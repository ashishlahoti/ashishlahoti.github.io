[{"categories":["Agile"],"contents":"Comprehensive list of SAFe Government Practitioner SGP Exam Questions curated for cracking the exam in first attempt.\nDisclaimer: Scaled Agile Inc is a protected Brand. These exam questions are neither endorsed by nor affiliated with Scaled Agile. These are not the SAFe official exam questions/dumps. These questions are created from the web content of the Scaled Agile Framework and SAFe for Government (SGP) Workbook. These questions cover all the domains and topics of the SAFe SGP official exam and once you go through these questions and their concepts, you are more than ready to crack the exam in first attempt.\nFull Exam Questions Consider buying the full set of questions from below links:- Complete Set of SAFe Government Practitioner SGP Exam Questions with Answers and Explanation at a very reasonable price.\n● All the Questions have duly verified answers supported with explanations and official weblinks of Scaled Agile Framework.\n● All the Questions marked with * are important from the exam perspective, give more attention!\n● All the Questions are unique and categorized by the topics of the SAFe SGP official exam as below:-\n\u0026ndash; Topic 1: SAFe Lean-Agile in government \u0026ndash; Topic 2: Lean-Agile Mindset \u0026ndash; Topic 3: SAFe Lean-Agile Principles\n\u0026ndash; Topic 4: Agile Teams\n\u0026ndash; Topic 5: Agile Release Train (ART) and Solution Train \u0026ndash; Topic 6: PI Planning \u0026ndash; Topic 7: SAFe Events\n\u0026ndash; Topic 8: Program Vision and Roadmap \u0026ndash; Topic 9: Feature and Capability \u0026ndash; Topic 10: Agency Strategy\n\u0026ndash; Topic 11: Lean flow of Epics \u0026ndash; Topic 12: Lean Budgeting\n\u0026ndash; Topic 13: Lean Estimating and Forecasting\n\u0026ndash; Topic 14: Acquisition Practices\n\u0026ndash; Topic 15: Build in Quality and Compliance\n\u0026ndash; Topic 16: Governance Practices\n\u0026ndash; Topic 17: Implementation Roadmap\n\u0026ndash; Topic 18: Lean Agile Leadership\nPlease leave your feedback in the comment section or contact us from main menu.\nWish you all the best for the SAFe Government Practitioner SGP Exam!\n20 Sample Questions Topic 1: SAFe Lean-Agile in government What does it mean to \u0026ldquo;think in terms of value streams\u0026rdquo; when adopting a Lean-Agile mindset? ⬜ Prioritize individual tasks over broader objectives\n⬜ Focus on optimizing individual team performance\n✅ Identify and prioritize the end-to-end delivery of value to the customer\n⬜ Centralize decision-making authority\nWhat is the primary benefit of embracing a Lean-Agile mindset in an organization? ✅ Improved collaboration and innovation\n⬜ Slower decision-making\n⬜ Increased bureaucracy\n⬜ Reduced transparency\nTopic 2: Lean-Agile Mindset What principle from the SAFe Core Values explains the purpose of the PI Planning retrospective? ✅ Relentless improvement\n⬜ Respect for people and culture\n⬜ Innovation\n⬜ Flow\nWhat is the Manifesto for Agile Software Development? ✅ A shared set of values and principles intended to improve software development\n⬜ A set of four values and twelve principles that help organizations implement systems thinking\n⬜ A value system that only applies to software development\n⬜ A set of practices that originated from Scrum\nTopic 3: SAFe Lean-Agile Principles Which SAFe principle ensures the maximum benefit is generated by understanding trade-offs between risks, cost of delay, operational costs, and development costs for each program and Value Stream? ✅ Take an economic view\n⬜ Assume variability, preserve options\n⬜ Apply systems thinking\n⬜ Build incrementally with fast, integrated learning cycles\nWhich statement is true about optimizing batch sizes for newly-formed SAFe teams in a program? ✅ Batch size optimization happens over time as teams figure out the balance between holding cost and transaction cost\n⬜ Batch size is optimized immediately by looking at transaction and holding costs\n⬜ Batch size is already optimized if there is continuous flow\n⬜ Batch size is optimized when transaction and holding costs seldom change\nTopic 4: Agile Teams What are two attributes of high-performing teams? (Choose two.) ✅ Experience mutual trust\n✅ They are self-organizing\n⬜ High-performing teams can be larger than the typical Agile team\n⬜ High-performing teams do not require the assistance of a Scrum Master\n⬜ Members of the team value uniformity\nTopic 5: Agile Release Train (ART) and Solution Train A large government program currently kicks off separate “siloed” projects for each new initiative, forming new teams that disband after delivering all contracted requirements. To adopt SAFe, what structural change should the agency make to improve the flow of value? ⬜ Continue using siloed project teams but add more documentation to coordinate between them.\n⬜ Move people to whichever project has the highest priority at the moment to maximize utilization.\n⬜ Plan all solution details upfront for each project, then assign teams to execute strictly to that plan.\n✅ Establish long-lived, cross-functional Agile Release Trains aligned to value streams, continuously delivering a flow of Epics from a prioritized backlog.\nWhen should government technology programs be organized as Solution Trains? ⬜ When the program crosses multiple agency divisions or branches\n⬜ When the cost of the program exceeds $100M\n⬜ When the program involves classified information\n✅ When the size and complexity require two or more ARTs\nTopic 6: PI Planning Which SAFe event provides a cadence for synchronization and alignment across multiple government agencies in a Large Solution? ✅ PI Planning\n⬜ Sprint Review\n⬜ System Demo\n⬜ Daily Standup\nIn SAFe for Government, what is the primary purpose of aligning governance reviews with the Program Increment (PI) cadence?​ ⬜ To enforce strict compliance with initial project plans\n✅ To streamline oversight and reduce delays by synchronizing reviews with development cycles\n⬜ To conduct comprehensive audits at the end of each fiscal year\n⬜ To separate governance activities from development processes​\nTopic 7: SAFe Events What are three reasons the Innovation and Planning (IP) Iteration is critical to success in SAFe? (Choose three.) ✅ It creates a guard band that ensures program predictability on cadence\n✅ It ensures time for creative problem-solving needed by knowledge workers\n✅ It provides a consistent time for PI Planning\n⬜ It creates a predictable window to allow for personal time off\n⬜ It is the only way a government ART can afford to do hackathons\n⬜ It provides the only time the Product Management can conduct backlog refinement\nSource: https://scaledagileframework.com/innovation-and-planning-iteration/\nWhat supports early risk identification and mitigation in a SAFe government program? ⬜ Holding risk reviews only at the end of a PI\n✅ Regular System Demos and Inspect \u0026amp; Adapt events\n⬜ Relying solely on contract status meetings\n⬜ Removing uncommitted objectives from tracking\nTopic 8: Program Vision and Roadmap What is the primary purpose of the Program Vision in SAFe government programs? ⬜ To define team-level tasks and responsibilities\n⬜ To outline the detailed technical specifications\n✅ To provide a shared direction and context for the Agile Release Train\n⬜ To establish fixed budget allocations for each team\nIn a SAFe Roadmap, which PIs are committed? ✅ The first one\n⬜ The first three\n⬜ All\n⬜ The first two\nTopic 9: Feature and Capability What is the primary purpose of defining system attributes using Features in a government SAFe program? ⬜ To define contract milestones in advance\n⬜ To delegate all compliance responsibility to vendors\n⬜ To document the entire system design before development begins\n✅ To express system behaviors and capabilities in a value-focused way\nHow does defining system attributes with Features support compliance in SAFe government programs? ⬜ By ensuring requirements are fixed before development starts\n⬜ By assigning all compliance testing to vendors\n✅ By enabling traceable, testable expressions of system capabilities\n⬜ By deferring reviews until final delivery\nTopic 10: Agency Strategy How does SAFe recommend structuring large government programs to stay aligned with agency strategy? ⬜ Use traditional project structures with fixed scope and milestones\n⬜ Prioritize system design before identifying mission goals\n✅ Organize around long-lived Value Streams and Agile Release Trains (ARTs)\n⬜ Separate teams based on vendor and government affiliation\nWhich represents a good example of an agency Strategic Theme? ⬜ Comply with agency development lifecycle regulations\n⬜ Reduce agency IT spend\n⬜ Improve cost and schedule performance of IT projects\n✅ Create a \u0026lsquo;one-stop shop\u0026rsquo; for all citizen-facing services provided by the agency\nTopic 11: Lean flow of Epics What are Business Epics in SAFe? ✅ System attributes that deliver business value\n⬜ Solution descriptions for the business\n⬜ An Agile term that is equivalent to a government program\n⬜ Solutions to address financial constraints\nWhich statement best describes the Lean-Agile alternative to traditional project-based development in government? ⬜ Create fixed-scope, fixed-funding projects with annual delivery targets\n✅ Manage Epics in continuous flow using a prioritized backlog and long-lived teams\n⬜ Outsource projects entirely to reduce delivery overhead\n⬜ Delay execution until detailed upfront planning is complete\nTopic 12: Lean Budgeting In Advanced Lean-Agile government, what is the primary benefit of using Lean budgeting? ⬜ It allows government agencies to spend freely without controls.\n⬜ It provides a way to allocate funds to individual teams.\n✅ It promotes continuous funding and adaptability.\n⬜ It centralizes budgeting decisions within the government.\nWhat two visual reporting tools can be used to help maintain fiduciary tracking of government technology programs? (Choose two.) ✅ Feature progress charts\n✅ Burn-up charts\n⬜ Detailed spreadsheets\n⬜ Pro formas\n⬜ Integrated master schedules\nTopic 13: Lean Estimating and Forecasting Which statement is true for estimating and forecasting in SAFe? ✅ ART velocities are used to conduct \u0026lsquo;what if\u0026rsquo; analysis\n⬜ Feature estimation is independent of team-level input\n⬜ Epics should be estimated by the Epic Owner\n⬜ Work breakdown structure to the task level helps increase estimation accuracy\nWhy is cadence-based forecasting preferred over traditional project planning in Lean-Agile environments? ⬜ It eliminates the need for scope negotiation\n⬜ It locks delivery into quarterly milestones\n✅ It allows for regular adjustments based on actual team throughput\n⬜ It reduces stakeholder involvement in estimation\nTopic 14: Acquisition Practices What change in acquisition practices is essential for enabling Lean-Agile development in government? ⬜ Awarding contracts based only on lowest cost proposals\n✅ Introducing modular contracts that support incremental delivery and collaboration\n⬜ Relying on fixed-scope, fixed-price contracts with detailed up-front planning\n⬜ Extending the timeline for vendor selection to ensure exhaustive documentation\nWhat is a primary risk of continuing to use traditional acquisition models in Agile programs? ⬜ Overspending on contractor rates\n⬜ Excessive documentation burdens\n⬜ Delayed system integration\n✅ Forcing vendors into waterfall behaviors, undermining Agile execution\nTopic 15: Build in Quality and Compliance What events in SAFe provide objective evidence of built-in quality and compliance? ✅ System Demos\n⬜ PI Planning\n⬜ Backlog refinement\n⬜ ART syncs\nIn SAFe, how is compliance integrated into the development process?​ ⬜ By conducting compliance checks only at the end of the project\n⬜ By assigning compliance responsibilities solely to a separate team\n✅ By embedding compliance activities throughout the development lifecycle\n⬜ By minimizing compliance to accelerate delivery​\nTopic 16: Governance Practices What is governance in a SAFe context? ✅ A framework for decision-making to ensure programs achieve desired mission outcomes\n⬜ Oversight that is provided by the PMO\n⬜ Agency level regulations that ensure compliance with federal laws\n⬜ Participatory budgeting events conducted twice annually by LPM\nWhat is the purpose of governance on a Lean-Agile government program? ✅ To provide a framework for decision-making to achieve desired mission outcomes\n⬜ To ensure the Solution is delivered within fixed time, cost, and scope constraints\n⬜ To ensure compliance with statutory and regulatory requirements\n⬜ To ensure contractors are accountable for how taxpayer money is spent\nTopic 17: Implementation Roadmap Which two statements accurately describe the SAFe Implementation Roadmap? (Choose two.) ✅ It reflects success patterns from previous implementations of SAFe\n✅ It provides recommended sequencing of role-based training\n⬜ It ensures standardization with software life cycle best practices\n⬜ The steps in the roadmap must be followed in order\n⬜ All steps must be complete with the first ART before launching additional trains\nSource: https://framework.scaledagile.com/government-creating-high-performing-teams-of-teams/\nAn agency is ready to undertake a large modernization effort. Though the agency ensures the correct implementation of all stage gates in their SDLC, projects seem to always be over budget. They are exploring the use of SAFe due to the success stories from other agencies. What is the appropriate first step? ⬜ Map all stage gates to Agile events\n⬜ Train Product Owners in the principles, practices, and benefits of adopting SAFe\n⬜ Start an Agile project with an internal Agile Scrum Master\n✅ Identify and train change agents that will lead the effort\nTopic 18: Lean Agile Leadership Which aspect of leading by example challenges Lean-Agile leaders to \u0026lsquo;walk the talk\u0026rsquo; by being a role model of desired professional and ethical behaviors by acting with honesty, integrity, and transparency? ✅ Authenticity\n⬜ Emotional intelligence\n⬜ Life-long learning\n⬜ Decentralized decision-making\nWhat occurs when leaders create an environment for risk-taking that supports change without fear of negative consequences to self-image, status, or career? ✅ Psychological safety\n⬜ Decentralized decision-making\n⬜ Emotional intelligence\n⬜ Alignment\n","permalink":"https://codingnconcepts.com/agile/safe-government-practitioner-sgp-exam-questions/","tags":["SAFe","Certification"],"title":"SAFe Government Practitioner SGP Exam Questions"},{"categories":["Agile"],"contents":"Comprehensive list of SAFe DevOps Practitioner SDP 6.0 Exam Questions curated for cracking the exam in first attempt.\nDisclaimer: Scaled Agile Inc is a protected Brand. These exam questions are neither endorsed by nor affiliated with Scaled Agile. These are not the SAFe official exam questions/dumps. These questions are created from the web content of the Scaled Agile Framework and SAFe DevOps 6.0 Workbook. These questions cover all the domains and topics of the SAFe DevOps SDP official exam and once you go through these questions and their concepts, you are more than ready to crack the exam in first attempt.\nFull Exam Questions Consider buying the full set of questions from below links:- Complete Set of SAFe DevOps Practitioner SDP 6.0 Exam Questions with Answers and Explanation at a very reasonable price.\n● All the Questions have duly verified Answers supported with explanations and official weblinks of Scaled Agile Framework.\n● All the Questions marked with * are important from the exam perspective, give more attention!\n● All the Questions are unique and categorized by the topics of the SAFe DevOps SDP 6.0 official exam as below:-\n\u0026ndash; Topic 1: DevOps\n\u0026ndash; Topic 2: Value Stream Mapping\n\u0026ndash; Topic 3: Continuous Exploration\n\u0026ndash; Topic 4: Continuous Integration\n\u0026ndash; Topic 5: Continuous Deployment\n\u0026ndash; Topic 6: Release on Demand\nPlease leave your feedback in the comment section or contact us from main menu.\nWish you all the best for the SAFe DevOps Practitioner SDP 6.0 Exam!\n25 Sample Questions Topic 1: DevOps What are two benefits of DevOps? (Choose two) ⬜ Less frequent deployments\n⬜ More lead time\n⬜ Less time spent on new work\n✅ Fewer defects\n✅ Less time spent fixing security issues\nWhich statement describes a measurable benefit of adopting DevOps practices and principles? ✅ It results in faster lead time, and more frequent deployments\n⬜ It identifies key Value Streams\n⬜ It guarantees an increase in profits and decrease in downtime\n⬜ It creates a highly functional, cross-team culture\nWhich teams should coordinate when responding to production issues? ✅ Teams across the Value Stream\n⬜ Dev teams and Ops teams\n⬜ Support teams and Dev teams\n⬜ SRE teams and System teams\nWhat is the main goal of a SAFe DevOps transformation? ⬜ To create a strong DevOps team with leadership support\n⬜ To create immutable infrastructure to avoid changes to the production environment\n⬜ To implement an advanced tool chain to automate the entire Continuous Delivery Pipeline\n✅ To align people across the Value Stream to deliver value continuously\nDevOps is a key enabler of continuous delivery. What does continuous mean in this context? ⬜ To deploy to production and release on cadence every Iteration\n⬜ To deploy to production and release at least once every Program Increment\n✅ To deploy to production as often as possible and release when the business needs it\n⬜ To deploy to production multiple times per year and release on a cadence once or twice a year\nTopic 2: Value Stream Mapping What is the primary benefit of value stream mapping? ⬜ It fosters collaboration among development and operations managers\n⬜ It creates the hypothesis of which Solution to build\n⬜ It identifies how to build Agile Release Trains\n✅ It provides insight into organizational efficiency and value flow\nAfter the team maps the steps of the current state Value Stream during value stream mapping, what are the next two steps? (Choose two.) ⬜ Identify who is involved in each step\n✅ Create a future state value stream map\n⬜ Perform a SWOT analysis\n✅ Calculate the Activity Ratio\n⬜ Measure the performance at each step\nWhat does the %C\u0026amp;A metric measure in the Continuous Delivery Pipeline? ⬜ The percent concurrent and accurate process times of each pipeline activity\n⬜ The percent complete and average times of each pipeline activity\n⬜ The percent of change averages of each pipeline activity\n✅ The percent of time downstream customers receive work that is usable as-is\nTopic 3: Continuous Exploration Which of the following aspects of the continuous delivery pipeline focuses on understanding customer needs? ⬜ Release on Demand\n⬜ Continuous Integration\n⬜ Continuous Deployment\n✅ Continuous Exploration\nWhat are two activities performed as part of defining the hypothesis in Continuous Exploration? (Choose two) ✅ Identify metrics based on leading indicators\n✅ Define the minimum viable product\n⬜ Elicit feedback\n⬜ Use value stream mapping\n⬜ Develop a detailed business case\nWhich statement describes the Lean startup lifecycle? ⬜ Define the minimum viable product (MVP), build the MVP, implement Epic Features until all Features are delivered\n✅ Define the hypothesis, build a minimum viable product (MVP), continuously evaluate the MVP while implementing additional Features until WSJF determines work can stop\n⬜ Define a Lean business case, build a minimum viable product, implement the Epic Features until all Features are delivered\n⬜ Create a hypothesis statement, assign an Epic owner, deliver Features created from the Lean business case, deploy\nWhen preparing a DevOps backlog, prioritizing features using WSJF includes which two factors? (Choose two) ✅ Cost of delay\n✅ Duration/job size\n⬜ Business value\n⬜ Total count of items on the Program Backlog\n⬜ Team velocity\nTopic 4: Continuous Integration Scanning application code for security vulnerabilities is an important step in which aspect of the Continuous Delivery Pipeline? ⬜ Continuous Exploration\n⬜ Continuous Deployment\n⬜ Release on Demand\n✅ Continuous Integration\nWhat is the correct order of activities in the Continuous Integration aspect? ⬜ Stage, Develop, Build, Test end-to-end\n⬜ Build, Develop, Stage, Test end-to-end\n⬜ Develop, Test end-to-end, Build, Stage\n✅ Develop, Build, Test end-to-end, Stage\nWhich two quality practices apply to Agile teams? (Choose two.) ⬜ Providing architectural runway\n✅ Peer review and pairing\n⬜ Decentralized decision-making\n⬜ Using nonfunctional requirements\n✅ Establishing flow\nWhich two security skills are part of the Continuous Integration aspect? (Choose two) ✅ Application security\n✅ Penetration testing\n⬜ Security board review\n⬜ SOX compliance analysis\n⬜ Network security practices\nTopic 5: Continuous Deployment What is the primary purpose of a dark launch in the Continuous Deployment pipeline? ⬜ To release new functionality to all end users immediately\n✅ To deploy new functionality to production without releasing it to end users\n⬜ To test new features in a development environment\n⬜ To roll back deployed features that caused issues\nContinuous Deployment enables which key business objective? ⬜ Manage technical debt\n⬜ Release continuously\n✅ Time-to-market\n⬜ Business value\nFeature toggles are useful for which activity? ⬜ To accelerate the deployment process\n✅ To decouple deployment from release\n⬜ To enable continuous code integration\n⬜ To enable continuous testing\nWhich two skills appear under the Respond activity? (Choose two.) ✅ Cross-team collaboration\n✅ Version control\n⬜ Telemetry\n⬜ Automatic rollback\n⬜ Service virtualization\nTopic 6: Release on Demand What differentiates Deployment and Release in the Continuous Delivery Pipeline? ⬜ Deployment occurs multiple times per day; release occurs on demand\n⬜ Deployment occurs multiple times per day; release occurs in PI boundaries\n⬜ Deployment involves moving changes to staging; release involves moving them to production\n✅ Deployment involves moving changes to production; release involves making them available to end users\nThe Release on Demand aspect enables which key business objective? ✅ Business value\n⬜ Quality\n⬜ Time-to-market\n⬜ Alignment\nWhat falls outside the scope of the Stabilize activity? ⬜ Features are monitored after release\n⬜ Continuous security monitoring is done\n✅ Blue/green deployment\n⬜ Failover and recovery processes are in place\nWhat are two important items to monitor in production to support the Release on Demand aspect in SAFe? (Choose two) ✅ System performance\n✅ Business value\n⬜ Number of concurrent users\n⬜ Unit test coverage\n⬜ Percent Complete and Accurate (%C\u0026amp;A)\nWhy is it important to take a structured approach to analyze problems in the delivery pipeline? ⬜ It provides a structured roadmap for the SAFe implementation\n✅ It helps ensure that actual causes of problems are addressed, rather than symptoms\n⬜ It ensures that solutions are more likely to be approved for implementation\n⬜ It allows the solution to be demoed to key stakeholders\nComplete Set of SAFe DevOps Practitioner SDP 6.0 Exam Questions with Answers and Explanation at a very reasonable price.\n","permalink":"https://codingnconcepts.com/agile/safe-devops-practitioner-sdp-exam-questions/","tags":["SAFe","Certification"],"title":"SAFe DevOps Practitioner SDP 6.0 Exam Questions"},{"categories":["Agile"],"contents":"Comprehensive list of Free SAFe Practitioner SP 6.0 (SAFe for Teams) Exam Questions curated for cracking the exam in first attempt.\nDisclaimer: Scaled Agile Inc is a protected Brand. These exam questions are neither endorsed by nor affiliated with Scaled Agile. These are not the SAFe official exam questions/dumps. These questions are created from the web content of the Scaled Agile Framework and SAFe for Teams (SP) 6.0 Workbook. These questions cover all the domains and topics of the SAFe SP official exam and once you go through these questions and their concepts, you are more than ready to crack the exam in first attempt.\nFull Exam Questions Consider buying the full set of questions from below links:- Complete Set of SAFe Practitioner SP 6.0 Exam Questions with Answers and Explanation at a very reasonable price.\nAll the Questions have duly verified answers supported with explanations and official weblinks of Scaled Agile Framework. All the Questions marked with * are important from the exam perspective, give more attention! All the Questions are unique and categorized by the topics of the official SAFe SP 6.0 Exam as below:- Domain Topic 1. Introducing SAFe (6-12%) Topic 1: Business AgilityTopic 2: Lean-Agile MindsetTopic 3: SAFe Core CompetenciesTopic 4: SAFe Core ValuesTopic 5: SAFe Lean-Agile Principles 2. Forming Agile Teams as Trains (15-21%) Topic 6: Agile Teams characteristics and rolesTopic 7: Scrum and KanbanTopic 8: Agile Release Train (ART) characteristics and roles 3. Connect to the Customer (9-14%) Topic 9: Customer-centric mindsetTopic 10: Product vision and roadmapTopic 11: Story and Feature 4. Plan the Work (21-25%) Topic 12: Team Backlog and Backlog RefinementTopic 13: Iteration PlanningTopic 14: PI Planning 5. Deliver Value (13-18%) Topic 15: Continuous Delivery PipelineTopic 16: Team and ART Sync EventsTopic 17: Built-in quality 6. Get Feedback (6-12%) Topic 18: Feedback techniquesTopic 19: Iteration ReviewTopic 20: System Demo 7. Improve Relentlessly (13-18%) Topic 21: Iteration RetrospectiveTopic 22: Inspect and Adapt (I\u0026amp;A) EventTopic 23: FlowTopic 24: Outcomes Please leave your feedback in the comment section or contact us from main menu.\nWish you all the best for the SAFe Practitioner SP 6.0 (SAFe for Teams) Exam!\n50 Sample Questions Topic 1: Business agility The primary need for SAFe is to scale the idea of what? ⬜ Technical Solution Delivery\n⬜ Organizational and Functional Alignment\n⬜ Lean Portfolio Management\n✅ Business Agility\nRestoring the speed and innovation of the entrepreneurial network while leveraging the stability of the hierarchical system is a benefit of what? ✅ Dual operating system\n⬜ Customer centricity\n⬜ Continuous learning culture\n⬜ Functional silos\nTopic 2: Lean-Agile Mindset What is one of the Lean Thinking Principles? ✅ Make value flow without interruptions\n⬜ Working software over comprehensive documentation\n⬜ Responding to change over following a plan\n⬜ Individuals and Iterations over processes and tools\nWhat is \u0026ldquo;precisely specify value by specific product\u0026rdquo; central to? ✅ Lean Thinking ⬜ SAFe Principles ⬜ Agile Manifesto\n⬜ SAFe Core Values\nTopic 3: SAFe Core Competencies Which of the following SAFe Core Competencies of Business Agility includes the Customer Centricity and Design Thinking dimension? ⬜ Lean Portfolio Management\n⬜ Continuous Learning Culture\n⬜ Lean-Agile Leadership\n✅ Agile Product Delivery\nWhich of the following SAFe Core Competencies of Business Agility includes the built-in quality dimension? ⬜ Enterprise Solution Delivery\n⬜ Organizational Agility\n✅ Team and Technical Agility\n⬜ Agile Product Delivery\nTopic 4: SAFe Core Values Turn mistakes into learning moments, create a trust-based environment, and visualize work are examples of which SAFe Core Value?? ⬜ Alignment ⬜ Respect for People ⬜ Relentless Improvement\n✅ Transparency\nWhich of the following SAFe Core Values involves coaching aspiring developers to grow their skillsets and fill new roles throughout the organization? ⬜ Transparency\n✅ Respect for People\n⬜ Built-In Quality\n⬜ Alignment\nTopic 5: SAFe Lean-Agile Principles Which SAFe Lean-Agile Principle includes an emphasis on \u0026ldquo;deliver early and often\u0026rdquo;? ✅ Take an economic view\n⬜ Build incrementally with fast, integrated learning cycles ⬜ Organize around value ⬜ Make value flow without interruptions\nWhich of the following SAFe Lean-Agile principles involves delivering a continuous flow of value to customers in the shortest sustainable lead time? ⬜ Decentralized decision-making\n⬜ Apply systems thinking\n⬜ Take an economic view\n✅ Make value flow without interruptions\nTopic 6: Agile Teams characteristics and roles What are two ways to describe a cross-functional Agile Team? (Choose two.) ✅ They are optimized for communication and delivery of value\n⬜ They deliver value every six weeks\n⬜ They are made up of members, each of whom can define, develop, test, and deploy the system\n✅ They can define, build, and test an increment of value\n⬜ They release customer products to production continuously\nWhich of the following statements describes the Product Owner role? ⬜ Ensuring quality by testing the Solution\n⬜ Estimating Stories in the Product Backlog\n⬜ Prioritizing the ART Backlog\n✅ Representing the Customer to the Agile Team\nTopic 7: Scrum and Kanban What is one key difference between SAFe Scrum and SAFe Team Kanban? ✅ SAFe Scrum uses fixed-length iterations, while SAFe Team Kanban operates on a continuous flow model\n⬜ SAFe Scrum lacks defined roles, whereas SAFe Team Kanban includes specific roles like Scrum Master and Product Owner\n⬜ SAFe Scrum is suitable for support and operations teams, while SAFe Team Kanban is designed for software development teams\n⬜ SAFe Team Kanban requires time-boxed sprints, whereas SAFe Scrum does not\nTeam A is a maintenance team that cannot always predictably plan their work. They like to meet daily to review the needs of the system and plan for how they can quickly address those needs during the workday. Which of the following SAFe Lean-Agile methods should Team A use to plan and execute their work? ⬜ SAFe Enabling Team\n⬜ SAFe XP Team\n✅ SAFe Team Kanban\n⬜ SAFe Platform Team\nTopic 8: Agile Release Train (ART) characteristics and roles What are the three key items communicated on the ART planning board? (Choose three.) ⬜ ART PI risks\n✅ Feature delivery ⬜ Team velocity\n✅ Dependencies between teams\n⬜ PI Objectives\n✅ Milestones\nWhich of the following statements describes the Release Train Engineer role? ⬜ To maintain Team Backlogs\n✅ To serve as the ART Chief Coach\n⬜ To ensure technical integrity of all development within the ART\n⬜ To serve as the ART-level content authority\nTopic 9: Customer-centric mindset Which of the following roles act as proxies for the customer in representing their needs to the teams? ⬜ Executive roles\n✅ Product roles\n⬜ Architecture roles\n⬜ Developer roles\nWhich of the following design-thinking techniques helps breakdown Features while considering the end-to-end user flow? ✅ Story mapping\n⬜ Market research\n⬜ Gemba walks\n⬜ Personas\nTopic 10: Product vision and roadmap What is the product vision? ⬜ A set of prioritized Features\n⬜ The User Stories required to meet customer needs\n⬜ An explanation of the architectural runway needed to deliver products to the customer\n✅ A method for aligning to the product direction\nWhich of the following statements is true about Roadmaps? ⬜ Roadmaps are commitment\n⬜ Roadmaps provide a single planning horizon\n⬜ Roadmaps are only adjusted at PI boundaries\n✅ Roadmaps communicate intent\nTopic 11: Story and Feature Which statement is true about Features and Stories? ⬜ Features should be small enough to fit into an Iteration\n✅ Features can be larger than an Iteration but Stories should be small enough to fit into an Iteration\n⬜ They are prioritized by the team\n⬜ They are estimated like User Stories\nWhat is one benefit of Story acceptance criteria? ⬜ To provide Story details from a release-planning point of view\n✅ To provide Story details from a testing point of view\n⬜ To provide Story details from a designer point of view\n⬜ To provide Story details from a deployment point of view\nThe Scrum Master/Team Coach wants to establish a team\u0026rsquo;s initial capacity. The team has two testers, three developers, one full-time Scrum Master/Team Coach, and a Product Owner split between two teams. What is their capacity before calculating for time off? ⬜ 52\n⬜ 32\n✅ 40\n⬜ 48\nTopic 12: Team Backlog and Backlog Refinement What is the primary purpose of backlog refinement in SAFe? ⬜ To assign tasks to individual team members ✅ To define, discuss, estimate, and establish acceptance criteria for upcoming backlog items ⬜ To prioritize the backlog items based on business value\n⬜ To review the outcomes of the previous iteration\nWhat types of items are typically found in the Team Backlog? ⬜ Only user stories\n⬜ Only enablers\n✅ User stories and enablers\n⬜ Epics and capabilities\nTopic 13: Iteration Planning Which statement defines the purpose of Iteration Planning? ⬜ It is to analyze, approve, and ready Features for implementation\n⬜ It is to explore and implement program Epics and split them into Features to be further explored\n⬜ It is to break Stories into tasks that are achievable in the team\u0026rsquo;s capacity\n✅ It is to organize the work and define a realistic scope for the Iteration\nWhat is one result from Iteration Planning for SAFe Scrum Teams? ✅ Iteration goals\n⬜ Iteration sequencing\n⬜ Iteration demo\n⬜ Iteration estimate\nTopic 14: PI Planning What is the example of applying cadence and synchronization in SAFe? ⬜ Creating cross-functional ARTs and Agile teams ⬜ Allocating budgets to Value Streams ⬜ Using a Portfolio Kanban system ✅ Conducting a PI Planning event\nWhich of the following is an output of the PI Planning process? ⬜ PI Vision\n✅ PI Objectives\n⬜ PI Goals\n⬜ Actual PI Business Value\nDuring which of the following PI Planning activities are Business Owners asked to accept the plans? ⬜ The second team breakout session\n⬜ The Management Review and Problem-Solving workshop\n⬜ The draft plan review\n✅ The final plan review\nTopic 15: Continuous Delivery Pipeline What represents the workflow, activities, and automation needed to deliver new functionality more frequently? ⬜ The Portfolio Kanban\n⬜ The Lean budget Guradrails\n⬜ The PI Planning process\n✅ The Continuous Delivery Pipeline\nWhich of the following continuous delivery pipeline aspects focuses on enabling the organization to deliver value aligned with business needs? ⬜ Continuous Ideation\n⬜ Continuous Deployment\n✅ Release on Demand\n⬜ Continuous Integration\nTopic 16: Team and ART Sync Events What is the focus of the Team Sync? ⬜ PI objectives versus outcomes\n✅ Progress towards the Iteration and PI goals ⬜ Scrum Master goals versus Development Team goals ⬜ Plan objectives versus Program Owner objectives\nWhich of the following events does SAFe recommend running regularly throughout the PI? ✅ ART Sync\n⬜ Product Sync\n⬜ Design Sync\n⬜ Business Owner Sync\nTopic 17: Built-in quality Which of the following basic quality practices applies to all teams? ⬜ Rapid prototyping\n⬜ Agile architecture\n⬜ Modeling and simulation\n✅ Collective ownership and standards\nWhich of the following stakeholders primarily develops the definition of done for the team increment? ⬜ Solution Architect\n⬜ Business Owners\n⬜ Release Train Engineer\n✅ Agile Teams\nTopic 18: Feedback techniques What is the purpose of an empathy map? ✅ To help develop a deeper understanding of the customer\n⬜ To facilitate collaboration with other team members\n⬜ To identify the customer\n⬜ To gain deeper insight to the members of an Agile Team\nWhich of the following methods for gathering customer feedback relies on building analytic systems to deliver information about how customers are using the Solution? ⬜ Continuous exploration\n✅ Telemetry\n⬜ Refactoring\n⬜ Continuous integration\nTopic 19: Iteration Review What is the purpose of the Iteration review? ⬜ To identify where there is too much work in the system and where the teams are being overloaded\n✅ To measure the team\u0026rsquo;s progress by showing working Stories to the stakeholders and getting feedback from them\n⬜ To show the backlog items and work on possible Solutions for the backlog items ⬜ To serve as a forecasting meeting where the work is estimated for the Program Increments\nWhich of the following types of information is shown in a cumulative flow diagram? ⬜ Costs of producing artifacts\n⬜ Time to complete a Feature by the rollup of Stories\n✅ Work that is in process across the whole team\n⬜ Team velocity\nTopic 20: Team and System demo Which of the following team-level events does SAFe recommend running on a cadence during the PI for SAFe Scrum Teams? ⬜ Portfolio Demo\n✅ Iteration Demo\n⬜ Iteration Review\n⬜ Portfolio Review\nWhat is one method for showing true progress of business outcomes? ⬜ Analyze ART metrics\n✅ Conduct a System Demo\n⬜ Review the Kanban board\n⬜ Discuss during PI Planning\nTopic 21: Iteration Retrospective Which of the following Agile Team responsibilities is associated with the Iteration Retrospective? ⬜ Take an economic view\n✅ Improve relentlessly\n⬜ Connect to the customer\n⬜ Apply systems thinking\nAccording to SAFe, what is one output of a successful Iteration Retrospective? ⬜ Updated ART metrics ✅ Improvement Stories ⬜ Updated dependencies between stories ⬜ Iteration Goals\nTopic 22: Inspect and Adapt (I\u0026amp;A) Event What are the three parts of an Inspect and Adapt? ⬜ Backlog refinement, confidence vote, and Problem-Solving workshop\n✅ The PI System Demo, qualitative and quantitative measurement, and Problem-solving workshop\n⬜ Backlog refinement, qualitative and quantitative measurement, and ROAMing risks\n⬜ The PI System Demo, confidence vote, and ROAMing risks\nWhich of the following activities does SAFe recommend as the first activity of the Inspect and Adapt event? ✅ PI System Demo\n⬜ Quantitative measurement\n⬜ Agreement on the problems to solve\n⬜ Retrospective and problem-solving workshop\nTopic 23: Flow What are the three flow accelerators for making value flow without interruptions? (Choose three.) ✅ Reduce Queue Length ⬜ Frequent context switching ⬜ Increase capacity ⬜ Address the systemic problems ✅ Work in Smaller Batches ✅ Visualize and limit work in process (WIP)\nWhat is the formula to calculate flow efficiency? ⬜ Total wait time + Flow time\n⬜ Total wait time / Flow time\n⬜ Total active time + Flow time\n✅ Total active time / Flow time\nTopic 24: Outcomes Which of the following measures tracks progress toward achieving desired outcomes? ⬜ ART actual business value\n⬜ Burn-down charts\n⬜ Cumulative flow diagrams\n✅ Objectives and key results\nWhich of the following role guide the ART towards outcome? ⬜ Product Management ⬜ Solution Management ⬜ Release Train Engineer ✅ Business Owner\nComplete Set of SAFe Practitioner SP 6.0 (SAFe for Teams) Exam Questions with Answers and Explanation at a very reasonable price.\n","permalink":"https://codingnconcepts.com/agile/safe-practitioner-for-teams-exam-questions/","tags":["SAFe","Certification"],"title":"SAFe Practitioner SP 6.0 Exam Questions"},{"categories":["Agile"],"contents":"Comprehensive list of Free SAFe Release Train Engineer RTE 6.0 Exam Questions curated for cracking the exam in first attempt.\nDisclaimer: Scaled Agile Inc is a protected Brand. These exam questions are neither endorsed by nor affiliated with Scaled Agile. These are not the SAFe official exam questions/dumps. These questions are created from the web content of the Scaled Agile Framework and SAFe Release Train Engineer (RTE) 6.0 Workbook. These questions cover all the domains and topics of the SAFe RTE official exam and once you go through these questions and their concepts, you are more than ready to crack the exam in first attempt.\nFull Exam Questions Consider buying the full set of questions from below links:- Premium SAFe Release Train Engineer (RTE) 6.0 Exam Questions with Answers and Explanation at a very reasonable price.\nAll the Questions have duly verified answers supported with explanations and official weblinks of Scaled Agile Framework. All the Questions are unique and categorized by the topics of the official exam material. All the Questions marked with * are important from the exam perspective, give more attention! Please leave your feedback in the comment section or contact us from main menu.\nWish you all the best for the SAFe Release Train Engineer (RTE) 6.0 Exam!\n50 Sample Questions Topic 1: RTE Characteristics and Responsibilities What behavior is an important part of the Release Train Engineer (RTE) role? ⬜ Manage dependencies for teams\n✅ Encourage teams to self-organize\n⬜ Drive teams to specific outcomes\n⬜ Provide teams with answers about Features\nA Release Train Engineer (RTE) would like to try a new retrospective technique at the next Inspect and Adapt event. However, the RTE is unsure how to prepare for it and thinks there may be some pitfalls. How could an RTE get help? ✅ Share and receive feedback from other RTEs in a community of practice ⬜ Start a discussion with the Architects to see how they would re-design the retro-spectives\n⬜ Post it in an internal communications forum and inspire others to try this technique as well\n⬜ Ask leadership to decide whether or not this technique should be used with the Agile Release Train\nWhat are two responsibilities of the Release Train Engineer as chief Scrum Master for the Agile Release Train (ART)? (Choose two.) ⬜ Provide the go/no-go decision for large initiatives\n⬜ Analyze Epics in the Portfolio Kanban\n✅ Escalate ART impediments\n⬜ Break down Features into Stories\n✅ Facilitate Program Increment (PI) Planning\nTopic 2: SAFe Concepts Which Core Competency of Business Agility includes aligning strategy with execution? ⬜ Lean-Agile Leadership\n⬜ Agile Product Delivery\n⬜ Organizational Agility\n✅ Lean Portfolio Management\nHow can a Release Train Engineer (RTE) support decentralized decision-making? ⬜ Evaluate the strategy for the Value Stream\n✅ Empower knowledge workers to manage their dependencies with other teams\n⬜ Change the cadence of the ART\n⬜ Update team PI Objectives when handling a time-critical release\nWhich SAFe Principle is applied when a Release Train Engineer (RTE) treats Suppliers as partners? ⬜ Assume variability; preserve options\n✅ Apply systems thinking\n⬜ Decentralize decision-making\n⬜ Build incrementally with fast, integrated learning cycles\nTopic 3: PI Planning - Preparation Activities While facilitating Program Increment (PI) Planning readiness activities, the Release Train Engineer (RTE) notices a Feature that is risky to the teams because the technology is new. Which of the following could reduce the risk? ⬜ Ensure engineering managers are directing the development process\n✅ Coach the teams to create exploration Enablers\n⬜ Ask for a presentation of a detailed design before the PI Planning meeting\n⬜ Make sure all the technical specifications are written before PI execution\nWhat is one key responsibility of the Release Train Engineer (RTE) during PI Planning preparation? ⬜ Prioritize the Features in the Program Backlog\n✅ Ensure that the PI Planning agenda is distributed to all Agile Teams\n⬜ Assign user stories to individual team members\n⬜ Finalize the architectural runway\nWhich two actions should the RTE take to ensure facility readiness for PI Planning? (Choose two.) ✅ Engage audio-visual technical support\n⬜ Assign seating arrangements for all participants\n✅ Secure communication channels for remote participants\n⬜ Prepare individual workstations for each team member\nTopic 4: PI Planning - Facilitation Activities During PI Planning, the Release Train Engineer (RTE) sees that the team\u0026rsquo;s excitement deviates from Product Management priorities, including for the top ten Features, at the ART Planning. In addition, Product Management is asking for new estimations, timelines, and scope changes. What is the likely reason for this behavior? ⬜ People are over-committed and under-utilized\n⬜ Cadence and synchronization are not in alignment\n✅ Lack of training and preparation for PI Planning\n⬜ Management stakeholders are not involved in changing the system\nWhat are the three key items communicated on the ART planning board? (Choose three.) ⬜ ART PI risks\n✅ Feature delivery dates\n⬜ Team velocity\n✅ Dependencies between teams\n⬜ PI Objectives\n✅ Milestones\nDuring the management review and problem-solving meeting, one team raises the risk of not finishing a Feature before the end of the Program Increment (PI). How can the management team help ensure they complete the Feature within the PI? ⬜ Redefine the definition of done (DoD)\n⬜ ROAM the risk appropriately\n⬜ Use buffer resources as a guard band\n✅ Negotiate a reduction in the scope\nTopic 5: PI Planning - Planning Development and Commitment Which activity occurs during Team Breakout #2 on the second day of PI Planning? ⬜ The RTE modifies the PI Iteration schedule, if needed, based on the scope of high priority Features\n⬜ All Feature delivery and dependencies are visualized on the ART planning board\n⬜ Business Owners independently assign business value to normalize business value across all teams\n✅ The Release Train Engineer (RTE) combines all Team PI Objectives into ART PI Objectives\nWhat information is covered during the final plan review? ⬜ Team Features, Stories, and team-level Enablers\n⬜ Changes to capacity and load, final PI Objectives, ART PI Risks, and impediments\n⬜ Changes to Iteration Goals, measured velocity, and dependencies\n✅ Planned Features, uncommitted objectives, and ROAMed risks\nA confidence vote is taken at the end of PI Planning after dependencies are resolved and risks are addressed. What best describes the process of the confidence vote? ⬜ Each person votes ⬜ The managers vote ✅ The teams and the ARTs vote ⬜ The business owners vote\nTopic 6: PI Planning - Multi-location Facilitation When planning for a distributed PI Planning with a significant difference in time zones, what is a key preparation and facilitation focus? ⬜ Share the outcomes of preparation meetings with local Scrum Masters/Team Coaches (SM/TC) so they can arrange local rooms\n✅ Adjust the PI agenda to 2.5–3 days, allowing for overlapping hours\n⬜ Have a single Release Train Engineer (RTE) and technical support person that acts as a central point of communication for all locations\n⬜ Split up the PI Planning event per time zone and then have the final plan review, confidence vote, and planning retrospective as one centralized meeting\nWhich two actions can enhance the effectiveness of distributed PI Planning? (Choose two.) ✅ Assigning a dedicated RTE proxy and tech support person at each location ⬜ Centralizing all decision-making to the primary location\n✅ Utilizing collaborative digital tools for real-time planning and documentation\n⬜ Restricting breakout sessions to a single time zone\nWhy is it beneficial to rotate key stakeholder\u0026rsquo;s physical presence across different locations during distributed PI Planning? ⬜ To centralize authority and decision-making\n✅ To foster inclusivity and ensure all teams feel equally supported\n⬜ To reduce travel expenses\n⬜ To delegate planning responsibilities to local teams\nTopic 7: ART and Team Roles and Responsibilities Which statement describes what stream-aligned teams do? ⬜ Analyze Value Streams and transform them using Lean-Agile Principles\n⬜ Promote a better flow of communications between leadership, ARTs, and teams\n⬜ Use process-mapping to identify and eliminate process bottlenecks\n✅ Build and deliver Customer value with minimal dependencies on other teams\nA Release Train Engineer (RTE) should build a relationship with which SAFe role to effectively assign business value to a team PI Objective? ⬜ Lean Agile Leaders\n⬜ Objective Owners\n✅ Business Owners ⬜ Solution Managers\nThe Release Train Engineer (RTE) collaborates with which other two roles to help focus the ART on delivering value and operational excellence? ⬜ Solution Management and Solution Architect\n⬜ Enterprise Architect and Solution Management\n⬜ Solution Architect and Enterprise Architect\n✅ Product Management and System Architect\nTopic 8: ART and Team Iteration Events Which statement is true about the definition of done (DoD)? ✅ The DoD should evolve as system capabilities evolve\n⬜ The DoD is used as a method to manage technical debt across the ART\n⬜ At the higher levels there is only one DoD for everything that passes through the ART to a Solution increment\n⬜ The teams share one common DoD\nWhat is one benefit of an Iteration and PI calendar? ⬜ Ability to know the cycle time between important team and ART events\n⬜ Ability to ensure that key events do not conflict with non-SAFe events\n✅ Ability to visualize the ART cadence and synchronization\n⬜ Ability to create a big visible information radiator (BVIR) of the important team and ART milestones\nWhich statement is true about SAFe Iteration Goals? ✅ They enable teams to keep aligned with PI Objectives\n⬜ They provide key performance indicators (KPIs) for tracking progress and value realization\n⬜ They provide quantifiable metrics to be used in retrospectives\n⬜ They describe the value of planned Features and Enablers\nWhat are two of the Agile Release Train Sync meetings? (Choose two.) ✅ PO Sync ⬜ System Demo\n⬜ Solution Demo\n✅ Coach Sync\n⬜ Inspect and Adapt\nTopic 9: Optimizing ART Flow An ART frequently discovers compatibility issues between the developed Solution and the Enterprise information architecture. What can the Release Train Engineer (RTE) do to prevent this from occurring? ⬜ Add data architects to the ART\n⬜ Develop more detailed Feature definitions\n⬜ Conduct the entire data architecture design upfront\n✅ Confirm attendance of architectural representatives at PI Planning\nThe Release Train Engineer (RTE) learns the teams feel the business value needs to reflect the effort and progress. What is one technique the RTE can use to provide the Business Owners a better understanding of the value the teams have created? ✅ Work with the teams to ensure they are actively involved when the Business Owners view the business value achieved\n⬜ Illustrate the link between business values and the market communication/measures tied to the value\n⬜ Publish the team business values and coach teams that these values are for tracking such ART deliverables\n⬜ Educate teams that business value provides the Enterprise with a metric of how fast the team executed work during the PI\nTeams are reporting a high level of success through their individual quantitative measurements, but the system results say otherwise. What should the Release Train Engineer do to help the teams deliver more value? ✅ Coach the Scrum Masters on good retrospective techniques and ensure teams are defining and taking a systems view approach to improvements\n⬜ Share the quantitative measurement results with Product Management and leadership and ask for their input\n⬜ Diagnose the differences between the measurements and the results and suggest improvement items to each team\n⬜ Work with the team that is struggling the most to discover patterns that can be applied to the other teams\nTopic 10: Metrics What are two main reasons why the ART predictability measure is important? (Choose two.) ✅ It allows the business and other stakeholders to plan effectively\n⬜ It identifies under-performing teams\n✅ It focuses the Agile Release Train on predictable value delivery\n⬜ It demonstrates the need to fix the scope at the beginning of the Program Increment (PI)\n⬜ It indicates whether the Solution is ready to be released\nWhich SAFe tool might the Release Train Engineer (RTE) use to identify areas for an ART to improve how its Continuous Delivery Pipeline (CDP) functions? ⬜ Conduct empathy interviews with system architects/engineers\n⬜ Facilitate a CDP sync every Iteration\n✅ Agile Product Delivery core competency assessment\n⬜ Architectural Runway ART Backlog Enablers\nProduct Management wants to prioritize a list of Features likely to be planned in the upcoming PI meeting. What metric is used as the denominator (the number under the line) of weighted shortest job first (WSJF) calculations? ⬜ Feature size expressed in a T-shirt size\n✅ Job size based on relative estimation\n⬜ The actual business value of a Feature\n⬜ Feature size expressed in Story points\nTopic 11: Innovation and Planning (IP) Iteration What is the primary goal of the Innovation and Planning (IP) Iteration in SAFe? ⬜ To finalize all work from the current Program Increment (PI) ⬜ Finalize the PI Objectives for the upcoming PI ✅ To serve as a buffer for teams to meet PI objectives and provide time for innovation, planning, and learning ⬜ To increase the velocity of the Agile Release Train (ART)\nThe ART is near the end of the final Iteration of its first PI. Integration into staging is more challenging than estimated. The ART adds a week to the Innovation and Planning (IP) Iteration for integration and testing. Why is this action considered an anti-pattern? ⬜ It substantially decreases the predictability of the Solution Intent\n⬜ It decreases job satisfaction by removing autonomy and purpose\n⬜ Overall, train velocity goes up, and the time-to-market goes down\n✅ It reduces the overall predictability established through cadence and synchronization\nWhich of the following is a bad practice for the IP Iteration? ⬜ Dedicate time for I\u0026amp;A event and PI planning event ⬜ Ensure all Stories and Team\u0026rsquo;s PI plans are completed before the IP Iteration ⬜ Spend time on innovation, exploration, and tech debt in IP Iteration\n✅ Plan work for the IP Iteration during PI Planning and wait for the IP Iteration to fix defects\nTopic 12: Inspect and Adapt (I\u0026amp;A) Event What can a Release Train Engineer (RTE) use to support relentless improvement for the Program Increment? ⬜ Release management meeting\n⬜ Product Owner sync\n⬜ Iteration retrospective\n✅ Inspect and Adapt event\nWhich statement is true about the retrospective and problem-solving part of the Inspect \u0026amp; Adapt (I\u0026amp;A) workshop? ⬜ The improvement backlog items resulting from the problem-solving workshop should be items that only leadership can address\n⬜ Encourage teams to sit together during the retrospective portion to ensure an effective outcome\n✅ Key ART stakeholders, including Business Owners, Customers, and management can participate along with the teams\n⬜ The Release Train Engineer (RTE) gathers the list of problems to be solved during the final Coach Sync of the PI\nWhat is one anti-pattern related to System Demos? ⬜ People from outside the Agile Release Train attend the System Demo\n✅ Team demos are accepted in place of a System Demo to avoid redundancy\n⬜ System demo takes more than 15 minutes\n⬜ Different team members conduct the demo each Iteration\nTopic 13: DevOps and Release on Demand What best describes the DevOps? ⬜ A high-performing DevOps Team\n⬜ Combine Deployment and Releases ⬜ Strong organizational structure\n✅ A culture, a mindset and a set of technical practices\nHow does \u0026ldquo;C\u0026rdquo; in CALMR approach to DevOps help to organize around value and deliver continuous value? ⬜ By automating the continuous delivery pipeline ⬜ By Measuring the flow, quality \u0026amp; value ✅ By creating a culture of shared responsibility ⬜ By accelerating delivery using lean flow\nWhat are the three components of the Continuous Delivery Pipeline? (Choose three.) ⬜ Continuous Planning\n⬜ Continuous Improvement\n✅ Continuous Integration\n⬜ Continuous Cadence\n✅ Continuous Deployment\n✅ Continuous Exploration\nWhy is it important to decouple deployment from release? ⬜ To remove the need to respond quickly to production issues\n⬜ To allow inspection of Agile maturity based on different cycle times\n⬜ To make deploying of assets a business decision\n✅ To enable releasing functionality on demand to meet business needs\nTopic 14: Systems Thinking and Value Stream Mapping Which action describes the behavior of applying Systems Thinking for a Release Train Engineer (RTE)? ⬜ Demonstrates appreciation to team members in many ways\n⬜ Facilitates individual decision-making over team-level decision-making\n✅ Examines what may be missing to make the environment better for the team\n⬜ Encourages the team to express opinions in all circumstances\nWhat is one way to use the results from Value Stream mapping? ⬜ Identify methods for developers to code faster\n⬜ Calculate the metrics and share them with the ART\n✅ Move from bottleneck to bottleneck, eliminating as many as possible\n⬜ Focus on one component to optimize\nWhat practice can help to identify bottlenecks in the flow of work? ⬜ Visualizing the flow of all work and track progress of individual items ⬜ Comparing transaction costs, holding costs and business value realization ⬜ Measuring lead time for all work in progress ✅ Modeling overall process flow during value stream identification\nTopic 15: Facilitation and Coaching Techniques Why is it important for the Release Train Engineer (RTE) to understand Tuckman\u0026rsquo;s group dynamic stages? ⬜ Tuckman helps RTEs to better understand Team and ART topologies\n✅ An ART is a team of teams and will likely progress through the Tuckman stages\n⬜ The Tuckman dynamic nature of the stages requires that we assume variability and preserve options\n⬜ The Tuckman four stages should be reflected in the design of the ART Kanban\nA group of developers, Scrum Master/Team Coaches (SM/TCs), and Product Owners (POs) are interested in sharing knowledge and learning more about DevOps concepts. How can the Release Train Engineer (RTE) help them collaborate to gain knowledge about DevOps? ⬜ Schedule a DevOps bi-weekly synchronization\n⬜ Provide DevOps training\n✅ Help them launch a DevOps Community of Practice (CoP)\n⬜ Align them with the System Team\nSeveral Scrum Masters/Team Coaches (SM/TCs) disagree on what to do about a shared issue. What is an appropriate coaching technique for the Release Train Engineer (RTE)? ⬜ Being the final decision maker once all feedback has been heard\n⬜ Performing an empirical assessment of the problem using metrics\n⬜ Bringing new emphasis to Lean-Agile\u0026rsquo;s Respect for People\n✅ Asking powerful questions to invite creativity and new possibilities\nBecoming a coach requires a shift from old behaviors to new ones. What are three examples of new coaching behaviors? (Choose three.) ✅ Facilitate team problem-solving\n✅ Focus on business value delivery\n✅ Ask the team for the answer\n⬜ Drive toward specific outcomes\n⬜ Fix problems for the team\n⬜ Focus on deadlines\nTopic 16: One-Team Culture What is one technique for building a one-team culture across the ART? ⬜ Rotate team members to new teams to facilitate relationship building\n✅ Foster an environment in which the whole ART succeeds and fails together\n⬜ Review each teams\u0026rsquo; predictability measure with the ART\n⬜ Ensure the team leader does not show vulnerability\nWhat is a key characteristic of a \u0026ldquo;one-team culture\u0026rdquo; within an Agile Release Train (ART)? ⬜ Each team operates independently with minimal interaction\n⬜ Teams prioritize their own goals over ART objectives\n✅ Teams collaborate across boundaries to achieve shared ART goals\n⬜ Communication is limited to formal meetings only\nConsider buying the full set of questions from below links:- Premium SAFe Release Train Engineer (RTE) 6.0 Exam Questions with Answers and Explanation at a very reasonable price.\n","permalink":"https://codingnconcepts.com/agile/safe-release-train-engineer-rte-exam-questions/","tags":["SAFe","Certification"],"title":"SAFe Release Train Engineer RTE 6.0 Exam Questions"},{"categories":["Agile"],"contents":"If you are planning or preparing for SAFe Product Owner/Producer Manager POPM 6.0 (Scaled Agile Framework) certification then this article is for you to get started.\nOverview Prepare well for the exam. Understand all SAFe concepts and you can crack it like me! Requires 1 to 3 weeks of preparation depending upon your commitment per day. It is an online exam. You need to solve 45 questions (multiple choice = 1 answer and multiple select = 2-3 answers) in 90 mins. It is a web-based, closed book exam without any supervision, they trust you that you won\u0026rsquo;t cheat 😉 Passing score is 36/45 (80%) means you should answer at least 36 (out of 45) questions correctly. No negative scoring so answer all the questions! You get the score and result (Pass or Fail) immediately after you submit the exam. First attempt is included in the course registration fee if taken within 30 days of course completion. Each retake or attempt past the 30-day window is $50 You can download the SAFe Product owner/Product Manager 6.0 Workbook after the course registration from https://community.scaledagile.com/ Refer to the official Exam Details for more information. Refer to the official SAFe Website for exam material. Exam Questions Disclaimer: Scaled Agile Inc is a protected Brand. These exam questions are neither endorsed by nor affiliated with Scaled Agile. These are not the SAFe official exam questions/dumps. These questions are created from the web content of the Scaled Agile Framework and SAFe Product Owner/Producer Manager (POPM) 6.0 Workbook. These questions cover all the domains and topics of the SAFe POPM official exam and once you go through these questions and their concepts, you are more than ready to crack the exam in first attempt.\n200+ Leading SAFe Product Owner/Producer Manager POPM 6.0 Questions with Answers and Explanations at a very reasonable price and ace the exam.\nProduct Owner Free Practice Questions --- primary_color: steelblue secondary_color: \"#f2f2f2\" text_color: black shuffle_questions: false shuffle_answers: false --- ###### How does the Product Owner support the team in delivering continuous value? 1. [x] by Fostering Built-in Quality 1. [ ] by Testing benefit hypotheses 1. [ ] by Accepting Stories 1. [ ] by Prioritizing backlog items \u003e __Correct Answer:__ by Fostering Built-in Quality These are the areas of responsibilities of the Product Owner in SAFe 6.0:- 1- Connecting with the Customer 2- Contributing to the Vision and Roadmap 3- Managing and Prioritizing the Team Backlog 4- __Supporting the Team in Delivering Value__ 5- Getting and Applying Feedback These are the Product Owner's responsibilities in supporting the Team in Delivering Continuous Value:- 1- Balance Stakeholder perspectives 2- Elaborate Stories 3- __Foster Built-in Quality__ 4- Participate in team and ART Events Reference: https://scaledagileframework.com/product-owner/ ###### What is part of the role of the Product Owner? 1. [x] Managing and Prioritizing the Team Backlog 1. [ ] Managing and Prioritizing the ART Backlog 1. [ ] Managing and Prioritizing the Solution Train Backlog 1. [ ] Managing and Prioritizing the Portfolio Backlog \u003e __Correct Answer:__ Managing and Prioritizing the Team Backlog These are the areas of responsibilities of the Product Owner in SAFe 6.0:- 1- Connecting with the Customer 2- Contributing to the Vision and Roadmap 3- __Managing and Prioritizing the Team Backlog__ 4- Supporting the Team in Delivering Value 5- Getting and Applying Feedback Reference: https://scaledagileframework.com/product-owner/ ###### Which does NOT come under Product Owner responsibility? 1. [ ] Connecting with the Customer 1. [ ] Contributing to the Vision and Roadmap 1. [ ] Managing and Prioritizing the Team Backlog 1. [x] Facilitate PI Planning \u003e __Correct Answer:__ Facilitate PI Planning \u003e Facilitating PI Planning is the responsibility of the Scrum Master, not of the Product Owner. \u003e Responsibilities of the Product Owners are:- 1- __Connecting with the Customer__ 2- __Contributing to the Vision and Roadmap__ 3- __Managing and Prioritizing the Team Backlog__ 4- Supporting the Team in Delivering Value 5- Getting and Applying Feedback \u003e Reference: https://scaledagileframework.com/product-owner/ ###### Product Management has content authority over the Program Backlog. What do Product Owners have content authority over? 1. [ ] Value Streams 1. [ ] Portfolio Backlog 1. [ ] Portfolio Vision 1. [x] Team Backlog \u003e __Correct Answer:__ Team Backlog \u003e __Product managers__ concentrate on the program backlog and features, look one to three program increments ahead, and focus on product viability. They collaborate with business owners and those at the solution and strategic levels within SAFe \u003e __Product owners__ concentrate on the team backlog and stories, look one to three months ahead, collaborate with the team, and focus on product feasibility. \u003e Reference: https://scaledagileframework.com/product-owner/ \u003e Reference: https://scaledagile.com/blog/product-owners-product-managers-and-the-feature-factory/ ###### Who acts as a customer proxy for Agile teams? 1. [ ] The Scrum Master 1. [x] The Product Owner 1. [ ] The Release Train Engineer 1. [ ] The Business Analyst \u003e __Correct Answer:__ The Product Owner \u003e __Product Owner__ acts as a customer proxy for the Agile teams. Product Owner is an integral part of the team and voice of the customer \u003e Reference: https://scaledagileframework.com/product-owner/ ###### Who owns and prioritizes the Team backlog during an Iteration? 1. [ ] Scrum Master 1. [x] Product Owner 1. [ ] Business Owner 1. [ ] Product Management \u003e __Correct Answer:__ Product Owner \u003e Product owner is responsible for prioritization of User Stories and Enablers within an Iteration Backlog of an Agile Team. \u003e Reference: https://scaledagileframework.com/team-backlog/ ###### What does the Product Owner do as part of the prep work for Iteration Planning? 1. [ ] They collaborate with their team to detail stories with acceptance criteria and acceptance tests. 1. [x] They review and reprioritize the backlog. 1. [ ] They elaborate backlogs into user stories for implementation. 1. [ ] They build, edit, and maintain the team backlog. \u003e __Correct Answer:__ They review and reprioritize the backlog. \u003e Reference: https://scaledagileframework.com/iteration-planning/ ###### What is the primary purpose of PO Sync meeting? 1. [ ] To build PI Objectives and improve alignment 1. [ ] To align with Coach Sync participants on the status of the PI 1. [x] To assess the progress of the PI and adjust scope and priorities as needed 1. [ ] To conduct backlog refinement \u003e __Correct Answer:__ To assess the progress of the PI and adjust scope and priorities as needed \u003e The Product Owner (PO Sync) is an ART event used to gain visibility into the ART’s progress toward meeting its PI objectives and to make any necessary adjustments. \u003e Reference: https://scaledagileframework.com/planning-interval/ ###### Who has content authority to make decisions at the User Story level during Program Increment (PI) Planning? 1. [ ] Scrum Masters 1. [ ] Agile Team 1. [x] Product Owner 1. [ ] Release Train Engineer \u003e __Correct Answer:__ Product Owner • __Product managers__ concentrate on the __program backlog and features__, look one to three program increments ahead, and focus on product viability. They collaborate with business owners and those at the solution and strategic levels within SAFe • __Product owners__ concentrate on the __team backlog and stories__, look one to three months ahead, collaborate with the team, and focus on product feasibility. Reference: https://scaledagileframework.com/product-owner/ ###### When working with the team backlog, what is the specific function of the Product Owner? 1. [ ] Helping surface problems with the current plan. 1. [ ] Investing all their time in developing specific acceptance tests. 1. [ ] Holding all features that are planned to be delivered by an ART. 1. [x] Protecting the team from the problem of multiple stakeholders. \u003e __Correct Answer:__ Protecting the team from the problem of multiple stakeholders. A Product Owner (PO) of the team connects with Customers and multiple stakeholders and helps a team to answer all their questions Reference: https://scaledagileframework.com/agile-teams/ Reference: https://scaledagileframework.com/product-owner/ Product Management Free Practice Questions --- primary_color: steelblue secondary_color: \"#f2f2f2\" text_color: black shuffle_questions: false shuffle_answers: false --- ###### What is one responsibility of Product Management? 1. [ ] Building High-Performing Teams 1. [x] Defining Product Strategy, Vision and Roadmap 1. [ ] Managing and Prioritize the Team Backlog 1. [ ] Facilitate PI Planning \u003e __Correct Answer:__ Defining Product Strategy, Vision and Roadmap These are the areas of responsibilities of the __Product Management__ in SAFe 6.0:- 1- Exploring Markets and Users 2- Connecting with the Customer 3- __Defining Product Strategy, Vision and Roadmap__ 4- Managing and Prioritizing the ART Backlog 5- Delivering Value • Building High-Performing Teams and Facilitate PI Planning are the responsibility of Scrum Master. • Managing and Prioritize the Team Backlog is the responsibility of Product Owner. Reference: https://scaledagileframework.com/product-management/ ###### What is part of the role of Product Management? 1. [x] Managing and Prioritizing the ART Backlog 1. [ ] Managing and Prioritizing the Team Backlog 1. [ ] Managing and Prioritizing the Solution Train Backlog 1. [ ] Managing and Prioritizing the Portfolio Backlog \u003e __Correct Answer:__ Managing and Prioritizing the ART Backlog These are the areas of responsibilities of the __Product Management__ in SAFe 6.0:- 1- Exploring Markets and Users 2- Connecting with the Customer 3- Defining Product Strategy, Vision and Roadmap 4- __Managing and Prioritizing the ART Backlog__ 5- Delivering Value • Team Backlog is a Kanban system to capture and manage Stories by __Product Owner__ • ART Backlog is a Kanban system to capture and manage Features by __Product Management__ • Solution Train Backlog is a Kanban system to capture and manage Capabilities by __Solution Management__ • Portfolio Backlog is a Kanban system to capture and manage Portfolio Epics by __Epic Owner__ and __Lean Portfolio Management (LPM)__ Reference: https://scaledagileframework.com/product-management/ ###### Who is responsible for gathering qualitative and quantitative insights about market dynamics and user preferences? 1. [ ] Lean Portfolio Management 1. [ ] Solution Management 1. [x] Product Management 1. [ ] Product Owner \u003e __Correct Answer:__ Product Management These are the areas of responsibilities of the __Product Management__ in SAFe 6.0:- 1- __Exploring Markets and Users__ 2- Connecting with the Customer 3- Defining Product Strategy, Vision and Roadmap 4- Managing and Prioritizing the ART Backlog 5- Delivering Value Product Management conduct the primary and secondary research, apply market segmentation to gather the insights and understand the end-user needs Reference: https://scaledagileframework.com/product-management/ ###### Who has the content authority over the ART Backlog? 1. [ ] Product Owner 1. [x] Product Management 1. [ ] Release Train Engineer 1. [ ] Solution Architect/Engineer \u003e __Correct Answer:__ Product Management These are the areas of responsibilities of the __Product Management__ in SAFe 6.0:- 1- Exploring Markets and Users 2- Connecting with the Customer 3- Defining Product Strategy, Vision and Roadmap 4- __Managing and Prioritizing the ART Backlog__ 5- Delivering Value Reference: https://scaledagileframework.com/product-management/ ###### Who owns the Feature priorities during the PI Planning? 1. [ ] Business Owner 1. [x] Product Management 1. [ ] Release Train Engineer 1. [ ] Solution Architect/Engineer \u003e __Correct Answer:__ Product Management \u003e __Product Management__ presents the product vision including the __top ten features__ in the ART backlog during Day 1 of PI Planning \u003e Reference: https://scaledagileframework.com/pi-planning/ ###### Who owns the decision to release the changes into Production in SAFe 6.0? 1. [ ] Solution Owner 1. [ ] System Architect 1. [ ] Release Train Engineer 1. [x] Product Management \u003e __Correct Answer:__ Product Management \u003e In collaboration with other stakeholders, __Product Management__ establishes policies that govern the release process. \u003e Reference: https://scaledagileframework.com/release-on-demand/ ###### You need someone in your organization who will be the authority on the ART backlog and is the internal voice of the Customer. What SAFe Program-level role must you fill? 1. [ ] Epic Owner 1. [ ] Product Owner 1. [ ] Solution Management 1. [x] Product Management \u003e __Correct Answer:__ Product Management • Team Backlog is a Kanban system to capture and manage Features by __Product Owner__ • ART Backlog is a Kanban system to capture and manage Features by __Product Management__ • Solution Train Backlog is a Kanban system to capture and manage Capabilities by __Solution Management__ • Portfolio Backlog is a Kanban system to capture and manage Portfolio Epics by __Epic Owner__ and __Lean Portfolio Management (LPM)__ \u003e Reference: https://scaledagileframework.com/product-management/ ###### Which role does the Product management collaborate with to support the ongoing development and maintenance of the Architectural Runway? 1. [ ] Enterprise Architect 1. [ ] Release Train Engineer 1. [x] System Architect 1. [ ] Solution Architect \u003e __Correct Answer:__ System Architect System Architect works with Product Management to prioritize enabler stories for Architectural Runway Reference: https://scaledagileframework.com/product-management/ Reference: https://scaledagileframework.com/system-architect/ ###### Which technique is used by Product Management to prioritize the features in ART Backlog? 1. [x] WSJF 1. [ ] Empathy Maps 1. [ ] 5 Whys 1. [ ] SWOT Analysis \u003e __Correct Answer:__ WSJF Product Management use the WSJF (Weighted Shortest Job First) technique to prioritize the features in the ART backlog before each PI Planning session. Reference: https://scaledagileframework.com/product-management/ ###### What is the recommended way to express a Feature? 1. [x] Name, Benefit hypothesis, and Acceptance criteria 1. [ ] Lean business case 1. [ ] Name, Problem statement, and Definition of done 1. [ ] Name, Non-Functional Requirements, and Architecture \u003e __Correct Answer:__ Name, Benefit hypothesis, and Acceptance criteria A feature or capability is described using the name, benefit hypothesis, and acceptance criteria Reference: https://scaledagileframework.com/features-and-capabilities/ 200+ Leading SAFe Product Owner/Producer Manager POPM 6.0 Questions with Answers and Explanations at a very reasonable price and ace the exam.\nExam Domains Domain 1 - Product Owner/Product Management Roles and Responsibilities (14-16%) SAFe for Product Owner / Product Management The Lean-Agile mindset Value Streams Product Owner / Product Management Responsibilities Domain 2 - PI Planning Preparation (21-24%) PI Planning The Solution Vision Solution and PI Roadmaps Customer-centric Features ART Backlog and Kanban Domain 3 - Leadership for PI Planning (14-16%) The Vision and PI Planning PI Objectives ART Planning Board and Dependencies Risks and the End of PI Planning Domain 4 - Iteration Execution (30-32%) Stories and Story Maps Iteration Planning The Team Kanban Backlog Refinement Iteration Review and Iteration Retrospective DevOps and Release on Demand Domain 5 - PI Execution (14-16%) PO Sync System Demo The Innovation and Planning Iteration Inspect and Adapt Other Links to Refer to:-\nSAFe Big Picture Extended SAFe Guidance What’s New in SAFe 6.0 SAFe Product Owner/Product Manager 6.0 Workbook Exam Notes Lesson 1: Product Owner/Product Management Roles and Responsibilities What are the Lean Thinking Principles? Lean Thinking is to deliver the maximum value (a solution) to the customer in the shortest sustainable lead time from the trigger (the identification of the need or opportunity) to the point at which the customer receives the value. The five principles of Lean thinking are:-\nPrecisely specify value by product Identify the Value Stream for each product Make value flow without interruptions Let the Customer pull value from the producer Pursue perfection Reference: https://scaledagileframework.com/lean-agile-mindset/\nWhat are the Agile Values? Agile Manifesto uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value:\nIndividuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan\nThat is, while there is a value in the items on the right, we value the items on the left more Lean Thinking and Agile Values are two core building blocks of SAFe.\nReference: https://scaledagileframework.com/lean-agile-mindset/\nReference: https://agilemanifesto.org/\nWhat are the 12 Agile Manifesto Principles? Our highest priority is to satisfy the customer through early and continuous delivery of valuable software. Welcome changing requirements, even late in development. Agile processes harness change for the customer’s competitive advantage. Deliver working software frequently, from a couple of weeks to a couple of months, with a preference for a shorter timescale. Business people and developers must work together daily throughout the project. Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done. The most efficient and effective method of conveying information to and within a development team is face-to-face conversation. Working software is the primary measure of progress. Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely. Continuous attention to technical excellence and good design enhances agility. Simplicity – the art of maximizing the amount of work not done – is essential. The best architectures, requirements, and designs emerge from self-organizing teams. At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly. Reference: https://scaledagileframework.com/lean-agile-mindset/\nReference: https://agilemanifesto.org/principles.html\nWhat are the 10 SAFe Lean-Agile Principles? Take an economic view Deliver Early and Often Apply a Comprehensive Economic Framework:- -Operate Within Lean Budgets and Guardrails\n-Understand Solution Economic Trade-Offs: Development expense, Lead time, Product cost, Value, and Risk\n-Leverage Suppliers\n-Sequencing Jobs for Maximum Benefit: Weighted Shortest Job First (MSJF) Apply systems thinking The Solution Is a System\n-Optimizing a component does not optimize the whole system -For the system to behave well, teams must understand the intended behavior and architecture\n-The value of a system passes through its interconnections -A system can evolve no faster than its slowest integration point The Enterprise Building the System Is a System, Too Understand and Optimize the Full Development Value Stream Only Management Can Change the System Assume variability; preserve options\n-Flexible requirements and design, the Cone of uncertainty, set-based over point-based approach Build incrementally with fast, integrated learning cycles\n-PDCA = Plan – Do – Check – Adjust, The shorted the cycles, the faster the learning\n-Integration points control product development and reduce risk Base milestones on objective evaluation of working systems\n-Phase-gate milestones force design decisions too early, false-positive feasibility, they assume a point Solution exists, huge batches and long queues, centralized requirements and design.\n-Use Objective milestones instead, PI System Demos, continuous, cost-effective adjustments towards an optimum Solution Make value flow without interruptions\n-Reduce batch size for higher predictability. Total cost = Holding cost + Transaction cost. Reducing transaction costs increases predictability, accelerates feedback, reduces rework, and lowers cost.\n-Little’s Law: Wq = Lq / Lambda, Average wait time = Average queue length / Average processing rate Apply cadence, synchronize with cross-domain planning Cadence – converts unpredictable events into predictable occurrences and lowers cost, makes waiting times for new work predictable, supports regular planning and cross-functional coordination, limits batch sizes to a single interval, controls the injection of new work, provides scheduled integration points;\nSynchronization – causes multiple events to happen simultaneously, facilitates cross-functional trade-offs, provides routine dependency management, supports full system integration and assessment, provides multiple feedback perspectives Unlock the intrinsic motivation of knowledge workers -Workers are most qualified to make decisions about how to perform their work\n-The workers must be heard and respected for management to lead effectively\n-Knowledge workers must manage themselves. They need autonomy -Continuing innovation must be part of the work, the tasks, and the responsibilities of knowledge workers. -Unlocking intrinsic motivation with autonomy, mastery, and purpose Decentralize decision-making Centralize – Infrequent, Long-lasting, Significant economies of scale\nDecentralize – Frequent, Time critical, Requires local information Organize around value -Value doesn’t follow silos\n-Organize around Development Value Streams. Reference: https://scaledagileframework.com/safe-lean-agile-principles/\nWhat are the four SAFe core values? SAFe has the following four core values:-\nAlignment Transparency Respect for people Relentless improvement Reference: https://scaledagileframework.com/safe-core-values/\nWhat is Value Streams? Value streams are the most fundamental construct of Lean thinking and are foundational to SAFe. There are two types of value streams described in SAFe:-\nDevelopment Value Streams (DVS) is the sequence of activities needed to convert a business hypothesis into a digitally-enabled product or service that delivers customer value.\n\u0026mdash; Example of DVS: designing and developing a medical device, developing and deploying a CRM system, or building an e-commerce website.\n\u0026mdash; Systems and software developers, product managers, engineers, scientists, and IT practitioners work primarily in the DVS.\n\u0026mdash; DVS structure contains: Trigger, Steps, Bar, and Elipses (\u0026hellip;) Operational Value Streams (OVS) is the sequence of activities needed to deliver a product or service to a customer. \u0026mdash; Example: manufacturing a product, fulfilling an e-commerce order, admitting and treating a patient, providing a loan, or delivering a professional service.\n\u0026mdash; Common OVS Patterns are Fulfillment, Manufacturing, Software products, and Supporting.\n\u0026mdash; OVS flow contains: Steps, People, System, and information \u0026amp; material DVS develops and supports the product and services used by OVS.\nReference: https://scaledagileframework.com/development-value-streams/\nReference: https://scaledagileframework.com/operational-value-streams/\nWhat is an Agile team? An Agile Team is a cross-functional group of typically ten or fewer individuals with all the skills necessary to define, build, test, and deploy increments of value to their customers. Agile teams are optimized for communication and the continuous delivery of value to the customer. Agile Teams visualize flow with SAFe Scrum or SAFe Kanban Agile Team Events are:- Team Sync, Backlog Refinement, Iteration Review, Iteration Retro, Iteration Planning The Agile Team\u0026rsquo;s responsibilities are:-\nConnecting with the Customer (led by PO) Planning the Work: ART Planning (PI Planning), Team planning using SAFe Scrum or SAFe Team Kanban, and refining the Team Backlog. Delivering Value: Frequently integrate and test, Sync with other teams in ART through ART Sync (includes Coach Sync and PO Sync), build continuous delivery pipeline, release frequently. Getting Feedback: with the help of PO and through System Demos Improving relentlessly: participate in ART\u0026rsquo;s joint Inspect \u0026amp; Adapt, address the problems as they occur Reference: https://scaledagileframework.com/agile-teams/\nWhat are the two specialty roles in Agile Teams? The Agile Team contains two specialty roles:- Product Owner (PO) and Scrum Master/Team Coach (SM/TC).\nProduct Owner (PO) responsibilities are:- Connect with the customer Contribute to the Vision and Roadmap Manage and prioritize the Team Backlog Support the team in delivering value Get and apply fast feedback Scrum Master/Team Coach (SM/TC) responsibilities are:- Facilitate SAFe Scrum (or SAFe Kanban) and PI planning Supports Iteration Execution Improves Flow Build a high-performing team Optimizes and improves the team and ART performance Reference: https://scaledagileframework.com/agile-teams/\nWhat is Agile Release Train (ART)? ART is a team of cross-functional Agile Teams and has the capabilities to define, build, validate, and release to deliver a continuous flow of value. ART is a virtual organization of 5-12 teams (50-125+ individuals) All the teams in ART are synchronized on a common cadence - a Program Increment (PI), aligned to a common mission via a single Program Backlog (ART Backlog). Critical Roles in the ART are:-\nRelease Train Engineer (RTE) is a servant leader (chief scrum master and coach) who facilitates ART execution, impediment removal, risk and dependency management, and continuous improvement. Product Management is largely responsible for ‘what gets built,’ as defined by the Vision, Roadmap, and new Features in the ART Backlog. They work with customers, teams, and Product Owners to understand and communicate their needs and participate in solution validation. System Architect is an individual or team that defines the system’s overall architecture. They work at a level of abstraction above the teams and components and typically define Non-functional Requirements (NFRs), major system elements, subsystems, and interfaces. Business Owners are key stakeholders of the ART, with final responsibility for the business outcomes of the train. Customers are the ultimate economic buyers or value users of the solution. Other essential roles in the ART are:-\nSystem Teams typically assist in building and maintaining development, continuous integration, and test environments. Shared Services are specialists necessary for the success of an ART but cannot be dedicated to a specific train. They often include data security, information architects, site reliability engineering (SRE), database administrators (DBAs), and many more. Three Sync events to keep ART on track:-\nCoach Sync: (timeboxed: 1 hour) focuses on executing the current PI, including risk, dependencies, progress, and impediments PO Sync: manages the PI’s scope, reviews progress, adjusts priorities, and prepares for the following PI ART Sync: usually replaces the Coach Sync and PO Sync for a particular iteration to reduce overhead. The ROAM board created during PI planning can be reviewed during the ART Sync. ART Planning board is used during ART sync to track and manage dependencies, ensuring they do not block other teams. Other events in ART:-\nPI Planning (timeboxed: 2 days) Each ART begins with the PI Planning using the PI planning board, the outcome is PI Objectives to be completed in the PI iteration. System Demos (timeboxed: 1 hour) Occur at the end of Iteration to review deliverables and receive feedback from stakeholders, business owners, and customers. Inspect \u0026amp; Adapt (timeboxed: 1/2 day) Each PI concludes with I\u0026amp;A event for retrospection where ART reviews and improves its process before the next PI. Reference: https://scaledagileframework.com/agile-release-train/ Reference: https://scaledagileframework.com/planning-interval/\nWhat are the responsibilities of the Product Owner? Product Owner (PO) is the team’s primary customer advocate and primary link to business and technology strategy. They have the following responsibilities:-\nConnecting with the Customer Contributing to the Vision and Roadmap Managing and Prioritizing the Team Backlog Supporting the Team in Delivering Value Getting and Applying Feedback Reference: https://scaledagileframework.com/product-owner/\nWhat are the responsibilities of Product Management? Product Management has the following responsibilities:-\nExploring Markets and Users Connecting with the Customer Defining Product Strategy, Vision, and Roadmaps Managing and Prioritizing the ART Backlog Delivering Value Reference: https://scaledagileframework.com/product-management/\nWhat are the content authorities of the Product Owner? Has Team Backlog content authority Works with the System Architect to prioritize Enablers Guides Iteration Goals and content via prioritized Stories Establish Story acceptance criteria Has authority for accepting Stories and team increments Helps guide PI Objectives at the team level Reference: https://scaledagileframework.com/product-owner/\nWhat are the content authorities of Product Management? Has ART Backlog content authority Works with the System Architect and team to prioritize Enablers Has content authority for Vision and Roadmap Helps guide PI Objectives Establish Features and acceptance criteria Reference: https://scaledagileframework.com/product-management/\nLesson 2 - Preparing for PI Planning What is Planning Interval (PI)? A Planning Interval (PI) is a cadence-based timebox in which Agile Release Trains deliver continuous value to customers in alignment with PI Objectives. PIs are typically 8 – 12 weeks long. The most common pattern for a PI is four development Iterations, followed by one Innovation and Planning (IP) Iteration. Reference: https://scaledagileframework.com/planning-interval/\nWhat is PI Planning and its events? PI Planning stands for Program Increment Planning. PI Planning is a cadence-based event that serves as the heartbeat of the ART, aligning all teams on the ART to a shared mission and Vision. PI Planning sessions are regularly scheduled events held throughout the year where multiple teams within the same Agile Release Train (ART) meet to align to a shared vision, discuss features, plan the roadmap, and identify cross-team dependencies. PI Planning is a 2 full day event that typically runs every 8-12 weeks (10 weeks typical). The two-day agenda is as follows:- Day 1 08:00 - 09:00 Business Context 09:00 - 10:30 Product/Solution Vision 10:30 - 11:30 Architecture Vision and Development Practices 11:30 - 01:00 Planning Context and Lunch 01:00 - 04:00 Team breakouts 04:00 - 05:00 Draft Plan Review 05:00 - 06:00 Management review and problem solving Day 2 08:00 - 09:00 Planning Adjustment 09:00 - 11:00 Team breakouts 11:00 - 01:00 Final Plan Review and Lunch 01:00 - 02:00 ART Risks 02:00 - 02:15 Confidence Vote 02:15 - ?? Plan Rework (if needed) When ready Planning Retrospective and moving forward Primary Inputs to the PI Planning include: 1. Business context, 2. Roadmap \u0026amp; vision, and 3. Highest priority Features (typically top 10) of the ART backlog Primary Outputs of the PI Planning include: 1. Committed PI objectives, and 2. ART planning board Product Management provides the vision and backlog (typically represented by the top ten or so upcoming features) and owns the feature priorities Business Owner provides the business context and assigns a business value (BV) to each PI Objective on a scale from 1 to 10 Development Teams own Story planning and high-level estimates ART Planning Board is used for PI Planning showing: 1. Features, 2. Significant Dependency, and 3. Milestone or Event Architects and UX work as intermediaries for governance, interfaces, and dependencies Reference: https://scaledagileframework.com/pi-planning/\nWhat are PI\u0026rsquo;s Uncommitted Objectives? Uncommitted objectives are used to identify work that can be variable within the scope of a PI. The work is planned, but the outcome is simply not certain. Teams can apply uncommitted objectives whenever there is low confidence in meeting the objective. This can be due to many circumstances:\nDependencies with another team or supplier that cannot be guaranteed. The team has little to no experience with functionality of this type. In this case the teams may plan ‘Spikes’ early in the PI to reduce uncertainty. There are a large number of fairly critical objectives that the business is depending on and the team is already loaded close to full capacity. Reference: https://scaledagileframework.com/pi-planning/\nWhat is a Feature? Features are maintained in the ART Backlog Feature are sized to fit in a Program Increment (PI) and delivered by a single Agile Release Train (ART) Features are split into Stories and fit in one Iteration for one team Features include a definition of Minimum Marketable Feature (MMF), a benefit hypothesis (to justify development cost) and Acceptance criteria (defined during program backlog refinement). Features are prioritized using WSJF and the top 10 features are presented to the team during PI planning Typically Product Management creates business features and System Architect creates enabler features Reference: https://scaledagileframework.com/features-and-capabilities/\nLesson 3: Leading PI Planning What do the Product Owner and Product Management do on day 1 of PI Planning? Communicate the Vision\n\u0026mdash; Present to the ART how the vision aligns with Strategic Themes and Solution Context \u0026mdash; Prepare material and provide user personas to illustrate and explain the Vision\n\u0026mdash; Explain the importance of NFR Communicate the Roadmap\n\u0026mdash; Show how PI roadmaps in this PI help fulfill the vision \u0026mdash; Describe how PI roadmaps support Key Epics and Milestones Communicate the top 10 features to the ART\n\u0026mdash; Discuss and refine features through backlog refinement before PI Planning\n\u0026mdash; Prepare to explain why these features were chosen\n\u0026mdash; Top 10 features is a guideline. ART may pull more or less. Collaborate to decompose Features into Stories Negotiate Scope Review draft PI plans and provide feedback Participate in management review of draft plans What do the Product Owner and Product Management do on day 2 of PI Planning? Support team breakouts Accept team PI Objectives Establish business value with Business Owners Participate in the final plan review Provide feedback on ART PI Risks Participate in the confidence vote Lesson 4: Executing Iterations What is a Feature? Features are maintained in the ART Backlog Feature are sized to fit in a Program Increment (PI) and delivered by a single Agile Release Train (ART) Features are split into Stories and fit in one Iteration for one team Features include a definition of Minimum Marketable Feature (MMF), a benefit hypothesis (to justify development cost) and Acceptance criteria (defined during program backlog refinement). Features are prioritized using WSJF and the top 10 features are presented to the team during PI planning Typically Product Management creates business features and System Architect creates enabler features Reference: https://scaledagileframework.com/features-and-capabilities/\nWhat is a Story? Features are implemented by Stories Stories are small increments of value that can be developed in days and are relatively easy to estimate Stories are created during PI planning as the team collaborates with POs and Product Management Features fit in one PI for one ART; Stories fit in one iteration for one team. A Story Point is a relative number that represents: Volume, Complexity, Knowledge, and Uncertaity. Team breaks down Features into one or more User Stories and Enabler Stories User Stories expressed desired end-user functionality written in the user\u0026rsquo;s language Enabled Stories support exploration, architecture, infrastructure, and compliance Reference: https://scaledagileframework.com/story/\nWhen is a Story complete? A Story is complete when it satisfies the definition of done (DoD). The DoD requires that the story:-\n\u0026mdash; Satisfies the acceptance criteria\n\u0026mdash; Is accepted by the Product Owner\nLesson 5: Executing the PI What is PO Sync? Provides visibility into how well the ART is progressing toward meeting the ART PI Objectives Provides an opportunity to access scope and priority adjustments Is facilitated by RTE or Product Management Includes Product Managers, POs, Stakeholders, and SMEs, as necessary Occurs weekly or more frequently and lasts 30-60 minutes long POs communicate the adjustments to their team after the PO Sync\nReference: https://scaledagileframework.com/planning-interval/\nWhat is Inspect and Adapt (I\u0026amp;A) Event? The Inspect and Adapt (I\u0026amp;A) is a significant event (Timebox: 3-4 hours) held at the end of each PI, where All ART stakeholders and the Agile Team participate. The I\u0026amp;A event consists of three parts:- – PI System Demo (Timebox: 45-40 mins) - team demonstrates the current state of the solution\n– Quantitative and qualitative measurement - Team PI performance report is created which includes team\u0026rsquo;s planned vs actual business value. Individual team totals are rolled up into the ART predictability report.\n– Retrospective and problem-solving workshop Reference: https://scaledagileframework.com/inspect-and-adapt/\nWhat is a PI System Demo? The PI System Demo is the event (Timebod: 1 hour) held at the end of each PI, to demonstrate the current state of the solution to appropriate stakeholders Often led by Product Management, Product Owners, and the System Team Attended by Business Owners, ART Stakeholders, Product Management, RTE, Scrum Masters/Team Coaches, and teams. Reference: https://scaledagileframework.com/system-demo/\nWhat is Innovation and Planning (IP) Iteration? The Innovation and Planning (IP) Iteration is a unique iteration that occurs every PI, which provides dedicated time for Innovation and Planning where:- –\u0026ndash; Innovation includes opportunities for innovation, hackathons, infrastructure improvements, continuing education, certifications, etc. \u0026ndash;– Planning includes PI Planning Readiness, Inspect and Adapt (I\u0026amp;A), and PI Planning events, etc. It provides:- \u0026mdash; an estimating buffer for meeting PI Objectives\n\u0026mdash; an estimating guard band for cadence-based delivery \u0026mdash; sufficient capacity margin to enable cadence Without the IP Iteration\n– Lack of delivery capacity buffer impacts predictability\n– Little innovation; the tyranny of the urgent\n– Technical debt grows uncontrollably\n– People burn out\n– No time for teams to plan, demo, or improve together Reference: https://scaledagileframework.com/innovation-and-planning-iteration/\nOther SAFe Certification Exam Notes Read Leading SAFe Agilist 6.0 (Scaled Agile) Exam Notes\nRead SAFe Scrum Master SSM 6.0 Exam Notess\n","permalink":"https://codingnconcepts.com/agile/safe-product-owner-product-manager-popm-exam-notes/","tags":["SAFe","Certification"],"title":"SAFe Product Owner/Producer Manager POPM 6.0 Exam Notes"},{"categories":["SG"],"contents":"A comprehesive guide for Singapore Driving - Basic Theory Test (BTT) including the free practice quiz.\nDisclaimer: These are NOT the Basic Theory Test (BTT) real exam questions or exam dumps. These questions are created from the content of the Basic Theory of Driving Official Handbook. These questions cover all the topics of the BTT official exam and once you go through these questions and their concepts, you are more than ready to crack the exam in first attempt.\nAbout Basic Theory Test (BTT) Exam BTT has 50 questions to be answered in 50 min Passing score is 45/50 (90%) means you should answer at least 45 (out of 50) questions correctly. No negative scoring so attempt all the questions! You should be able to finish the exam in 20-30 min, spend some time to review the answers before submission. You will see the test result after the submission on your computer screen. You can download the Basic Driving Theory Handbook You can practice the mock theory test in English, Mandarin, Malay, and Tamil using this link BTT Full Exam Questions Consider buying the full set of 500 questions from below links:-\nComplete set of 500 Singapore Driving - Basic Theory Test (BTT) Questions and Answers at a very reasonable price.\nPlease leave your feedback in the comment section or contact us from main menu.\nWish you all the best for the Basic Theory Test (BTT) Exam!\nBTT Free Practice Quiz Quiz 1 - Signs And Signals --- primary_color: steelblue secondary_color: \"#f2f2f2\" text_color: black shuffle_questions: false shuffle_answers: false --- ###### What does this prohibitory sign means? ![BTT-Quiz](/img/sg/btt-quiz-sign-no-entry.png) 1. [x] No entry for all vehicles at all times 1. [ ] No stopping of vehicles at all times 1. [ ] No waiting and stopping of vehicles at all times \u003e __Correct Answer:__ No entry for all vehicles at all times ###### What does this prohibitory sign means? ![BTT-Quiz](/img/sg/btt-quiz-sign-no-stopping.png) 1. [ ] No Waiting 1. [x] No Stopping 1. [ ] No Entry \u003e __Correct Answer:__ No Stopping ###### What does this prohibitory sign means? ![BTT-Quiz](/img/sg/btt-quiz-sign-no-right-turn.svg) 1. [ ] No waiting or stopping to the right 1. [x] No right turn 1. [ ] Road on the right is temporarily closed \u003e __Correct Answer:__ No right turn ###### What does this sign means? ![BTT-Quiz](/img/sg/btt-quiz-sign-stop-children.svg) 1. [ ] children must not cross the road. 1. [ ] you can proceed as only children have to stop. 1. [x] you must stop for children to cross the road. \u003e __Correct Answer:__ you must stop for children to cross the road. ###### What does this sign mean? ![BTT-Quiz](/img/sg/btt-quiz-sign-no-overtaking.png) 1. [ ] No entry for cars 1. [x] No overtaking 1. [ ] No racing \u003e __Correct Answer:__ No overtaking ###### What does this warning represent? ![BTT-Quiz](/img/sg/btt-quiz-sign-erp.png) 1. [ ] Express Road Pathway ahead 1. [x] Electronic Road Pricing Zone ahead 1. [ ] Express Roadside Parking ahead \u003e __Correct Answer:__ Electronic Road Pricing Zone ahead *(Pay a road user charge when entering the zone during restricted hours)* ###### What does this warning sign mean? ![BTT-Quiz](/img/sg/btt-quiz-sign-gated-crossing.png) 1. [ ] Private property ahead, drive carefully and do not breach 1. [x] Gated level crossing ahead, slow down and beware of gate closing 1. [ ] Forest zone ahead, slow down and beware of animals \u003e __Correct Answer:__ Gated level crossing ahead, slow down and beware of gate closing ###### What does this warning sign mean? ![BTT-Quiz](/img/sg/btt-quiz-sign-ungated-crossing.png) 1. [ ] Railway Track ahead 1. [x] Ungated Level Crossing ahead 1. [ ] Train Accident ahead \u003e __Correct Answer:__ Ungated Level Crossing ahead *(Slow down and beware of train approaching)* ###### What does this warning sign mean? ![BTT-Quiz](/img/sg/btt-quiz-sign-vehicle-breakdown.png) 1. [ ] Road is clear and no traffic, you can drive within the maximum speed limit 1. [x] Vehicle breakdown sign to be placed at least 20 metres from the rear of the vehicle 1. [ ] Slow down and drive carefully, look and follow the warning signs \u003e __Correct Answer:__ Vehicle breakdown sign to be placed at least 20 metres from the rear of the vehicle ###### What does this warning sign mean? ![BTT-Quiz](/img/sg/btt-quiz-sign-road-leading-to-quay.png) 1. [ ] Road hump ahead 1. [x] Road leading to quay 1. [ ] Steep downward slope \u003e __Correct Answer:__ Road leading to quay, river bank or sea *(Slow down and Beware of road ending ahead)* ###### What does this warning sign mean? ![BTT-Quiz](/img/sg/btt-quiz-sign-tunnel-ahead.png) 1. [ ] The strech of road leading to an expressway 1. [x] The stretch of road ahead is in a tunnel 1. [ ] The stretch of road ahead is narrow \u003e __Correct Answer:__ The stretch of road ahead is in a tunnel ###### What does this warning sign mean? ![BTT-Quiz](/img/sg/btt-quiz-sign-expressway.svg) 1. [x] You are about to enter an expressway 1. [ ] You are about to enter a tunnel 1. [ ] You are about to cross an MRT line \u003e __Correct Answer:__ You are about to enter an expressway *(Certain types of vehicles are prohibited from using the expressway)* ###### What does this warning sign mean? ![BTT-Quiz](/img/sg/btt-quiz-sign-cross-junction.svg) 1. [ ] drive carefully and give way to the 'Red Cross' ambulance. 1. [ ] slow down and drive carefully in the hospital area. 1. [x] slow down and beware of the traffic approaching cross-road junction. \u003e __Correct Answer:__ slow down and beware of the traffic approaching cross-road junction. ###### What does this information sign mean? ![BTT-Quiz](/img/sg/btt-quiz-sign-rain-shelter.png) 1. [ ] Umbrella available for rent 1. [x] Rain shelter 1. [ ] Heavy Rain Zone \u003e __Correct Answer:__ Rain shelter *(mainly for motorcyclists)* ###### What does this sign mean? ![BTT-Quiz](/img/sg/btt-quiz-sign-must-turn-left.svg) 1. [ ] warning sign, you should look out for traffic from the right before proceeding straight ahead. 1. [ ] information sign, you must keep left to allow other vehicles to overtake. 1. [x] mandatory sign, you must turn left. \u003e __Correct Answer:__ mandatory sign, you must turn left. ###### What does this mandatory sign mean? ![BTT-Quiz](/img/sg/btt-quiz-sign-keep-left.svg) 1. [ ] sharp turn on the left 1. [ ] turn to the left side only 1. [x] keep to the left side of the road. \u003e __Correct Answer:__ keep to the left side of the road. *(It is a compulsary sign and an offence to disobey.)* ###### What does this sign mean? ![BTT-Quiz](/img/sg/btt-quiz-sign-roundabout.svg) 1. [ ] Give way to traffic on the left and right 1. [ ] Give way to traffic on the left 1. [x] Give way to traffic on the right \u003e __Correct Answer:__ Give way to traffic on the right. *(It is a Roundabout sign, slow down and stop if necessary. Give way to traffic on the right)* ###### This sign means 'No Entry' for vehicles? ![BTT-Quiz](/img/sg/btt-quiz-sign-wide.svg) 1. [ ] higher than 2.3 metres. 1. [x] wider than 2.3 metres. 1. [ ] longer than 2.3 metres. \u003e __Correct Answer:__ wider than 2.3 metres. ###### This does this sign mean? ![BTT-Quiz](/img/sg/btt-quiz-sign-speed.svg) 1. [x] Cannot drive beyond 80 km/h. 1. [ ] Cannot drive slower than 80 km/h. 1. [ ] Maintain your speed at 80 km/h. \u003e __Correct Answer:__ Cannot drive beyond 80 km/h. ###### When you see this sign, you should? ![BTT-Quiz](/img/sg/btt-quiz-sign-narrow.svg) 1. [ ] slow down and prepare to stop - road ends ahead. 1. [ ] slow down and drive carefully - dual-carriageway ahead. 1. [x] slow down, do not overtake - road narrows ahead. \u003e __Correct Answer:__ slow down, do not overtake - road narrows ahead. Quiz 2 - 20 Questions --- primary_color: steelblue secondary_color: \"#f2f2f2\" text_color: black shuffle_questions: false shuffle_answers: false --- ###### When driving behind a bus, you should? 1. [x] Keep a longer following distance 1. [ ] Keep the normal following distance 1. [ ] Keep more to the right to have a better view \u003e __Correct Answer:__ Keep a longer following distance ###### When you are driving along a main road, you should 1. [ ] Stop at every road junction before proceeding 1. [ ] Accelerate through each junction 1. [x] Check every side road before you drive past them \u003e __Correct Answer:__ Check every side road before you drive past them ###### You notice a vehicle trying to pull out between a row of stationary vehicles. You should 1. [ ] Drive on regardless of danger ahead 1. [ ] Sound the horn and maintain your speed 1. [x] Slow down and be prepared to stop \u003e __Correct Answer:__ Slow down and be prepared to stop ###### As vehicle speed increases, your field of vision would be 1. [x] Reduced 1. [ ] Remained the same 1. [ ] Increased \u003e __Correct Answer:__ Reduced ###### When driving, your arms should be 1. [ ] Straighten 1. [x] Slightly bent 1. [ ] Bent at 90 degree \u003e __Correct Answer:__ Slightly bent ###### Some of the common vehicle defects that can cause accidents are 1. [ ] Faulty seat belts, seats, child restraint and alarm system 1. [x] Faulty tyres, brakes, shock absorbers and windscreen wipers 1. [ ] Dull paint, dented bumpers and old seats \u003e __Correct Answer:__ Faulty tyres, brakes, shock absorbers and windscreen wipers ###### The handbreaks of a car acts 1. [x] Only on the rear wheels 1. [ ] Only on the front wheels 1. [ ] On both the front and rear wheels \u003e __Correct Answer:__ Only on the rear wheels ###### When passing the stationary bus at bus-stop, you should 1. [ ] Sound the horn to warn the bus driver of your presence 1. [ ] Speed up to overtake the bus before it moves off 1. [x] Slow down and be ready to stop for pedestrian who may cross the road in front of the bus \u003e __Correct Answer:__ Slow down and be ready to stop for pedestrian who may cross the road in front of the bus ###### Both the van and the car are entering a car park, the car should not overtake the van ![BTT-Quiz](/img/sg/btt-quiz-1-1.png) 1. [ ] Because of the centre white line 1. [ ] Because of the double yellow line 1. [x] Because it is not safe to do so \u003e __Correct Answer:__ Because it is not safe to do so ###### How does alcohol affect you 1. [x] It reduces your concentration 1. [ ] It increases your awareness 1. [ ] It improves your co-ordination \u003e __Correct Answer:__ It reduces your concentration ###### The \"Two-second Rule\" is a sufficient distance between your vehicle and the car in front when road condition is 1. [ ] Wet 1. [x] Good 1. [ ] Hazy \u003e __Correct Answer:__ Good ###### You will not be allowed to apply for a Provisional Driving License (PDL) if you have 1. [ ] 6 demerit points 1. [ ] 9 demerit points 1. [x] 13 demerit points \u003e __Correct Answer:__ 13 demerit points ###### You will not be allowed to take your driving test if you have 1. [x] 13 demerit points 1. [ ] 12 demerit points 1. [ ] 24 demerit points \u003e __Correct Answer:__ 13 demerit points ###### When you hear the siren of an emergency vehicle, you should 1. [ ] Stop immediately where you are in order to make way 1. [x] Pull over to the left or right side of the road 1. [ ] Sound your horn to warn the front vehicles to speed up \u003e __Correct Answer:__ Pull over to the left or right side of the road ###### The safe following distance is 1. [ ] 1 car-length for every 5 kmh of your speed 1. [x] 1 car-length for every 10 kmh of your speed 1. [ ] 1 car-length for every 20 kmh of your speed \u003e __Correct Answer:__ 1 car-length for every 10 kmh of your speed ###### After driving through flood the brakes may malfunction. In order to correct this, you must 1. [ ] Stop the vehicle on a slope to allow water to flow out from the brakes 1. [x] Pump the break pedal repeatedly to dry the brakes 1. [ ] Drive at normal speed to spin-dry the brakes \u003e __Correct Answer:__ Pump the break pedal repeatedly to dry the brakes ###### An automatic car has foot pedal/s 1. [ ] One 1. [x] Two 1. [ ] Three \u003e __Correct Answer:__ Two There are two pedals in an automatic car. The accelerator is on the right. The brake is on the left. You control both pedals with your right foot. ###### When entring the expressway, vehicles on the left lane ![BTT-Quiz](/img/sg/btt-quiz-1-2.png) 1. [x] Should adjust their speed in order to merge in smoothly 1. [ ] Must not slow down or stop 1. [ ] Must stop at the dotted line \u003e __Correct Answer:__ Should adjust their speed in order to merge in smoothly ###### The right-most outer lane of the expressway is meant for 1. [x] Emergency vehicles and overtaking 1. [ ] Vehicles travelling at the maximum speed limit 1. [ ] Slow moving vehicles \u003e __Correct Answer:__ Emergency vehicles and overtaking ###### Before reaching a junction, you should 1. [x] Form up into the correct lane which you intend to travel 1. [ ] Sound your horn to caution other road users 1. [ ] Speed up so as not to cause a traffic jam \u003e __Correct Answer:__ Form up into the correct lane which you intend to travel Quiz 3 - 20 Questions --- primary_color: steelblue secondary_color: \"#f2f2f2\" text_color: black shuffle_questions: false shuffle_answers: false --- ###### To assist braking when you are travelling downhill, you should use 1. [ ] The front brakes only 1. [ ] Both the front and rear brakes only 1. [x] the front and rear brakes and engine brake \u003e __Correct Answer:__ the front and rear brakes and engine brake ###### When the clutch pedal is depressed 1. [x] Engine-brake will no longer be effective 1. [ ] Engine-brake will be effective 1. [ ] Brake linings will be burnt \u003e __Correct Answer:__ Engine-brake will no longer be effective ###### With gear engaged, engine power is not connected to the wheel when the clutch 1. [ ] Is released 1. [x] Is applied 1. [ ] Is released halfway \u003e __Correct Answer:__ Is applied ###### You should not depress the clutch when 1. [ ] Stopping 1. [ ] Changing gears 1. [x] Going round a bend \u003e __Correct Answer:__ Going round a bend ###### When you see a bend ahead, you should 1. [ ] Change from the 3rd gear to the 4th gear before entering the bend 1. [ ] Depress the clutch as you enter the bend 1. [x] Change from the 4th gear to the 3rd gear before entering the bend \u003e __Correct Answer:__ Change from the 4th gear to the 3rd gear before entering the bend ###### To move off smoothly uphill, you must be good at co-ordinating 1. [ ] The clutch, foot-brake and handbrake 1. [x] The clutch, accelerator and handbrake 1. [ ] The clutch, foot-brake and accelerator \u003e __Correct Answer:__ The clutch, accelerator and handbrake ###### You may overtake another vehicle on the left 1. [ ] If it is a heavy vehicle 1. [x] If the driver in front signals his intention to turn right 1. [ ] If you are certain that vehicle would not change direction \u003e __Correct Answer:__ If the driver in front signals his intention to turn right ###### While reversing you should 1. [ ] Look at the rear view and side view mirrors 1. [ ] Look through the rear windscreen 1. [x] All of them \u003e __Correct Answer:__ All of them ###### Reversing from a side road into a main road is 1. [ ] Allowed if there is no vehicle around 1. [x] Not allowed for it is unsafe and could cause road accident 1. [ ] Allowed during the daytime only \u003e __Correct Answer:__ Not allowed for it is unsafe and could cause road accident ###### Driving with a \"bald\" tyre is dangerous because 1. [ ] It skids more easily 1. [ ] There is a greater probability of punctures 1. [x] All of them \u003e __Correct Answer:__ All of them ###### If the front tyre is punctured, 1. [ ] The car will pull away from the side of the punctured tyre 1. [x] The car will pull to the side of the punctured tyre 1. [ ] The car will move from side to side \u003e __Correct Answer:__ The car will pull to the side of the punctured tyre ###### When negotiating a sharp bend, centrifugal force pushes your vehicle 1. [ ] Inwards 1. [x] Outwards 1. [ ] Forward \u003e __Correct Answer:__ Outwards ###### Reaction distance is the distance travelled from the moment, the 1. [ ] Break is applied to the time the vehicle comes to a stop 1. [ ] Hazard is seen to the time the vehicle comes to a stop 1. [x] Hazard is seen to the time the brake is applied \u003e __Correct Answer:__ Hazard is seen to the time the brake is applied ###### Why is free wheeling wrong? 1. [ ] It will cause the vehicle to skid 1. [x] There is no engine braking 1. [ ] The engine will run faster \u003e __Correct Answer:__ There is no engine braking Freewheeling is a bad driving practice involving the driver either driving with the clutch depressed or the gear stick in neutral, or both together. Freewheeling disengages the wheels from the engine. ###### All drivers are required by law to switch on the vehicle's headlights whilst driving between 1. [x] 7:00 pm and 7:00 am 1. [ ] 7:15 pm and 7:15 am 1. [ ] 7:30 pm and 7:30 am \u003e __Correct Answer:__ 7:00 pm and 7:00 am ###### Switching on the headlights whilst driving in heavy rain during the daytime 1. [x] Is advisable because you can be readily seen by others 1. [ ] Is not advisable because it will shorten the battery life 1. [ ] Is not adbisable because the rain water will reflect the light \u003e __Correct Answer:__ Is advisable because you can be readily seen by others ###### As you approach a controlled junction the lights change to green. Elderly people are halfway across, you should 1. [ ] Rev your engine to make them hurry 1. [x] Wait because they will take longer to cross 1. [ ] Flash your light to hurry them \u003e __Correct Answer:__ Wait because they will take longer to cross ###### When approaching a green traffic light signal at a juction, you must 1. [ ] Speed up to drive through the junction 1. [x] Slow down and be prepared to stop 1. [ ] Maintain your speed \u003e __Correct Answer:__ Slow down and be prepared to stop ###### In this situation, the \"red car\" should pay special attention to ![BTT-Quiz](/img/sg/btt-quiz-1-3.png) 1. [x] The school chidren 1. [ ] The turning vehicle 1. [ ] The traffic light \u003e __Correct Answer:__ The school chidren ###### When following behind a large vehicle, you should 1. [ ] Move more to the right to have a better view 1. [x] Keep a longer following distance as you would with other vehicles 1. [ ] Follow at the same distance as you would with other vehicles \u003e __Correct Answer:__ Keep a longer following distance as you would with other vehicles Quiz 4 - 10 Questions --- primary_color: steelblue secondary_color: \"#f2f2f2\" text_color: black shuffle_questions: false shuffle_answers: false --- ###### The right most outer lane on a three-lane carriageway is for 1. [x] Emergency vehicles and overtaking only 1. [ ] Vehicles travelling at the maximum speed limit of the road 1. [ ] Vehicles which are capable of travelling at high speed \u003e __Correct Answer:__ Emergency vehicles and overtaking only ###### Generally, overheating of the engine is largely due to 1. [ ] Long distance driving 1. [x] Poor vehicle maintenance 1. [ ] Lack of petrol \u003e __Correct Answer:__ Poor vehicle maintenance ###### The engine oil level on the dip stick should be 1. [ ] Below the lower marking 1. [ ] Above the upper marking 1. [x] Between the lower and upper markings \u003e __Correct Answer:__ Between the lower and upper markings ###### When your vehicle starts to skid while braking, you should 1. [ ] Step harder on the brake pedal 1. [x] Release the brakes and apply intermittent braking 1. [ ] Neutralise the gear and apply the front brakes \u003e __Correct Answer:__ Release the brakes and apply intermittent braking ###### While driving along an expressway and your handphone (without hands-free kit) rings, you should 1. [ ] Answer the call immediately 1. [x] Carry on driving and find a safe spot out of the expressway before answering the call 1. [ ] Stop at the road shoulder and answer the call \u003e __Correct Answer:__ Carry on driving and find a safe spot out of the expressway before answering the call ###### Is it an offence to drive on the road shoulder of the expressway? 1. [x] Yes, it is an offence 1. [ ] No, It is not an offence 1. [ ] It is not an offence if the road is congested \u003e __Correct Answer:__ Yes, it is an offence ###### When passing a cyclist, what is the minimum side clearance you must ensure between the cyclist and your car? 1. [ ] 0.5m 1. [ ] 1m 1. [x] 1.5m \u003e __Correct Answer:__ 1.5m ###### When a cyclist in front of you is glancing to the rear, it means that 1. [x] He might change direction 1. [ ] He is going to stop 1. [ ] He is about to speed up \u003e __Correct Answer:__ He might change direction ###### A turn signal that keeps flashing after a turn is very likely to 1. [x] Confuse other drivers and cause them to turn into your path 1. [ ] Damage the electric system 1. [ ] Keep other drivers from seeing your break light \u003e __Correct Answer:__ Confuse other drivers and cause them to turn into your path ###### The rear view mirror inside the cabin should be adjusted so that 1. [ ] Only the right hand side of the rear windscreen can be seen in the mirror 1. [ ] Only the left hand side of the rear windscreen can be seen in the mirror 1. [x] A full view of the rear windscreen can be seen in the mirror \u003e __Correct Answer:__ A full view of the rear windscreen can be seen in the mirror ###### When approaching a sharp bend or a steep hill, you 1. [ ] may overtake other vehicles. 1. [x] should not overtake other vehicles. 1. [ ] should stay close behind the front vehicle. \u003e __Correct Answer:__ should not overtake other vehicles. ###### If you are in doubt of the depth of flood water (whether passable to vehicles), you should 1. [x] take an alternative route. 1. [ ] drive through slowly. 1. [ ] drive through as quickly as possible. \u003e __Correct Answer:__ take an alternative route. Consider buying the full set of 500 questions from below links:- Complete set of 500 Singapore Driving - Basic Theory Test (BTT) Questions and Answers at a very reasonable price.\n","permalink":"https://codingnconcepts.com/sg/driving-basic-theory-test-btt/","tags":null,"title":"Singapore Driving - Basic Theory Test (BTT)"},{"categories":["Agile"],"contents":"Comprehensive list of Free SAFe Scrum Master SSM 6.0 Exam Questions curated for cracking the exam in first attempt.\nDisclaimer: Scaled Agile Inc is a protected Brand. These exam questions are neither endorsed by nor affiliated with Scaled Agile. These are not the SAFe official exam questions/dumps. These questions are created from the web content of the Scaled Agile Framework and SAFe Scrum Master (SSM) 6.0 Workbook. These questions cover all the domains and topics of the SAFe SSM official exam and once you go through these questions and their concepts, you are more than ready to crack the exam in first attempt.\nFull Exam Questions Consider buying the full set of 270 questions from below links:-\nSAFe Scrum Master (SSM) 6.0 Exam Questions with Answers and Explanation at SAFe Scrum Master (SSM) 6.0 Exam Questions with Answers and Explanation at a very reasonable price.\nAll the Questions have duly verified answers supported with explanations and official weblinks of Scaled Agile Framework. All the Questions are unique and categorized by the topics of the official exam material. All the Questions marked with * are important from the exam perspective, give more attention! Refer to the SAFe scrum master exam notes to revise the topics. Please leave your feedback in the comment section or contact us from main menu.\n50 Sample Questions Topic 1: Agile Development Concepts What is the key differentiation of Agile development from Waterfall? ⬜ Architectural Runway and Incremental delivery of value\n⬜ Fast Feedback and Face-to-face interaction\n✅ Incremental delivery of value and Fast Feedback ⬜ Architectural Runway and Staggered Integration\nWhat is one example of an Agile Team development practice? ⬜ Writing requirements\n⬜ Tracking regulations\n⬜ Visualizing effort\n✅ Demoing frequently\nWhich statement is a value from the Agile Manifesto? ⬜ Customer collaboration over a constant indefinite pace ✅ Customer collaboration over contract negotiation ⬜ Customer collaboration over Feature negotiation ⬜ Customer collaboration over ongoing internal conversation\nTopic 2: Scrum Basics Which statement is true about Scrum? ⬜ It is a methodology for collecting requirements ✅ It is a Team-based Agile Framework ⬜ It is a Lean Portfolio management technique ⬜ It is a set of development practices\nWhich SAFe Core Value includes \u0026ldquo;use common terminology\u0026rdquo; and \u0026ldquo;understand your customer\u0026rdquo;? ✅ Alignment ⬜ Respect for People ⬜ Relentless Improvement\n⬜ Transparency\nWhich of the following correctly matches equivalent terms in SAFe and Scrum? ⬜ Product Increment (Scrum) = Planning Interval (SAFe) ✅ Sprint (Scrum) = Iteration (SAFe)\n⬜ Scrum Master (Scrum) = Release Train Engineer (SAFe)\n⬜ Product Owner (Scrum) = Epic Owner (SAFe)\nTopic 3: Agile Teams in a SAFe Enterprise What is one way to describe a cross-functional Agile Team? ⬜ They release customer products to production continuously ⬜ They deliver value every 6 weeks ✅ They are optimized for communication and delivery of value ⬜ They are made up of individuals, each of whom can define, develop, test, and deploy the system\nProduct Management has content authority over the Program Backlog. What do Product Owners have content authority over? ⬜ Value Streams\n⬜ Portfolio Backlog\n⬜ Portfolio Vision\n✅ Team Backlog\nAccording to Patrick Lencioni\u0026rsquo;s \u0026ldquo;Five Dysfunctions of a Team\u0026rdquo;, which issue most often leads to Team Dysfunctions? ✅ Absence of Trust ⬜ Fear of Conflict ⬜ Lack of Commitment ⬜ Avoidance of Accountability\nTopic 4: High-performing team characteristics Scrum master\u0026rsquo;s responsibility is to prevent which trait from a high-performing team? ⬜ Self Organizing\n⬜ Value Diversity\n⬜ Mutual Trust\n✅ Competition\nTeam A works collaboratively on new functionality for a customer application. The acceptance criteria have each been minimally met. Team A decides to release the functionality with a method for collecting direct customer feedback. Which of the following high-performing team characteristics is Team A demonstrating? ✅ Focusing on success over trying to avoid failures ⬜ Taking appropriate risks without fear of failure ⬜ Using regular Feedback Loops built into the learning cycle ⬜ Balancing abilities on the Team with the challenge of the work\nDuring which of the following stages of team development do team members stop focusing on their own goals and begin focusing on developing better ways of working together? ⬜ Performing ✅ Norming ⬜ Storming ⬜ Forming\nTopic 5: DevOps and Release on Demand What best describes the DevOps? ⬜ A high-performing DevOps Team\n⬜ Combine Deployment and Releases ⬜ Strong organizational structure\n✅ A culture, a mindset and a set of technical practices\nHow does the \u0026ldquo;C\u0026rdquo; in the CALMR approach to DevOps help teams manage tensions caused by differing needs? ⬜ By establishing communication between different teams\n✅ By creating a culture of shared responsibility ⬜ By committing to a balance of speed and quality ⬜ By identifying a collaborative approach to deployment\nWhy is it important to decouple deployment from release? ⬜ To remove the need to respond quickly to production issues ⬜ To allow inspection of Agile maturity based on different cycle times ⬜ To make deploying of assets a business decision ✅ To enable releasing functionality on demand to meet business needs\nTopic 6: Scrum Master / Team Coach characteristics The Scrum Master is what above all else? (Choose two.) ✅ A Servant Leader ✅ A Team Coach ⬜ A SAFe Agilist ⬜ A Lean-Agile Leader\nWhat is one trait of a servant leader? ⬜ Determines the day-to-day activities for the team ⬜ Solves problems on behalf of the team ⬜ Deflects information that could change the team\u0026rsquo;s work ✅ Persuades rather than using authority\nAccording to SAFe, which of the following key traits of effective Scrum Masters/Team Coaches is crucial for team members to accept coaching? ⬜ Optimism\n✅ Empathy\n⬜ Fairness ⬜ Agreeableness\nTopic 7: Scrum Master / Team Coach responsibilities What is one responsibility of a Scrum Master/Team Coach in an Agile Team? ⬜ Prioritizing the backlog ⬜ Testing the system ⬜ Demoing the system ✅ Improving flow\nThe Scrum Master facilitates which three events in SAFe? (Choose three.) ⬜ System Demo ✅ Retrospective ✅ Daily stand-up ⬜ Scrum of Scrums\n✅ Iteration Planning ⬜ LPM Backlog Refinement\nWhat is one way SAFe suggests Scrum Masters/Team Coaches support productive team Backlog Refinement? ⬜ Manage the meeting so that team members can pair with one another\n✅ Schedule the meeting on a predictable cadence\n⬜ Advocate for the meeting to be held twice per iteration\n⬜ Begin the meeting by reviewing Story estimates\nTopic 8: Iteration Basics What is a Team\u0026rsquo;s primary goal in an Iteration? ⬜ Sharing progress with the organization during the Iteration ⬜ Managing scope at the start of the Iteration\n⬜ Maintaining steady Team Syncs across the Iteration\n✅ Delivering working functionality at the end of the Iteration\nWhat is one potential root cause of Team Sync anti-patterns? ⬜ Overcommunication between team members ⬜ Occasional conflict within the team\n✅ Lack of collective ownership ⬜ Frequent verification and integration during the Iteration\nAccording to SAFe, which of the following types of work should fit into one Iteration for one team? ⬜ Epics ⬜ Tasks ⬜ Features ✅ Stories\nTopic 9: Iteration Planning What is the primary purpose of Iteration Planning in SAFe? ⬜ To assign tasks to individual team members ⬜ To define detailed design specifications for features ✅ To define and commit to a set of stories that can be completed in the iteration ⬜ To create the architectural runway for the iteration\nDuring which of the following Agile Team events do team members estimate relative story sizes? ⬜ Backlog Refinement ⬜ Iteration Review ✅ Iteration Planning ⬜ Iteration Retrospective\nWho commits to the Iteration goals at the end of Iteration Planning? ⬜ Product Owner\n⬜ Scrum Master\n✅ Agile Team ⬜ Business Owner\nTopic 10: Backlog Refinement What is the primary purpose of backlog refinement in SAFe? ⬜ To assign tasks to individual team members ✅ To define, discuss, estimate, and establish acceptance criteria for upcoming backlog items ⬜ To prioritize the backlog items based on business value\n⬜ To review the outcomes of the previous iteration\nWhat is the intended value of the Backlog Refinement Event? ⬜ The Team aligns on the progress of Iteration Goals\n⬜ The Team reviews and improves processes before the next Iteration\n⬜ The Team is able to commit to a set of goals to be delivered in the Iteration\n✅ The Team is able to prepare requirements for Iteration Planning\nWhich of the following statements is true about the team backlog? ⬜ The team backlog is where stories are reviewed\n⬜ The team backlog is where the team commits to work\n✅ The team backlog is where the team refines possible stories\n⬜ The team backlog is where the team finalizes work\nTopic 11: Iteration Review During Iteration Review, what is one activity performed by an Agile Team? ⬜ Identify opportunities for improvement ⬜ Gather Features requirements from stakeholders ✅ Demonstrate a working, tested team increment ⬜ Modify Iteration Goals\nThe team has struggled in this Iteration to meet their goals, so they want to cancel the Iteration Review. Which risk is introduced by cancelling the Iteration Review? ✅ The Team is unable to incorporate Feedback during the next Iteration Planning ⬜ The Team does not know when the prior Iteration ends and the next Iteration begins\n⬜ The Team can\u0026rsquo;t accept work as complete\n⬜ The Team can\u0026rsquo;t improve their internal processes\nWhat is one way to ensure a team is holding successful Iteration Reviews and demos? ✅ The team demos working functionality ⬜ The team swarms to prepare for demos ⬜ The team shares improving metrics ⬜ The team ensures they complete every Story\nTopic 12: Iteration Retrospective What is the primary purpose of the Iteration Retrospective in SAFe? ⬜ To demonstrate the working increment to stakeholders\n✅ To review the iteration\u0026rsquo;s results, discuss practices, and identify improvement opportunities\n⬜ To evaluate the team performance metrics ⬜ To assess individual team member performance\nAccording to SAFe, what is one output of a successful Iteration Retrospective? ⬜ Updated ART metrics ✅ Improvement Stories ⬜ Updated dependencies between stories ⬜ Iteration Goals\nAccording to SAFe, what is one Iteration Retrospective anti-pattern? ⬜ The Team only shares issues that are too small to result in real improvement ⬜ The Team only shares issues that are too big to be solved\n✅ The Team only brings up issues that are outside of team\u0026rsquo;s control to address ⬜ The Team only shares issues that can\u0026rsquo;t be measured\nTopic 13: Agile Release Train (ART) The Agile Release Train uses which type of teams to get work done? ⬜ Solution teams\n⬜ Phased-review-process teams\n⬜ Management teams\n✅ Cross-functional teams\nWhat are two of the Agile Release Train (ART) Sync meetings? (Choose two.) ✅ PO Sync ⬜ System Demo\n⬜ Solution Demo\n✅ Coach Sync\n⬜ Inspect and Adapt\nWhich role serves as a servant leader for the Agile Release Train? ⬜ Business Owner\n✅ Release Train Engineer ⬜ Agile Coach\n⬜ Scrum Master\nTopic 14: PI Planning What is the example of applying cadence and synchronization in SAFe? ⬜ Creating cross-functional ARTs and Agile teams ⬜ Allocating budgets to Value Streams ⬜ Using a Portfolio Kanban system ✅ Conducting a PI Planning event\nA successful PI planning event delivers which primary outputs? (Choose two.) ⬜ Roadmap and Vision ⬜ Business Context\n✅ Committed PI objectives ✅ ART planning board ⬜ Scope\nThe ART Planning board shows which two items? (Choose two.) ⬜ Epics\n⬜ Capacity and Load\n✅ Features ✅ Significant Dependencies ⬜ Risks\nOn day two of PI Planning, adjustments are made by the group based on the previous day\u0026rsquo;s management review and problem-solving meeting. What are three possible types of changes? (Choose three.) ✅ Adjustment to PI Objectives ⬜ User Stories ⬜ Planning requirements reset ✅ Movement of people ✅ Changes to scope\nA confidence vote is taken at the end of PI Planning after dependencies are resolved and risks are addressed. What best describes the process of the confidence vote? ⬜ Each person votes ⬜ The managers vote ✅ The teams and the ARTs vote ⬜ The business owners vote\nTopic 15: IP Iteration What is one benefit of having an IP Iteration every PI? ⬜ It creates a timeboxed opportunity for team growth ✅ It creates an estimating buffer for meeting PI objectives ⬜ It creates a guardrail for teams working too hard ⬜ It creates a chance for teams to manage quality\nTeam A has decided to use the IP Iteration to continue the finalizing Feature delivery work they have been working on for the past two Iterations. What is one effect Team might experience by continuing to stay heads-down rather than using the IP Iteration as intended? ✅ Individual Team members could lose an opportunity to consider their Team work more holistically ⬜ Individual Team members could lose an opportunity to refresh their motivation ⬜ Individual Team members could lose an opportunity to learn from one another ⬜ Individual Team members could lose an opportunity to keep their technical skills updated\nTeam A wants to use the IP Iteration to continue their \u0026lsquo;usual work.\u0026rsquo; What is one benefit the Scrum Master/Team Coach could share with the Team about using the IP Iteration as intended? ⬜ The team can consider additional retrospective action items ⬜ The team can perform needed system maintenance ✅ The team can participate in hackathons ⬜ The team can find time to participate in ad hoc groups\nTopic 16: Inspect and Adapt (I\u0026amp;A) What is the primary purpose of the Inspect and Adapt (I\u0026amp;A) event in SAFe? ⬜ To finalize all pending features before release ✅ To demonstrate the current state of the solution and identify improvement backlog items\n⬜ To conduct team-building activities ⬜ To reassign team roles and responsibilities\nWhat is one anti-pattern of the Inspect and Adapt? ⬜ Too many ideas enter the problem-solving workshop\n⬜ Not enough Team members attend the PI System Demo\n⬜ Only one problem is identified by each Team in the retrospective\n✅ No actionable improvement Features are created\nWhat is the first step of the problem-solving workshop in the Inspect and Adapt event? ⬜ Restate the new problem for the biggest root cause\n⬜ Identify the biggest root cause\n⬜ Perform a root-cause analysis\n✅ Agree on the problem(s) to solve\nConsider buying the full set of 270 questions from below links:- SAFe Scrum Master (SSM) 6.0 Exam Questions with Answers and Explanation at SAFe Scrum Master (SSM) 6.0 Exam Questions with Answers and Explanation at a very reasonable price.\n","permalink":"https://codingnconcepts.com/agile/safe-scrum-master-ssm-exam-questions/","tags":["SAFe","Certification"],"title":"SAFe Scrum Master SSM 6.0 Exam Questions"},{"categories":["Agile"],"contents":"If you are planning or preparing for SAFe Scrum Master SSM 6.0 (Scaled Agile Framework) certification then this article is for you to get started.\nOverview Prepare well for the exam. Understand all SAFe concepts and you can crack it like me! Requires 1 to 3 weeks of preparation depending upon your commitment per day. You need to solve 45 questions (multiple choice = 1 answer and multiple select = 2-3 answers) in 90 mins from your laptop without any supervision. It is an open-book online exam where you can search for the answers. Passing score is 33/45 (73%) means you should answer at least 33 (out of 45) questions correctly. No negative scoring so answer all the questions! You get the result (Pass or Fail) once you submit the exam. First attempt is included in the course registration fee if taken within 30 days of course completion. Each retake or attempt past the 30-day window is $50 You can download the SAFe Scrum Master SSM Student Workbook after the course registration from https://community.scaledagile.com/ Refer to the official Exam Details for more information. Refer to the official SAFe Website for exam material. Exam Questions • Read Leading SAFe Scrum Master SSM 6.0 Exam Questions for free. • Buy Leading SAFe Scrum Master (SSM) 6.0 Questions with Answers and Explaination at a very reasonable price.\n• Practice SAFe Scrum Master (SSM) 6.0 Exam Questions with Answers and Explanation at Exam Domains Domain 1 - Introducing Scrum in SAFe (22-28%) Basic Agile development concepts Basic Scrum concepts Agile Teams in a SAFe Enterprise High-performing team characteristics Team events overview DevOps and Release on Demand Domain 2 - Defining the Scrum Master / Team Coach role (26-30%) Scrum Master / Team Coach characteristics Scrum Master / Team Coach responsibilities Agile team coaching Domain 3 - Supporting Team Events (17-21%) Iteration planning Team sync Backlog refinement Iteration review Iteration retrospective Domain 4 - Supporting ART Events (25-29%) PI Planning IP Iteration Inspect and Adapt event Other Links to Refer to:-\nSAFe Big Picture Extended SAFe Guidance What’s New in SAFe 6.0 SAFe Scrum Master 6.0.1 Workbook Exam Notes Lesson 1 - Introducing Scrum in SAFe What are the SAFe Agile Frameworks? Agile Frameworks:- • SAFe\n• Scrum\n• Kanban\nPractices:- • Timeboxing\n• Frequent demos\n• Information radiators\n• Stories\n• Test-driven development • Retrospectives\n• Team Syncs\n• Behavior-driven development\n• Continuous Integration • Pair/mob programming\n• DevOps\nWhat is the Agile Manifesto? Agile Manifesto uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value:\nIndividuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan\nThat is, while there is a value in the items on the right, we value the items on the left more Reference: https://scaledagileframework.com/lean-agile-mindset/\nReference: https://agilemanifesto.org/\nWhat are the 12 Agile Manifesto Principles? Our highest priority is to satisfy the customer through early and continuous delivery of valuable software. Welcome changing requirements, even late in development. Agile processes harness change for the customer’s competitive advantage. Deliver working software frequently, from a couple of weeks to a couple of months, with a preference for the shorter timescale. Business people and developers must work together daily throughout the project. Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done. The most efficient and effective method of conveying information to and within a development team is face-to-face conversation. Working software is the primary measure of progress. Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely. Continuous attention to technical excellence and good design enhances agility. Simplicity – the art of maximizing the amount of work not done – is essential. The best architectures, requirements, and designs emerge from self-organizing teams. At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly. Reference: https://scaledagileframework.com/lean-agile-mindset/\nReference: https://agilemanifesto.org/principles.html\nWhat are the 10 SAFe Lean-Agile Principles? Take an economic view Deliver Early and Often Apply a Comprehensive Economic Framework:- -Operate Within Lean Budgets and Guardrails\n-Understand Solution Economic Trade-Offs: Development expense, Lead time, Product cost, Value, and Risk\n-Leverage Suppliers\n-Sequencing Jobs for Maximum Benefit: Weighted Shortest Job First (MSJF) Apply systems thinking The Solution Is a System\n-Optimizing a component does not optimize the whole system -For the system to behave well, teams must understand the intended behavior and architecture\n-The value of a system passes through its interconnections -A system can evolve no faster than its slowest integration point The Enterprise Building the System Is a System, Too Understand and Optimize the Full Development Value Stream Only Management Can Change the System Assume variability; preserve options\n-Flexible requirements and design, the Cone of uncertainty, set-based over point-based approach Build incrementally with fast, integrated learning cycles\n-PDCA = Plan – Do – Check – Adjust, The shorted the cycles, the faster the learning\n-Integration points control product development and reduce risk Base milestones on objective evaluation of working systems\n-Phase-gate milestones force design decisions too early, false-positive feasibility, they assume a point Solution exists, huge batches and long queues, centralized requirements and design.\n-Use Objective milestones instead, PI System Demos, continuous, cost-effective adjustments towards an optimum Solution) Make value flow without interruptions\n-Reduce batch size for higher predictability. Total cost = Holding cost + Transaction cost. Reducing transaction costs increases predictability, accelerates feedback, reduces rework, and lowers cost.\n-Little’s Law: Wq = Lq / Lambda, Average wait time = Average queue length / Average processing rate Apply cadence, synchronize with cross-domain planning Cadence – converts unpredictable events into predictable occurrences and lowers cost, makes waiting times for new work predictable, supports regular planning and cross-functional coordination, limits batch sizes to a single interval, controls injection of new work, provides scheduled integration points;\nSynchronization – causes multiple events to happen simultaneously, facilitates cross-functional trade-offs, provides routine dependency management, supports full system integration and assessment, provides multiple feedback perspectives Unlock the intrinsic motivation of knowledge workers -Workers are most qualified to make decisions about how to perform their work\n-The workers must be heard and respected for management to lead effectively\n-Knowledge workers must manage themselves. They need autonomy -Continuing innovation must be part of the work, the tasks, and the responsibilities of knowledge workers. -Unlocking intrinsic motivation with autonomy, mastery, and purpose Decentralize decision-making Centralize – Infrequent, Long-lasting, Significant economies of scale\nDecentralize – Frequent, Time critical, Requires local information Organize around value -Value doesn’t follow silos\n-Organize around Development Value Streams. Reference: https://scaledagileframework.com/safe-lean-agile-principles/\nWhat are the four SAFe core values? SAFe has the following four core values:-\nAlignment Transparency Respect for people Relentless improvement Reference: https://scaledagileframework.com/safe-core-values/\nWhat are the Scrum pillars and values? The three pillars of Scrum are transparency, inspection, and adaptation The five values of the Scrum are Courage, Commitment, Focus, Respect, and Openness What are the SAFe vs Scrum Terminologies? SAFe Scrum Iteration Sprint Iteration Planning Sprint Planning Iteration Review Sprint Review Iteration Retrospective Sprint Retrospective Iteration Goals Sprint Goals Iteration Backlog Sprint Backlog Team Sync Daily Scrum Agile Team Scrum Team Team Increment Increment Program Increment (PI) Typically 5 Sprints Program Increment (PI) Planning Planning for typically 5 Sprints Cadence and Synchronization Velocity Agile Release Train (ART) - Teams of Agile teams Teams of Scrum teams Release Train Engineer Chief scrum Master ART Sync Scrum of Scrum What is an Iteration? Definition: Iterations are a single development cycle where each Agile Team defines, builds, integrates, and tests the Stories from their Iteration Backlog Duration: Each Iteration is the same length, running back-to-back.SAFe advises two-week Iterations Goal: To deliver new value to the Customer at the end of each Iteration Avoid adding scope once the Iteration has begun What is an Agile team? An Agile Team is a cross-functional group of typically ten or fewer individuals with all the skills necessary to define, build, test, and deploy increments of value to their customers. Agile teams are optimized for communication and the continuous delivery of value to the customer. Agile Teams visualize flow with SAFe Scrum or SAFe Kanban Agile Team Events are:- Team Sync, Backlog Refinement, Iteration Review, Iteration Retro, Iteration Planning The Agile Team\u0026rsquo;s responsibilities are:-\nConnecting with the Customer (led by PO) Planning the Work: ART Planning (PI Planning), Team planning using SAFe Scrum or SAFe Team Kanban, and refining the Team Backlog. Delivering Value: Frequently integrate and test, Sync with other teams in ART through ART Sync (includes Coach Sync and PO Sync), build continuous delivery pipeline, release frequently. Getting Feedback: with the help of PO and through System Demos Improving relentlessly: participate in ART\u0026rsquo;s joint Inspect \u0026amp; Adapt, address the problems as they occur Reference: https://scaledagileframework.com/agile-teams/\nWhat are the two specialty roles in Agile Teams? The Agile Team contains two specialty roles:- Product Owner (PO) and Scrum Master/Team Coach (SM/TC).\nProduct Owner (PO) responsibilities are:- Connect with the customer Contribute to the Vision and Roadmap Manage and prioritize the Team Backlog Support the team in delivering value Get and apply fast feedback Scrum Master/Team Coach (SM/TC) responsibilities are:- Facilitate SAFe Scrum (or SAFe Kanban) and PI planning Supports Iteration Execution Improves Flow Build a high-performing team Optimizes and improves the team and ART performance Reference: https://scaledagileframework.com/agile-teams/\nWhat is Agile Release Train (ART)? ART is a team of cross-functional Agile Teams and has the capabilities to define, build, validate, and release to deliver a continuous flow of value. ART is a virtual organization of 5-12 teams (50-125+ individuals) All the teams in ART are synchronized on a common cadence - a Program Increment (PI), aligned to a common mission via a single Program Backlog (ART Backlog). Critical Roles in the ART are:-\nRelease Train Engineer (RTE) is a servant leader (chief scrum master) who facilitates ART execution, impediment removal, risk and dependency management, and continuous improvement. Product Management is largely responsible for ‘what gets built,’ as defined by the Vision, Roadmap, and new Features in the ART Backlog. They work with customers, teams, and Product Owners to understand and communicate their needs and participate in solution validation. System Architect is an individual or team that defines the system’s overall architecture. They work at a level of abstraction above the teams and components and typically define Non-functional Requirements (NFRs), major system elements, subsystems, and interfaces. Business Owners are key stakeholders of the ART, with final responsibility for the business outcomes of the train. Customers are the ultimate economic buyers or value users of the solution. Other essential roles in the ART are:-\nSystem Teams typically assist in building and maintaining development, continuous integration, and test environments. Shared Services are specialists necessary for the success of an ART but cannot be dedicated to a specific train. They often include data security, information architects, site reliability engineering (SRE), database administrators (DBAs), and many more. Three Sync events to keep ART on track:-\nCoach Sync: (timeboxed: 1 hour) focuses on executing the current PI, including risk, dependencies, progress, and impediments PO Sync: manages the PI’s scope, reviews progress, adjusts priorities, and prepares for the following PI ART Sync: usually replaces the Coach Sync and PO Sync for a particular iteration to reduce overhead. The ROAM board created during PI planning can be reviewed during the ART Sync. ART Planning board is used during ART sync to track and manage dependencies, ensuring they do not block other teams. Other events in ART:-\nPI Planning (timeboxed: 2 days) Each ART begins with the PI Planning using the PI planning board, the outcome is PI Objectives to be completed in the PI iteration. System Demos (timeboxed: 1 hour) Occur at the end of Iteration to review deliverables and receive feedback from stakeholders, business owners, and customers. Inspect \u0026amp; Adapt (timeboxed: 1/2 day) Each PI concludes with I\u0026amp;A event for retrospection where ART reviews and improves its process before the next PI. Reference: https://scaledagileframework.com/agile-release-train/ Reference: https://scaledagileframework.com/planning-interval/\nLesson 2 - Characterizing the Role of the Scrum Master What are the characteristics of an effective Scrum Master? A Scrum Master acts as a Servant Leader. An effective scrum master should have the following characteristics:-\nEmpathetic: Understand and empathize with others. Conflict navigator: Listen to and support team members in problem identification and decision-making Servant Leader: Persuade rather than use authority. Seeks to help without diminishing the commitment of others. Mentor: Encourage and support the personal development of each individual Transparent: Is open and appreciates openness in others Coach: Thinks beyond day-to-day activities Reference: https://scaledagileframework.com/scrum-master-team-coach/\nWhat are the responsibilities of the Scrum Master in the SAFe enterprise? Works with the RTE to ensure the train meets its overall PI Objectives Coordinates with other Scrum Masters, the System Team, and Shared Services during PI Planning Works with the teams throughout each Iteration and PI Participates in the Coach Sync Fosters normalized estimating within the team Helps teams operate under architectural and portfolio governance, system integration, and System Demos Reference: https://scaledagileframework.com/scrum-master-team-coach/\nWhat are the common attributes of the high-performing team? The Scrum master\u0026rsquo;s responsibility is to build a high-performing team with the following common attributes:-\nSelf-organizing Effective decision-making Open and clear communication Value diversity Mutual trust Healthy conflict Clear goals and purpose Concentration and focus Ownership and accountability Understand work’s impact on an organization Aligned and collaborative Safe atmosphere to take risks Effective, timely feedback Sufficient resources for local control Success focus over failure avoidance Abilities balanced with challenge Engagement Have fun with work and each other There are 4 stages of building a high-performing team:-\nForming Storming Norming Performing Reference: https://scaledagileframework.com/agile-teams/\nWhat are the Team Events? Event Timebox Value Backlog Refinement 1 hour Team prepares requirements for Iteration Planning Iteration Planning 2 to 4 hours Team commits to a set of goals to be delivered in theIteration Team sync 15 minutes Teammembers sync regarding the progress of theIteration Goals Iteration Review 1 hour Team meets with stakeholders to review thedeliverables and provide feedback Iteration Retrospective 1 to 1.5 hours Teamreviews and improves its process before thenext Iteration Reference: https://scaledagileframework.com/essential-safe/\nLesson 3 - Experiencing PI Planning What is PI Planning and its events? PI Planning stands for Program Increment Planning. PI Planning is a cadence-based event that serves as the heartbeat of the ART, aligning all teams on the ART to a shared mission and Vision. PI Planning sessions are regularly scheduled events held throughout the year where multiple teams within the same Agile Release Train (ART) meet to align to a shared vision, discuss features, plan the roadmap, and identify cross-team dependencies. PI Planning is a 2 full day event that typically runs every 8-12 weeks (10 weeks typical). The two-day agenda is as follows:- Day 1 08:00 - 09:00 Business Context 09:00 - 10:30 Product/Solution Vision 10:30 - 11:30 Architecture Vision and Development Practicies 11:30 - 01:00 Planning Context and Lunch 01:00 - 04:00 Team breakouts 04:00 - 05:00 Draft Plan Review 05:00 - 06:00 Management review and problem solving Day 2 08:00 - 09:00 Planning Adjustment 09:00 - 11:00 Team breakouts 11:00 - 01:00 Final Plan Review and Lunch 01:00 - 02:00 ART Risks 02:00 - 02:15 Confidence Vote 02:15 - ?? Plan Rework (if needed) When ready Planning Retrospective and moving forward Primary Inputs to the PI Planning include: 1. Business context, 2. Roadmap \u0026amp; vision, and 3. Highest priority Features (typically top 10) of the ART backlog Primary Outputs of the PI Planning include: 1. Committed PI objectives, and 2. ART planning board Product Management provides the vision and backlog (typically represented by the top ten or so upcoming features) and owns the feature priorities Business Owner provides the business context and assigns business value (BV) to each PI Objective on a scale from 1 to 10 Development Teams own Story planning and high-level estimates ART Planning Board is used for PI Planning showing: 1. Features, 2. Significant Dependency, and 3. Milestone or Event Architect and UX work as intermediaries for governance, interfaces,and dependencies Reference: https://scaledagileframework.com/pi-planning/\nWhat are PI Uncommitted Objectives? Uncommitted objectives are used to identify work that can be variable within the scope of a PI. The work is planned, but the outcome is simply not certain. Teams can apply uncommitted objectives whenever there is low confidence in meeting the objective. This can be due to many circumstances:\nDependencies with another team or supplier that cannot be guaranteed. The team has little to no experience with functionality of this type. In this case the teams may plan ‘Spikes’ early in the PI to reduce uncertainty. There are a large number of fairly critical objectives that the business is depending on and the team is already loaded close to full capacity. Reference: https://scaledagileframework.com/pi-planning/\nWhat is a Feature? Features are maintained in the ART Backlog Feature are sized to fit in a Program Increment (PI) and delivered by a single Agile Release Train (ART) Features are split into Stories and fit in one Iteration for one team Features include a definition of Minimum Marketable Feature (MMF), a benefit hypothesis (to justify development cost) and Acceptance criteria (defined during program backlog refinement). Features are prioritized using WSJF and the top 10 features are presented to the team during PI planning Typically Product Management creates business features and System Architect creates enabler features Reference: https://scaledagileframework.com/features-and-capabilities/\nWhat is a Story? Features are implemented by Stories Stories are small increments of value that can be developed in days and are relatively easy to estimate Features fits in one PI for one ART; Stories fits in one iteration for one team. A Story Point is a relative number that represents: Volume, Complexity, Knowledge, and Uncertaity. Team breaks down Features into User Stories and Enabler Stories Reference: https://scaledagileframework.com/story/\nHow to write a good Story? User Stories are short descriptions of a small piece of desired functionalitywritten in the user’s language The recommended form of expression is the user-voice form, as follows: As a (user role), I want to (activity), so that (business value). As a driver, I want to get a receipt after fueling so that I can expense the purchase. Use personas to better understand users. Personas are fictional characters. A good story has INVEST attributes:-\nI = Independent (can be developed on their own)\nN = Negotiable (fexible scope)\nV = Valuable (useful to Customer) E = Estimable (can be estimated)\nS = Small (can fit in iteration) T = Testable (can be tested) A good story is written using 3Cs:- Cards = Written on a card or in a digital tool and can be annotated with notes Conversation = The details are in a conversation with the PO Confirmation = Acceptance criteria confirms the Story correctness Reference: https://scaledagileframework.com/story/\nWhat is an Acceptance criteria? Provide the details of the Story from a testing point of view Are created by the Agile Team Can be written in the Given-When-Then format As a driver, I want to get a receipt after fueling so that I can expense the purchase. Acceptance criteria:- Given that the fueling is over When the driver asks for the receipt Then it is printed and includes: amount fueled, the amount paid, tax, date, time Reference: https://scaledagileframework.com/story/\nWhat are Enabler Stories? Enabler Stories build the groundwork for future User Stories. There are four types of Enabler Stories:-\nInfrastructure: Build development and testing frameworks that enable a faster and more efficient development process Architecture: Build the Architectural Runway, which enables smoother and faster development Exploration: Build an understanding of what is needed by the Customer to understand prospective Solutions and evaluate alternatives Compliance: Facilitate specific activities such as verification and validation, documentation, signoffs, regulatory submissions, and approvals Reference: https://scaledagileframework.com/story/\nWhat is a Story Point? Fibonacci is used for Story Point estimation A Story point is a singular number that represents:-\n– Volume: How much is there?\n– Complexity: How hard is it?\n– Knowledge: What do we know?\n– Uncertainty: What’s not known? Story points are relative. They are not connected to any specific unit of measure. – An 8-point Story should take four times longer than a 2-point Story to complete\n– Typically, a 1-point Story would take one day to developand test Estimating Poker can be used for fast and relative estimation Reference: https://scaledagileframework.com/story/\nLesson 4 - Facilitating Iteration Execution What is Iteration Planning? Iteration planning is the first event of the Iteration. Timeboxed to 4 hours or less. Typically 90 minutes for a two-week iteration Inputs to the Iteration Planning include:- – A refined Team Backlog – The Team and ART PI Objectives – Feedback from System Demos and prior iterations A successful iteration planning event delivers the following outputs:- – Stories planned for the upcoming iteration, including Enablers. Each has defined acceptance criteria and an estimate and is recorded in the iteration backlog. – Committed iteration goals. – Dependencies with other teams are understood and planned. Reference: https://scaledagileframework.com/iteration-planning/\nWhat is the Scrum Master\u0026rsquo;s role in Iteration Planning? Maintain the timebox Ensure that the team commits to the Iteration Goals Verify that the PO or other stakeholders don’t influence the team to overcommit Challenge the team to exceed their previous accomplishments Ensure that the improvement items from the retrospective are put into action Ensure time is allocated for technical debt activities Reference: https://scaledagileframework.com/iteration-planning/\nWhat is the Team Sync? The team sync is a short meeting (usually 15 minutes or less), typically held about daily, to inspect progress toward the iteration goal, communicate, and adjust upcoming planned work.\nThe Basic Team Sync agenda where Each person answers:-\nWhat did I do yesterday to advance the Iteration Goals? What will I do today to advance the Iteration Goals? Are there any impediments that will prevent the team from meeting the Iteration Goals? The meet-after Agenda:-\nReview topics captured on the meet-after board Involved parties discuss, and uninvolved people may leave Reference: https://scaledagileframework.com/safe-scrum/\nWhat is the Scrum Master\u0026rsquo;s role in tracking Iteration Progress? Facilitate mid-PI re-planning Encourage the team to point out as early as possible if they think they will miss Iteration Goals or Pl Objectives. Communicate to and from the Coach Sync Encourage the use of engineering practices Make sure defects are not pushed to the IP Iteration Facilitate preparation for the next PI Support release activities Reference: https://scaledagileframework.com/safe-scrum/\nWhat is Backlog Refinement Event? Timebox: 1 –2 hours per Iteration Purpose: Provides time to identify dependencies and issues that could impact the next Iteration. Ensures that there is a ready backlog for Iteration Planning. Attendees: – Agile Team members are in attendance and actively engaged; subjectmatter experts (SMEs) and other teams’ members are invited as needed. – Scrum Master or Product Owner facilitates. Reference: https://scaledagileframework.com/safe-scrum/\nWhat is the Scrum Master\u0026rsquo;s role in Backlog Refinement Event? Maintain the timebox Maintain the right level of a deep backlog vs a full set of specified Stories for two Iterations Make sure all the team members participate Invite the right subject matter experts Hold the event at regular intervals Reference: https://scaledagileframework.com/safe-scrum/\nWhat is DevOps? DevOps is a mindset, culture, and set of technical practices that supports the integration, automation, and collaboration needed to effectively develop and operate a solution. It is a combination of Dev (Development) and Ops (Operations). DevOps enable the Continuous Delivery Pipeline (CDP) to release on demand and deliver value whenever there is a business need Reference: https://scaledagileframework.com/devops/\nWhat is the CALMR approach to DevOps? CALMR is a DevOps mindset that guides the ART toward achieving continuous value delivery by enhancing culture, automation, lean flow, measurement, and recovery.\nCulture - Establish a culture of shared responsibility for development, deployment, and operations. Automation - Automate the Continuous Delivery Pipeline. Lean flow - Keep batch sizes small, limit WIP, and provide extreme visibility. Measurement - Measure the flow through the pipeline. Implement full-stack telemetry. Recovery - Architect and enable low-risk releases. Establish fast recovery, fast reversion, and fast fix-forward. Reference: https://scaledagileframework.com/calmr/\nLesson 5 - Finishing the PI What is Innovation and Planning (IP) Iteration? The Innovation and Planning (IP) Iteration is a unique iteration that occurs every PI, which provides dedicated time for Innvoation and Planning where:-\n– Innovation includes opportunities for innovation, hackathons, infrastructure improvements, continuing education, certifications, etc.\n– Planning includes PI Planning Readiness, Inspect and Adapt (I\u0026amp;A), and PI Planning events, etc. It provides an estimating buffer for meeting PI Objectives and sufficient capacity margin to enable cadence Without the IP Iteration\n– Lack of delivery capacity buffer impacts predictability\n– Little innovation; the tyranny of the urgent\n– Technical debt grows uncontrollably\n– People burn out\n– No time for teams to plan, demo, or improve together Reference: https://scaledagileframework.com/innovation-and-planning-iteration/\nWhat is Inspect and Adapt (I\u0026amp;A) Event? The Inspect and Adapt (I\u0026amp;A) is a significant event (Timebox: 3-4 hours) held at the end of each PI, where All ART stakeholders and Agile Team participate. The I\u0026amp;A event consists of three parts:- – PI System Demo (Timebox: 45-40 mins) - team demonstrates the current state of the solution\n– Quantitative and qualitative measurement - Team PI performance report is created which includes team\u0026rsquo;s planned vs actual business value. Individual team totals are rolled up into the ART predictability report.\n– Retrospective and problem-solving workshop Reference: https://scaledagileframework.com/inspect-and-adapt/\nWhat is Scrum Master Role in I\u0026amp;A Event? Facilitate the team’s preparation for the PI System Demo Provide team data Facilitate one of the teams in the problem-solving workshop Help the RTE make sure improvement items are included during the PI If using ad hoc teams for the I\u0026amp;A, then ScrumMasters may be participants rather than facilitators Reference: https://scaledagileframework.com/inspect-and-adapt/\nOther SAFe Certification Exam Notes Read Leading SAFe Agilist 6.0 (Scaled Agile) Exam Notes Read SAFe Product Owner/Producer Manager POPM 6.0 Exam Notes\n• Read Leading SAFe Scrum Master SSM 6.0 Exam Questions for free. • Buy Leading SAFe Scrum Master (SSM) 6.0 Questions with Answers and Explaination at a very reasonable price.\n• Practice SAFe Scrum Master (SSM) 6.0 Exam Questions with Answers and Explanation at ","permalink":"https://codingnconcepts.com/agile/safe-scrum-master-ssm-exam-notes/","tags":["SAFe","Certification"],"title":"SAFe Scrum Master SSM 6.0 Exam Notes"},{"categories":["Java"],"contents":"In this quick tutorial, we\u0026rsquo;ll learn how to calculate the age in human-readable format in Java.\nRequirement We want to create a utility method in Java that calculates the age from birth date as of today in human-readable format i.e. in Years, Months, and Days.\nThese are few examples of expected age as of date 2024-01-01 from the given birth date:-\nBirth Date As of date Age 2000-01-01 2024-01-01 24 Years 1990-05-01 2024-01-01 33 Years and 8 Months 1980-08-22 2024-01-01 43 Years, 4 Months and 10 Days Let\u0026rsquo;s build a utility method to achieve this.\nDate Utility Method Let\u0026rsquo;s create a LocalDate Java utility method DateUtils.calculateAge(birthDate) to calculate age from the birth date as of today in human-readable format i.e. in Years, Months, and Days.\npackage com.example.util; import java.time.LocalDate; import java.time.Period; public class DateUtils { public static String calculateAge(LocalDate birthDate) { return getDiffInHumanReadableFormat(birthDate, LocalDate.now()); } public static String getDiffInHumanReadableFormat(LocalDate startDate, LocalDate endDate) { Period period = Period.between(startDate, endDate); StringBuilder token = new StringBuilder(); appendUnits(token, period.getYears(), \u0026#34;Year\u0026#34;); yearSeparator(period, token); appendUnits(token, period.getMonths(), \u0026#34;Month\u0026#34;); monthSeparator(period, token); appendUnits(token, period.getDays(), \u0026#34;Day\u0026#34;); return token.toString().trim(); } private static void appendUnits(StringBuilder token, int units, String unitName) { if (units \u0026gt; 0) { token.append(units).append(\u0026#34; \u0026#34;).append(unitName); if (units \u0026gt; 1) token.append(\u0026#34;s\u0026#34;); } } private static void yearSeparator(Period period, StringBuilder token) { if (period.getYears() \u0026gt; 0 \u0026amp;\u0026amp; period.getMonths() \u0026gt; 0) { token.append(period.getDays() \u0026gt; 0 ? \u0026#34;, \u0026#34; : \u0026#34; and \u0026#34;); } } private static void monthSeparator(Period period, StringBuilder token) { boolean appendAnd = (period.getMonths() \u0026gt; 0 \u0026amp;\u0026amp; period.getDays() \u0026gt; 0) || (period.getMonths() == 0 \u0026amp;\u0026amp; period.getYears() \u0026gt; 0 \u0026amp;\u0026amp; period.getDays() \u0026gt; 0); if (appendAnd) token.append(\u0026#34; and \u0026#34;); } } Let\u0026rsquo;s call the utility method to calculate Age:-\nLocalDate birthDate = LocalDate.of(1980, 8, 22); System.out.println(\u0026#34;BirthDate: \u0026#34; + birthDate); System.out.println(\u0026#34;Age: \u0026#34; + calculateAge(birthDate)); //BirthDate: 1980-08-22 //Age: 43 Years, 4 Months and 13 Days Please note that the Age calculation will be different depending on today\u0026rsquo;s date.\nThat\u0026rsquo;s it! You can copy the code and use it. Thanks for reading!\n","permalink":"https://codingnconcepts.com/java/age-calculator-java/","tags":["Java Date"],"title":"Calculate Age from Birth Date in Java"},{"categories":["Java"],"contents":"In this quick tutorial, we\u0026rsquo;ll learn how to print the difference between two dates in human-readable format in Java.\nRequirement We want to create a utility method in Java that returns the difference between two Dates in human-readable format i.e. in Years, Months, and Days.\nThese are few examples of expected difference between two dates:-\nStart Date End Date Expected Difference 2000-01-01 2001-01-01 1 Year 2000-01-01 2020-01-01 20 Years 2000-01-01 2000-02-01 1 Month 2000-01-01 2000-11-01 10 Months 2000-01-01 2000-01-02 1 Day 2000-01-01 2000-01-26 25 Days 2000-01-01 2001-02-02 1 Year, 1 Month and 1 Day 2000-01-01 2005-05-04 5 Years, 4 Months and 3 Days 2000-01-01 2001-01-26 1 Year and 25 Days 2000-01-01 2000-05-04 4 Months and 3 Days Let\u0026rsquo;s build a utility method to achieve this.\nDate Utility Method Let\u0026rsquo;s create a Java utility method DateUtils.getDiffInHumanReadableFormat(startDate, endDate) to find the difference between two LocalDate in human-readable format i.e. in Years, Months, and Days.\npackage com.example.util; import java.time.LocalDate; import java.time.Period; public class DateUtils { public static String getDiffInHumanReadableFormat(LocalDate startDate, LocalDate endDate) { Period period = Period.between(startDate, endDate); StringBuilder token = new StringBuilder(); appendUnits(token, period.getYears(), \u0026#34;Year\u0026#34;); yearSeparator(period, token); appendUnits(token, period.getMonths(), \u0026#34;Month\u0026#34;); monthSeparator(period, token); appendUnits(token, period.getDays(), \u0026#34;Day\u0026#34;); return token.toString().trim(); } // Helper Methods private static void appendUnits(StringBuilder token, int units, String unitName) { if (units \u0026gt; 0) { token.append(units).append(\u0026#34; \u0026#34;).append(unitName); if (units \u0026gt; 1) token.append(\u0026#34;s\u0026#34;); } } private static void yearSeparator(Period period, StringBuilder token) { if (period.getYears() \u0026gt; 0 \u0026amp;\u0026amp; period.getMonths() \u0026gt; 0) { token.append(period.getDays() \u0026gt; 0 ? \u0026#34;, \u0026#34; : \u0026#34; and \u0026#34;); } } private static void monthSeparator(Period period, StringBuilder token) { boolean appendAnd = (period.getMonths() \u0026gt; 0 \u0026amp;\u0026amp; period.getDays() \u0026gt; 0) || (period.getMonths() == 0 \u0026amp;\u0026amp; period.getYears() \u0026gt; 0 \u0026amp;\u0026amp; period.getDays() \u0026gt; 0); if (appendAnd) token.append(\u0026#34; and \u0026#34;); } } Let\u0026rsquo;s write a test case to validate our method:-\npackage com.example.util; import org.junit.jupiter.api.Test; import java.time.LocalDate; import static org.junit.jupiter.api.Assertions.assertEquals; public class DateUtilsTest { @Test void testGetHumanReadableDiff() { assertEquals(\u0026#34;1 Year\u0026#34;, DateUtils.getDiffInHumanReadableFormat(LocalDate.of(2000, 1, 1), LocalDate.of(2001, 1, 1))); assertEquals(\u0026#34;20 Years\u0026#34;, DateUtils.getDiffInHumanReadableFormat(LocalDate.of(2000, 1, 1), LocalDate.of(2020, 1, 1))); assertEquals(\u0026#34;1 Month\u0026#34;, DateUtils.getDiffInHumanReadableFormat(LocalDate.of(2000, 1, 1), LocalDate.of(2000, 2, 1))); assertEquals(\u0026#34;10 Months\u0026#34;, DateUtils.getDiffInHumanReadableFormat(LocalDate.of(2000, 1, 1), LocalDate.of(2000, 11, 1))); assertEquals(\u0026#34;1 Day\u0026#34;, DateUtils.getDiffInHumanReadableFormat(LocalDate.of(2000, 1, 1), LocalDate.of(2000, 1, 2))); assertEquals(\u0026#34;25 Days\u0026#34;, DateUtils.getDiffInHumanReadableFormat(LocalDate.of(2000, 1, 1), LocalDate.of(2000, 1, 26))); assertEquals(\u0026#34;1 Year, 1 Month and 1 Day\u0026#34;, DateUtils.getDiffInHumanReadableFormat(LocalDate.of(2000, 1, 1), LocalDate.of(2001, 2, 2))); assertEquals(\u0026#34;5 Years, 4 Months and 3 Days\u0026#34;, DateUtils.getDiffInHumanReadableFormat(LocalDate.of(2000, 1, 1), LocalDate.of(2005, 5, 4))); assertEquals(\u0026#34;5 Years and 4 Months\u0026#34;, DateUtils.getDiffInHumanReadableFormat(LocalDate.of(2000, 1, 1), LocalDate.of(2005, 5, 1))); assertEquals(\u0026#34;1 Year and 25 Days\u0026#34;, DateUtils.getDiffInHumanReadableFormat(LocalDate.of(2000, 1, 1), LocalDate.of(2001, 1, 26))); assertEquals(\u0026#34;4 Months and 3 Days\u0026#34;, DateUtils.getDiffInHumanReadableFormat(LocalDate.of(2000, 1, 1), LocalDate.of(2000, 5, 4))); } } That\u0026rsquo;s it! You can copy the code and use it. Thanks for reading!\n","permalink":"https://codingnconcepts.com/java/date-difference-human-readable/","tags":["Java Date"],"title":"Date Difference in Human Readable format in Java"},{"categories":["Agile"],"contents":"Comprehensive list of Free 200+ Leading SAFe Agilist SA 6.0 Exam Questions curated for cracking the exam in first attempt.\nDisclaimer: Scaled Agile Inc is a protected Brand. These exam questions are neither endorsed by nor affiliated with Scaled Agile. These are not the SAFe official exam questions/dumps. These questions are created from the web content of the Scaled Agile Framework and Leading SAFe Agilist (SA) 6.0 Workbook. These questions cover all the domains and topics of the SAFe SA official exam and once you go through these questions and their concepts, you are more than ready to crack the exam in first attempt.\nFull Exam Questions Consider buying the full set of questions with answers and explanation from below links:-\nLeading SAFe Agilist 6.0 Practice Exams with Answers and Explaination at Leading SAFe Agilist 6.0 Questions with Answers and Explaination at a very reasonable price.\nAll the Questions have duly verified answers supported with explanations and official weblinks of Scaled Agile Framework. All the Questions are unique and categorized by the topics of the official exam material. All the Questions marked with * are important from the exam perspective, give more attention! Refer to the SAFe Agilist Exam Notes to revise the topics. Please leave your feedback in the comment section or contact us from main menu.\nArea 1: Business Agility We are currently in which technological revolution? ⬜ Industrial Revolution ⬜ Age of Oil and Mass Production\n⬜ Age of Software and Digital ⬜ Age of Steal and Heavy Engineering\nWhere we are in the age of software and digital? ⬜ Installation Period ⬜ Turning Point ⬜ Deployment Period ⬜ Exploration Period\nWhat is the primary goal of SAFe? ⬜ Organizing around value ⬜ Lean Portfolio Management ⬜ Business Agility\n⬜ Customer centricity\nThe primary need for SAFe is to scale the idea of what? ⬜ Technical Solution Delivery\n⬜ Organizational and Functional Alignment\n⬜ Lean Portfolio Management\n⬜ Business Agility\nWhat is the ultimate goal of SAFe? ⬜ Faster Delivery\n⬜ Servant Leadership\n⬜ Delivering Continuous Value ⬜ Functional Teams\nWhat is Business Agility? ⬜ Applying Lean-Agile principles and practices to the specification, development, deployment, operation, and evolution of the world’s largest and most sophisticated systems.\n⬜ How Lean-thinking people and Agile Teams optimize their business processes, evolve strategy with clear and decisive new commitments, and quickly adapt the organization as needed to capitalize on new opportunities.\n⬜ A customer-centric approach to defining, building, and releasing a continuous flow of valuable products and services to customers and users.\n⬜ The ability to compete and thrive in the digital age by quickly responding to market changes and emerging opportunities with innovative business Solutions.\nAchieving the business agility using SAFe requires? ⬜ The network ⬜ The hierarchy ⬜ The dual operating system\nKeeping the innovation and adaptation to market changes of the entrepreneurial network while leveraging the stability of the hierarchical system is a benefit of what? ⬜ Dual operating system\n⬜ Customer centricity\n⬜ Continuous learning culture\n⬜ Functional silos\nTo compete in the age of software we need to? ⬜ Think only about the newer technologies ⬜ Balance the stability of hierarchy and Speed of innovation via Network ⬜ Discard hierarchy completely ⬜ Think only about speed of innovation\nHow does the second operating system in SAFe deliver value? ⬜ Organize development around the flow of value while maintaining the hierarchies ⬜ Build a small entrepreneurial network focused on the Customer in place of the existing hierarchies ⬜ Decide whether to apply a hierarchical or Value Stream organizational model across the Enterprise ⬜ Reorganize the hierarchies around the flow of value\nHow does SAFe provide a second operating system that enables Business Agility? ⬜ By achieving economies of scale\n⬜ By focusing on customers, products, innovation, and growth\n⬜ By building up large departments and matrixed organizations to support rapid growth\n⬜ By creating stability and hierarchy\nWhat are the top two reasons for adopting Agile in an organization? (Choose two.) ⬜ Accelerate product delivery\n⬜ Reduce changes\n⬜ Centralize decision-making\n⬜ Enable changing priorities ⬜ Reduce project cost\nWhat are the top two reasons for adopting Agile in an organization? (Choose two.) ⬜ Increase predictability by reducing changes\n⬜ Reduce risk by centralizing decision-making\n⬜ Enhance ability to manage changing priorities ⬜ Accelerate product delivery, Reduce project cost\nWhat is one issue when organizing around hierarchical functions? ⬜ It moves the decision to where the information is\n⬜ It reduces political tensions\n⬜ It creates Agile business teams\n⬜ It hinders the value flow\nWhat is one issue to organize a system around functional silos? ⬜ They impede how value flows\n⬜ Operational activities are not included\n⬜ They do not provide development opportunities for employees\n⬜ Corporate responsibilities are not a focus\nWhich SAFe Configuration provides the benefit of all seven core competencies? ⬜ Essential ⬜ Large Solution ⬜ Portfolio ⬜ Full\nWhich is NOT an element of SAFe Foundation? ⬜ Lean-Agile Mindset ⬜ Core Values\n⬜ SAFe Principles\n⬜ Core Competencies\nA quantified way to measure your progress in SAFe is? ⬜ ROAMING ⬜ Measure and Grow ⬜ WSJF ⬜ Planning Poker\nWhich is NOT a SAFe measurement to evaluate the progress of Business Agility? ⬜ Outcomes ⬜ Flow ⬜ Competency ⬜ Demo\nArea 2: SAFe Core Competencies SAFe has the following 7 Core Competencies:-\nTeam and Technical Agility has 3 Dimensions:-\n-Agile Teams\n-Teams of Agile Teams (ART)\n-Built-In Quality Agile Product Delivery has 3 Dimensions:-\n-Customer Centricity and Design Thinking\n-Develop on cadence and release on demand\n-DevOps and the Continuous Delivery Pipeline Enterprise Solution Delivery has 3 Dimensions:-\n-Lean System Engineering -Coordinating Trains and Suppliers\n-Continually Evolve Live Systems Lean Portfolio Management has 3 Dimensions:-\n-Strategy \u0026amp; Investment Funding\n-Agile Portfolio Operations\n-Lean Governance Organizational Agility has 3 Dimensions:-\n-Lean-thinking People and Agile Teams -Lean Business Operations\n-Strategy Agility Continuous Learning Culture has 3 Dimensions:-\n-Learning Organization\n-Innovation Culture\n-Relentless Improvement - Inspect \u0026amp; Adapt (I\u0026amp;A) - Plan Do Check Adjust Lean-Agile Leadership has 3 Dimensions:-\n-Lean-Agile Mindset, Core Values, and SAFe Principles\n-Leading by Example -Leading Change Reference: https://scaledagileframework.com/safe/\nWhat are Lean Portfolio Management, Agile Product Delivery, and Lean-Agile Leadership called in SAFe? ⬜ Agile values ⬜ SAFe Lean-Agile Principles ⬜ SAFe Core Competencies ⬜ Steps in the Business Agility Value Stream\nWhat is the foundation of SAFe core competencies? ⬜ Lean-Agile Leadership\n⬜ Organizational Agility\n⬜ Continuous Learning Culture\n⬜ Team and Technical Agility\nWho is at the center of the seven SAFe core competencies? ⬜ The Business ⬜ The Customer\n⬜ The Agile Team ⬜ The Economic Benefit\nWhich core competency of the Lean Enterprise helps drive Built-In Quality practices? ⬜ Team and Technical Agility\n⬜ DevOps and Release on Demand\n⬜ Lean Portfolio Management\n⬜ Business Solutions and Lean Systems Engineering\nWhich of the core competencies of the Lean Enterprise helps align strategy and execution? ⬜ Business Solutions and Lean Systems Engineering\n⬜ Lean Portfolio Management\n⬜ DevOps and Release on Demand\n⬜ Team and Technical Agility\nWhat is the dimension of Customer Centricity? ⬜ To interpret market rhythms\n⬜ To understand the Customer\u0026rsquo;s needs\n⬜ To build small, partial systems just in time\n⬜ To design custom-built Customer Solutions\nHow does SAFe describe customer centricity? ⬜ As a set of practices employed to make products focused on the Customer\n⬜ As a strategy to meet the needs of an ever-changing Customer market\n⬜ As a mindset focused on Customer behaviors that produce the best innovations ⬜ As a way of working to include the Customer in daily work processes and planning\nHow many dimensions does the Agile product delivery competency have? ⬜ two\n⬜ three ⬜ four ⬜ five\nWhat are the three dimensions of Lean-Agile Leadership? (Choose three.) ⬜ Mindset and principles\n⬜ Emotional intelligence\n⬜ SAFe Core Values ⬜ Lead by example\n⬜ Support organizational change ⬜ Lead the change\nWhat is one of the dimensions of Lean-Agile Leadership? ⬜ Support organizational change ⬜ Relentless improvement ⬜ Emotional intelligence ⬜ Mindset and Principles\nWhat must management do for a successful Agile transformation? ⬜ Send someone to represent management, and then delegate tasks to these individuals\n⬜ Change Scrum Masters in the team every two weeks\n⬜ Strive to think of adoption as an area they can control\n⬜ Commit to quality and be the change agent in the system\nWhat is one way Lean-Agile leaders lead by example? ⬜ By mastering the Seven Core Competencies of the Lean Enterprise ⬜ By modeling SAFe\u0026rsquo;s Lean-Agile Mindset, values, principles, and practices ⬜ By using the SAFe Implementation Roadmap to script the path for change ⬜ By applying empathic design and focus on Customer Centricity\nLifelong learning is a requirement for Lean-Agile Leaders, and it helps them do what? ⬜ Provide the personnel, resources, direction, and support to the Enterprise\n⬜ Act as an effective enabler for teams\n⬜ Demonstrate the values they want the teams to embody\n⬜ Commit to quality and productivity\nArea 3: Lean-Agile Mindset Agile Manifesto Values:-\nIndividuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan Reference: https://scaledagileframework.com/lean-agile-mindset/\nWhich statement is a value from the Agile Manifesto? ⬜ Customer collaboration over a constant indefinite pace ⬜ Customer collaboration over contract negotiation ⬜ Customer collaboration over Feature negotiation ⬜ Customer collaboration over ongoing internal conversation\nWhich statement is a value from the Agile Manifesto? ⬜ Responding to a plan over responding to customer collaboration\n⬜ Responding to a plan over responding to change\n⬜ Responding to change over following a system\n⬜ Responding to change over following a plan\nWhich statement is a value from the Agile Manifesto? ⬜ Customer collaboration over a constant indefinite pace\n⬜ Individuals and interactions over contract negotiation\n⬜ Customer collaboration over following a plan\n⬜ Individuals and interactions over processes and tools\nWhich statement is a value from the Agile Manifesto? ⬜ Respond to change\n⬜ Respect for people and culture\n⬜ Build incrementally with fast, integrated learning cycles\n⬜ Limit work in process\nWhich statement is a principle of the Agile Manifesto? ⬜ Measure everything\n⬜ Simplicity–the art of maximizing the amount of work not done–is essential\n⬜ Visualize and limit WIP, reduce batch sizes, and manage queue lengths\n⬜ Respect for people and culture\nWhat is the goal of Lean Thinking? ⬜ Lean-Agile Leadership as an organizational culture\n⬜ Deliver maximum Value with the shortest sustainable lead time\n⬜ Aligning principles and values to a fixed cause\n⬜ Building a Lean Mindset as opposed to a Fixed Mindset\nWhat is the main focus point of Lean Thinking? ⬜ Ensuring respect for people and culture ⬜ Moving to an iterative development process ⬜ Implementing objective measures of progress ⬜ Reducing delays\nThe principle \u0026ldquo;precisely specify value by specific product\u0026rdquo; belongs to? ⬜ Lean Thinking\n⬜ SAFe Principles ⬜ Agile Manifesto\n⬜ SAFe Core Values\nArea 4: SAFe Core Values SAFe has the following four core values:-\nAlignment Transparency Respect for people Relentless improvement What are two of the SAFe Core Values? (Choose two.) ⬜ Program execution\n⬜ Transparency ⬜ Flow ⬜ Culture ⬜ Relentless improvement\nWhat is one of the SAFe Core Values? ⬜ Flow ⬜ Transparency ⬜ Culture ⬜ Built-in quality\nWhich SAFe Core Value includes \u0026ldquo;use common terminology\u0026rdquo; and \u0026ldquo;understand your customer\u0026rdquo;? ⬜ Alignment ⬜ Respect for People ⬜ Relentless Improvement\n⬜ Transparency\nWhich SAFe Core Value includes \u0026ldquo;create trust-based environment\u0026rdquo; and \u0026ldquo;learn from mistakes\u0026rdquo;? ⬜ Alignment ⬜ Respect for People ⬜ Relentless Improvement\n⬜ Transparency\nWhich SAFe Core Value includes \u0026ldquo;build long-term partnership\u0026rdquo; and \u0026ldquo;value diversity\u0026rdquo;? ⬜ Alignment ⬜ Respect for People ⬜ Relentless Improvement ⬜ Transparency\nWhich SAFe Core Value focuses on the Customer being the consumer of the work? ⬜ Alignment ⬜ Respect for People ⬜ Relentless Improvement ⬜ Transparency\nWhich SAFe Core Value includes \u0026ldquo;build problem-solving culture\u0026rdquo; and \u0026ldquo;reflect and adapt\u0026rdquo; ? ⬜ Alignment ⬜ Respect for People ⬜ Relentless Improvement ⬜ Transparency\nWhich SAFe Core Value emphasize on Learning and Growth? ⬜ Alignment ⬜ Respect for People ⬜ Relentless Improvement ⬜ Transparency\nHow can trust be gained between the business and technology? ⬜ Maintain Iterations as a safe zone for the team ⬜ Release new value to production every day\n⬜ Deliver predictably ⬜ Automate the delivery pipeline\nArea 5: SAFe Lean-Agile Principles Take an economic view Deliver Early and Often Apply a Comprehensive Economic Framework:- -Operate Within Lean Budgets and Guardrails\n-Understand Solution Economic Trade-Offs: Development expense, Lead time, Product cost, Value, and Risk\n-Leverage Suppliers\n-Sequencing Jobs for Maximum Benefit: Weighted Shortest Job First (MSJF) Apply systems thinking The Solution Is a System\n-Optimizing a component does not optimize the whole system -For the system to behave well, teams must understand the intended behavior and architecture\n-The value of a system passes through its interconnections -A system can evolve no faster than its slowest integration point The Enterprise Building the System Is a System, Too Understand and Optimize the Full Development Value Stream Only Management Can Change the System Assume variability; preserve options\n-Flexible requirements and design, the Cone of uncertainty, set-based over point-based approach Build incrementally with fast, integrated learning cycles\n-PDCA = Plan – Do – Check – Adjust, The shorted the cycles, the faster the learning\n-Integration points control product development and reduce risk Base milestones on objective evaluation of working systems\n-Phase-gate milestones force design decisions too early, false-positive feasibility, they assume a point Solution exists, huge batches and long queues, centralized requirements and design.\n-Use Objective milestones instead, PI System Demos, continuous, cost-effective adjustments towards an optimum Solution) Make value flow without interruptions\n-Reduce batch size for higher predictability. Total cost = Holding cost + Transaction cost. Reducing transaction costs increases predictability, accelerates feedback, reduces rework, and lowers cost.\n-Little’s Law: Wq = Lq / Lambda, Average wait time = Average queue length / Average processing rate Apply cadence, synchronize with cross-domain planning Cadence – converts unpredictable events into predictable occurrences and lowers cost, makes waiting times for new work predictable, supports regular planning and cross-functional coordination, limits batch sizes to a single interval, controls injection of new work, provides scheduled integration points;\nSynchronization – causes multiple events to happen simultaneously, facilitates cross-functional trade-offs, provides routine dependency management, supports full system integration and assessment, provides multiple feedback perspectives Unlock the intrinsic motivation of knowledge workers -Workers are most qualified to make decisions about how to perform their work\n-The workers must be heard and respected for management to lead effectively\n-Knowledge workers must manage themselves. They need autonomy -Continuing innovation must be part of the work, the tasks, and the responsibilities of knowledge workers. -Unlocking intrinsic motivation with autonomy, mastery, and purpose Decentralize decision-making Centralize – Infrequent, Long-lasting, Significant economies of scale\nDecentralize – Frequent, Time critical, Requires local information Organize around value -Value doesn’t follow silos\n-Organize around Development Value Streams. Reference: https://scaledagileframework.com/safe-lean-agile-principles/\nWhich SAFe Lean-Agile Principle includes \u0026ldquo;deliver early and often\u0026rdquo;? ⬜ Take an economic view\n⬜ Build incrementally with fast, integrated learning cycles ⬜ Organize around value ⬜ Make value flow without interruptions\nSAFe\u0026rsquo;s first Lean-Agile Principle includes \u0026ldquo;Deliver early and often\u0026rdquo; and what else? ⬜ Decentralize decision-making\n⬜ Apply cadence\n⬜ Apply systems thinking\n⬜ Apply a Comprehensive Economic Framework\nIn the economic framework, how are development expense, lead time, product cost, value and risk used? ⬜ To take into account sunk costs ⬜ To recover money already spent ⬜ To limit work in process (WIP) ⬜ To understand solution tradeoffs\nWhich is an aspect of system thinking? ⬜ Mastery drives intrinsic motivation\n⬜ Optimizing a component does not optimize the whole system\n⬜ Cadence makes routine that which is routine\n⬜ The length of the queue impacts the wait time\nWhich two are primary aspects of system thinking? (Choose two) ⬜ Mastery drives intrinsic motivation ⬜ Optimize the full Value Stream ⬜ Agile Release Train (ART) is a System ⬜ The Solution itself is a System ⬜ The System is composed of People\nWhich is not a true statement about system thinking? ⬜ Optimizing a component of the system optimizes the whole system ⬜ For the system to behave well as a system, a higher-level understanding of behavior and architecture is required ⬜ The value of a system passes through its interconnections ⬜ A system can evolve no faster than its slowest integration point\nYou\u0026rsquo;re working on a complex multi-component software project and would like to control the variability of the development process. What SAFe mechanism can you employ? ⬜ Integration points\n⬜ Stand-up meetings\n⬜ Detailed upfront planning\n⬜ Decentralized decision making\nWhat is the principal advantage of using objective evaluation of working systems as milestones in the Scaled Agile Framework? ⬜ Centralized decisions regarding design and requirements\n⬜ Increased system performance\n⬜ Significantly lower solution bug rate\n⬜ Risk mitigation\nIn the Kanban boards some steps have work in process (WIP) limits. Why is this necessary? ⬜ To enable multitasking\n⬜ To ensure large queues are not being built\n⬜ To help Continuous Deployment\n⬜ To keep timebox goals\nWhich statement is true about batch size? ⬜ Large batch sizes limit the ability to preserve options\n⬜ When stories are broken into tasks it means there are small batch sizes\n⬜ Large batch sizes ensure time for built-in quality\n⬜ When there is flow it means there are small batch sizes\nWhich statement is true about batch size? ⬜ When Stories are broken into tasks it means there are small batch sizes ⬜ The handoff batch should be made as large as possible ⬜ Large batch sizes ensure time for built-in quality ⬜ Large batch sizes increase variability\nIf small batches go through the system faster with lower variability, then which two statements are true about batch size? (Choose two.) ⬜ Good infrastructure enables large batches ⬜ Proximity (co-location) enables small batch size\n⬜ Batch sizes cannot influence our behavior\n⬜ Severe project slippage is the most likely result of large batches ⬜ Low utilization increases variability\nYou\u0026rsquo;re managing a team that has embodied SAFe. It takes about 5 days for each of your internal clients\u0026rsquo; requests for a feature to be answered by the development lead and, on average, only about 2 days to implement a feature. The testing team also takes another day, but the handover from development to testing takes two days per feature request. What\u0026rsquo;s the immediate SAFe tip you would implement to ensure that value is delivered sooner? ⬜ Remove, or minimize, the implementation time.\n⬜ Remove the development lead and educate a self-organizing team.\n⬜ Have the developers carry out the testing of their own work and remove the testing team completely.\n⬜ Remove, or minimize, the request wait time and the testing handover time.\nSAFe is a flow-based system. Which is not a property of a flow system? ⬜ Batch\n⬜ Queue\n⬜ Work in process (WIP)\n⬜ Retro\nWhat should the team focus on to optimize flow? ⬜ Cost\n⬜ Requests\n⬜ Delays\n⬜ Results\nOptimizing flow means identifying what? ⬜ Key performance indicators\n⬜ Delays\n⬜ Predictability issues of the train\n⬜ Activities that lack innovation\nWhat are the three flow accelerators for making value flow without interruptions? (Choose three.) ⬜ Reduce Queue Length ⬜ Frequent context switching ⬜ Increase capacity ⬜ Address the systemic problems ⬜ Work in Smaller Batches ⬜ Visualize and limit work in process (WIP)\nWhen using the Scaled Agile Framework, what is one benefit of adding synchronization to the cadence of multiple teams? ⬜ System-wide development variability is reduced to zero\n⬜ System-wide demos are possible since all the team demos happen at the same time\n⬜ Each team will work faster since they all start at the same time\n⬜ Overall work-in-progress is reduced\nWhat is an example of applying cadence-based synchronization in SAFe? ⬜ Teams decide their own Iteration length\n⬜ Teams align their Iterations to the same schedule to support communication, coordination, and system integration\n⬜ Teams allow batch sizes across multiple intervals\n⬜ Teams meet twice every Program Increment (PI) to plan and schedule capacity\nEach team is \u0026ldquo;sprinting\u0026rdquo; on a cadence in a SAFe Agile Release Train, which adds synchronization to this cadence? ⬜ Backlog refinement ⬜ Routine System Demos ⬜ Innovation and Planning (IP) iteration ⬜ ART Sync\nWhat benefit does cadence-based planning and development provide? ⬜ Limits WIP ⬜ Limits variability ⬜ Synchronization ⬜ Causes multiple events to occur at the same time\nPeter Drucker defines knowledge workers as individuals who know more about the work they perform than who? ⬜ Their coworkers\n⬜ Their team\n⬜ Their organization ⬜ Their bosses\nWhat else does the SAFe principle, unlock the intrinsic motivation of knowledge workers, require besides purpose and mastery? ⬜ Transparency ⬜ Innovation ⬜ Incentive-based compensation ⬜ Autonomy\nWhat is one benefit of unlocking the intrinsic motivation of knowledge workers? ⬜ To centralize decision-making\n⬜ To provide autonomy with purpose, mission, and minimum constraints\n⬜ To lower work in process (WIP) limits\n⬜ To strive to achieve a state of continuous flow\nWhat else does the SAFe Lean-Agile principle, unlock the intrinsic motivation of knowledge workers, require besides purpose and autonomy? ⬜ Innovation\n⬜ Transparency\n⬜ Mastery ⬜ Incentive-based compensation\nWhich is not an extrinsic motivation factor? ⬜ Compensation ⬜ Recognition\n⬜ Stacked Ranking ⬜ Promotion Opportunities\nWhat is the biggest benefit of decentralized decision-making? ⬜ Ensuring strategic decisions are not made in a vacuum\n⬜ Delivering value in the shortest sustainable lead time\n⬜ Creating better visualization\n⬜ Removing accountability from leaders\nWhen should a decision be decentralized? ⬜ If it\u0026rsquo;s long-lasting\n⬜ If it requires local information\n⬜ If it provides large economies of scale\n⬜ If it\u0026rsquo;s infrequent\nWhich two types of decisions should remain centralized even in a decentralized decision-making environment? (Choose two.) ⬜ Decisions that are made frequently\n⬜ Decisions that come with a high cost of delay\n⬜ Decisions that require local information\n⬜ Decisions that deliver large and broad economic benefits\n⬜ Decisions unlikely to change in the short term\nWhich type of decision should remain centralized even in a decentralized decision-making environment? ⬜ Decisions that come with a high cost of delay ⬜ Decisions that require local information ⬜ Decisions that are made frequently ⬜ Decisions that deliver large and broad economic benefits\nWhich is an example of a decision that should remain centralized even in a decentralized decision-making environment? ⬜ Team and ART Backlog ⬜ Point release to Customer ⬜ Feature Criteria ⬜ Compensation Strategy\nWhich is an example of a decision that should be decentralized? ⬜ Internationalization Strategy ⬜ Common technology platform ⬜ Team and ART Backlog ⬜ Compensation Strategy\nWhich SAFe Lean-Agile Principle includes the critical part of \u0026ldquo;delaying decisions to the last responsible moment\u0026rdquo;? ⬜ Make value flow without interruptions\n⬜ Assume variability; preserve options\n⬜ Build incrementally with fast, integrated learning cycles ⬜ Base milestones on objective evaluation of working systems\nAccording to the SAFe Lean-Agile Principle #10, what should the Enterprise do when markets and customers demand change? ⬜ Recognize the network to address emerging opportunities\n⬜ Apply development cadence and synchronization to operate effectively and manage uncertainty ⬜ Create a reliable decision-making framework to empower employees ⬜ Create a new Portfolio to manage the change\nWhat is the basic building block when organizing around value? ⬜ Agile Teams\n⬜ Hierarchies\n⬜ Individuals\n⬜ Agile Release Trains\nArea 6: Cross-functional Agile Teams How do you describe a cross-functional Agile Team? ⬜ They release customer products to production continuously ⬜ They deliver value every 6 weeks ⬜ They are optimized for communication and delivery of value ⬜ They are made up of individuals, each of whom can define, develop, test, and deploy the system\nWhat are two ways to describe a cross-functional Agile Team? (Choose two.) ⬜ They are optimized for communication and delivery of value\n⬜ They deliver value every six weeks\n⬜ They are made up of members, each of whom can define, develop, test, and deploy the system\n⬜ They can define, build, and test an increment of value\n⬜ They release customer products to production continuously\nWhen working with the team backlog, what is the specific function of the Product Owner? ⬜ Helping surface problems with the current plan.\n⬜ Investing all their time in developing specific acceptance tests.\n⬜ Holding all features that are planned to be delivered by an ART.\n⬜ Protecting the team from the problem of multiple stakeholders.\nWhich does NOT come under Product Owner responsibility? ⬜ Connecting with the Customer ⬜ Contributing to the Vision and Roadmap ⬜ Managing and Prioritize the Team Backlog ⬜ Facilitate PI Planning\nProduct Management has content authority over the Program Backlog. What do Product Owners have content authority over? ⬜ Value Streams\n⬜ Portfolio Backlog\n⬜ Portfolio Vision\n⬜ Team Backlog\nWho acts as a customer proxy for Agile teams? ⬜ The Scrum Master ⬜ The Product Owner ⬜ The Release Train Engineer ⬜ The Business Analyst\nThe Scrum Master is what above all else? (Choose two.) ⬜ A Servant Leader ⬜ A Team Coach ⬜ A SAFe Agilist ⬜ A Lean-Agile Leader\nWhich activity is a Scrum Master\u0026rsquo;s responsibility? ⬜ Coaching the Release Train Engineer(s)\n⬜ Owning the Daily stand-up\n⬜ Coaching the Agile team\n⬜ Prioritizing the Team Backlog\nWhat is part of the role of the Scrum Master? ⬜ To prioritize and identify what is ready for Iteration Planning ⬜ To escalate ART impediments ⬜ To understand Customer needs ⬜ To facilitate the team events\nWhich two behaviors should a SAFe scrum master represent as a Coach? (Choose two.) ⬜ Be a facilitator\n⬜ Focus on deadlines and technical options\n⬜ Drive towards specific outcomes\n⬜ Provide subject matter expertise\n⬜ Develop team skillsets\nWhat falls outside the Scrum Master\u0026rsquo;s responsibility? ⬜ Facilitating the Innovation and Planning event\n⬜ Facilitating team events\n⬜ Attending Scrum of scrums\n⬜ Estimating stories for the team\nWhat is a characteristic of an effective Scrum Master? ⬜ Supports the autonomy of the team ⬜ Articulates Architectural solutions ⬜ Is a technical expert\n⬜ Understands customer needs\nWhat is the focus of the Team Sync? ⬜ PI objectives versus outcomes\n⬜ Progress towards the Iteration and PI goals ⬜ Scrum Master goals versus Development Team goals ⬜ Plan objectives versus Program Owner objectives\nWhy do teams have an Iteration Retrospective? ⬜ To iterate on stories ⬜ To identify acceptance criteria ⬜ To adjust and identify ways to improve\n⬜ To evaluate metrics\nYou want to store, version, and index binary software artifacts. What type of tool do you use? ⬜ Code Repository\n⬜ Linter\n⬜ Artifact Management Repository\n⬜ Code Generator\nWhich team type is organized to deliver the value directly to the customer? ⬜ Stream-aligned team ⬜ Platform team ⬜ Complicated subsystem team ⬜ Enabling team\nWhich team type is organized to build the internal and supporting services to reduce the cognitive load? ⬜ Stream-aligned team ⬜ Platform team ⬜ Complicated subsystem team ⬜ Enabling team\nWhich team type is organized to build and maintain a part of the system? ⬜ Stream-aligned team ⬜ Platform team ⬜ Complicated subsystem team ⬜ Enabling team\nWhich team type is organized to assist other teams with specialized capabilities and help them become more proficient in new technologies? ⬜ Stream-aligned team\n⬜ Platform team\n⬜ Complicated subsystem team\n⬜ Enabling team\nArea 7: Agile Release Train (ART) The Agile Release Train uses which type of teams to get work done? ⬜ Solution teams\n⬜ Phased-review-process teams\n⬜ Management teams\n⬜ Cross-functional teams\nWhat are two of the Agile Release Train Sync meetings? (Choose two.) ⬜ PO Sync\n⬜ System Demo\n⬜ Solution Demo\n⬜ Coach Sync\n⬜ Inspect and Adapt\nWhat is one of the Agile Release Train sync meetings? ⬜ Solution Demo\n⬜ Coach Sync\n⬜ Iteration Retrospective\n⬜ Iteration Review\nWho facilitates the PO Sync meeting? ⬜ Release Train Engineer ⬜ Product Owner ⬜ Business Owner ⬜ Scrum Master\nWhat is the primary purpose of PO Sync meeting? ⬜ To build PI Objectives and improve alignment ⬜ To align with Coach Sync participants on the status of the PI ⬜ To assess the progress of the PI and adjust scope and priorities as needed ⬜ To conduct backlog refinement\nWho facilitates the Coach Sync meeting? ⬜ Release Train Engineer ⬜ Product Owner ⬜ Business Owner ⬜ Scrum Master\nWhich statement is true about ART events? ⬜ The daily stand-up is an ART event that requires the scrum of scrums and Program Owner sync involvement in the closed-loop system\n⬜ The Inspect and Adapt is the only ART event required to create a closed-loop system\n⬜ Team events run inside the ART events, and the ART events create a closed-loop system\n⬜ ART events run inside the team events, and the team events create a closed-loop system\nWhich is not an ART event? ⬜ System Demo ⬜ Inspect and Adapt\n⬜ PI Planning ⬜ Retro\nWhich role serves as a servant leader for the Agile Release Train? ⬜ Business Owner\n⬜ Release Train Engineer\n⬜ Agile Coach\n⬜ Scrum Master\nWho acts as the ‘Chief Scrum Master’ for the Agile Release Train? ⬜ Scrum Master\n⬜ Business Owner\n⬜ Product Manager\n⬜ Release Train Engineer\nWhich role defines the Nonfunctional Requirements (NFRs) in ART? ⬜ Scrum Master ⬜ Release Train Engineer ⬜ Product Manager ⬜ System Architect\nWhich role serves as a critical guardrail for the ART\u0026rsquo;s budgetary spending? ⬜ Scrum Master ⬜ Release Train Engineer ⬜ Product Manager ⬜ Business Owner\nThe Agile Release Train passes through four steps to deliver Solutions which include: defining new functionality, building, validating, and what else? ⬜ Completing phase-gate steps\n⬜ Releasing\n⬜ Regulatory compliance\n⬜ DevOps testing\nWhat do Shared Services represent? ⬜ A future view of the solution to be developed, reflecting customer and stakeholder needs. ⬜ A community of practice is an informal group of team members and other experts. ⬜ A team that provides assistance in building and using the continuous delivery pipeline. ⬜ The specialty roles, people, and services required for the success of an Agile Release Train or Solution Train.\nYou need someone in your organization who will be the authority on the ART backlog and is the internal voice of the Customer. What SAFe Program-level role must you fill? ⬜ Customer Support Representative\n⬜ Product Owner\n⬜ Release Train Engineer\n⬜ Product Management\nWhat is part of the role of Product Management? ⬜ Managing and Prioritizing the ART Backlog ⬜ Managing and Prioritizing the Team Backlog ⬜ Managing and Prioritizing the Solution Train Backlog ⬜ Managing and Prioritizing the Portfolio Backlog\nWhat is the main reason for System Demo? ⬜ To provide an optional quality check\n⬜ To enable faster feedback by integration across teams\n⬜ To fulfill SAFe PI Planning requirement\n⬜ To allow product owner to provide feedback on team increment\nWhat is the best measure of progress for complex system development? ⬜ Inspect and Adapt\n⬜ System Demo\n⬜ Prioritized backlog\n⬜ Iteration Review\nHow often should System Demos occur in the default SAFe cadence? ⬜ Every 4 weeks\n⬜ When requested\n⬜ Weekly\n⬜ Every 2 week\nHow often should a System Demo occur? ⬜ Every Release\n⬜ Every Week ⬜ Every Month\n⬜ Every Iteration\nIf a program repeatedly shows separate Feature branches rather than a true System Demo, which practice should be reviewed to address the issue? ⬜ Test first\n⬜ Roadmap creation\n⬜ Continuous Integration\n⬜ Scrum of scrums\nDuring Inspect and Adapt, teams identified a large number of action items aimed at solving their biggest problem as a train. How should the team proceed? ⬜ Load all improvement items into the Program Backlog to ensure the problem is documented and solved\n⬜ Select an improvement item using WSJF\n⬜ Identify two or three improvement items and load them into the Program Backlog\n⬜ Keep all the items and if there is extra capacity in the PI, load as many as will fit into the Program Backlog\nWhich one is part of the Inspect and Adapt (I\u0026amp;A) event? ⬜ PI System Demo ⬜ PI Planning\n⬜ Iteration Demo ⬜ Iteration Review\nWhich is NOT a part of the Inspect and Adapt (I\u0026amp;A) event? ⬜ PI System Demo ⬜ PI Planning ⬜ Quantitative and qualitative measurement ⬜ Retrospective and problem-solving workshop\nWhat is the first step of the problem-solving workshop in the Inspect and Adapt event? ⬜ Restate the new problem for the biggest root cause\n⬜ Identify the biggest root cause\n⬜ Perform a root-cause analysis\n⬜ Agree on the problem(s) to solve\nWhat is the outcome of the problem-solving workshop in the Inspect and Adapt event? ⬜ Improvement Backlog Items ⬜ Team Backlog ⬜ PI Objectives ⬜ Enabler Stories\nWhat are two reasons the 5 Whys technique is effective in performing root cause analysis in the Inspect and Adapt (I\u0026amp;A) event? (Choose two.) ⬜ It allows for assumptions and logic traps\n⬜ It is an effective way for the team to collaborate\n⬜ It allows problems of a similar nature to be combined into groups\n⬜ It reveals the nature of the problem by repeating \u0026ldquo;why\u0026rdquo; five times ⬜ It explores the cause-and-effect relationship underlying a particular problem\nWhen the 5 Whys technique is used? ⬜ To identify the root cause of the problem ⬜ To define the acceptance criteria of a Story ⬜ To brainstorm on the customer journey ⬜ To define the requirements of MVP\nWhich statement is true about the Innovation and Planning (IP) Iteration? ⬜ It is used annually when the team needs to refocus on work processes\n⬜ It is used as a weekly sync point between the Scrum Masters\n⬜ Without the IP Iteration, there is a risk that the \u0026rsquo;tyranny of the urgent\u0026rsquo; outweighs all innovation activities\n⬜ The Scrum Master can decide if the IP Iteration is necessary\nWhat is the recommended way to express a Feature? ⬜ Name, Benefit hypothesis, and Acceptance criteria ⬜ Lean business case ⬜ Name, Problem statement, and Definition of done ⬜ Name, Non-Functional Requirements, and Architecture\nArea 8: Solution Train Who is responsible for the Solution Backlog? ⬜ Product Owners\n⬜ Solution Train Engineer\n⬜ Product Management\n⬜ Solution Management\nWhich role accepts capabilities as complete? ⬜ Solution Management\n⬜ Product Management\n⬜ Solution Architect\n⬜ Solution Train Engineer\nWhich two statements describe a Capability? (Choose two.) ⬜ It is maintained in the Portfolio Backlog\n⬜ It must be structured to fit within a single PI\n⬜ It is written using a phrase, benefit hypothesis, and acceptance criteria\n⬜ It remains complete and becomes a Feature for implementation\n⬜ It is developed and approved without any dependency on the Solution Kanban\nWhich statement describes the connection between Features and Capabilities in a large Solution? ⬜ Some Features may not have parent Capabilities\n⬜ There cannot be more than 5 Features for each\n⬜ Some Capabilities may not have child Features\n⬜ Every Feature has a parent Capability\nArea 9: Built-in Quality Which basic Agile quality practice reduces bottlenecks and ensures consistency? ⬜ Establish flow\n⬜ Peer-review and pairing\n⬜ Collective ownership and standards\n⬜ Definition of done\nWhich two quality practices apply to Agile teams? (Choose two.) ⬜ Providing architectural runway\n⬜ Peer review and pairing\n⬜ Decentralized decision-making\n⬜ Using nonfunctional requirements\n⬜ Establishing flow\nArea 10: Design Thinking Design Thinking identifies at least four new ways to measure success. What are two of those ways? (Choose two.) ⬜ Reliability\n⬜ Scalability\n⬜ Marketability\n⬜ Sustainability\n⬜ Desirability\nWhat are the 4 D\u0026rsquo;s associated with the Core Design Thinking process that appears as a \u0026lsquo;Double Diamond\u0026rsquo;? ⬜ Discover, Define, Develop, Deliver ⬜ Define, Direct, Design, Deliver ⬜ Discover, Direct, Design, Deliver ⬜ Define, Direct, Develop, Deliver\nWhat is one of the tools associated with Design Thinking? ⬜ Set-based design ⬜ Empathy Maps ⬜ Portfolio Canvas ⬜ Behavior-driven development\nWhich is a design thinking tool that promotes customer identification by helping teams develop a deep, shared understanding of others? ⬜ Personas ⬜ Customer Journey Map ⬜ Story Map ⬜ Empathy Maps\nWhich tool is used to illustrate the high-level user experience in Design Thinking? ⬜ Benefit-feature Matrix ⬜ Empathy Maps ⬜ Customer Journey Map ⬜ Story Map\nWhich design thinking tool is useful to create features that include workflow? ⬜ Prototype ⬜ Story Map ⬜ Portfolio canvas ⬜ Benefit-feature Matrix\nArea 11: WSJF Weighted Shortest Job First gives preference to jobs with which two characteristics? (Choose two.) ⬜ Higher Cost of Delay\n⬜ Lower Cost of Delay\n⬜ Fixed date\n⬜ Shorter duration\n⬜ Revenue impact\nUser business value and time criticality are components of what? ⬜ Story point estimation\n⬜ Product Vision\n⬜ Cost of Delay ⬜ Feature Acceptance Criteria\nGiven the equal Cost of Delay (CoD), which job should do first based on Job Duration (JD)? ⬜ CoD = $$, JD = 1 day ⬜ CoD = $$, JD = 3 days\n⬜ CoD = $$, JD = 10 days\n⬜ CoD = $$, JD = 1 month\nGiven the equal Job Duration (JD), which job should do first based on Cost of Delay (CoD)? ⬜ CoD = $$$$, JD = 3 days ⬜ CoD = $$$, JD = 3 days\n⬜ CoD = $$, JD = 3 days\n⬜ CoD = $, JD = 3 days\nWSJF is particularly not useful to prioritize what? ⬜ Team Backlog ⬜ Portfolio Backlog ⬜ Solution Backlog ⬜ ART Backlog\nArea 12: PI Planning PI Planning aligns all the teams on the ART to a shared mission and vision. ⬜ True ⬜ False\nWhat is the example of applying cadence and synchronization in SAFe? ⬜ Creating cross-functional ARTs and Agile teams ⬜ Allocating budgets to Value Streams ⬜ Using a Portfolio Kanban system ⬜ Conducting a PI Planning event\nWhen is a Pre-PI Planning event needed? ⬜ When there is only one day to run PI Planning, so more time is needed to prepare to run it effectively\n⬜ When Product Owners and Scrum Masters need to coordinate dependencies within the Agile Release Train\n⬜ When multiple Agile Release Trains working on the same Solution need to align and coordinate\n⬜ When teams cannot identify and estimate Stories in PI Planning and need more time to prepare\nWhat are the two inputs to PI Planning? (Choose two.) ⬜ Vision\n⬜ Ready user stories ⬜ Program Roadmap ⬜ Top-10 Features list ⬜ A set of PI Objectives\nA successful PI planning event delivers which primary outputs? (Choose two.) ⬜ Roadmap and Vision ⬜ Business Context\n⬜ Committed PI objectives ⬜ ART planning board ⬜ Scope\nWhat is the recommended timeframe for cadence-based PI Planning? ⬜ 4-6 weeks ⬜ 6-8 weeks ⬜ 8-12 weeks ⬜ 12-16 weeks\nWhat is found on the ART Planning board? ⬜ Features\n⬜ User Stories\n⬜ Tasks\n⬜ Epics\nThe ART Planning board shows which two items? (Choose two.) ⬜ Epics\n⬜ Capacity and Load\n⬜ Features ⬜ Significant Dependencies\n⬜ Risks\nWhich three items are found on an ART Planning board? (Choose three.) ⬜ Significant Dependencies\n⬜ Milestones or Events\n⬜ Tasks\n⬜ Backlog items\n⬜ Features\n⬜ User Stories\nYou can find the Features and Milestones on the ART Planning board, what else? ⬜ Requirements ⬜ Dependencies ⬜ Risk ⬜ Objectives\nWhat are two problems that can be understood from the ART Planning Board? (Choose two.) ⬜ Events for future PI\n⬜ Too many dependencies leading to a single milestone\n⬜ Too much Work-in-Process in one Iteration\n⬜ Too many Features are placed in a team\u0026rsquo;s swim lane with no strings\n⬜ A significant dependency leading to a Feature\nWhen looking at an ART Planning Board, what does it mean when a feature is placed in a team\u0026rsquo;s swim lane with no strings? ⬜ That the feature can be completed independently from the other teams\n⬜ That all the risks have been ROAMed\n⬜ That the team has little confidence it will happen\n⬜ That the feature should be completed before any other feature\nWho has content authority to make decisions at the User Story level during Program Increment (PI) Planning? ⬜ Scrum Masters\n⬜ Agile Team\n⬜ Product Owner\n⬜ Release Train Engineer\nWhat does the Product Owner do as part of the prep work for iteration planning? ⬜ They collaborate with their team to detail stories with acceptance criteria and acceptance tests.\n⬜ They review and reprioritize the backlog.\n⬜ They elaborate backlogs into user stories for implementation.\n⬜ They build, edit, and maintain the team backlog.\nWho owns the Feature priorities during the PI Planning? ⬜ Business Owner\n⬜ Product Management\n⬜ Release Train Engineer\n⬜ Solution Architect/Engineer\nWhose active participation in the PI Planning event provides an essential Guardrail on budgetary spending? ⬜ Business Owner ⬜ Product Management ⬜ Product Owner ⬜ Solution Architect/Engineer\nWho assigns business value (BV) to the team PI Objectives? ⬜ Release Train Engineer\n⬜ Product Owner ⬜ Business Owner ⬜ Scrum Master\nWho decides the Team PI Objective Business Value scoring after negotiation? ⬜ The RTE ⬜ The Agile Team\n⬜ Business Owner ⬜ Product Management\nWhy do Business Owners assign business value to team PI Objectives? (Choose two.) ⬜ To determine the highest value using WSJF ⬜ To ensure the teams do not work on architectural Enablers\n⬜ To provide guidance on the business value of the team objectives ⬜ To override the decisions made in WSJF prioritization\n⬜ To empower teams to make decisions around work\n⬜ To determine what the teams should work on first\nWhat is considered an anti-pattern when assigning business values to team PI Objectives? ⬜ Business Owners assigning the business value\n⬜ Assigning business values to uncommitted objectives\n⬜ All PI Objectives are given a value of 10\n⬜ Business Owners assign high values to important Enabler work\nDuring PI Planning, which two tasks are part of the Scrum Master\u0026rsquo;s role in the first team breakout? (Choose two.) ⬜ Review and Reprioritize the team backlog as part of the preparatory work for the second team breakout\n⬜ Facilitate the coordination with other teams for dependencies\n⬜ Provide clarifications necessary to assist the team with their story estimating and sequencing\n⬜ Identify as many risks and dependencies as possible for the management review\n⬜ Be involved in the program backlog refinement and preparation\nWhich two statements are true about uncommitted objectives? (Choose two.) ⬜ The work to deliver the uncommitted objectives is not planned into the iterations during PI Planning\n⬜ Uncommitted objectives are extra things the team can do in case they have time ⬜ Uncommitted objectives are not included in the team\u0026rsquo;s commitment\n⬜ Uncommitted objectives do not get assigned a planned business value score\n⬜ Uncommitted objectives help improve predictability\nWhich statement applies to uncommitted objectives? ⬜ They are items the team has high confidence in ⬜ They are extra things teams can do if they have time ⬜ They are counted when calculating load ⬜ They are included in the commitment\nWhich statement correctly describes one aspect of the team\u0026rsquo;s commitment at the end of PI Planning? ⬜ A team commits only to the PI Objectives with the highest business value\n⬜ A team does not commit to uncommitted objectives\n⬜ A team commits to all the Features they put on the program board\n⬜ A team commits to all the Stories they put on their PI plan\nWhich is not a valid reason to move a PI Objective to uncommitted? ⬜ Dependencies with another team or supplier that cannot be guaranteed ⬜ The team has little to no experience with functionality of this type ⬜ There are a large number of critical objectives that the business depends on, and the team is already loaded close to full capacity.\n⬜ There is an objective with low business value\nDuring the final plan review, ART PI risks are ROAM\u0026rsquo;ed. What do the letters in ROAM represent? ⬜ Resolved, Owned, Assigned, Mitigated ⬜ Resolved, Owned, Accepted, Mitigated ⬜ Resolved, Owned, Approved, Mitigated ⬜ Resolved, Owned, Active, Mitigated\nDuring the final plan review, team categorize the ART PI Risk as Resolved? ⬜ when the teams agree that the risk is no longer a concern ⬜ when the team identify a plan to reduce the impact of the risk ⬜ when someone takes the responsibity to own the risk ⬜ All of the above\nDuring the PI Planning event when are planning adjustments agreed upon? ⬜ During breakout sessions\n⬜ During the management review and problem-solving meeting ⬜ During the Coach sync\n⬜ During the draft plan review\nWhich of the below statement about draft plan review is NOT True? ⬜ Entire ART is present during Draft Plan review ⬜ Initial PI Objectives in the Draft Plan do not include \u0026lsquo;uncommitted objectives\u0026rsquo; ⬜ Draft Plan review usually happens on Day 1 of PI planning ⬜ ART planning board is used for the Draft Plan review\nOn day two of PI Planning, management presents adjustments based on the previous day\u0026rsquo;s management review and problem-solving meeting. What is one possible type of adjustment they could make? ⬜ Change a team’s plan\n⬜ Create new User Stories\n⬜ Adjust business priorities\n⬜ Adjust the length of the PI\nOn day two of PI Planning, adjustments are made by the group based on the previous day\u0026rsquo;s management review and problem-solving meeting. What are three possible types of changes? (Choose three.) ⬜ Adjustment to PI Objectives ⬜ User Stories ⬜ Planning requirements reset ⬜ Movement of people ⬜ Changes to scope\nWhy is a confidence vote held at the end of PI Planning? ⬜ To remove the risks for the PI\n⬜ To build shared commitment to the Program plan\n⬜ To ensure that Business Owners accept the plan\n⬜ To hold the team accountable if the Agile Release Train does not deliver on its commitment\nA confidence vote is taken at the end of PI Planning after dependencies are resolved and risks are addressed. What best describes the process of the confidence vote? ⬜ Each person votes ⬜ The managers vote ⬜ The teams and the ARTs vote\n⬜ The business owners vote\nAt the end of PI Planning after dependencies are resolved and risks are addressed, a confidence vote is taken. What is the default method used to vote? ⬜ A vote by team then a vote of every person for the train ⬜ A vote by every person then normalized for the train ⬜ A vote by team normalized for the train ⬜ A single vote by every person for the train\nTeams make their PI objectives SMART, what SMART stands for? ⬜ Specific, Measurable, Achievable, Realistic, Time-Bound ⬜ Strategic, Maintainable, Achievable, Realistic, Testable ⬜ Specific, Measurable, Achievable, Reusable, Testable ⬜ Strategic, Maintainable, Achievable, Realistic, Time-Bound\nDuring PI planning, what are two key purposes of the hourly Scrum of Scrums checkpoint meeting? (Choose two.) ⬜ To help keep teams on track ⬜ To support early identification of risk ⬜ To align milestones with PI objectives\n⬜ To keep stretch objectives within scope\n⬜ To ensure PI objectives have direct user value\nArea 13: Continuous Delivery Pipelines with DevOps What represents the workflow, activities, and automation needed to deliver new functionality more frequently? ⬜ The Portfolio Kanban\n⬜ The Lean budget Guradrails\n⬜ The PI Planning process\n⬜ The Continuous Delivery Pipeline\nWhat is an explanation of the Continuous Delivery Pipeline at the Scaled Agile Framework Program Level? ⬜ It encompasses everything needed to go from untested software artifacts to tested software artifacts.\n⬜ It encompasses everything needed to provide a continuous stream of value to clients.\n⬜ It encompasses everything needed to deploy working software artifacts from a test environment to a production environment.\n⬜ It encompasses everything needed to go from source code to working software artifacts.\nWhat is SAFe’s release strategy? ⬜ Release on demand\n⬜ Release continuously\n⬜ Release every Program Increment\n⬜ Release on cadence\nWhat is one component of the Continuous Delivery Pipeline? ⬜ Continuous Planning\n⬜ Continuous Improvement\n⬜ Continuous Cadence\n⬜ Continuous Exploration\nWhat are the three components of the Continuous Delivery Pipeline? (Choose three.) ⬜ Continuous Planning\n⬜ Continuous Improvement\n⬜ Continuous Integration\n⬜ Continuous Cadence\n⬜ Continuous Deployment\n⬜ Continuous Exploration\nDeploy, verify, monitor, and respond are all activities of what? ⬜ Continuous Integration ⬜ Continuous Deployment\n⬜ Continuous Exploration ⬜ Release on Demand\nWhy is it important to decouple deployment from release? ⬜ To remove the need to respond quickly to production issues\n⬜ To allow inspection of Agile maturity based on different cycle times\n⬜ To make deploying of assets a business decision\n⬜ To enable releasing functionality on demand to meet business needs\nWhen is the best time to release software in SAFe? ⬜ After every PI\n⬜ After every Iteration\n⬜ As soon as the software meets the Solution Definition of Done\n⬜ Whenever the Business needs it\nWho owns the decision to release the changes into Production in SAFe 6.0? ⬜ Solution Owner ⬜ System Architect ⬜ Release Train Engineer ⬜ Product Management\nWhat do feature toggles enable? ⬜ Continuous Deployment ⬜ Scheduled Release ⬜ Automated Testing ⬜ Release on Demand\nWhat best describes the DevOps? ⬜ A high-performing DevOps Team\n⬜ Combine Deployment and Releases ⬜ Strong organizational structure\n⬜ A culture, a mindset and a set of technical practices\nWhich statement is true about DevOps? ⬜ DevOps is an approach to bridge the gap between development and operations\n⬜ DevOps automation of testing reduces the holding cost\n⬜ Measurements are not a top priority for DevOps\n⬜ Lean-Agile principles are not necessary for a successful DevOps implementation\nWhat is one key purpose of DevOps? ⬜ DevOps joins development and operations to enable continuous delivery\n⬜ DevOps enables continuous release by building a scalable Continuous Delivery Pipeline\n⬜ DevOps focuses on a set of practices applied to large systems\n⬜ DevOps focuses on automating the delivery pipeline to reduce transaction cost\nWhich statement is true when continuously deploying using a DevOps model? ⬜ It alleviates the reliance on the skill sets of Agile teams\n⬜ It increases the transaction cost\n⬜ It lessens the severity and frequency of release failures\n⬜ It ensures that changes deployed to production are always immediately available to end-users\nWhat does SAFe\u0026rsquo;s CALMR approach apply to? ⬜ PI Planning\n⬜ DevOps\n⬜ Economic Framework\n⬜ Continuous Deployment\nHow does \u0026ldquo;C\u0026rdquo; in CALMR approach to DevOps help to organize around value and deliver continuous value? ⬜ By automating the continuous delivery pipeline ⬜ By Measuring the flow, quality \u0026amp; value ⬜ By creating a culture of shared responsibility ⬜ By accelerating delivery using lean flow\nWhich of the below is not part of the CALMR approach to DevOps? ⬜ Automation ⬜ Measurement ⬜ Continuous Improvement ⬜ Lean Flow\nArea 14: Lean Portfolio Management A SAFe Portfolio is a collection of what? ⬜ Development Value Streams ⬜ Solutions\n⬜ Functional teams\n⬜ Business units\nYou need someone on your team who will work across value streams and programs to help provide the strategic technical direction that can optimize portfolio outcomes. What portfolio level must you fill? ⬜ Scrum Master\n⬜ Lean Portfolio Management\n⬜ Epic Owners\n⬜ Enterprise Architect\nWhat is one output of enterprise strategy formulation? ⬜ Portfolio Budgets\n⬜ Portfolio Governance\n⬜ Portfolio Vision\n⬜ Portfolio Canvas\nWhich option is a Scaled Agile Framework Portfolio-level highlight? ⬜ Lean Budgets\n⬜ Program Increment\n⬜ Economic Framework\n⬜ Solution Intent\nWhich phase of the Portfolio Kanban is used to capture all the new big business or technology ideas? ⬜ Funnel ⬜ Reviewing ⬜ Analyzing ⬜ Ready\nWhich phase of the Portfolio Kanban is used to estimate the preliminary cost of the epic and define its intent and definition? ⬜ Funnel ⬜ Reviewing ⬜ Analyzing ⬜ Ready\nIn which phase of Portfolio Kanban, LPM makes a go/no-go decision of an Epic for implementation? ⬜ Funnel ⬜ Reviewing ⬜ Analyzing\n⬜ Ready\nIn which phase of Portfolio Kanban, Epics are approved and sequenced using WSJF? ⬜ Funnel ⬜ Reviewing ⬜ Analyzing ⬜ Ready\nThe analyzing step of the Portfolio Kanban system has a new Epic with a completed Lean business case. What best describes the next step for the Epic? ⬜ It will remain in the analyzing step until one or more Agile Release Trains have the capacity to implement it\n⬜ It will be implemented once the Epic Owner approves the Lean business case\n⬜ It will be moved to the ready state in the Portfolio Kanban if it receives a \u0026lsquo;go\u0026rsquo; decision from Lean Portfolio Management\n⬜ It will be implemented if it has the highest weighted shortest job first (WSJF) ranking\nWhat Portfolio-level highlight has the role of describing the purpose of the Scaled Agile Framework portfolio? ⬜ Portfolio Retrospective\n⬜ Portfolio Value Stream\n⬜ Portfolio Canvas\n⬜ Portfolio Kanban\nWhat is used to capture the current state of the Portfolio and a primer to the future state? ⬜ Portfolio Canvas ⬜ Portfolio Backlog ⬜ Portfolio Kanban ⬜ Portfolio Vision\nWhich brings structure to analysis and decision-making around Epics? ⬜ Portfolio Canvas ⬜ Portfolio Backlog ⬜ Portfolio Kanban ⬜ Portfolio Vision\nHow is the flow of Portfolio Epics managed? ⬜ In the Program Kanban\n⬜ In the Portfolio Backlog\n⬜ In the Program Backlog\n⬜ In the Portfolio Kanban\nWhat is a minimum viable product (MVP)? ⬜ A minimal product that can be built to achieve market dominance\n⬜ A minimal Story a team can deliver in an Iteration\n⬜ A prototype that can be used to explore user needs\n⬜ A minimal version of a new product used to test a hypothesis\nWhat portfolio-level role takes responsibility for coordinating portfolio Epics through the Portfolio Kanban system? ⬜ Epic Owners\n⬜ Enabler Epic\n⬜ Lean Portfolio Management\n⬜ Enterprise Architect\nWho is responsible for managing the Portfolio Kanban? ⬜ Release Train Engineer\n⬜ Solution Management\n⬜ Product Management\n⬜ Lean Portfolio Management\nWhat is one example of differentiating business objectives? ⬜ Solution Intent ⬜ Enterprise Goals ⬜ Strategic Themes ⬜ Portfolio Vision\nWhich statement accurately characterizes Strategic Themes? ⬜ They are business objectives that connect the SAFe portfolio to the Enterprise business strategy\n⬜ They are a high-level summary of each program’s Vision and are updated after every PI\n⬜ They are requirements that span Agile Release Trains but must fit within a single Program Increment\n⬜ They are large initiatives managed in the Portfolio Kanban that require weighted shortest job first prioritization and a lightweight business case\nWhat is used to brainstorm potential Portfolio future states? ⬜ KPIs and Lean budget Guardrails\n⬜ Epics and Enablers\n⬜ SWOT and TOWS ⬜ Enterprise business drivers\nWhat is one of the Lean budget Guardrail? ⬜ Learning Milestones as objective measurements\n⬜ Spending caps for each Agile Release Train\n⬜ Participatory budgeting\n⬜ Continuous Business Owner engagement\nWhat is one component of a Guardrail in Lean Portfolio Management? ⬜ Allocation of centralized vs decentralized decisions in the Enterprise ⬜ Capacity allocation to optimize value and solution integrity\n⬜ Participatory budgeting forums that lead to Value Stream budget changes ⬜ Determining if business needs meet the Portfolio Threshold\nWhich Lean budget Guardrail helps ensure the appropriate allocation of budgets to balance near-term opportunities with long-term strategy and growth? ⬜ Applying capacity allocation\n⬜ Coutinuous Business Owner engagement\n⬜ Guiding investments by horizon ⬜ Approving significant initiatives\nWhich two practices, together pave the path of architecture runway? (Choose two.) ⬜ Intentional architecture ⬜ Emergent design ⬜ Strategic architecure\n⬜ Conventional design\nYou are trying to coordinate the architectural runway through different layers, You want to increase velocity in your portfolio. What could you do to accomplish this? ⬜ Follow built-in quality practices\n⬜ Implement enablers\n⬜ Implement epics\n⬜ Follow QMS guidelines\nWhat is the recommended frequency for updating Lean budget distribution? ⬜ Every iteration\n⬜ Annually\n⬜ On demand\n⬜ Twice annually\nArea 15: SAFe Implementation Roadmap SAFe Implementation Roadmap has the following 13 Steps:-\nReaching the Tipping Point Train Lean-Agile Change Agents Create a Lean-Agile Center of Excellence Train Executives, Managers, and Leaders Lead in the Digital Age Organize Around Value Create the Implementation Plan Prepare for ART Launch Train Teams and Launch ART Coach ART Execution Launch More ARTs and Value Streams Enhance the Portfolio Accelerate Reference: https://scaledagileframework.com/implementation-roadmap/\nWhat can be used as a template for putting SAFe into practice within an organization? ⬜ SAFe Principles\n⬜ SAFe Core Values\n⬜ SAFe Implementation Roadmap\n⬜ SAFe Competencies\nWhich pathway would a LACE use on the Agile growth lifecycle? ⬜ The 7 Core Competencies of Business Agility\n⬜ The SAFe Implementation Roadmap\n⬜ Agile Maturity Roadmaps\n⬜ The Scaled Agile Framework\nWhich is NOT a responsibility of LACE? ⬜ Supporting Lean Portfolio Management ⬜ Managing the Transformation Backlog ⬜ Managing the Portfolio Backlog ⬜ Coaching Leadership\nWhat is this statement defining: \u0026ldquo;A series of activities that have proven to be effective in successfully implementing SAFe\u0026rdquo;? ⬜ SAFe Principles\n⬜ SAFe Core Values\n⬜ SAFe Implementation Roadmap\n⬜ SAFe House of Lean\nWhat can be used to script the change to SAFe? ⬜ The Implementation Roadmap\n⬜ The Program Kanban\n⬜ The Lean-Agile Center of Excellence (LACE) charter\n⬜ The portfolio canvas\nWhat is the first step of the SAFe Implementation Roadmap? ⬜ Reach the tipping point ⬜ Create the Implementation Plan\n⬜ Prepare for ART Launch\n⬜ Coach ART Execution\nWhat are the first three steps of the SAFe Implementation Roadmap? ⬜ Train Lean-Agile change agents, train executives, managers and leaders, and then prepare for Agile Release Train launch\n⬜ Reach the tipping point, Train Lean-Agile change agents, and then train the identified support personnel\n⬜ Charter a Lean-Agile Center of Excellence, Train Lean-Agile change agents, and then train executives, managers and leaders\n⬜ Reach the tipping point, train Lean-Agile change agents, and then train executives, managers and leaders\nWhat are the last three steps of the SAFe Implementation Roadmap? ⬜ Train Lean-Agile change agents, extend to the portfolio, accelerate\n⬜ Launch trains, coach Agile Release Train execution, train executives and managers\n⬜ Train Lean-Agile change agents, identify Value Streams and Agile Release Trains, extend to the portfolio\n⬜ Launch more Agile Release Trains and Value Streams, extend to the portfolio, accelerate\nA Team has just adopted the SAFe Implementation Roadmap and is in the process of training executives, managers, and leaders. What is their next step? ⬜ Identify Value Stream and Agile Release Train ⬜ Create the Implementation Plan\n⬜ Prepare for ART Launch\n⬜ Coach ART Execution\nWhich implementation step follows Coach ART Execution on the SAFe Implementation Roadmap? ⬜ Launch more ARTs and Value Streams ⬜ Train Executives, Leaders, and Managers ⬜ Accelerate ⬜ Organize around value\nAn Enterprise has just adopted the SAFe Implementation Roadmap and is in the process of training executives, managers, and leaders. What is their next step? ⬜ Train the leaders in Portfolio and Product Management to solve problems before fixing symptoms\n⬜ Perform process mapping on the current state\n⬜ Train Lean-Agile change agents to push out the roadmap and build consensus\n⬜ Identify Value Streams and Agile Release Trains to start alignment of the organization\nWhen should new approaches be anchored in an organization\u0026rsquo;s culture? ⬜ Culture change needs to happen before the SAFe implementation can begin ⬜ Culture change comes right after a sense of urgency is created in the organization ⬜ Culture should not be changed because SAFe respects the current culture ⬜ Culture change comes last as a result of changing work habits\nWhat is the last step in Kotter\u0026rsquo;s approach to change management? ⬜ Institute Change ⬜ Sustain Acceleration ⬜ Create A Sense of Urgency\n⬜ Generate short-term wins\nImplementing SAFe requires buy-in from all levels of the organization. What level of leadership is most important for effecting cultural change? ⬜ Release Train Engineers\n⬜ Solution Management\n⬜ Product Owners\n⬜ Executive Management\nWhen does a Roadmap become a queue? ⬜ When it is longer than one Program Increment\n⬜ When it is fully committed\n⬜ When it includes no commitments\n⬜ When it contains Features and not Epics\nWhat does the Program Roadmap do in the Scaled Agile Framework? ⬜ It provides visibility into the Portfolio Epics being implemented in the next year ⬜ It describes technical dependencies between Features ⬜ It communicates the delivery of Features over a near term timeline ⬜ It describes the program commitment for the current and next two Program Increments\nWhich is not a roadmap defined by SAFe? ⬜ PI Roadmap ⬜ Solution Roadmap ⬜ Portfolio Roadmap ⬜ ART Roadmap\nWhat is the recommended time horizon of the PI Roadmap? ⬜ 1-3 years ⬜ 6-9 months ⬜ 3-4 weeks ⬜ 3-4 months\nConsider buying the full set of questions with answers and explanation from below links:- Leading SAFe Agilist 6.0 Practice Exams with Answers and Explaination at Leading SAFe Agilist 6.0 Questions with Answers and Explaination at a very reasonable price.\n","permalink":"https://codingnconcepts.com/agile/leading-safe-exam-questions/","tags":["SAFe","Certification"],"title":"Leading SAFe Agilist SA 6.0 Exam Questions"},{"categories":["Spring Boot"],"contents":"In this quick tutorial, we\u0026rsquo;ll configure Log4j 2 logging by replacing it with the default Logback config in the Spring Boot application.\nAdd Log4j 2 Dependency in Spring Boot We need to do two things here:-\nExclude default logging dependency org.springframework.boot:spring-boot-starter-logging from all the other dependencies. Add Log4j 2 org.springframework.boot:spring-boot-starter-log4j2 dependency. The way to do this in Maven pom.xml and Gradle build.gradle is a bit different. Let\u0026rsquo;s look at the examples:-\nMaven pom.xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-logging\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-log4j2\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Gradle build.gradle dependencies { implementation \u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39; implementation \u0026#39;org.springframework.boot:spring-boot-starter-log4j2\u0026#39; } configurations { all { exclude group: \u0026#39;org.springframework.boot\u0026#39;, module: \u0026#39;spring-boot-starter-logging\u0026#39; } } We can also use Module replacement feature to tell Gradle that any occurrences of the default logging starter should be replaced by the Log4j 2 starter, as shown in the following example:-\ndependencies { implementation \u0026#34;org.springframework.boot:spring-boot-starter-log4j2\u0026#34; modules { module(\u0026#34;org.springframework.boot:spring-boot-starter-logging\u0026#34;) { replacedBy(\u0026#34;org.springframework.boot:spring-boot-starter-log4j2\u0026#34;, \u0026#34;Use Log4j2 instead of Logback\u0026#34;) } } } That\u0026rsquo;s it. You have replaced Logback with Log4j 2 logging framework. Spring Boot auto-configures log4j2 once it finds the jar in the classpath.\nLoggingException in Project Startup If you have configured Log4j 2 in your Spring Boot Project using Maven or Gradle and getting the following Logging Exception on application startup:-\nCaused by: org.apache.logging.log4j.LoggingException: log4j-slf4j-impl cannot be present with log4j-to-slf4j\nSolution: According to Spring Boot Documentation, you should exclude the spring-boot-starter-logging module from all the dependencies, not just from spring-boot-starter-web.\nYou should not face this issue if you have followed this tutorial for Log4j 2 configuration.\nAdd Log4j 2 Properties in Spring Boot Spring boot application uses the default configuration when no configuration or property file is located in the classpath. You can provide your configuration by placing the log4j2.xml file in src/main/resources folder of your project.\n📖springboot-application ⏷📁src ⏷📁main ⏵📁java ⏷📁resources ✅log4j2.xml ⚙application.yml pom.xml build.gradle log4j2.xml Example of log4j2.xml configuration which prints the messages to the console and in rolling file:-\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;Configuration status=\u0026#34;WARN\u0026#34; monitorInterval=\u0026#34;30\u0026#34;\u0026gt; \u0026lt;Properties\u0026gt; \u0026lt;Property name=\u0026#34;LOG_PATH\u0026#34;\u0026gt;target/logs\u0026lt;/Property\u0026gt; \u0026lt;Property name=\u0026#34;ROLLING_FILE_NAME\u0026#34;\u0026gt;application\u0026lt;/Property\u0026gt; \u0026lt;Property name=\u0026#34;LOG_PATTERN\u0026#34;\u0026gt;%clr{%d{yyyy-MM-dd HH:mm:ss.SSS}}{faint} %clr{%5p} %clr{${sys}}{magenta} %clr{---}{faint} %clr{[%15.15t]}{faint} %clr{%-40.40c{1.}}{cyan} %clr{:}{faint} %msg%n%throwable \u0026lt;/Property\u0026gt; \u0026lt;/Properties\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34; follow=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;${LOG_PATTERN}\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;RollingFile name=\u0026#34;RollingFile\u0026#34; fileName=\u0026#34;${LOG_PATH}/${ROLLING_FILE_NAME}.log\u0026#34; filePattern=\u0026#34;${LOG_PATH}/${ROLLING_FILE_NAME}_%d{yyyy-MM-dd}.log\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;${LOG_PATTERN}\u0026#34;/\u0026gt; \u0026lt;Policies\u0026gt; \u0026lt;!-- Causes a rollover if the log file is older than the current JVM\u0026#39;s start time --\u0026gt; \u0026lt;OnStartupTriggeringPolicy/\u0026gt; \u0026lt;!-- Causes a rollover once the date/time pattern no longer applies to the active file --\u0026gt; \u0026lt;TimeBasedTriggeringPolicy interval=\u0026#34;1\u0026#34; modulate=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;!-- Causes a rollover once the size of file exceed --\u0026gt; \u0026lt;SizeBasedTriggeringPolicy size=\u0026#34;10MB\u0026#34; /\u0026gt; \u0026lt;/Policies\u0026gt; \u0026lt;!-- Max 10 files will be created everyday --\u0026gt; \u0026lt;DefaultRolloverStrategy max=\u0026#34;10\u0026#34;\u0026gt; \u0026lt;Delete basePath=\u0026#34;${LOG_PATH}\u0026#34; maxDepth=\u0026#34;10\u0026#34;\u0026gt; \u0026lt;!-- Delete all files older than 30 days --\u0026gt; \u0026lt;IfLastModified age=\u0026#34;30d\u0026#34; /\u0026gt; \u0026lt;/Delete\u0026gt; \u0026lt;/DefaultRolloverStrategy\u0026gt; \u0026lt;/RollingFile\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;INFO\u0026#34; additivity=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Console\u0026#34;/\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RollingFile\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt; Let\u0026rsquo;s test our log4j2.xml configuration by writing a JUnit test case:-\n@SpringBootTest public class Log4J2ConfigTest { private static final Logger log = LogManager.getLogger(Log4J2ConfigTest.class); @Test public void testLoggingLevels() { log.trace(\u0026#34;Trace message is logged!\u0026#34;); log.debug(\u0026#34;Debug message is logged!\u0026#34;); log.info(\u0026#34;Info message is logged!\u0026#34;); log.warn(\u0026#34;Warning message is logged!\u0026#34;); try { throw new RuntimeException(\u0026#34;Unknown error\u0026#34;); } catch (Exception e) { log.error(\u0026#34;Error message is logged!\u0026#34;, e); } } } We see the logging output is printed to the console as per the log-pattern specified in the log4j2.xml\n2023-10-07 14:21:55.239 INFO 16816 --- [Test worker] c.e.d.c.Log4J2ConfigTest : Started Log4J2ConfigTest in 7.159 seconds (JVM running for 11.532) 2023-10-07 14:21:55.959 DEBUG 16816 --- [Test worker] c.e.d.c.Log4J2ConfigTest : Debug message is logged! 2023-10-07 14:21:55.960 INFO 16816 --- [Test worker] c.e.d.c.Log4J2ConfigTest : Info message is logged! 2023-10-07 14:21:55.960 WARN 16816 --- [Test worker] c.e.d.c.Log4J2ConfigTest : Warning message is logged! 2023-10-07 14:21:55.961 ERROR 16816 --- [Test worker] c.e.d.c.Log4J2ConfigTest : Error message is logged! java.lang.RuntimeException: Unknown error ","permalink":"https://codingnconcepts.com/spring-boot/configure-log4j-2-logging-spring-boot/","tags":["Spring Boot Basics","Log4j"],"title":"Configure Log4j 2 Logging in Spring Boot"},{"categories":["Postman"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to generate a Code snippet from Postman for HTTP GET, POST, PUT, DELETE requests in various programming languages like C, C#, cURL, Dart, Go, Java, Javascript, NodeJs, Objective-C, PHP, Python, R, Ruby, etc.\nSteps to Generate Code Snippet from Postman You can generate the code snippet from Postman in supported programming languages by following these 5 steps:-\nCreate an HTTP request in Postman or Open an existing request in Postman Click on the code icon \u0026lt;/\u0026gt; in the right pane. It will open a \u0026ldquo;Code Snippet\u0026rdquo; dialog on the right side. Select the programming language from the dropdown list Change the code snippet settings such as formatting, indentation, set request timeout, etc. Available settings change based on your programming language selection from the dropdown list. Select the copy icon ⧉ to copy the code snippet to your clipboard. Supported Languages in Postman Postman is a very powerful tool and can be used to generate code snippets in various languages. Postman supports the following languages and frameworks to generate code snippet, which are available in the dropdown list:-\nC# - HttpClient C# - RestSharp cURL Dart - http Go - Native HTTP Java - OkHttp Java - Unirest Javascript - Fetch Javascript - JQuery Javascript - XHR C - libcurl NodeJs - Axios NodeJs - Native NodeJs - Request NodeJs - Unirest Objective C -NSURLSession OCaml\t- Cohttp PHP\t- cURL PHP\t- Guzzle PHP\t- Http_Request2 PHP\t- pecl_http PowerShell- RestMethod Python - http.client Python - Requests R\t- httr R\t- RCurl Ruby - NET::Http Shell - Httpie Shell\t- wget Swift\t- URLSession ","permalink":"https://codingnconcepts.com/postman/how-to-generate-code-snippet-from-postman/","tags":null,"title":"How to generate Code Snippet from Postman"},{"categories":["Spring Boot","Kafka"],"contents":"This post describes how to configure Multiple Kafka Consumer in Spring Boot application from a property file having different configurations such as Kafka cluster, topic, etc.\nSetup Spring Boot Project It is recommended to use Spring Initializr to generate the initial project. Our project should have Web and Kafka dependencies.\nMaven Project Click on the below link to generate maven project with pre-defined configuration:-\nhttps://start.spring.io/#!type=maven-project\u0026amp;language=java\u0026amp;platformVersion=3.0.10\u0026amp;packaging=jar\u0026amp;jvmVersion=11\u0026amp;groupId=com.example\u0026amp;artifactId=springboot-kafka\u0026amp;name=springboot-kafka\u0026amp;description=Kafka%20producer%20and%20consumer%20configuration\u0026amp;packageName=com.example.kafka\u0026amp;dependencies=web,kafka,lombok\nA typical pom.xml file for a kafka project look like this:-\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.kafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-kafka\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.kafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-kafka-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; Gradle Project Click on the below link to generate gradle project with pre-defined configuration:-\nhttps://start.spring.io/#!type=gradle-project\u0026amp;language=java\u0026amp;platformVersion=3.0.10\u0026amp;packaging=jar\u0026amp;jvmVersion=11\u0026amp;groupId=com.example\u0026amp;artifactId=springboot-kafka\u0026amp;name=springboot-kafka\u0026amp;description=Kafka%20producer%20and%20consumer%20configuration\u0026amp;packageName=com.example.kafka\u0026amp;dependencies=web,kafka,lombok\nA typical build.gradle file for a kafka project look like this:-\ndependencies { implementation \u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39; implementation \u0026#39;org.springframework.kafka:spring-kafka\u0026#39; testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; testImplementation \u0026#39;org.springframework.kafka:spring-kafka-test\u0026#39; compileOnly \u0026#39;org.projectlombok:lombok\u0026#39; annotationProcessor \u0026#39;org.projectlombok:lombok\u0026#39; } Kafka Custom Properties Spring Boot Kafka only provides support for single-consumer configuration. For example:-\nspring: kafka: bootstrap-servers: localhost:9092, localhost:9093, localhost:9094 consumer: group-id: group-1 auto-offset-reset: earliest key-deserializer: org.apache.kafka.common.serialization.StringDeserializer value-deserializer: org.apache.kafka.common.serialization.StringDeserializer We will write a custom class KafkaCustomProperties which will read the property file having multiple consumer configurations. A typical example of multiple consumer configuration properties where we have two consumers consumer1 and consumer2 consuming messages from different topics configured in different Kafka clusters:-\napplication.yml kafka: consumer: consumer1: bootstrap-servers: server1:9092, server2:9092, server3:9092 topic: topic1 group-id: group1 auto-offset-reset: earliest key-deserializer: org.apache.kafka.common.serialization.StringDeserializer value-deserializer: org.apache.kafka.common.serialization.StringDeserializer consumer2: bootstrap-servers: another.server1:9092, another.server2:9092, another.server3:9092 topic: topic2 group-id: group2 auto-offset-reset: latest key-deserializer: org.apache.kafka.common.serialization.StringDeserializer value-deserializer: org.apache.kafka.common.serialization.StringDeserializer consumer3: ... consumer4: ... The value for bootstrap-servers in the above configuration is shown for illustration purposes. It should be replaced with Kafka cluster servers in comma-separated values.\nHere is a list of important custom auto-configuration properties:-\nProperty Description kafka.bootstrap-servers Comma separated list of kafka servers (host:port) running as a cluster. Applies to all the consumers unless overridden. kafka.consumer.consumer1.bootstrap-servers Kafka bootstrap server for consumer1. Overrides kafka.bootstrap-servers kafka.client-id Client-ID to pass to the server when making requests. Used for server-side logging. kafka.consumer.consumer1.client-id Client-ID to pass for consumer1. Overrides kafka.client-id kafka.ssl.* Kafka SSL configuration is to provide secure communication between producer/consumer and Kafka server. You need to generate key-store and trust-store files and configure the location and password kafka.consumer.consumer1.* Kafka Consumer related configurations. Support all configurations same as spring.kafka.consumer.* The custom Kafka configuration class which reads the above custom configuration:-\nKafkaCustomProperties.java package com.example.kafka.config; @Configuration @ConfigurationProperties(prefix = \u0026#34;kafka\u0026#34;) @Getter @Setter public class KafkaCustomProperties { private List\u0026lt;String\u0026gt; bootstrapServers = new ArrayList\u0026lt;\u0026gt;(Collections.singletonList(\u0026#34;localhost:9092\u0026#34;)); private String clientId; private Map\u0026lt;String, String\u0026gt; properties = new HashMap\u0026lt;\u0026gt;(); private Map\u0026lt;String, KafkaProperties.Producer\u0026gt; producer; private Map\u0026lt;String, KafkaProperties.Consumer\u0026gt; consumer; private KafkaProperties.Ssl ssl = new KafkaProperties.Ssl(); private KafkaProperties.Security security = new KafkaProperties.Security(); public Map\u0026lt;String, Object\u0026gt; buildCommonProperties() { Map\u0026lt;String, Object\u0026gt; properties = new HashMap\u0026lt;\u0026gt;(); if (this.bootstrapServers != null) { properties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, this.bootstrapServers); } if (this.clientId != null) { properties.put(CommonClientConfigs.CLIENT_ID_CONFIG, this.clientId); } properties.putAll(this.ssl.buildProperties()); properties.putAll(this.security.buildProperties()); if (!CollectionUtils.isEmpty(this.properties)) { properties.putAll(this.properties); } return properties; } } Spring Boot Kafka Multiple Consumer Let\u0026rsquo;s initialize a ConcurrentKafkaListenerContainerFactory bean for each consumer using @Qualifier annotation:-\nKafkaMultipleConsumerConfig.java package com.example.kafka.multi.config; @Configuration @RequiredArgsConstructor @Slf4j public class KafkaMultipleConsumerConfig { private final KafkaCustomProperties kafkaCustomProperties; @Bean @Qualifier(\u0026#34;consumer1\u0026#34;) public ConcurrentKafkaListenerContainerFactory\u0026lt;String, String\u0026gt; consumer1ContainerFactory() { ConcurrentKafkaListenerContainerFactory\u0026lt;String, String\u0026gt; factory = new ConcurrentKafkaListenerContainerFactory\u0026lt;\u0026gt;(); factory.setConsumerFactory(consumerFactory(\u0026#34;consumer1\u0026#34;)); return factory; } @Bean @Qualifier(\u0026#34;consumer2\u0026#34;) public ConcurrentKafkaListenerContainerFactory\u0026lt;String, String\u0026gt; consumer2ContainerFactory() { ConcurrentKafkaListenerContainerFactory\u0026lt;String, String\u0026gt; factory = new ConcurrentKafkaListenerContainerFactory\u0026lt;\u0026gt;(); factory.setConsumerFactory(consumerFactory(\u0026#34;consumer2\u0026#34;)); return factory; } private ConsumerFactory\u0026lt;String, Object\u0026gt; consumerFactory(String consumerName) { Map\u0026lt;String, Object\u0026gt; properties = new HashMap\u0026lt;\u0026gt;(kafkaCustomProperties.buildCommonProperties()); if (nonNull(kafkaCustomProperties.getConsumer())) { KafkaProperties.Consumer consumerProperties = kafkaCustomProperties.getConsumer().get(consumerName); if (nonNull(consumerProperties)) { properties.putAll(consumerProperties.buildProperties()); } } log.info(\u0026#34;Kafka Consumer \u0026#39;{}\u0026#39; properties: {}\u0026#34;, consumerName, properties); return new DefaultKafkaConsumerFactory\u0026lt;\u0026gt;(properties); } } Let\u0026rsquo;s create two consumer service classes, which consume messages from different topics configured in different clusters:-\n// Consumer 1 @Service @Slf4j public class KafkaFirstConsumerServiceImpl implements KafkaConsumerService { @KafkaListener(topics = {\u0026#34;${kafka.consumer.consumer1.topic}\u0026#34;}, groupId = \u0026#34;${kafka.consumer.consumer1.group-id}\u0026#34;, containerFactory = \u0026#34;consumer1ContainerFactory\u0026#34;) public void receive(@Payload String message) { log.info(\u0026#34;message received in consumer1: {}\u0026#34;, message); } } // Consumer 2 @Service @Slf4j public class KafkaSecondConsumerServiceImpl implements KafkaConsumerService { @KafkaListener(topics = {\u0026#34;${kafka.consumer.consumer2.topic}\u0026#34;}, groupId = \u0026#34;${kafka.consumer.consumer2.group-id}\u0026#34;, containerFactory = \u0026#34;consumer2ContainerFactory\u0026#34;) public void receive(@Payload String message) { log.info(\u0026#34;message received in consumer2: {}\u0026#34;, message); } } Make note of containerFactory passed in @KafkaListener annotation, which tells which consumer configuration to use.\nSummary Spring boot doesn\u0026rsquo;t provide support for multiple Kafka consumer configurations through a property file but we can leverage existing Kafka properties to create a custom configuration to support multiple consumers.\nThis solves the use case where you want to consume the messages from different Kafka topics configured in different Kafka clusters from the same Spring Boot Project.\nYou can download the complete source code from github and read the official spring documentation Spring for Apache Kafka for further exploration.\nAlso Read How to Configure Multiple Kafka Producer in Spring Boot\n","permalink":"https://codingnconcepts.com/spring-boot/configure-multiple-kafka-consumer/","tags":["Spring Boot Kafka"],"title":"Configure Multiple Kafka Consumer in Spring Boot"},{"categories":["Spring Boot","Kafka"],"contents":"This post describes how to configure Multiple Kafka Producer in Spring Boot application from a property file having different configurations such as Kafka cluster, topic, etc.\nSetup Spring Boot Project It is recommended to use Spring Initializr to generate the initial project. Our project should have Web and Kafka dependencies.\nMaven Project Click on the below link to generate maven project with pre-defined configuration:-\nhttps://start.spring.io/#!type=maven-project\u0026amp;language=java\u0026amp;platformVersion=3.0.10\u0026amp;packaging=jar\u0026amp;jvmVersion=11\u0026amp;groupId=com.example\u0026amp;artifactId=springboot-kafka\u0026amp;name=springboot-kafka\u0026amp;description=Kafka%20producer%20and%20consumer%20configuration\u0026amp;packageName=com.example.kafka\u0026amp;dependencies=web,kafka,lombok\nA typical pom.xml file for a kafka project look like this:-\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.kafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-kafka\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.kafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-kafka-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; Gradle Project Click on the below link to generate gradle project with pre-defined configuration:-\nhttps://start.spring.io/#!type=gradle-project\u0026amp;language=java\u0026amp;platformVersion=3.0.10\u0026amp;packaging=jar\u0026amp;jvmVersion=11\u0026amp;groupId=com.example\u0026amp;artifactId=springboot-kafka\u0026amp;name=springboot-kafka\u0026amp;description=Kafka%20producer%20and%20consumer%20configuration\u0026amp;packageName=com.example.kafka\u0026amp;dependencies=web,kafka,lombok\nA typical build.gradle file for a kafka project look like this:-\ndependencies { implementation \u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39; implementation \u0026#39;org.springframework.kafka:spring-kafka\u0026#39; testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; testImplementation \u0026#39;org.springframework.kafka:spring-kafka-test\u0026#39; compileOnly \u0026#39;org.projectlombok:lombok\u0026#39; annotationProcessor \u0026#39;org.projectlombok:lombok\u0026#39; } Kafka Custom Properties Spring Boot Kafka only provides support for single-producer configuration. For example:-\nspring: kafka: bootstrap-servers: localhost:9092, localhost:9093, localhost:9094 producer: retries: 0 acks: all key-serializer: org.apache.kafka.common.serialization.StringSerializer value-serializer: org.apache.kafka.common.serialization.StringSerializer We will write a custom class KafkaCustomProperties which will read the property file having multiple producer configurations. A typical example of multiple producer configuration properties where we have two producers producer1 and producer2 publishing messages to different Kafka clusters defined in bootstrap-servers:-\napplication.yml kafka: producer: producer1: topic: topic1 bootstrap-servers: server1:9092, server2:9092, server3:9092 retries: 0 acks: all producer2: topic: topic2 bootstrap-servers: another.server1:9092, another.server2:9092, another.server3:9092 retries: 2 acks: 1 producer3: ... producer4: ... The value for bootstrap-servers in the above configuration is shown for illustration purposes. It should be replaced with Kafka cluster servers in comma-separated values.\nHere is a list of important custom auto-configuration properties:-\nProperty Description kafka.bootstrap-servers Comma separated list of kafka servers (host:port) running as a cluster. Applies to all the producers unless overridden. kafka.producer.producer1.bootstrap-servers Kafka bootstrap server for producer1. Overrides kafka.bootstrap-servers kafka.client-id Client-ID to pass to the server when making requests. Used for server-side logging. kafka.producer.producer1.client-id Client-ID to pass for producer1. Overrides kafka.client-id kafka.ssl.* Kafka SSL configuration is to provide secure communication between producer/consumer and Kafka server. You need to generate key-store and trust-store files and configure the location and password kafka.producer.producer1.* Kafka Producer related configurations. Support all configurations same as spring.kafka.producer.* The custom Kafka configuration class which reads the above custom configuration:-\nKafkaCustomProperties.java package com.example.kafka.config; @Configuration @ConfigurationProperties(prefix = \u0026#34;kafka\u0026#34;) @Getter @Setter public class KafkaCustomProperties { private List\u0026lt;String\u0026gt; bootstrapServers = new ArrayList\u0026lt;\u0026gt;(Collections.singletonList(\u0026#34;localhost:9092\u0026#34;)); private String clientId; private Map\u0026lt;String, String\u0026gt; properties = new HashMap\u0026lt;\u0026gt;(); private Map\u0026lt;String, KafkaProperties.Producer\u0026gt; producer; private Map\u0026lt;String, KafkaProperties.Consumer\u0026gt; consumer; private KafkaProperties.Ssl ssl = new KafkaProperties.Ssl(); private KafkaProperties.Security security = new KafkaProperties.Security(); public Map\u0026lt;String, Object\u0026gt; buildCommonProperties() { Map\u0026lt;String, Object\u0026gt; properties = new HashMap\u0026lt;\u0026gt;(); if (this.bootstrapServers != null) { properties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, this.bootstrapServers); } if (this.clientId != null) { properties.put(CommonClientConfigs.CLIENT_ID_CONFIG, this.clientId); } properties.putAll(this.ssl.buildProperties()); properties.putAll(this.security.buildProperties()); if (!CollectionUtils.isEmpty(this.properties)) { properties.putAll(this.properties); } return properties; } } Spring Boot Kafka Multiple Producer Let\u0026rsquo;s initialize a KafkaTemplate bean for each producer using @Qualifier annotation:-\nKafkaMultipleProducerConfig.java package com.example.kafka.config; @Configuration @RequiredArgsConstructor @Slf4j public class KafkaMultipleProducerConfig { private final KafkaCustomProperties kafkaCustomProperties; @Bean @Qualifier(\u0026#34;producer1\u0026#34;) public KafkaTemplate\u0026lt;String, Object\u0026gt; producer1KafkaTemplate() { return new KafkaTemplate\u0026lt;\u0026gt;(producerFactory(\u0026#34;producer1\u0026#34;)); } @Bean @Qualifier(\u0026#34;producer2\u0026#34;) public KafkaTemplate\u0026lt;String, Object\u0026gt; producer2KafkaTemplate() { return new KafkaTemplate\u0026lt;\u0026gt;(producerFactory(\u0026#34;producer2\u0026#34;)); } private ProducerFactory\u0026lt;String, Object\u0026gt; producerFactory(String producerName) { Map\u0026lt;String, Object\u0026gt; properties = new HashMap\u0026lt;\u0026gt;(kafkaCustomProperties.buildCommonProperties()); if (nonNull(kafkaCustomProperties.getProducer())) { KafkaProperties.Producer producerProperties = kafkaCustomProperties.getProducer().get(producerName); if (nonNull(producerProperties)) { properties.putAll(producerProperties.buildProperties()); } } log.info(\u0026#34;Kafka Producer \u0026#39;{}\u0026#39; properties: {}\u0026#34;, producerName, properties); return new DefaultKafkaProducerFactory\u0026lt;\u0026gt;(properties); } } Let\u0026rsquo;s create two producer service classes, which publish messages to different topics configured in different clusters:-\n// Producer 1 @Service @Slf4j public class KafkaFirstProducerService implements KafkaProducerService { @Qualifier(\u0026#34;producer1\u0026#34;) private KafkaTemplate\u0026lt;String, String\u0026gt; kafkaTemplate; @Value(\u0026#34;${kafka.producer.producer1.topic}\u0026#34;) private String topic; @Override public void send(String message) { log.info(\u0026#34;sending message from first producer: {}\u0026#34;, message); kafkaTemplate.send(topic, message); } } // Producer 2 public class KafkaSecondProducerService { @Qualifier(\u0026#34;producer2\u0026#34;) private KafkaTemplate\u0026lt;String, String\u0026gt; kafkaTemplate; @Value(\u0026#34;${kafka.producer.producer2.topic}\u0026#34;) private String topic; public void send(String message) { log.info(\u0026#34;sending message from second producer: {}\u0026#34;, message); kafkaTemplate.send(topic, message); } } Summary Spring boot doesn\u0026rsquo;t provide support for multiple Kafka producer configurations through a property file but we can leverage existing Kafka properties to create a custom configuration to support multiple producers.\nThis solves the use case where you want to publish the messages to different Kafka topics configured in different Kafka clusters from the same Spring Boot Project.\nYou can download the complete source code from github and read the official spring documentation Spring for Apache Kafka for further exploration.\nAlso Read How to Configure Multiple Kafka Consumer in Spring Boot\n","permalink":"https://codingnconcepts.com/spring-boot/configure-multiple-kafka-producer/","tags":["Spring Boot Kafka"],"title":"Configure Multiple Kafka Producer in Spring Boot"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to use TreeMap to sort a Map by its keys in Java\nSort Map by Simple Key Let\u0026rsquo;s initialize a Map, where the map key is a simple key of type String\n// initialize map in random order of keys Map\u0026lt;String, String\u0026gt; map = Map.of( \u0026#34;key5\u0026#34;, \u0026#34;value5\u0026#34;, \u0026#34;key2\u0026#34;, \u0026#34;value2\u0026#34;, \u0026#34;key4\u0026#34;, \u0026#34;value4\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;, \u0026#34;key3\u0026#34;, \u0026#34;value3\u0026#34;); Sort Map in ascending order When you initialize a TreeMap by passing an existing map in the constructor e.g. new TreeMap\u0026lt;\u0026gt;(map), it is sorted according to the natural ordering of its keys, which is the ascending alphabetical order for String key type.\nMap\u0026lt;String, String\u0026gt; sortedTreeMap = new TreeMap\u0026lt;\u0026gt;(map); System.out.println(sortedTreeMap); // {key1=value1, key2=value2, key3=value3, key4=value4, key5=value5} Sort Map in descending order You can also initialize a TreeMap by passing a Comparator in the constructor new TreeMap\u0026lt;\u0026gt;(Comparator.reverseOrder()). In this case, all the map entries added to this TreeMap are sorted by the passed Comparator.\nSince we have provided the Comparator to sort by reverse natural order, the resulting map is sorted by descending alphabetical order for String key type.\nMap\u0026lt;String, String\u0026gt; reverseSortedTreeMap = new TreeMap\u0026lt;\u0026gt;(Comparator.reverseOrder()); reverseSortedTreeMap.putAll(map); System.out.print(reverseSortedTreeMap); // {key5=value5, key4=value4, key3=value3, key2=value2, key1=value1} Sort Map by Object Key Let’s initialize a Map, where the map key is a complex key of type ComplexKey class, which has multiple properties - name and priority.\npublic class ComplexKey { String name; int priority; } // initialize map in random order of keys Map\u0026lt;ComplexKey, String\u0026gt; map1 = Map.of( new ComplexKey(\u0026#34;key5\u0026#34;, 1), \u0026#34;value5\u0026#34;, new ComplexKey(\u0026#34;key2\u0026#34;, 1), \u0026#34;value2\u0026#34;, new ComplexKey(\u0026#34;key4\u0026#34;, 1), \u0026#34;value4\u0026#34;, new ComplexKey(\u0026#34;key1\u0026#34;, 2), \u0026#34;value1\u0026#34;, new ComplexKey(\u0026#34;key3\u0026#34;, 2), \u0026#34;value3\u0026#34;); Sort Map in ascending order When you initialize a TreeMap by passing a Comparator in the constructor e.g. new TreeMap\u0026lt;\u0026gt;(Comparator.comparing(ComplexKey::getName)), then all the map entries added to this TreeMap are sorted by the passed Comparator.\nWe passed the Comparator to sort the Map by Key Object\u0026rsquo;s name property in ascending order. Let\u0026rsquo;s see the result:-\nMap\u0026lt;ComplexKey, String\u0026gt; sortedTreeMap = new TreeMap\u0026lt;\u0026gt;(Comparator.comparing(ComplexKey::getName)); sortedTreeMap.putAll(map); System.out.println(sortedTreeMap); // {ComplexKey(name=key1, priority=2)=value1, // ComplexKey(name=key2, priority=1)=value2, // ComplexKey(name=key3, priority=2)=value3, // ComplexKey(name=key4, priority=1)=value4, // ComplexKey(name=key5, priority=1)=value5} Sort by multiple key properties We can also pass a Comparator that sorts the map by multiple Key Object\u0026rsquo;s properties. For example, sort the map by priority property and then by name property in ascending order.\nMap\u0026lt;ComplexKey, String\u0026gt; sortedTreeMap = new TreeMap\u0026lt;\u0026gt;(Comparator.comparingInt(ComplexKey::getPriority).thenComparing(ComplexKey::getName)); sortedTreeMap.putAll(map); System.out.println(sortedTreeMap); // {ComplexKey(name=key2, priority=1)=value2, // ComplexKey(name=key4, priority=1)=value4, // ComplexKey(name=key5, priority=1)=value5, // ComplexKey(name=key1, priority=2)=value1, // ComplexKey(name=key3, priority=2)=value3} Sort Map in descending order Let\u0026rsquo;s initialize a TreeMap by passing a Comparator, which sorts the map by Key Object\u0026rsquo;s name property in descending order.\nMap\u0026lt;ComplexKey, String\u0026gt; reverseSortedTreeMap = new TreeMap\u0026lt;\u0026gt;(Comparator.comparing(ComplexKey::getName).reversed()); reverseSortedTreeMap.putAll(map); System.out.println(reverseSortedTreeMap); // {ComplexKey(name=key5, priority=1)=value5, // ComplexKey(name=key4, priority=1)=value4, // ComplexKey(name=key3, priority=2)=value3, // ComplexKey(name=key2, priority=1)=value2, // ComplexKey(name=key1, priority=2)=value1} Sort by multiple key properties We can also pass a Comparator that sorts the map by multiple Key Object\u0026rsquo;s properties. For example, sort the map by priority property in ascending order and then by name property in descending order.\nMap\u0026lt;ComplexKey, String\u0026gt; reverseSortedTreeMapWithMultipleFields = new TreeMap\u0026lt;\u0026gt;(Comparator.comparingInt(ComplexKey::getPriority) .thenComparing((c1, c2) -\u0026gt; c2.getName().compareTo(c1.getName()))); reverseSortedTreeMapWithMultipleFields.putAll(map); System.out.println(reverseSortedTreeMapWithMultipleFields); // {ComplexKey(name=key5, priority=1)=value5, // ComplexKey(name=key4, priority=1)=value4, // ComplexKey(name=key2, priority=1)=value2, // ComplexKey(name=key3, priority=2)=value3, // ComplexKey(name=key1, priority=2)=value1} ","permalink":"https://codingnconcepts.com/java/sort-map-by-key-using-treemap/","tags":["Java Collection","Sorting"],"title":"Sort Map by Key using TreeMap in Java"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to sort an Array List of Objects in Java\nBuild a List of Objects Let\u0026rsquo;s understand the sorting of an Array List of Objects with an Example. Let\u0026rsquo;s create a class Book:-\n@Data public class Book { private final String name; private final String author; private final double rating; } Let\u0026rsquo;s create some Book objects and build a library with an Array List of Book Objects:-\nBook book1 = new Book(\u0026#34;book1\u0026#34;, \u0026#34;author1\u0026#34;, 3.0); Book book2 = new Book(\u0026#34;book2\u0026#34;, \u0026#34;author2\u0026#34;, 5); Book book3 = new Book(\u0026#34;book3\u0026#34;, \u0026#34;author1\u0026#34;, 4.0); Book book4 = new Book(\u0026#34;book4\u0026#34;, \u0026#34;author2\u0026#34;, 2.5); Book book5 = new Book(\u0026#34;book5\u0026#34;, \u0026#34;author1\u0026#34;, 4.0); List\u0026lt;Book\u0026gt; library = Arrays.asList(book1, book2, book3, book4, book5); Sort List of Objects using Collections.sort() We use Collections.sort() method to sort the existing list of objects using Comparators. This sorting takes place on the same list of objects and no new list is created. If you want to keep the existing list as it is after sorting then use Streams instead, which generates a new sorted list.\nSort in ascending order Sort a list of book objects by author in ascending order (author name in alphabetical increasing order)\nCollections.sort(library, Comparator.comparing(Book::getAuthor)); System.out.println(library); // [Book(name=book1, author=author1, rating=3.0), // Book(name=book3, author=author1, rating=4.0), // Book(name=book5, author=author1, rating=4.0), // Book(name=book2, author=author2, rating=5.0), // Book(name=book4, author=author2, rating=2.0)] Sort in descending order Sort a list of book objects by rating in descending order (rating number in decreasing order)\nCollections.sort(library, Comparator.comparingDouble(Book::getRating).reversed()); System.out.println(library); // [Book(name=book2, author=author2, rating=5.0), // Book(name=book3, author=author1, rating=4.0), // Book(name=book5, author=author1, rating=4.0), // Book(name=book1, author=author1, rating=3.0), // Book(name=book4, author=author2, rating=2.0)] Sort by multiple fields in ascending order Sort a list of book objects first by author name in alphabetical increasing order, then rating in increasing order and then by book name in alphabetical increasing order.\nCollections.sort(library, Comparator.comparing(Book::getAuthor) .thenComparing(Book::getRating) .thenComparing(Book::getName)); System.out.println(library); // [Book(name=book1, author=author1, rating=3.0), // Book(name=book3, author=author1, rating=4.0), // Book(name=book5, author=author1, rating=4.0), // Book(name=book4, author=author2, rating=2.0), // Book(name=book2, author=author2, rating=5.0)] Sort by multiple fields in ascending then descending order Sort a list of book objects by author name in alphabetical increasing order, then rating in decreasing order and then by book name in alphabetical decreasing order.\nCollections.sort(library, Comparator.comparing(Book::getAuthor) .thenComparing((b1, b2) -\u0026gt; Double.compare(b2.getRating(), b1.getRating())) .thenComparing((b1, b2) -\u0026gt; b2.getName().compareTo(b1.getName()))); System.out.println(library); // [Book(name=book5, author=author1, rating=4.0), // Book(name=book3, author=author1, rating=4.0), // Book(name=book1, author=author1, rating=3.0), // Book(name=book2, author=author2, rating=5.0), // Book(name=book4, author=author2, rating=2.0)] Sort List of Objects using Streams Use Java Streams for sorting the list of Objects when you don\u0026rsquo;t want to change the existing list and generate a new sorted list instead.\nSort in ascending order Sort a list of book objects by author in ascending order\nList\u0026lt;Book\u0026gt; sortedLibrary = library.stream() .sorted(Comparator.comparing(Book::getAuthor)) .collect(Collectors.toList()); System.out.println(sortedLibrary); // [Book(name=book1, author=author1, rating=3.0), // Book(name=book3, author=author1, rating=4.0), // Book(name=book5, author=author1, rating=4.0), // Book(name=book2, author=author2, rating=5.0), // Book(name=book4, author=author2, rating=2.0)] Sort in descending order Sort a list of book objects by rating in descending order\nList\u0026lt;Book\u0026gt; sortedLibrary = library.stream() .sorted(Comparator.comparingDouble(Book::getRating) .reversed()) .collect(Collectors.toList()); System.out.println(sortedLibrary); // [Book(name=book2, author=author2, rating=5.0), // Book(name=book3, author=author1, rating=4.0), // Book(name=book5, author=author1, rating=4.0), // Book(name=book1, author=author1, rating=3.0), // Book(name=book4, author=author2, rating=2.0)] Sort by multiple fields in ascending order Sort a list of book objects by author, then rating and then name in ascending order\nList\u0026lt;Book\u0026gt; sortedLibrary = library.stream() .sorted(Comparator.comparing(Book::getAuthor) .thenComparingDouble(Book::getRating) .thenComparing(Book::getName)) .collect(Collectors.toList()) System.out.println(sortedLibrary); // [Book(name=book1, author=author1, rating=3.0), // Book(name=book3, author=author1, rating=4.0), // Book(name=book5, author=author1, rating=4.0), // Book(name=book4, author=author2, rating=2.0), // Book(name=book2, author=author2, rating=5.0)] Sort by multiple fields in ascending then descending order Sort a list of book objects by author in ascending, then rating in descending and then name in descending order\nList\u0026lt;Book\u0026gt; sortedLibrary = library.stream() .sorted(Comparator.comparing(Book::getAuthor) .thenComparing((b1, b2) -\u0026gt; Double.compare(b2.getRating(), b1.getRating())) .thenComparing((b1, b2) -\u0026gt; b2.getName().compareTo(b1.getName()))) .collect(Collectors.toList()) System.out.println(sortedLibrary); // [Book(name=book5, author=author1, rating=4.0), // Book(name=book3, author=author1, rating=4.0), // Book(name=book1, author=author1, rating=3.0), // Book(name=book2, author=author2, rating=5.0), // Book(name=book4, author=author2, rating=2.0)] ","permalink":"https://codingnconcepts.com/java/sort-array-list-of-object-in-java/","tags":["Java Collection","Sorting"],"title":"Sort Array List of Objects in Java"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to print the elements of a given Array in Java.\nSimplest way The Arrays.toString() and Arrays.deepToString() methods are the simplest way to print Arrays in Java and work well for all type of Arrays i.e. int, double, byte, String, etc.\nQuick examples System.out.println(Arrays.toString(new int[]{1, 2, 3})); // prints [1, 2, 3] System.out.println(Arrays.deepToString(new int[][]{{1, 2}, {3, 4}, {5, 6}})); // prints [[1, 2], [3, 4], [5, 6]] System.out.println(Arrays.deepToString(new int[][][]{{{1, 2}, {3}, {4}}, {{5, 6, 7}, {8}}, {{9, 10}}})); // prints [[[1, 2], [3], [4]], [[5, 6, 7], [8]], [[9, 10]]] Arrays.toString() for simple Array The Arrays.toString() method is the simplest way to print elements of an Array in a single line in Java:-\nString[] strArray = new String[] {\u0026#34;John\u0026#34;, \u0026#34;Mary\u0026#34;, \u0026#34;Bob\u0026#34;}; System.out.println(Arrays.toString(strArray)); // prints [John, Mary, Bob] Limitation of Array.toString() The Arrays.toString() method works well with simple Array but prints ClassName@hashCode for nested Arrays like this:-\nString[][] nestedArray = new String[][] {{\u0026#34;John\u0026#34;, \u0026#34;Mary\u0026#34;}, {\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;}}; System.out.println(Arrays.toString(nestedArray)); // prints something like [[Ljava.lang.String;@566776ad, [Ljava.lang.String;@6108b2d7] Nothing to worry about! We have Arrays.deepToString() to the rescue.\nArrays.deepToString() for nested Array The Arrays.deepToString() method can be used to print all the nested elements of a multi-dimensional Array in a single line in Java:-\nString[][] nestedArray = new String[][] {{\u0026#34;John\u0026#34;, \u0026#34;Mary\u0026#34;}, {\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;}}; System.out.println(Arrays.deepToString(nestedArray)); // prints [[John, Mary], [Alice, Bob]] String[][][] deepNestedArray = new String[][][] {{{\u0026#34;John\u0026#34;}, {\u0026#34;Mary\u0026#34;}, {\u0026#34;Alice\u0026#34;}}, {{\u0026#34;Bob\u0026#34;}, {\u0026#34;Adam\u0026#34;}}, {{\u0026#34;Emily\u0026#34;}}}; System.out.println(Arrays.deepToString(deepNestedArray)); // prints [[[John], [Mary], [Alice]], [[Bob], [Adam]], [[Emily]]] Other ways There are many other ways to print an Array in Java:-\nUse Arrays.toString() and Arrays.deepToString() methods to convert an Array to a comma-separated String and print. Use Arrays.asList() and List.of() methods to convert an Array to a List and use its predefined toString() method to print elements in a comma-separated String. Use String.join() method to join elements of a String Array by any delimiter of your choice and print in a single line. Use Java Streams to map elements of an Array to String, join by any delimiter of your choice, and print in a single line. Use forEach loop to iterate through elements of an Array and print each element in a new line. Let\u0026rsquo;s look at the examples:-\n// Primitive int int[] ints = new int[] {1, 2, 3, 4, 5}; System.out.println(Arrays.toString(ints)); System.out.println(IntStream.of(ints).mapToObj(Integer::toString).collect(Collectors.joining(\u0026#34;, \u0026#34;))); System.out.println(IntStream.of(ints).boxed().map(Object::toString).collect(Collectors.joining(\u0026#34;, \u0026#34;))); IntStream.of(ints).forEach(System.out::println); Arrays.stream(ints).forEach(System.out::println); // Object String String[] strs = new String[] {\u0026#34;John\u0026#34;, \u0026#34;Mary\u0026#34;, \u0026#34;Bob\u0026#34;}; System.out.println(Arrays.toString(strs)); System.out.println(Arrays.asList(strs)); System.out.println(List.of(strs)); System.out.println(String.join(\u0026#34;, \u0026#34;, strs)); System.out.println(Stream.of(strs).collect(Collectors.joining(\u0026#34;, \u0026#34;))); Stream.of(strs).forEach(System.out::println); Arrays.stream(strs).forEach(System.out::println); Arrays.asList(strs).forEach(System.out::println); List.of(strs).forEach(System.out::println); // Object Enum DayOfWeek [] days = { FRIDAY, MONDAY, TUESDAY }; System.out.println(Arrays.toString(days)); System.out.println(Arrays.asList(days)); System.out.println(Stream.of(days).map(Object::toString).collect(Collectors.joining(\u0026#34;, \u0026#34;))); System.out.println(Arrays.toString(days)); Stream.of(days).forEach(System.out::println); Arrays.stream(days).forEach(System.out::println); Arrays.asList(days).forEach(System.out::println); List.of(days).forEach(System.out::println); ","permalink":"https://codingnconcepts.com/java/print-array-in-java/","tags":["Java Collection"],"title":"Print Array in Java"},{"categories":["AWS"],"contents":"If you are planning or preparing for AWS Certified Solutions Architect Associate (SAA-C02) exam then this article is for you to get started.\nOverview FAQs Prepare well for the exam, it is the toughest exam I cracked in recent years. Requires 2 to 3 month of preparation depending upon your commitment per day. Exam code is SAA-C03 (third version) and cost you 150 USD per attempt. You need to solve 65 questions in 130 mins from your laptop under the supervision of online proctor. Passing score is 720 (out of 1000) means you should answer at least 47 (out of 65) questions correctly. No negative scoring so answer all the questions! You get the result (Pass or Fail) once you submit the exam, however you don\u0026rsquo;t receive any email immediately. It generally takes 2-3 days. I received an email with digital certificate, score-card and badge after two days. You can also login to AWS training to get them later. You can schedule exam with Pearson VUE or PSI. I heard bad reviews about PSI and chose Pearson VUE for my exam. The exam went smooth. You get discount vouchers under Benefits tab of AWS training portal once you crack at least one AWS exam. You can use these vouchers for subsequent exams. Exam Guide for more details. Learning Path I followed these four steps for the preparation of AWS exam:-\n1. Watch Videos First step to your learning path is to go through AWS lecture and training videos, which is easiest way to get familiar with AWS Services. It might take 1-2 months to cover all the AWS services depending upon your daily commitment. I recommend the following lecture videos:-\nCloudGuru - 35+ hours videos by Ryan Kroonenburg with course quizzes, Hands-on labs, 1 practice exam. They also provide AWS Sandbox for unlimited Hands-on when you buy subscription. Udemy - 26+ hours videos by Stephane Maarek with course quizzes, Hands-on labs, 1 practice exam. FreeCodeChamp - 10+ hours of amazing video on Youtube, absolutely free! Hands-on AWS Services is very important to visualize AWS services and retain your AWS learning for a long time.\n2. Practice Exam Watching videos are not enough! You must solve as many practice exams as you can. They gives you a very fair understanding of what to expect in real exam. I recommend the following practice exams:-\nWhizlabs - 7 practice tests (65 questions each) and also has topic-wise practice tests. Udemy - 6 practice tests (65 questions each) by John Bonso AWS - Sample Questions - 10 sample questions 3. Next Step You can read the following to build confidence:-\nAWS services FAQs - You will find the answers in FAQs for most of the questions VPC Analogy - Interesting read to understand the difficult topic VPC \u0026amp; Networking AWS Well-Architected White papers AWS Ramp-Up Guide: Architect 4. Last Step Once you are done with your preparation and ready for the exam, go through the below exam notes for your last day of preparation:-\nThese AWS certification exam notes are the result of watching 50+ hours of AWS training videos, solving 1000+ AWS exam questions, reading AWS services FAQs and White papers. Best of luck with your exam preparation!\nAWS Infrastructure AWS Region AWS regions are physical locations around the world having a cluster of data centers. | AWS Region | Code | |---------------------------|----------------| | US East (N. Virginia) | us-east-1 | | US East (Ohio) | us-east-2 | | US West (N. California) | us-west-1 | | US West (Oregon) | us-west-2 | | Africa (Cape Town) | af-south-1 | | Asia Pacific (Hong Kong) | ap-east-1 | | Asia Pacific (Mumbai) | ap-south-1 | | Asia Pacific (Osaka) | ap-northeast-3 | | Asia Pacific (Seoul) | ap-northeast-2 | | Asia Pacific (Singapore) | ap-southeast-1 | | Asia Pacific (Sydney) | ap-southeast-2 | | Asia Pacific (Tokyo) | ap-northeast-1 | | Canada (Central) | ca-central-1 | | Europe (Frankfurt) | eu-central-1 | | Europe (Ireland) | eu-west-1 | | Europe (London) | eu-west-2 | | Europe (Milan) | eu-south-1 | | Europe (Paris) | eu-west-3 | | Europe (Stockholm) | eu-north-1 | | Middle East (Bahrain) | me-south-1 | | South America (São Paulo) | sa-east-1 | You need to select the region first for most of the AWS services such as EC2, ELB, S3, Lambda, etc. You can not select region for Global AWS services such as IAM, AWS Organizations, Route 53, CloudFront, WAF, etc. Each AWS Region consists of multiple, isolated, and physically separate AZs (Availability Zones) within a geographic area. AZ (Availability zones) An AZ is one or more discrete data centers with redundant power, networking, and connectivity All AZs in an AWS Region are interconnected with high-bandwidth, low-latency networking. Customer deploy applications across multiple AZs in same region for high-availability, scalability, fault-tolerant and low-latency. AZs in a region are usually 3, min is 2 and max is 6 for e.g. 3 AZs in Ohio are us-east-2a, us-east-2b, and us-east-2c. For high availability in us-east-2 region with min 6 instances required either place 3 instances in each 3 AZs or place 6 instances in each 2 AZs (choose any 2 AZs out of 3) so that it works normal when 1 AZ goes down. Security, Identity \u0026amp; Compliance IAM (Identity and Access Management) IAM is used to manage access to users and resources IAM is a global service (applied to all the regions at the same time). IAM is a free service. Root account is created by default with full administrator, shouldn\u0026rsquo;t be used Users mapped to physical user, should login to AWS console with their own account and password Groups can have one or more users, can not have other groups Policies are JSON documents that Allow or Deny the access on action can be performed on AWS resource by any user, group and role Version policy language version. 2012-10-17 is latest version. Statement container for one or more policy statements Sid (optional) a way of labeling your policy statement Effect set whether the policy Allow or Deny Principal user, group, role, or federated user to which you would like to allow or deny access Action one or more actions that can be performed on AWS resources Resource one or more AWS resource to which actions apply Condition (optional) one or more conditions to satisfy for policy to be applicable, otherwise ignore the policy { \u0026#34;Version\u0026#34;:\u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;:[{ \u0026#34;Sid\u0026#34;: \u0026#34;Deny-Barclay-S3-Access\u0026#34;, \u0026#34;Effect\u0026#34;:\u0026#34;Deny\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: [\u0026#34;arn:aws:iam:123456789012:barclay\u0026#34;] }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:List*\u0026#34; ], \u0026#34;Resource\u0026#34;: [\u0026#34;arn:aws:s3:::mybucket/*\u0026#34;] },{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;iam:CreateServiceLinkedRole\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringLike\u0026#34;: { \u0026#34;iam:AWSServiceName\u0026#34;: [ \u0026#34;rds.amazonaws.com\u0026#34;, \u0026#34;rds.application-autoscaling.amazonaws.com\u0026#34; ] } } }] } Roles are associated with trusted entities - AWS services (EC2, Lambda, etc), Another AWS account, Web Identity (Cognito or any OpenID provider), or SAML 2.0 federation (your corporate directory). You attach policy to the role, these entities assume the role to access the AWS resources. Least Privilege Principle should be followed in AWS, don\u0026rsquo;t give more permission than a user needs. Resource Based Policies are supported by S3, SNS, and SQS IAM Permission Boundaries to set at individual user or role for maximum allowed permissions IAM Policy Evaluation Logic ➔ Explicit Deny ➯ Organization SCPs ➯ Resource-based Policies (optional) ➯ IAM Permission Boundaries ➯ Identity-based Policies If you got SSL/TLS certificates from third-party CA, import the certificate into AWS Certificate Manager (ACM) or upload it to the IAM Certificate Store Access AWS programmatically AWS Management Console - Use password + MFA (multi factor authentication) AWS CLI or SDK - Use Access Key ID (~username) and Secret Access Key (~password) $ aws --version $ aws configure AWS Access Key ID [None]: AES Secret Access Key [None]: Default region name [None]: Default output format [None]: $ aws iam list-users AWS CloudShell - CLI tool from AWS browser console - Require login to AWS Access AWS for Non-IAM users Non-IAM user first authenticate from Identity Federation. Then provide a temporary token (IAM Role attached) generated by calling a AssumeRole API of STS (Security Token Service). Non-IAM user access the AWS resource by assuming IAM Role attached with token. You can authenticate and authorize Non-IAM users using following Identity Federation:- SAML 2.0 (old) to integrate Active Directory/ADFS, use AssumeRoleWithSAML STS API Custom Identity Broker used when identity provider is not compatible to SAML 2.0, use AssumeRole or GetFederationToken STS API Web Identity Federation is used to sign in using well-known external identity provider (IdP), such as login with Amazon, Facebook, Google, or any OpenID Connect (OIDC)-compatible IdP. Get the ID token from IdP, use AWS Cognito api to exchange ID token with cognito token, use AssumeRoleWithWebIdentity STS API to get temp security credential to access AWS resources AWS Cognito is recommended identity provider by Amazon Amazon Single Sign On gives single sign-on token to access AWS, no need to call STS API You can use AWS Directory Service to manage Active Directory (AD) in AWS for e.g. AWS Managed Microsoft AD is managed Microsoft Windows Server AD with trust connection to on-premise Microsoft AD. Best choice when you need all AD features to support AWS applications or Windows workloads. can be used for single sign-on for windows workloads. AD Connector is proxy service to redirect requests to on-premise Microsoft AD. Best choice to use existing on-premise AD with compatible AWS services. Simple AD is standalone AWS managed compatible AD powered by Samba 4 with basic directory features. You cannot connect it to on-premise AD. Best choice for basic directory features. Amazon Cognito is a user directory for sign-up and sign-in to mobile and web application using Cognito User Pools. Nothing to do with Microsoft AD. Amazon Cognito Cognito User Pools (CUP) User Pools is a user directory for sign-up and sign-in to mobile and web applications. User pool is mainly used for authentication to access AWS services Use to authenticate mobile app users through user pool directory, or federated through third-party identity provider (IdP). The user pool manages the overhead of handling the tokens that are returned from social sign-in through Facebook, Google, Amazon, and Apple, and from OpenID Connect (OIDC) and SAML IdPs. After successful authentication, your web or mobile app will receive user pool JWT tokens from Amazon Cognito. JWT token can be used in two ways:- You use JWT tokens to retrieve temporary AWS credentials that allow your app to access other AWS services. You create group in user pool with IAM role to access API Gateway, then you can use JWT token (for that group) to access Amazon API Gateway. Cognito Identity Pools (Federated Identity) Identity pool is mainly used for authorization to access AWS services You first authenticate user using User Pools and then exchange token with Identity Pools which further use AWS STS to generate temporary AWS credentials to access AWS Resources. You can provide temporary access to write to S3 bucket using facebook/google login to your mobile app users. Supports guest users AWS Key Management Service (KMS) AWS managed centralized key management service to create, manage and rotate customer master keys (CMKs) for encryption at rest. You can create customer-managed Symmetric (single key for both encrypt and decrypt operations) or Asymmetric (public/private key pair for encrypt/decrypt or sign/verify operations) master keys You can enable automatic master key rotation once per year. Service keeps the older version of master key to decrypt old encrypted data. AWS CloudHSM AWS managed dedicated hardware security model (HSM) in AWS Cloud Enables you to securely generate, store, and manage your own cryptographic keys Integrate with your application using industry-standard APIs, such as PKCS#11, Java Cryptography Extensions (JCE), and Microsoft CryptoNG (CNG) libraries. Use case: Use KMS to create a CMKs in a custom key store and store non-extractable key material in AWS CloudHSM to get a full control on encryption keys AWS Systems Manager Parameter Store is centralized secrets and configuration data management e.g. passwords, database details, and license code Parameter value can be type String (plain text), StringList (comma separated) or SecureString (KMS encrypted data) Use case: Centralized configuration for dev/uat/prod environment to be used by CLI, SDK, and Lambda function Run Command allows you to automate common administrative tasks and perform one-time configuration changes on EC2 instances at scale Session Manager replaces the need for Bastions to access instances in private subnet AWS Secrets Manager Secret Manager is mainly used to store, manage, and rotate secrets (passwords) such as database credentials, API keys, and OAuth tokens. Secret Manager has native support to rotate database credentials of RDS databases - MySQL, PostgreSQL and Amazon Aurora For other secrets such as API keys or tokens, you need to use the lambda for customized rotation function AWS Shield AWS managed Distributed Denial of Service (DDoS) protection service Protect against Layer 3 and 4 (Network and Transport) attacks AWS Shield Standard is automatic and free DDoS protection service for all AWS customers for CloudFront and Route 53 resources AWS Shield Advanced is paid service for enhanced DDoS protection for EC2, ELB, CloudFront, and Route 53 resources AWS WAF Web Application Firewall protects web applications against common web exploits Protect against Layer 7 (HTTP) attack and block common attack patterns, such as SQL injection or Cross-site scripting (XSS) You can deploy WAF on CloudFront, Application Load Balancer, API Gateway and AWS AppSync AWS Firewall Manager Use AWS Firewall Manager to centrally configure and manage AWS WAF rules, AWS Shield Advanced, Network Firewall rules, and Route 53 DNS Firewall Rules across accounts and resources in AWS Organization Use case: Meet Gov regulations to deploy AWS WAF rule to block traffic from embargoed countries across accounts and resources AWS GuardDuty Read VPC Flow Logs, DNS Logs, and CloudTrail events. Apply machine learning algorithms and anomaly detections to discover threats Can protect against CryptoCurrency attacks Amazon Inspector Automated Security Assessment service for EC2 instances by installing an agent in the OS of EC2 instance. Inspector comes with pre-defined rules packages:- Network Reachability rules package checks for unintended network accessibility of EC2 instances Host Assessment rules package checks for vulnerabilities and insecure configurations on EC2 instance. Includes Common Vulnerabilities and Exposures (CVE), Center for Internet Security (CIS) Operating System configuration benchmarks, and security best practices. Amazon Macie Managed service to discover and protect your sensitive data in AWS Macie identify and alert for sensitive data, such as Personally Identifiable Information (PII) in your selected S3 buckets AWS Config Managed service to assess, audit, and evaluate configurations of your AWS resources in multi-region, multi-account You are notified via SNS for any configuration change Integrated with CloudTrail, provide resource configuration history Use case: Customers need to comply with standards like PCI-DSS (Payment Card Industry Data Security Standard) or HIPAA (U.S. Health Insurance Portability and Accountability Act) can use this service to assess compliance of AWS infra configurations Compute EC2 (Elastic Compute Cloud) Infrastructure as a Service (IaaS) - virtual machine on the cloud You must provision nitro-based EC2 instance to achieve 64000 EBS IOPS. Max 32000 EBS IOPS with Non-Nitro EC2. When you restart an EC2 instance, its public IP can change. Use Elastic IP to assign a fixed public IPv4 to your EC2 instance. By default, all AWS accounts are limited to five (5) Elastic IP addresses per Region. Get EC2 instance metadata such as private \u0026amp; public IP from http://169.254.169.254/latest/meta-data and user-defined data from http://169.254.169.254/latest/user-data Place all the EC2 instances in same AZ to reduce the data transfer cost EC2 Hibernate saves the contents of instance memory (RAM) to the Amazon EBS root volume. When the instance restarts, the RAM contents are reloaded, brings it to last running state, also known as pre-warm the instance. You can hibernate an instance only if it\u0026rsquo;s enabled for hibernation and it meets the hibernation prerequisites Use VM Import/Export to import virtual machine image and convert to Amazon EC2 AMI to launch EC2 instances EC2 Instance Types You can choose EC2 instance type based on requirement for e.g. m5.2xlarge has Linux OS, 8 vCPU, 32GB RAM, EBS-Only Storage, Up to 10 Gbps Network bandwidth, Up to 4,750 Mbps IO Operations.\nInstance Class Usage Type Usage Example T, M General Purpose Web Server, Code Repo, Microservice, Small Database, Virtual Desktop, Dev Environment C Compute Optimized High Performance Computing (HPC), Batch Processing, Gaming Server, Scientific Modelling, CPU-based machine learning R, X, Z Memory Optimized In-memory Cache, High Performance Database, Real-time big data analytics F, G, P Accelerated Computing High GPU, Graphics Intensive Applications, Machine Learning, Speech Recognition D, H, I Storage Optimized EC2 Instance Storage, High I/O Performance, HDFS, MapReduce File Systems, Spark, Hadoop, Redshift, Kafka, Elastic Search EC2 Launch Types On-Demand - pay as you use, pay per hour, costly Reserved - up-front payment and reserve for 1 year or 3 year, two classes:- Standard unused instanced can be sold in AWS reserved instance marketplace Convertible can be exchanged for another Convertible Reserved Instance with different instance attributes Scheduled Reserved Instances - reserve capacity that is scheduled to recur daily, weekly, or monthly, with a specified start time and duration, for a one-year term. After you complete your purchase, the instances are available to launch during the time windows that you specified. Spot Instances - up-to 90% discount, cheapest useful for applications with flexible in timing, can handle interruptions and recover gracefully. Spot blocks can also be launched with a required duration, which are not interrupted due to changes in the Spot price Spot Fleet is a collection, or fleet, of Spot Instances, and optionally On-Demand Instances, which attempts to launch the number of Spot and On-Demand Instances to meet the specified target capacity Dedicated Instance - Your instance runs on dedicated hardware provides physical isolation, single-tenant Dedicated Hosts - Your instances run on a dedicated physical server. More visibility of how instances are placed on server. Let you use existing server-bound software licenses and address corporate compliance and regulatory requirements. You have a limit of 20 Reserved instances, 1152 vCPU On-demand standard instances, and 1440 vCPU spot instances. You can increase the limit by submitting the EC2 limit increase request form.\nEC2 Enhanced Networking Elastic Network Interface (ENI) is a virtual network card, which you attach to EC2 instance in same AZ. ENI has one primary private IPv4, one or more secondary private IPv4, one Elastic IP per private IPv4, one public IPv4, one or more IPv6, one or more security groups, a MAC address and a source/destination check flag While primary ENI cannot be detached from an EC2 instance, A secondary ENI with private IPv4 can be detached and attached to a standby EC2 instance if primary EC2 becomes unreachable (failover) Elastic Network Adapter (ENA) for C4, D2, and M4 EC2 instances, Upto 100 Gbps network speed. Elastic Fabric Adapter (EFA) is ENA with additional OS-bypass functionality, which enables HPC and Machine Learning applications to bypass the operating system kernel and communicate directly with EFA device resulting in very high performance and low latency. for M5, C5, R5, I3, G4, metal EC2 instances. Intel 82599 Virtual Function (VF) Interface for C3, C4, D2, I2, M4, and R3 EC2 instances, Upto 10 Gbps network speed. EC2 Placement Groups Strategy Placement groups can span across AZs only, cannot span across regions\nCluster - Same AZ, Same Rack, Low latency and High Network, High-Performance Computing (HPC) Spread - Different AZ, Distinct Rack, High Availability, Critical Applications, Limited to 7 instances per AZ per placement group. Partition - Same or Different AZ, Different Rack (or Partition), Distributed Applications like Hadoop, Cassandra, Kafka etc, Upto 7 Partition per AZ AMI (Amazon Machine Image) Customized image of an EC2 instance, having built-in OS, softwares, configurations, etc. You can create an AMI from EC2 instance and launch a new EC2 instance from AMI. AMI are built for a specific region and can be copied across regions ELB (Elastic Load Balancing) AWS load balancer provides a static DNS name provided for e.g. http://myalb-123456789.us-east-1.elb.amazonaws.com AWS load balancer routes the request to Target Groups. Target group can have one or more EC2 instances, IP Addresses or lambda functions. Three types of ELB - Classic Load Balancer, Application Load Balancer, and Network Load Balancer Application Load Balancer (ALB): Routing based on hostname, request path, params, headers, source IP etc. Support Request tracing, add X-Amzn-Trace-Id header before sending the request to target Client IP and port can be found in X-Forwarded-For and X-Forwarded-Porto header integrate with WAF with rate-limiting (throttle) rules to prevent from DDoS attacks Network Load Balancer (NLB): Handle volatile workloads and extreme low-latency Provide static IP/Elastic IP for the load balancer per AZ allows registering targets by IP address Use NLB with Elastic IP in front of ALBs when there is a requirement of whitelisting ALB Stickiness: works in CLB and ALB. Stickiness and its duration can be set at Target Group level. Doesn\u0026rsquo;t work with NLB ELB Types Supported Protocol Application Load Balancer HTTP, HTTPS, WebSocket Network Load Balancer TCP, UDP, TLS Gateway Load Balancer Thirdparty appliances Classic Load Balancer (old) HTTP, HTTPS, TCP ASG (Auto Scaling Group) Scale-out (add) or scale-in (remove) EC2 instances based on scaling policy - CPU, Network, Custom metric or Scheduled. You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. ASG runs EC2 instances at desired capacity if no policy specified. Minimum and maximum capacity are boundaries within ASG scale-in or scale-out. min \u0026lt;= desired \u0026lt;= max Instances are created in ASG using Launch Configuration (legacy) or Launch Template (newer) You cannot change the launch configuration for an ASG, you must create a new launch configuration and update your ASG with it. You can create ASG that launches both Spot and On-Demand Instances or multiple instance types using launch template, not possible with launch configuration. Dynamic Scaling Policy Target Tracking Scaling - can have more than one policy for e.g. add or remove capacity to keep the average aggregate CPU utilization of your Auto Scaling group at 40% and request count per target of your ALB target group at 1000 for your ASG. If both policies occurs at same time, use largest capacity for both scale-out and scale-in. Simple Scaling - e.g. CloudWatch alarm CPUUtilization (\u0026gt;80%) - add 2 instances Step Scaling - e.g. CloudWatch alarm CPUUtilization (60%-80%)- add 1, (\u0026gt;80%) - add 3 more, (30%-40%) - remove 1, (\u0026lt;30%) - remove 2 more Scheduled Action - e.g. Increase min capacity to 10 at 5pm on Fridays Default Termination Policy - Find AZ with most number of instances, and delete the one with oldest launch configuration, in case of tie, the one closest to next billing hour Cooldown period is the amount of time to wait for previous scaling activity to take effect. Any scaling activity during cooldown period is ignored. Health check grace period is the amount of wait time to check the health status of EC2 instance, which has just came into service to give enough time to warmup. You can add lifecycle-hooks to ASG to perform custom action during:- scale-out to run script, install softwares and send complete-lifecycle-action command to continue scale-in e.g. download logs, take snapshot before termination Lambda FaaS (Function as a Service), Serverless Lambda function supports many languages such as Node.js, Python, Java, C#, Golang, Ruby, etc. Lambda limitations:- execution time can\u0026rsquo;t exceed 900 seconds or 15 min min required memory is 128MB and can go till 10GB with 1-MB increment /temp directory size to download file can\u0026rsquo;t exceed 512 MB max environment variables size can be 4KB compressed .zip and uncompressed code can\u0026rsquo;t exceed 50MB and 250MB respectively Lambda function can be triggered on DynamoDB database trigger, S3 object events, event scheduled from EventBridge (CloudWatch Events), message received from SNS or SQS, etc. Assign IAM Role to lambda function to give access to AWS resource for e.g. create snapshot of EC2, process image and store in S3, etc. Lambda can auto scale in seconds to handle sudden burst of traffic. EC2 require minutes to auto scale. You are charged based on number of requests, execution time and resource (memory) usage. Cheaper than EC2. You can use Lambda@Edge to run code at CloutFront Edge globally You can optionally setup a dead-letter queue (DLQ) with SQS or SNS to forward unprocessed or failed requests payload You can enable and watch the lambda execution logs in CloudWatch Application Integration SQS (Amazon Simple Queue Service) Fully managed service with following specifications for Standard SQS:- can have unlimited number of messages waiting in queue default retention period is 4 days and max 14 days can send message upto 256KB in size unlimited throughput and low latency (\u0026lt;10ms on publish and receive) can have duplicate messages (At least once delivery) can have out-of-order messages (best-effort ordering) Consumer (can be EC2 instance or lambda function) poll the messages in batches (upto 10 messages) and delete them from queue after processing. If don\u0026rsquo;t delete, they stay in Queue and may process multiple times. You should allow Producer and Consumer to send and receive messages from SQS Queue Access Policy Message Visibility Timeout when a message is polled by a consumer, it becomes invisible to other consumers for timeout period. You can setup a Dead-letter queue (DLQ) which is another SQS to keep the messages which are failed to process by consumers multiple times and exceed the Maximum receives threshold in SQS. You use SQS Temporary Queue Client to implement SQS Request-Response System. You can delay message (consumers don\u0026rsquo;t see them immediately) up to 15 minutes (default 0 seconds). You can do it using Delivery Delay configuration at queue level or DelaySeconds parameter at message level. Long polling is when the ReceiveMessageWaitTimeSeconds property of a queue is set to a value greater than zero. Long polling reduces the number of empty responses by allowing Amazon SQS to wait until a message is available before sending a response to a ReceiveMessage request, helps to reduce the cost. You can create SQS of type FIFO which guarantee ordering and exactly once processing with limited throughput upto 300 msg/s without and 3000 msg/s with batching. FIFO queue name must end with suffix .fifo. You can not convert Standard SQS to FIFO SQS. Use case: Cloudwatch has custom metric on =(SQS queue length/Number of EC2 instances), which alarm ASG to auto scale EC2 instances (SQS consumer) based on number of messages in queue. SNS (Amazon Simple Notification Service) PubSub model, where publisher sends the messages on SNS topic and all topic subscribers receive those messages. Upto 100,000 topics and Upto 12,500,000 subscription per topic Subscribers can be: Kinesis Data Firehose, SQS, HTTP, HTTPS, Lambda, Email, Email-JSON, SMS Messages, Mobile Notifications. You can setup a Subscription Filter Policy which is JSON policy to send the filtered messages to specific subscribers. Fan out pattern: SNS topic has multiple SQS subscribers e.g. send all order messages to SNS topic and then send filtered messages based on order status to 3 different application services using SQS. Amazon MQ Amazon managed Apache ActiveMQ Migrate an existing message broker using MQTT protocol to AWS. Storage S3 (Simple Storage Service) S3 Bucket is an object-based storage, used to manage data as objects S3 Object is having:- Value - data bytes of object (photos, videos, documents, etc.) Key - full path of the object in bucket e.g. /movies/comedy/abc.avi Version ID - version object, if versioning is enabled Metadata - additional information S3 Bucket holds objects. S3 console shows virtual folders based on key. S3 is a universal namespace so bucket names must be globally unique (think like having a domain name) https://\u0026lt;bucket-name\u0026gt;.s3.\u0026lt;aws-region\u0026gt;.amazonaws.com or https://s3.\u0026lt;aws-region\u0026gt;.amazonaws.com/\u0026lt;bucket-name\u0026gt; Unlimited Storage, Unlimited Objects from 0 Bytes to 5 Terabytes in size. You should use multi-part upload for Object size \u0026gt; 100MB All new buckets are private when created by default. You should enable public access explicitly. Access control can be configured using Access Control List (ACL) (deprecated) and S3 Bucket Policies (recommended) S3 Bucket Policies are JSON based policy for complex access rules at user, account, folder, and object level Enable S3 Versioning and MFA delete features to protect against accidental delete of S3 Object. Use Object Lock to store object using write-once-read-many (WORM) model to prevent objects from being deleted or overwritten for a fixed amount of time (Retention period) or indefinitely (Legal hold). Each version of object can have different retention-period. You can host static websites on S3 bucket consisting of HTML, CSS, client-side JavaScript, and images. You need to enable Static website hosting and Public access for S3 to avoid 403 forbidden error. Also you need to add CORS Policy to allow cross-origin request. https://\u0026lt;bucket-name\u0026gt;.s3-website[.-]\u0026lt;aws-region\u0026gt;.amazonaws.com Generate a pre-signed URL from CLI or SDK (can\u0026rsquo;t from the web) to provide temporary access to an S3 object to either upload or download object data. You specify expiry (say 5 sec) while generating url:- aws s3 presign s3://mybucket/myobject --expires-in 300 S3 Select or Glacier Select can be used to query subset of data from S3 Objects using SQL query. S3 Objects can be CSV, JSON, or Apache Parquet. GZIP \u0026amp; BZIP2 compression is supported with CSV or JSON format with server-side encryption. using Range HTTP Header in a GET Request to download the specific range of bytes of S3 object, known as Byte Range Fetch You can create S3 event notification to push events e.g. s3:ObjectCreated:* to SNS topic, SQS queue or execute a Lambda function. It is possible that you receive single notification for two writes to a non-versioned object at the same time. Enable versioning to ensure you get all notifications. Enable S3 Cross-Region Replication for asynchronous replication of object across buckets in another region. You must have versioning enabled on both source and destination side. Only new S3 Objects are replicated after you enable them. Enable Server access logging for logging object-level fields object-size, total time, turn around time, and HTTP referrer. Not available with CloudTrail. Use VPC S3 gateway endpoint to access S3 bucket within AWS VPC to reduce the overall data transfer cost. Enable S3 Transfer Acceleration for faster transfer and high throughput to S3 bucket (mainly uploads), Create CloudFront distribution with OAI pointing to S3 for faster-cached content delivery (mainly reads) Restrict the access of S3 bucket through CloudFront only using Origin Access Identity (OAI). Make sure user can\u0026rsquo;t use a direct URL to the S3 bucket to access the file. S3 Storage Class Types Standard: Costly choice for very high availability, high durability and fast retrieval Intelligent Tiering: Uses ML to analyze your Object\u0026rsquo;s usage and move to the appropriate cost-effective storage class automatically Standard-IA: Cost-effective for infrequent access files which cannot be recreated One-Zone IA: Cost-effective for infrequent access files which can be recreated Glacier: Cheaper choice to Archive Data. You must purchase Provisioned capacity, when you require guaranteed Expedite retrievals. Glacier Deep Archive: Cheapest choice for Long-term storage of large amount of data for compliance S3 Storage Class Durability Availability AZ Min. Storage Retrieval Time Retrieval fee S3 Standard (General Purpose) 11 9\u0026rsquo;s 99.99% ≥3 N/A milliseconds N/A S3 Intelligent Tiering 11 9\u0026rsquo;s 99.9% ≥3 30 days milliseconds N/A S3 Standard-IA (Infrequent Access) 11 9\u0026rsquo;s 99.9% ≥3 30 days milliseconds per GB S3 One Zone-IA (Infrequent Access) 11 9\u0026rsquo;s 99.5% 1 30 days milliseconds per GB S3 Glacier 11 9\u0026rsquo;s 99.99% ≥3 90 days Expedite (1-5 mins)Standard (3-5 hrs)Bulk (5-12 hrs) per GB S3 Glacier Deep Archive 11 9\u0026rsquo;s 99.99% ≥3 180 days Standard (12 hrs)Bulk (48 hrs) per GB You can upload files in the same bucket with different Storage Classes like S3 standard, Standard-IA, One Zone-IA, Glacier etc. You can setup S3 Lifecycle Rules to transition current (or previous version) objects to cheaper storage classes or delete (expire if versioned) objects after certain days e.g. transition from S3 Standard to S3 Standard-IA or One Zone-IA can only be done after 30 days. transition from S3 Standard to S3 Intelligent Tiering, Glacier, or Glacier Deep Archive can be done immediately. You can also setup lifecycle rule to abort multipart upload, if it doesn\u0026rsquo;t complete within certain days, which auto delete the parts from S3 buckets associated with multipart upload. Encryption Encryption is transit between client and S3 is achieved via SSL/TLS You can add default encryption at bucket level and also override encryption at file level. Encryption at rest - Server Side Encryption (SSE) SSE-S3 AWS S3 managed keys, use AES-256 algorithm. Must set header: \u0026quot;x-amz-server-side-encryption\u0026quot;:\u0026quot;AES-256\u0026quot; SSE-KMS Envelope Encryption using AWS KMS managed keys. Must set header: \u0026quot;x-amz-server-side-encryption\u0026quot;:\u0026quot;aws:kms\u0026quot; SSE-C Customer provides and manage keys. HTTPS is mandatory. Encryption at rest - Client Side Encryption client encrypts and decrypts the data before sending and after receiving data from S3. To meet PCI-DSS or HIPAA compliance, encrypt S3 using SSE-C and Client Side Encryption Data Consistency S3 provides strong read-after-write consistency for PUTs and DELETEs of objects. PUTs applies to both writes to new objects as well as overwrite existing objects. Updates to a single key are atomic. For example, if you PUT to an existing key from one thread and perform a GET on the same key from a second thread concurrently, you will get either the old data or the new data, but never partial or corrupt data. AWS Athena You can use AWS Athena (Serverless Query Engine) to perform analytics directly against S3 objects using SQL query and save the analysis report in another S3 bucket. Use Case: one-time SQL query on S3 objects, S3 access log analysis, serverless queries on S3, IoT data analytics in S3, etc. Instance Store Instance Store is temporary block-based storage physically attached to an EC2 instance Can be attached to an EC2 instance only when the instance is launched and cannot be dynamically resized Also known as Ephemeral Storage Deliver very low-latency and high random I/O performance Data persists on instance reboot, data doesn\u0026rsquo;t persist on stop or termination EBS (Elastic Block Store) EBS is block-based storage, referred as EBS Volume EBS Volume think like a USB stick Can be attached to only one EC2 instance at a time. Can be detached \u0026amp; attached to another EC2 instance in that same AZ only Can attach multiple EBS volumes to single EC2 instance. Data persist after detaching from EC2 EBS Snapshot is a backup of EBS Volume at a point in time. You can not copy EBS volume across AZ but you can create EBS Volume from Snapshot across AZ. EBS Snapshot can copy across AWS Regions. Facts about EBS Volume encryption:- All data at rest inside the volume is encrypted All data in flight between the volume and EC2 instance is encrypted All snapshots of encrypted volumes are automatically encrypted All volumes created from encrypted snapshots are automatically encrypted Volumes created from unencrypted snapshots can be encrypted at the time of creation EBS supports dynamic changes in live production volume e.g. volume type, volume size, and IOPS capacity without service interruption There are two types of EBS volumes:- SSD for small/random IO operations, High IOPS means number of read and write operations per second, Only SSD EBS Volumes can be used as boot volumes for EC2 HDD for large/sequential IO operations, High Throughput means number of bytes read and write per second EBS Volumes with two types of RAID configuration:- RAID 0 (increase performance) two 500GB EBS Volumes with 4000 IOPS - creates 1000GB RAID0 Array with 8000 IOPS and 1000Mbps throughput RAID 1 (increase fault tolerance) two 500GB EBS Volumes with 4000 IOPS - creates 500GB RAID1 Array with 4000 IOPS and 500Mbps throughput EBS Volume Types Description Usage General Purpose SSD (gp2/gp3) Max 16000 IOPS boot volumes, dev environment, virtual desktop Provisioned IOPS SSD (io1/io2) 16000 - 64000 IOPS, EBS Multi-Attach critical business application, large SQL and NoSQL database workloads Throughput Optimized HDD (st1) Low-cost, frequently accessed, throughput intensive Big Data, Data warehouses, log processing Cold HDD (sc1) Lowest-cost, infrequently accessed Large data with lowest cost EFS (Elastic File System) EFS is a POSIX-compliant file-based storage EFS supports file systems semantics - strong read-after-write consistency and file locking highly scalable - can automatically scale from gigabytes to petabytes of data without needing to provision storage. With burst mode, the throughput increase, as file system grows in size. highly available - stores data redundantly across multiple Availability Zones Network File System (NFS) that can be mounted on and accessed concurrently by thousands of EC2 in multiple AZs without sacrificing performance. EFS file systems can be accessed by Amazon EC2 Linux instances, Amazon ECS, Amazon EKS, AWS Fargate, and AWS Lambda functions via a file system interface such as NFS protocol. Performance Mode: General Purpose for most file system for low-latency file operations, good for content-management, web-serving etc. Max I/O is optimized to use with 10s, 100s, and 1000s of EC2 instances with high aggregated throughput and IOPS, slightly higher latency for file operations, good for big data analytics, media processing workflow Use case: Share files, images, software updates, or computing across all EC2 instances in ECS, EKS cluster FSx for Windows Windows-based file system supports SMB protocol \u0026amp; Windows NTFS supports Microsoft Active Directory (AD) integration, ACLs, user quotas FSx for Lustre Lustre = Linux + Cluster is a POSIX-compliant parallel linux file system, which stores data across multiple network file servers High-performance file system for fast processing of workload with consistent sub-millisecond latencies, up to hundreds of gigabytes per second of throughput, and up to millions of IOPS. Use it for Machine learning, High-performance computing (HPC), video processing, financial modeling, genome sequencing, and electronic design automation (EDA). You can use FSx for Lustre as hot storage for your highly accessed files, and Amazon S3 as cold storage for rarely accessed files. Seamless integration with Amazon S3 - connect your S3 data sets to your FSx for Lustre file system, run your analyses, write results back to S3, and delete your file system FSx for Lustre provides two deployment options:- Scratch file systems - for temporary storage and short-term processing Persistent file systems - for high available \u0026amp; persist storage and long-term processing Database RDS (Relational Database Service) AWS Managed Service to create PostgreSQL, MySQL, MariaDB, Oracle, Microsoft SQL Server, and Amazon Aurora in the cloud Scalability: Upto 5 Read replicas, replication is asynchronous so reads are eventually consistent. Availability use Multi-AZ Deployment, synchronous replication You can create a read replica in a different region of your running RDS instance. You pay for replication cross Region, but not for cross AZ. Automatic failover by switching the CNAME from primary to standby database Enable Password and IAM Database Authentication to authenticate using database password and user credentials through IAM users and roles, works with MySQL and PostgreSQL Enable Enhanced Monitoring to see percentage of CPU bandwidth and total memory consumed by each database process (OS process thread) in DB instance Enable Automated Backup for daily storage volume snapshot of your DB instance with retention-period from 1 day (default from CLI, SDK) to 7 days (default from console) to 35 days (max). Use AWS Backup service for retention-period of 90 days. To encrypt an unencrypted RDS DB instance, take a snapshot, copy snapshot and encrypt new snapshot with AWS KMS. Restore the DB instance with the new encrypted snapshot. Amazon Aurora Amazon fully managed relational database compatible with MySQL and PostgreSQL Provide 5x throughput of MySQL and 3x throughput of PostgreSQL Aurora Global Database is single database span across multiple AWS regions, enable low-latency global reads and disaster recovery from region-wide outage. Use global database for disaster recovery having RPO of 1 second and RTO of 1 minute. Aurora Serverless capacity type is used for on-demand auto-scaling for intermittent, unpredictable, and sporadic workloads. Typically operates as a DB cluster consisting of one or more DB instances and a cluster volume that manages cluster data with each AZ having a copy of volume. Primary DB instance - Only one primary instance, supports both read and write operation Aurora Replica - Upto 15 replicas spread across different AZ, supports only read operation, automatic failover if primary DB instance fails, high availability Connections Endpoints Cluster endpoint - only one cluster endpoint, connects to primary DB instance, only this endpoint can perform write (DDL, DML) operations Reader endpoint - one reader endpoint, provides load-balancing for all read-only connections to read from Aurora replicas Custom endpoint - Upto 5 custom endpoint, read or write from a specified group of DB instances from Cluster, used for specialized workloads to route traffic to high-capacity or low-capacity instances Instance endpoint - connects to specified DB instance directly, generally used to improve connection speed after failover DynamoDB AWS proprietary, Serverless, managed NoSQL database Use to store JSON documents, or session data Use as distributed serverless cache with single-digit millisecond performance Planned Capacity provision WCU \u0026amp; RCU, can enable auto-scaling, good for predictable workloads On-demand Capacity unlimited WCU \u0026amp; RCU, more expensive, good for unpredictable workloads where read \u0026amp; write are less (low throughput) Add DAX (DynamoDB Accelerator) cluster in front of DynamoDB to cache frequently read values and offload the heavy read on hot keys of DynamoDB, prevent ProvisionedThroughputExceededException Enable DynamoDB Streams to trigger events on database and integrate with lambda function for e.g. send welcome email to user added into the table. Use DynamoDB Global Table to serve the data globally. You must enable DynamoDB Streams first to create global table. You can use Amazon DMS (Data Migration Service) to migrate from Mongo, Oracle, MySQL, S3, etc. to DynamoDB ElastiCache AWS Managed Service for Redis or Memcached Use as distributed cache with sub-millisecond performance Elasticache for Redis Offers Multi-AZ with Auto-failover, Cluster mode Use password/token to access data using Redis Auth HIPAA Compliant Elasticache for Memcached Intended for use in speeding up dynamic web applications Not HIPAA Compliant Redshift Columnar Database, OLAP (online analytical processing) supports Massive Parallel Query Execution (MPP) Use for Data Analytics and Data warehousing Integrate with Business Intelligence (BI) tools like AWS Quicksight or Tableau for analytics Use Redshift Spectrum to query S3 bucket directly without loading data in Redshift Amazon Kinesis Amazon Kinesis is a fully managed service for collecting, processing and analyzing streaming real-time data in the cloud. Real-time data generally comes from IoT devices, gaming applications, vehicle tracking, clickstream, etc.\nKinesis Data Streams capture, process and store data streams. Producer can be Amazon Kinesis Agent, SDK, or Kinesis Producer Library (KPL) Consumer can be Kinesis Data Analytics, Kinesis Data Firehose, or Kinesis Consumer Library (KCL) Data Retention period from 24 hours (default) to 365 days (max). Order is maintained at Shard (partition) level. Kinesis Data Firehose loads data streams into AWS data stores such as S3, Amazon Redshift and ElastiSearch. Transform data using lambda functions and store failed data in another S3 bucket. Kinesis Data Analytics analyzes data streams with SQL or Apache Flink Kinesis Video Streams capture, process and store video streams Amazon EMR EMR = Elastic MapReduce Big data cloud platform for processing vast data using open source tools such as Hadoop, Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. EMR can be used to perform data transformation workloads - Extract, transform, load (ETL) Use case: Analyze Clickstream data from S3 using Apache Spark and Hive to deliver more effective ads Neptune Graph Database Use case: high relationship data, social networking data, knowledge graphs (Wikipedia) ElasticSearch Amazon-managed Elastic Search service Integration with Kinesis Data Firehose, AWS IoT, and CloudWatch logs Use case: Search, indexing, partial or fuzzy search Migration AWS Snow Family AWS snow family is used for on-premises large-scale data migration to S3 buckets and processing data at low network locations. You need to install AWS OpsHub software to transfer files from your on-premises machine to snow device. You can not migrate directly to Glacier, you should create S3 first with a lifecycle policy to move files to Glacier. You can transfer to Glacier directly using DataSync. Family Member Storage RAM Migration Type DataSync Migration Size Snowcone 8TB 4GB online \u0026amp; offline yes GBs and TBs Snowball Edge Storage Optimized 80TB 80GB offline no petabyte scale Snowball Edge Compute Optimized 42TB 208GB offline no petabyte scale Snowmobile 100PB N/A offline no exabyte scale AWS Storage Gateway Store gateway is a hybrid cloud service to move on-premises data to the cloud and connect on-premises applications with cloud storage.\nStorage Gateway Protocol Backed by Use Case File Gateway NFS \u0026amp; SMB S3 -\u0026gt; S3-IA, S3 One Zone-IA Store files as object in S3, with a local cache for low-latency access, with user auth using Active Directory FSx File Gateway SMB \u0026amp; NTFS FSx -\u0026gt; S3 Windows or Lustre File Server, integration with Microsoft AD Volume Gateway iSCSI S3 -\u0026gt; EBS Block storage in S3 with backups as EBS snapshots.Use Cached Volume for low-latency and Stored Volume for scheduled backups Tape Gateway iSCSI VTL S3 -\u0026gt;S3 Glacier \u0026amp; Glacier Deep Archive Backup data in S3 and archive in Glacier using tape-based process AWS DataSync AWS DataSync is used for Data Migration at a large scale from On-premises storage systems (using NFS and SMB storage protocol) to AWS storage (like S3, EFS, or FSx for Windows, AWS Snowcone) over the internet AWS DataSync is used to archive on-premises cold data directly to S3 Glacier or S3 Glacier Deep Archive AWS DataSync can migrate data directly to any S3 storage class Use DataSync with Direct Connect to migrate data over secure private network to AWS service associated with VPC endpoint. AWS Backup AWS Backup to centrally manage and automate the backup process for EC2 instances, EBS Volumes, EFS, RDS databases, DynamoDB tables, FSx for Lustre, FSx for Window server, and Storage Gateway volumes Use case: Automate backup of RDS with 90 days retention policy. (Automate backup using RDS directly has max 35 days retention period) Database Migration Service (DMS) DMS helps you to migrate database to AWS with source remaining fully operational during the migration, minimizing the downtime You need to select EC2 instance to run DMS in order to migrate (and replicate) database from source =\u0026gt; target e.g. On-premise =\u0026gt; AWS, AWS =\u0026gt; AWS, or AWS =\u0026gt; On-premise DMS supports both homogenous migrations such as On-premise PostgreSQL =\u0026gt; AWS RDS PostgreSQL and heterogenous migrations such as SQL Server or Oracle =\u0026gt; MySQL, PostgreSQL, Aurora, or Teradata or Oracle =\u0026gt; Amazon Redshift You need to run AWS SCT (Schema Conversion Tool) at source for heterogenous migrations AWS Application Migration Service (MGN) Migrate virtual machines from VMware vSphere, Microsoft Hyper-V or Microsoft Azure to AWS AWS Application Migration Service (new) utilizes continuous, block-level replication and enables cutover windows measured in minutes AWS Server Migration Service (legacy) utilizes incremental, snapshot-based replication and enables cutover windows measured in hours. Networking \u0026amp; Content Delivery Amazon VPC CIDR block — Classless Inter-Domain Routing. An internet protocol address allocation and route aggregation methodology. CIDR block has two components - Base IP (WW.XX.YY.ZZ) and Subnet Mask (/0 to /32) for e.g. 192.168.0.0/32 means 232-32= 1 single IP 192.168.0.0/24 means 232-24= 256 IPs ranging from 192.168.0.0 to 192.168.0.255 (last number can change) 192.168.0.0/16 means 232-16= 65,536 IPs ranging from 192.168.0.0 to 192.168.255.255 (last 2 numbers can change) 192.168.0.0/8 means 232-8= 16,777,216 IPs ranging from 192.0.0.0 to 192.255.255.255 (last 3 numbers can change) 0.0 0.0.0.0/0 means 232-0= All IPs ranging from 0.0.0.0 to 255.255.255.255 (all 4 numbers can change) VPC (Virtual Private Cloud) A virtual network dedicated to your AWS account. VPCs are region specific they do not span across regions Every region comes with default VPC. You can create upto 5 VPC per region. You can assign Max 5 IPv4 CIDR blocks per VPC with min block size /28 = 16 IPs and max size /16 = 65,536 IPs. You can assign Secondary IP CIDR range later if primary CIDR IPs are exhausted. Only private IP ranges are allowed in IPv4 CIDR block - 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16. You VPC CIDR block should not overlap with other VPC networks within your AWS account. Enable DNS resolution and DNS hostnames at VPC, EC2 instances created in that VPC will be assigned a domain name address VPC Peering VPC peering connect two VPC over a direct network route using private IP addresses Instances on peered VPCs behave just like they are on the same network Must have no overlapping CIDR Blocks VPC peering connection are not transitive i.e. VPC-A peering VPC-B and VPC-B peering to VPC-C doesn\u0026rsquo;t mean VPC-A peering VPC-C Route tables must be updated in both VPC that are peered so that instances can communicate Can connect one VPC to another in same or different region. VPC peering in the different region called VPC inter-region peering Can connect one VPC to another in same or different AWS account Subnet A range of IP addresses in your VPC Each subnet is tied to one Availability Zone, one Route Table, and one Network ACL You assign one CIDR block per Subnet within CIDR range of your VPC. Should not overlap with other Subnet\u0026rsquo;s CIDR in your VPC. AWS reserve 5 IP address (first 4 and last 1) from CIDR block in each Subnet. For e.g. If you need 29 IP addresses to use, your should choose CIDR /26 = 64 IP and not /27 = 32 IP, since 5 IPs are reserved and can not use. Enable Auto assign public IPv4 address in public subnets, EC2 instances created in public subnets will be assigned a public IPv4 address If you have 3 AZ in a region then you create a total of 6 subnets - 3 private subnets (1 in each AZ) and 3 public subnets (1 in each AZ) for multi-tier and highly-available architecture. API gateway and ALB reside in the public subnet, EC2 instances, Lambda, Database resides in private subnet. Route Table A set of rules, called routes, are used to determine where network traffic is directed. Each subnet in your VPC must be associated with a route table. A subnet can only be associated with one route table at a time You can associate multiple subnets with the same route table For e.g. you create 4 subnets in your VPC where 2 subnets associated with one route table with no internet access rules known as private subnets and another 2 subnets are associated with another route table with internet access rules known as public subnets Each Route table route has Destination like IPs and Target like local, IG, NAT, VPC endpoint etc. public subnet is a subnet that\u0026rsquo;s associated with a route table having rules to connect to internet using Internet Gateway. private subnet is a subnet that\u0026rsquo;s associated with a route table having no rules to connect to internet using Internet Gateway. When our Subnets connected to the Private Route Table need access to the internet, we set up a NAT Gateway in the public Subnet. We then add a rule to our Private Route Table saying that all traffic looking to go to the internet should point to the NAT Gateway. Internet Gateway Internet Gateway allows AWS instances public subnet access to the internet and accessible from the internet Each Internet Gateway is associated with one VPC only, and each VPC has one Internet Gateway only (one-to-one mapping) NAT Gateway NAT Gateway allows AWS instances in private subnet access to the internet but not accessible from the internet NAT Gateway (latest) is a managed service that launches redundant instances within the selected AZ (can survive failure of EC2 instance) NAT Instances (legacy) are individual EC2 instances. Community AMIs exist to launch NAT Instances. Works same as NAT Gateway. You can only have 1 NAT Gateway inside 1 AZ (cannot span AZ). You should create a NAT Gateway in each AZ for high availability so that if a NAT Gateway goes down in one AZ, instances in other AZs are still able to access the internet. NAT Gateway resides in public subnet. You must allocate Elastic IP to NAT Gateway. You must add NAT Gateway in private subnet route table with Destination 0.0.0.0/0 and Target nat-gateway-id NAT Gateways are automatically assigned a public IP address NAT Gateway/Instances works with IPv4 NAT Gateway cannot be shared across VPC NAT Gateway cannot be used as Bastions whereas Nat Instance can Bastion Host Bastian Host is an individual small EC2 instance in public subnet. Community AMIs exist to launch Bastion Host. Bastian Host are used to access AWS instances in private subnet with private IPv4 address via SSH at port 22 Egress Only Internet Gateway Works same as NAT Gateway, but for IPv6 Egress Only means - outgoing traffic only IPv6 are public by default. Egress Only Internet Gateway allows IPv6 instances in private subnet access to the internet but accessible from internet Network ACL Network Access Control List is commonly known as NACL Optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. VPCs comes with a modifiable default NACL. By default, it allows all inbound and outbound traffic. You can create custom NACL. By default, each custom network ACL denies all inbound and outbound traffic until you add rules. Each subnet within a VPC must be associated with only 1 NACL If you don\u0026rsquo;t specify, auto associate with default NACL. If you associate with a new NACL, auto-remove previous association Apply to all instances in associated subnet Support both Allow and Deny rules Stateless means explicit rules for inbound and outbound traffic. return traffic must be explicitly allowed by rules Evaluate rules in number order, starting with lowest numbered rule. NACL rules have number(1 to 32766) and higher precedence to lowest number for e.g. #100 ALLOW \u0026lt;IP\u0026gt; and #200 DENY \u0026lt;IP\u0026gt; means IP is allowed Each network ACL also includes a rule with rule number as asterisk *. If any of the numbered rule doesn\u0026rsquo;t match, it\u0026rsquo;s denies the traffic. You can\u0026rsquo;t modify or remove this rule. Recommended creating numbered rules in increments (for example, increments of 10 or 100) so that you can insert new rules where you need to later on. You can block a single IP address using NACL, which you can\u0026rsquo;t do using Security Group Security Group Control inbound and outbound traffic at EC2 instance level Support Allow rules only. All traffic is deny by default unless a rule specifically allows it. Stateful means return traffic is automatically allowed, regardless of any rules When you first create a security group, It has no inbound rule means denies all incoming traffic and one outbound rule that allows all outgoing traffic. You can specify a source in the security group rule to be an IP range, A specific IP (/32), or another security group One security group can be associated with multiple instances across multiple subnets One EC2 instance can be associated with multiple Security Groups and rules are permissive (instead of restrictive). Meaning if you have one security group which has no Allow and you add an allow in another then it will Allow Evaluate all rules before deciding whether to allow traffic Transit gateway is used to create transitive VPC peer connections between thousands of VPCs hub-and-spoke (star) connection Support IP Multicast (not supported by any other AWS service) Use as gateway at Amazon side in VPN connection, not at customer side Can be attached to - one or more VPCs, AWS Direct Connect gateway, VPN Connection, peering connection to another Transit gateway VPC Flow Logs Allows you to capture IP traffic information in-and-out of Network Interfaces within your VPC You can turn on Flow Logs at VPC, Subnet or Network Interface level VPC Flow logs can be delivered to S3 or CloudWatch logs. Query VPC flow logs using Athena on S3 or CloudWatch logs insight VPC Flow logs have - Log Version version, AWS Account Id account-id, Network Interface Id interface-id, Source IP address and port srcaddr \u0026amp; srcport, destination IP address and port dstaddr \u0026amp; dstport VPC Flow logs contain source and destination IP addresses (not hostnames) IPv6 are all public addresses, all instances with IPv6 are publicly accessible. for private ranges, we still use IPv4. You can not disable IPv4. If you enable IPv6 for VPC and subnets, then your EC2 instance would get private IPv4 and public IPv6 Cost nothing: VPCs, Route Tables, NACLs, Internet Gateway, Security Groups, Subnets, VPC Peering Cost money: NAT Gateway, VPC Endpoints, VPN Gateway, Customer Gateway VPC endpoints VPC endpoints allow your VPC to connect to other AWS services privately within the AWS network Traffic between your VPC and other services never leaves the AWS network Eliminates the need for an Internet Gateway and NAT Gateway for instances in public and private subnets to access the other AWS services through public internet. There are two types of VPC endpoints:- Interface endpoint are Elastic Network Interfaces (ENI) with a private IP address. They serve as an entry point for traffic going to most of the AWS services. Interface endpoints are provided by AWS PrivateLink and have an hourly fee and per GB usage cost. Gateway endpoint is a gateway that is a target for a specific route in your route table, use to destined for a supported AWS service. Currently supports only Amazon S3 and DynamoDB. Gateway endpoints are free If EC2 instance wants to access S3 bucket or DynamoDB in different region privately within AWS network then we first need VPC inter-region peering to connect VPC in both regions and then use VPC gateway endpoint for S3 or DynamoDB. AWS PrivateLink is VPC interface endpoint service to expose a particular service to 1000s of VPCs cross-accounts AWS ClassicLink (deprecated) to connect EC2-classic instances privately to your VPC AWS VPN AWS Site-to-Site VPN connection is created to communicate between your remote network and Amazon VPC over the internet VPN connection: A secure connection between your on-premises equipment and your Amazon VPCs. VPN tunnel: An encrypted link where data can pass from the customer network to or from AWS. Each VPN connection includes two VPN tunnels which you can simultaneously use for high availability. Customer gateway: An AWS resource that provides information to AWS about your customer gateway device. Customer gateway device: A physical device or software application on the customer side of the Site-to-Site VPN connection. Virtual private gateway: The VPN concentrator on the Amazon side of the Site-to-Site VPN connection. You use a virtual private gateway or a transit gateway as the gateway for the Amazon side of the Site-to-Site VPN connection. Transit gateway: A transit hub that can be used to interconnect your VPCs and on-premises networks. You use a transit gateway or virtual private gateway as the gateway for the Amazon side of the Site-to-Site VPN connection. AWS Direct Connect Establish a dedicated private connection from On-premises locations to the AWS VPC network. Can access public resources (S3) and private (EC2) on the same connection Provide 1GB to 100GB/s network bandwidth for fast transfer of data from on-premises to Cloud Not an immediate solution, because it takes a few days to establish a new direction connection AWS VPN AWS Direct Connect Over the internet connection Over the dedicated private connection Configured in minutes Configured in days low to modest bandwidth high bandwidth 1 to 100 GB/s Amazon API Gateway Serverless, Create and Manage APIs that act as a front door for back-end systems running on EC2, AWS Lambda, etc. API Gateway Types - HTTP, WebSocket, and REST Allows you to track and control the usage of API. Set throttle limit (default 10,000 req/s) to prevent being overwhelmed by too many requests and returns 429 Too Many Request error response. It uses the bucket-token algorithm where the burst size is the max bucket size. For a throttle limit of 10000 req/s and a burst of 5000 requests, if 8000 requests are coming in the first millisecond, then 5000 are served immediately and throttle the rest 3000 in the one-second period. Caching can be enabled to cache your API response to reduce the number of API calls and improve latency API Gateway Authentication IAM Policy is used for authentication and authorization of AWS users and leverage Sig v4 to pass IAM credential in the request header Lambda Authorizer (formerly Custom Authorizer) use lambda for OAuth, SAML or any other 3rd party authentication Cognito User Pools only provide authentication. Manage your own user pool (can be backed by Facebook, Google, etc.) Amazon CloudFront It\u0026rsquo;s a Content Delivery Network (CDN) that uses AWS edge locations to cache and deliver cached content (such as images and videos) CloudFront can cache data from Origin for e.g. S3 bucket using OAI (Origin Access Identity) and S3 bucket policy EC2 or ALB if they are public and security group allows Origin Access Identity (OAI) can be used to restrict the content from S3 origin to be accessible from CloudFront only supports Geo restriction (Geo-Blocking) to whitelist or blacklist countries that can access the content supports Web download distribution (static, dynamic web content, video streaming) and RTMP Streaming distribution (media files from Adobe media server using RTMP protocol) You can generate a Signed URL (for a single file and RTMP streaming) or Signed Cookie (for multiple files) to share content with premium users integrates with AWS WAF, a web application firewall to protect from layer 7 attacks Objects are removed from the cache upon expiry (TTL), by default 24 hours. Invalidate the Object explicitly for web distribution only with the cost associated, which removes the object from CloudFront cache. Otherwise, you can change the object name, and versioning to serve new content. Amazon Route 53 AWS Managed Service to create DNS Records (Domain Name System) Browser cache the resolved IP from DNS for TTL (time to live) Expose public IP of EC2 instances or load balancer Domain Registrar If you want to use Route 53 for domains purchased from 3rd party websites like GoDaddy. AWS - You need to create a Hosted Zone in Route 53 GoDaddy - update the 3rd party registrar NS (name server) records to use Route 53. Private Hosted Zone is used to create an internal (intranet) domain name to be used within Amazon VPC. You can then add some DNS records and routing policies for that internal domain. That internal domain is accessible from EC2 instances or any other resource within VPC. DNS Record: Type CNAME points hostname to any other hostname. Only works with subdomains e.g. something.mydomain.com A or AAAA (Alias) points hostname to an AWS Resource like ALB, API Gateway, CloudFront, S3 Bucket, Global Accelerator, Elastic Beanstalk, VPC interface endpoint etc. Works with both root-domain and subdomains e.g. mydomain.com. AAAA is used for IPv6 addresses. DNS Record: Routing Policy Simple to route traffic to specific IP using a single DNS record. Also allows you to return multiple IPs after resolving DNS. Weighted to route traffic to different IPs based on weights (between 0 to 255) e.g. create 3 DNS records for weights 70, 20, and 10. Latency to route traffic to different IPs based on AWS regions nearest to the client for low-latency e.g. create 3 DNS records with region us-east-1, eu-west-2, and ap-east-1 Failover to route traffic from Primary to Secondary in case of failover e.g. create 2 DNS records for primary and secondary IP. It is mandatory to create health check for both IP and associate to record. Geolocation to route traffic to specific IP based on user geolocation (select Continent or Country). Should also create default (select Default location) policy in case there\u0026rsquo;s no match on location. Geoproximity to route traffic to specific IP based on user geolocation and bias value. Positive bias (1 to 99) for more traffic and negative bias (-1 to -99) for less traffic. You can control the traffic from specific geolocation using bias value. Multivalue Answer to return up to 8 healthy IPs after resolving DNS e.g. create 3 DNS records with an associated health check. Acts as client-side Load Balancer, expect a downtime of TTL, if an EC2 becomes unhealthy. DNS Failover active-active failover when you want all resources to be available the majority of the time. All records have the same name, same type, and same routing policy such as weighted or latency active-passive failover when you have active primary resources and standby secondary resources. You create two records - primary \u0026amp; secondary with failover routing policy AWS Global Accelerator Global Service Global Accelerator improves the performance of your application globally by lowering latency and jitter, and increasing throughput as compared to the public internet. Use Edge locations and AWS internal global network to find an optimal pathway to route the traffic. First, you create a global accelerator, which provisions two anycast static IP addresses. Then you register one or more endpoints with Global Accelerator. Each endpoint can have one or more AWS resources such as NLB, ALB, EC2, S3 Bucket or Elastic IP. You can set the weight to choose how much traffic is routed to each endpoint. Within the endpoint, global accelerator monitors health checks of all AWS resources to send traffic to healthy resources only Management \u0026amp; Governance Amazon CloudWatch CloudWatch is used to collect \u0026amp; track metrics, collect \u0026amp; monitor log files, and set alarms of AWS resources like EC2, ALB, S3, Lambda, DynamoDB, RDS etc. By default, CloudWatch will aggregate and store the metrics at Standard 1-minute resolution. You can set max high-resolution at 1 second. CloudWatch dashboard can include graphs from different AWS accounts and regions CloudWatch has the following EC2 instance metrics - CPU Utilization %, Network Utilization, and Disk Read Write. You need to set up a custom metric for Memory Utilization, Disk Space Utilization, SwapUtilization etc. You need to install CloudWatch Logs Agent on EC2 to collect custom metrics and logs on CloudWatch You can terminate or recover EC2 instances based on CloudWatch Alarm You can schedule a Cron job using CloudWatch Events Any AWS service should have access to log:CreateLogGroup, log:CreateLogStream, and log:PutLogEvents actions to write logs to CloudWatch AWS CloudTrail CloudTrail provides audit and event history of all the actions taken by any user, AWS service, CLI, or SDK across AWS infrastructure. CloudTrail is enabled (applied) by default for all regions CloudTrail logs can be sent to CloudWatch logs or S3 bucket Use case: check in the CloudTrail if any resource is deleted from AWS without anyone\u0026rsquo;s knowledge. AWS CloudFormation Infrastructure as Code (IaC). Enable modeling, provisioning, and versioning of your entire infrastructure in a text (.YAML) file Create, update, or delete your stack of resources using CloudFormation template as a JSON or YAML file CloudFormation template has a following components:- Resources: AWS resources declared in the template (mandatory) Parameters: input values to be passed in the template at stack creation time Mappings: Static variables in the template Outputs: Output which you want to see once the stack is created e.g. return ElasticIP address after attaching to VPC, return DNS of ELB after stack creation. Conditionals: List of conditions to perform resource creation Metadata Template helpers: References and Functions Allows DependsOn attribute to specify that the creation of a specific resource follows another Allows DeletionPolicy attribute to be defined for resources in the template retain to preserve resources like S3 even after stack deletion snapshot to backup resources like RDS after stack deletion Supports Bootstrap scripts to install packages, files and services on the EC2 instances by simply describing them in the template automatic rollback on error feature is enabled, by default, which will cause all the AWS resources that CF created successfully for a stack up to the point where an error occurred to be deleted AWS CloudFormation StackSets allow you to create, update or delete CloudFormation stacks across multiple accounts, regions, OUs in AWS organization with a single operation. Using CloudFormation itself is free, underlying AWS resources are charged Use case: Use to set up the same infrastructure in different environments e.g. SIT, UAT and PROD. Use to create DEV resources every day in working hours and delete them later to lower the cost AWS Elastic Beanstalk Platform as a Service (PaaS) Makes it easier for developers to quickly deploy and manage applications without thinking about underlying resources Automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling and application health monitoring You can launch an application with the following pre-configured platforms:- Apache Tomcat for Java applications, Apache HTTP Server for PHP and Python applications Nginx or Apache HTTP Server for Node.js applications Passenger or Puma for Ruby applications Microsoft IIS 7.5 for .NET applications Single and Multi Container Docker You can also launch an environment with the following environment tier:- An application that serves HTTP requests runs in a web server environment tier. A backend environment that pulls tasks from an Amazon Simple Queue Service (Amazon SQS) queue runs in a worker environment tier. It costs nothing to use Elastic Beanstalk, only the resources it provisions e.g. EC2, ASG, ELB, and RDS etc. supports custom AMI to be used supports multiple running environments for development, staging and production, etc. supports versioning and stores and tracks application versions over time allowing easy rollback to prior version AWS ParallelCluster Deploy and manage High-Performance Computing (HPC) clusters on AWS using a simple text file You have full control of the underlying resources. AWS ParallelCluster is free, and you pay only for the AWS resources needed to run your applications. You can configure HPC cluster with Elastic Fabric Adapter (EFA) to get OS-bypass capabilities for low-latency network communication AWS Step Functions (SF) Build serverless visual workflow to orchestrate your Lambda functions You write state machine in declarative JSON, you write a decider program to separate activity steps from decision steps. AWS Simple Workflow Service (SWF) Code runs on EC2 (not Serverless) Older service. Use SWF when you need external signal signals to intervene in the process or need the child process to pass value to the parent process, otherwise, use Step Functions for new applications. AWS Organization Global service to manage multiple AWS accounts e.g. accounts per department, per cost center, per environment (dev, test, prod) Pricing benefits from aggregated usage across accounts. Consolidate billing across all accounts - single payment method Organization has multiple Organization Units (OUs) (or accounts) based on department, cost center or environment, OU can have other OUs (hierarchy) Organization has one master account and multiple member accounts You can apply Service Control Policies (SCPs) at OU or account level, SCP is applied to all users and roles in that account SPC Deny take precedence over Allow in the full OU tree of an account e.g. allowed at the account level but deny at OU level is = deny Master account can do anything even if you apply SCP To merge Firm_A Organization with Firm_B Organization Remove all member accounts from Firm_A organization Delete the Firm_A organization Invite Firm_A master account to join Firm_B organization as a member account AWS Resource Access Manager (RAM) helps you to create your AWS resources once, and securely share across accounts within OUs in AWS Organization. You can share Transit Gateways, Subnets, AWS License Manager configurations, Route 53 resolver rules, etc. One account can share resources with another individual account within AWS organization with the help of RAM. You must enable resource sharing at AWS Organization level. AWS Control Tower integrated with AWS Organization helps you to quickly setup and configure a new AWS account with best practices from base called as landing zone AWS OpsWorks Provide managed instances of Chef and Puppet configuration management services, which help to configure and operate applications in AWS. Configuration as Code - OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across EC2 instances using Code. OpsWork Stack let you model your application as a stack containing different layers, such as load balancing, database, and application server. AWS Glue Serverless, fully managed ETL (extract, transform, and load) service AWS Glue Crawler scan data from data-source such as S3 or DynamoDB table, determine the schema for data, and then creates metadata tables in the AWS Glue Data Catalog. AWS Glue provides classifiers for CSV, JSON, AVRO, XML or database to determine the schema for data Containers ECR (Elastic Container Registry) is Docker Hub to pull and push Docker images, managed by Amazon. ECS (Elastic Container Service) ECS is a container management service to run, stop, and manage Docker containers on a cluster ECS Task Definition where you configure task and container definition Specify ECS Task IAM Role for ECS task (Docker container instance) to access AWS services like S3 bucket or DynamoDB Specify Task Execution IAM Role i.e. ecsTaskExecutionRole for EC2 (ECS Agent) to pull docker images from ECR, make API calls to ECS service and publish container logs to Amazon CloudWatch on your behalf Add container by specifying docker image, memory, port mappings, health-check, etc. You can create multiple ECS Task Definitions - e.g. one task definition to run a web application on the Nginx server and another task definition to run a microservice on Tomcat. ECS Service Definition where you configure cluster, ELB, ASG, task definition, and number of tasks to run multiple similar ECS Task, which deploys a docker container on EC2 instance. One EC2 instance can run multiple ECS tasks. Amazon EC2 Launch Type: You manage EC2 instances of ECS Cluster. You must install ECS Agent on each EC2 instance. Cheaper. Good for predictable, long-running tasks. ECS Agent The agent sends information about the EC2 instance\u0026rsquo;s current running tasks and resource utilization to Amazon ECS. It starts and stops tasks whenever it receives a request from Amazon ECS Fargate Launch Type: Serverless, EC2 instances are managed by Fargate. You only manage and pay for container resources. Costlier. Good for variable, short-running tasks EKS (Elastic Kubernetes Service) is managed Kubernetes clusters on AWS Cheat Sheet AWS Service Keywords Security Amazon CloudWatch Metrics, Logs, Alarms AWS CloudTrail Audit Events AWS WAF Firewall, SQL injection, Cross-site scripting (XSS), Layer 7 attacks AWS Shield DDoS attack, Layer 3 \u0026amp; 4 attacks Amazon Macie Sensitive Data, Personally Identifiable Information (PII) Amazon Inspector EC2 Security Assessment, Unintended Network Accessibility Amazon GuardDuty Analyze VPC Flow Logs, Threat Detection AWS VPN Online Network Connection, Long-term Continuous transfer, Low to Moderate Bandwidth AWS Direct Connect Private Secure Dedicated Connection, Long-term Continuous transfer, High Bandwidth Application Integration Amazon SNS Serverless, PubSub, Fan-out Amazon SQS Serverless, Decoupled, Queue, Fan-out Amazon MQ ActiveMQ Amazon SWF Serverless, Simple Workflow Service, Decoupled, Task Coordinator, Distributed \u0026amp; Background Jobs AWS Step Functions (SF) Orchestrate / Coordinate Lambda functions and ECS containers into a workflow AWS OpsWork Chef \u0026amp; Puppet Storage EBS Block Storage Volume for EC2 EFS Network File System for EC2, Concurrent access Amazon S3 Serverless, Block Storage - Photos \u0026amp; Videos, Website Hosting Amazon Athena Query data in S3 using SQL AWS Snow Family Offline Data Migration, Petabyte to exabyte Scale AWS DataSync Online Data Transfer, Immediate One-time transfer AWS Storage Gateway Hybrid Storage b/w On-premise and AWS Compute AWS Lambda Serverless, FaaS Database Amazon RDS Relational Database - PostgreSQL, MySQL, MariaDB, Oracle, and SQL Server Amazon Aurora Relational Database - Amazon-Owned Amazon DynamoDB Serverless, key-value NoSQL Database - Amazon-Owned Amazon DocumentDB Document Database, JSON documents - MongoDB Amazon Neptune Graph Database, Social Media Relationship Amazon Timestream Time Series Database Amazon Redshift Columnar Database, Analytics, BI, Parallel Query Amazon Elasticache Redis and Memcached, In-memory Cache Amazon EMR Elastic MapReduce, Big Data - Apache Hadoop, Spark, Hive, Hbase, Flink, Hudi Amazon Elasticsearch Service Elasticsearch, ELK Microservices Elastic Container Registry (ECR) Docker image repository, DockerHub Elastic Container Service (ECS) Docker container management system AWS Fargate Serverless ECS AWS X-Ray Trace Request, Debug Developer AWS CodeCommit like GitHub, Git-based Source Code Repository AWS CodeBuild like Jenkins CI, Code Compile, Build \u0026amp; Test AWS CodeDeploy Code deployment to EC2, Fargate, and Lambda AWS CodePipeline CICD pipelines, Rapid Software or Build Release AWS CloudShell CLI, Browser-based Shell AWS Elastic Beanstalk PaaS, Quick deploy applications - Java-Tomcat, PHP/Python-Apache HTTP Server, Node.js-Nginx Amazon Workspaces Desktop-as-a-Service, Virtual Windows or Linux Desktops Amazon AppStream 2.0 Install Applications on Virtual Desktop and access it from Mobile, Tab or Remote Desktop through Browser AWS CloudFormation Infrastructure as Code, Replicate Infrastructure AWS Certificate Manager (ACM) Create, renew, deploy SSL/TLS certificates to CloudFront and ELB AWS Migration Hub Centralized Tracking on the progress of all migrations across AWS AWS Glue Data ETL (extract, transform, load), Crawler, Data Catalogue AWS AppSync GraphQL Amazon Elastic Transcoder Media (Audio, Video) converter Important Ports Protocol/Database Port FTP 21 SSH 22 SFTP 22 HTTP 80 HTTPS 443 RDP 3389 NFS 2049 PostgresSQL 5432 MySQL 3306 MariaDB 3306 Aurora 3306 or 5432 Oracle RDS 1521 MSSQL Server 1433 White Papers Disaster Recovery RPO - Recovery Point Objective - How much data is lost to recover from a disaster e.g. last 20 min data lost before the disaster RTO - Recovery Time Objective - How much downtime require to recover from a disaster e.g. 1-hour downtime to start disaster recovery service Disaster Recovery techniques (RPO \u0026amp; RTO reduces and the cost goes up as we go down) Backup \u0026amp; Restore – Data is backed up and restored, with nothing running Pilot light – Only minimal critical service like RDS is running and the rest of the services can be recreated and scaled during recovery Warm Standby – Fully functional site with minimal configuration is available and can be scaled during recovery Multi-Site – Fully functional site with identical configuration is available and processes the load Use Amazon Aurora Global Database for RDS and DynamoDB Global Table for NoSQL databases for disaster recovery with stringent RPO of 1 second and RTO of 1 minute. 5 Pillars of the AWS Well-Architected Framework The 5 Pillars of AWS Well-Architected Framework are as follows:-\nOperational Excellence Use AWS Trusted Advisor to get recommendations on AWS best practices, optimize AWS infrastructure, improve security and performance, reduce costs, and monitor service quotas Use Serverless application API Gateway (Front layer for auth, cache, routing), Lambda (Compute), DynamoDB (Database), DAX (Caching), S3 (File Storage) and Cognito User Pools (Auth), CloudFront (Deliver content globally), SES (Send email), SQS \u0026amp; SNS (Publish \u0026amp; Notify events) Security Use AWS Shield and AWS WAF to prevent network, transport and application layer security attacks Reliability Performance Efficiency Cost Optimization Use AWS Cost Explorer to forecast daily or monthly cloud costs based on ML applied to your historical cost Use AWS Budget to set yearly, quarterly, monthly, daily or fixed cost or usage budget for AWS services and get notified when actual or forecast cost or usage exceeds budget limit. Use AWS Saving Plans to get a discount in exchange for usage commitment e.g. $10/hour for one-year or three-year period. AWS offers three types of Savings Plans – 1. Compute Savings Plans apply to usage across Amazon EC2, AWS Lambda, and AWS Fargate. 2. EC2 Instance Savings Plans apply to EC2 usage, and 3. SageMaker Savings Plans apply to SageMaker usage. Use VPC Gateway endpoint to access S3 and DynamoDB privately within AWS network to reduce data transfer cost Use AWS Organization for consolidated billing and aggregated usage benefits across AWS accounts Disclaimer I have created the exam notes after watching many training videos and solving tons of practice exam questions. I found that some information given in training videos and practice exams were not correct (or should say not updated). Amazon AWS is growing very fast, they keep enhancing their services with loads of new features as well as introducing new AWS services.\nI have personally verified each and every statement in this exam notes from AWS services documentation and FAQs at the time of writing these notes. Please comment and share if you find any statement has become stale or irrelevant after updates in AWS services. Let\u0026rsquo;s make this exam notes helpful and trustful for all AWS aspirants!\n","permalink":"https://codingnconcepts.com/aws/aws-certified-solutions-architect-associate/","tags":["AWS Certification","Certification"],"title":"AWS Certified Solutions Architect Associate (SAA-C03) Exam Notes"},{"categories":["Cloud"],"contents":"Comprehensive list of Free 200+ Hashicorp Terraform Associate (003) Exam Questions curated for cracking the exam in first attempt.\nDisclaimer: Hashicorp is a protected Brand. These exam questions are neither endorsed by nor affiliated with Hashicorp Terraform. These are not the Terraform official exam questions/dumps. These questions are created from the web content of the Hashicorp Terraform. These questions cover all the objectives and topics of the Terraform Associate (003) official exam and once you go through these questions and their concepts, you are more than ready to crack the exam in first attempt.\nFull Exam Questions Consider buying the full set of questions with answers and explanation from below links:-\nHashicorp Terraform Associate (003) Exam Questions with Answers and Explanation at a very reasonable price.\n● All the Questions have duly verified answers supported with explanations and official weblinks of Hashicorp Terraform.\n● All the Questions are unique and categorized by the topics of the Terraform Associate (003) official exam.\n● All the Questions marked with * are important from the exam perspective, give more attention!\n● Refer to the Hashicorp Terraform Associate (003) Exam Guide to revise the topics.\nPlease leave your feedback in the comment section or contact us from main menu.\nWish you all the best for the Hashicorp Terraform Associate (003) Exam!\nArea 1: Infrastructure as Code (IaC) What are the benefits of Infrastructure as Code tools like Terraform? ⬜ Manage and track infrastructure\n⬜ Automate infrastructure changes ⬜ Support Reusable configuration\n⬜ Collaboration using VCS (version control system)\n⬜ All of the above\nWhich one do you think is an advantage of using Terraform as the IaC tool? ⬜ Terraform can manage infrastructure on multiple cloud platforms.\n⬜ Terraform\u0026rsquo;s state allows you to track resource changes throughout your deployments.\n⬜ You can commit your configurations to version control to safely collaborate on infrastructure.\n⬜ All of the above\nWhat are the benefits of using an \u0026ldquo;Infrastructure as Code\u0026rdquo; tool like Terraform? (select three) ⬜ An imperative approach, offers flexibility to describe each step you want IaC to follow to reach the desired state\n⬜ You can commit your configurations to version control to safely collaborate on infrastructure ⬜ The human-readable configuration language helps you write infrastructure code quickly ⬜ Terraform can manage infrastructure on multiple cloud platforms\nWhich is NOT a benefit of using an IaC tool like Terraform?? ⬜ You can commit your configurations to version control to safely collaborate on infrastructure ⬜ You can manage infrastructure on multiple cloud platforms ⬜ Infrastructure code is easy to maintain and understand without any Cloud knowledge ⬜ The human-readable configuration language helps you write infrastructure code quickly\nWhich of the following methods, used to provision resources into a public cloud, demonstrates the concept of Infrastructure as Code (IaC)? ⬜ curl commands manually run from a terminal\n⬜ A sequence of REST requests you pass to a public cloud API endpoint\n⬜ A script that contains a series of public cloud CLI commands\n⬜ A series of commands you enter into a public cloud console\nDoes Terraform provide support for multiple cloud deployments? ⬜ True ⬜ False\nHow do you describe the Terraform precisely? ⬜ A programming language\n⬜ An infrastructure as code (IaC) tool ⬜ A cloud provider ⬜ A containerization tool\nTerraform is an Infrastructure as Code (IaC) tool that is? (select two) ⬜ Imperative ⬜ Declarative ⬜ Cloud-agnostic ⬜ Cloud-native\nWhich of the following describes the Terraform use case? ⬜ Provisioning infrastructure in both Azure and AWS ⬜ Enforce compliance and governance policies before any infrastructure changes\n⬜ Deploy a Kubernetes cluster and manage its resources\n⬜ All of the above\nThe activity of code provision and configuring your initial infrastructure in the Infrastructure Lifecycle is called? ⬜ Day 0 ⬜ Day 1 ⬜ Sprint 0\n⬜ Sprint 1\nWhich characteristic of an IaC tool ensures the same infrastructure state, if the same code is applied multiple times? ⬜ Consistent ⬜ Repeatable ⬜ Idempotent ⬜ Predictable\nYou have recently started a new job at a retailer as an engineer. As part of this new role, you have been tasked with evaluating multiple outages that occurred during peak shopping time during the holiday season. Your investigation found that the team is manually deploying new compute instances and configuring each compute instance manually. This has led to inconsistent configuration between each compute instance. How would you solve this using Infrastructure as Code? ⬜ Implement a ticketing workflow that makes engineers submit a ticket before manually provisioning and configuring a resource\n⬜ Implement a checklist that engineers can follow when configuring compute instances\n⬜ Replace the compute instance type with a larger version to reduce the number of required deployments\n⬜ Implement a provisioning pipeline that deploys infrastructure configurations committed to your version control system following code reviews\nArea 2: Terraform Cloud Which of these are features of Terraform Cloud? (Choose two.) ⬜ A web-based user interface (UI)\n⬜ Remote state storage\n⬜ Automated infrastructure deployment visualization\n⬜ Automatic backups\nWhat are the features of Terraform Cloud? (Choose three.) ⬜ Remote State Management ⬜ Remote Terraform Execution\n⬜ Private Module Registry\n⬜ Terraform Linting\nWhich of the following features are available in Terraform Cloud Free Tier? (Choose three.) ⬜ Private Module Registry ⬜ VCS Connection ⬜ Team Management ⬜ Application-level Logging ⬜ Single Sign-On (SSO)\nWhich of the following features are NOT available in Terraform Cloud Free Tier? (Choose three.) ⬜ Audit Logging\n⬜ No-code provisioning\n⬜ Remote Terraform execution\n⬜ Private Module Registry\n⬜ Team Management\nWhich of the following Terraform features is only available in the Enterprise edition? ⬜ Application-level Logging\n⬜ Single Sign-On (SSO)\n⬜ Drift Detection\n⬜ Audit Logging\nYour company requires a self-hosted instance of Terraform Cloud with features like audit-logging and SAML single sign-on. Which Terraform plan you will choose? ⬜ Terraform Cloud Free Tier ⬜ Terraform Cloud Standard\n⬜ Terraform Cloud Plus ⬜ Terraform Enterprise\nVCS Connection in Terraform Cloud provides additional features such as automatically initiate Terraform runs when changes are committed. Which of the following VCS providers are supported by Terraform Cloud? ⬜ Github.com\n⬜ Gitlab.com ⬜ Bitbucket Cloud\n⬜ CVS Version Control\nWhich of the following is NOT a correct statement about version control system (VCS) in Terraform Cloud? ⬜ Terraform Cloud can automatically initiate Terraform runs when changes are committed to the specified branch in VCS. ⬜ Terraform Cloud makes code review easier by automatically predicting how pull requests will affect infrastructure. ⬜ Github and Bitbucket VCS providers are supported by Terraform Cloud ⬜ One VCS provider can be configured at a time in Terraform Cloud\nIn a Terraform Cloud workspace linked to a version control repository, speculative plan runs start automatically when you merge or commit changes to version control. ⬜ True\n⬜ False\nArea 3: Terraform Configuration How are the Core Terrform Workflow steps played out by an Individual Practitioner? ⬜ Plan, Write, Apply\n⬜ Write, Plan, Apply\n⬜ Apply, Write, Plan\n⬜ Apply, Plan, Write\nWhich file is typically used to define resources in a Terraform configuration? ⬜ main.tf\n⬜ terraform.tfvars\n⬜ variables.tf\n⬜ outputs.tf\nWhat is the type of the block in below Terraform configuration? resource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { ami = \u0026#34;abc123\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; } ⬜ resource ⬜ aws_instance ⬜ t2.micro ⬜ instance_type\nWhich of the following is NOT a valid Terraform block type? ⬜ provider\n⬜ resource\n⬜ output\n⬜ module\n⬜ data\n⬜ bucket\nWhat is the workflow for deploying new infrastructure with Terraform? ⬜ Terraform plan to import the current infrastructure to the state file, make code changes, and terraform apply to update the infrastructure.\n⬜ Write a Terraform configuration, run terraform show to view proposed changes, and terraform apply to create new infrastructure.\n⬜ Terraform import to import the current infrastructure to the state file, make code changes, and terraform apply to update the infrastructure.\n⬜ Write a Terraform configuration, run terraform init, run terraform plan to view planned infrastructure changes, and terraform apply to create new infrastructure.\nTerraform configuration can be written in which language? (select two) ⬜ XML\n⬜ JSON\n⬜ Hashicorp Configuration Language (HCL)\n⬜ YAML\nTerraform language is widely expressed in Hashicorp Configuration Language (HCL) syntax. Which other syntax is supported? ⬜ XML ⬜ JSON ⬜ YAML\n⬜ Protobuf\nHow many Indent spaces are required at each nested level in the Terraform language style convention? ⬜ 1\n⬜ 2 ⬜ 3 ⬜ 4\nWhich of the statements accurately describes the Terraform language? (select two) ⬜ Terraform is declarative Infrastructure as Code provisioning language based on Hashicorp Configuration Language, or optionally YAML. ⬜ Terraform is declarative Infrastructure as Code provisioning language based on Hashicorp Configuration Language, or optionally JSON. ⬜ Code in the Terraform language is stored in plain text files with the .tf file extension, or optionally with the .tf.json file extension\n⬜ Code in the Terraform language is stored in plain text files with the .tf file extension, or optionally with the .tf.yml file extension\nTerraform binary is available to download and install on different operating systems and platforms. Which of the following operating systems are supported? ⬜ macOS ⬜ Solaris ⬜ FreeBSD ⬜ Windows ⬜ All of the above\nWhich Terraform files should be excluded from Git commit by adding them in .gitingnore file? (select two) ⬜ output.tf\n⬜ terraform.tfstate\n⬜ terraform.tfvars\n⬜ variables.tf\nWhen does Terraform create the .terraform.lock.hcl file? ⬜ After your first terraform plan\n⬜ After your first terraform apply\n⬜ After your first terraform init\n⬜ When you enable state locking\nYou want to deploy multiple subnets within a VPC in AWS and looking for a for loop-like expression to iterate over a list of subnet CIDR blocks. Which feature of Terraform configuration you can use? ⬜ terraform import ⬜ dynamic blocks ⬜ splat expression ⬜ dynamic backend\nHow do you describe a dynamic block? ⬜ It requests that Terraform read from a given data source and export the result under the given local name\n⬜ It declares a resource of a given type with a given local name ⬜ It iterates over a given complex value, and generates a nested block for each element of that complex value\n⬜ It exports a value exported by a module or configuration\nWhat is one disadvantage of using dynamic blocks in Terraform? ⬜ Dynamic blocks can construct repeatable nested blocks.\n⬜ Terraform will run more slowly.\n⬜ They cannot be used to loop through a list of values.\n⬜ They make configuration harder to read and understand.\nWhich one of the following is considered as a Terraform plugin? ⬜ Terraform provisioner ⬜ Terraform module ⬜ Terraform provider ⬜ Terraform registry\nTerraform remembers the compatible version of dependencies such as providers and modules through dependency lock file. What is the name of that file? ⬜ .terraform.lock.hcl ⬜ .terraform.lock.tf ⬜ .dependency.lock.hcl ⬜ .dependency.lock.tf\nTerraform Core is a statically-compiled binary written in the ______ programming language. ⬜ Java\n⬜ C#\n⬜ Python\n⬜ Go\nTerraform builds a dependency graph from the Terraform configurations. Which is NOT a correct step of building a Graph? ⬜ Resources are mapped to provisioners if they have any defined.\n⬜ Resources are mapped to providers.\n⬜ Resources nodes are added to the graph from the configuration.\n⬜ Resources are not added to the graph that are no longer present in the configuration but are present in the state file.\nWhich type of block fetches or computes information for use elsewhere in a Terraform configuration? ⬜ provider\n⬜ resource\n⬜ local\n⬜ data\nArea 4: Terraform State What is the purpose of state in Terraform? ⬜ State stores variables and lets you quickly reuse existing code.\n⬜ State maps real world resources to your configuration and keeps track of metadata.\n⬜ State lets you enforce resource configurations that relate to compliance policies.\n⬜ State codifies the dependencies of related resources.\nWhich of the following best describes a Terraform state file? ⬜ A file that contains a list of available Terraform providers\n⬜ A file that stores the current state of infrastructure managed by Terraform\n⬜ A file that contains a list of Terraform modules used in a configuration\n⬜ A file that stores the output of a Terraform plan\nThe terraform.tfstate file always matches your currently built infrastructure. ⬜ True\n⬜ False\nTerraform can inspect the real-world resources on every run to validate the desired state when the state file is missing. ⬜ True ⬜ False\nUsernames and passwords referenced in the Terraform code, even as variables, will end up in plain text in the state file. ⬜ True\n⬜ False\nTerraform encrypts sensitive values stored in your state file. ⬜ True\n⬜ False\nYou injected some secrets from variables into your Terraform configuration. What happens after you run the terraform apply command and they are loaded into state? ⬜ They are shown in clear-text.\n⬜ They are shown as their referenced variables.\n⬜ They are shown as encrypted values.\n⬜ They are omitted from state.\nWhich provider authentication method prevents credentials from being stored in the state file? ⬜ Using environment variables\n⬜ Specifying the login credentials in the provider block\n⬜ Setting credentials as Terraform variables\n⬜ None of the above\nWhat is the name of the default file where Terraform stores the state? Type your answer in the field provided. The text field is not case-sensitive and all variations of the correct answer are accepted.\n⬜ terraform.tfstate\nWhere is the default location that Terraform stores its state in? ⬜ The current working directory in which Terraform was run.\n⬜ At the users root directory.\n⬜ In the same location that Terraform is installed. E.g. /usr/bin/terraform\n⬜ In ~/.terraform.d/plugins\nWhat is the recommended way to implement Terraform\u0026rsquo;s state for larger teams? ⬜ By configuring a remote backend such that multiple teams can work in tandem and know which resources are being created and destroyed.\n⬜ By sticking the state in a cloud instance, and having team members SSH into the instance to work on their configuration files.\n⬜ Having your state synced to a github repo for members to compare to.\n⬜ By using the daily standup you are a part of sot that you can share changes to the state file.\nYou are part of a large DevOps team using the current version of Terraform, and there can be multiple changes going on to your terraform files across the company. What would you do to ensure that the state file is locked when you run terraform apply? ⬜ Add the -lock=True flag to the command.\n⬜ Nothing, terraform will manage the locking by itself.\n⬜ First run terraform plan to lock in your proposed changes. Then run terraform apply to commit them.\n⬜ Add the -state-lock=True to the command.\nIf supported by your backend, Terraform automatically locks the state for all operations that could write state. What is the purpose of state locking? ⬜ Prevents others from committing Terraform code that could override your updates.\n⬜ Locks colleagues from making manual changes to the managed infrastructure\n⬜ This prevents others from acquiring the lock and potentially corrupting your state. ⬜ Ensures the state file cannot be moved after the initial terraform apply\nWhich are forbidden actions when the Terraform state file is locked? (Choose three) ⬜ terraform destroy\n⬜ terraform fmt\n⬜ terraform state list\n⬜ terraform apply\n⬜ terraform plan\n⬜ terraform validate\nA developer on your team is going to tear down an existing deployment managed by Terraform and deploy a new one. However, there is a server resource named aws_instance.ubuntu[1] you would like to keep. What command should they use to tell Terraform to stop managing that specific resource? ⬜ terraform destroy aws_instance.ubuntu[1]\n⬜ terraform apply rm aws_instance.ubuntu[1]\n⬜ terraform plan rm aws_instance.ubuntu[1]\n⬜ terraform state rm aws_instance.ubuntu[1]\nYou manage the AWS cloud resources using Terraform and want to destroy all dev resources to save cost. However, your team member request you to keep the Amazon Aurora dev instance running. How can you destroy all cloud resources without impacting the database instance? ⬜ run terraform state rm command to remove the database instance from terraform state before running terraform destroy command ⬜ take a snapshot of database, run terraform destroy, and then recreate database by restoring the snapshot ⬜ run a terraform destroy, modify configuration file to include only database instance, and then run terraform apply ⬜ manually delete the other resource from AWS\nYou manage the AWS cloud resources using Terraform and want to follow new naming standard for the local name within resource block. However, you don\u0026rsquo;t want Terraform to replace the object after changing your configuration files. How can you change the local name from data-bucket to prod-aws-s3-bucket in the following resource block:- resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;data-bucket\u0026#34; { bucket = \u0026#34;prod-data-bucket\u0026#34; tags = { Name = \u0026#34;prod-data-bucket\u0026#34; Environment = \u0026#34;prod\u0026#34; } } After renaming the local name of the resource block, what command would you run to update the local name while ensuring Terraform does not replace the existing resource?\n⬜ terraform apply -refresh-only ⬜ terrafrorm apply -replace aws_s3_bucket.data-bucket\n⬜ terraform state mv aws_s3_bucket.data-bucket aws_s3_bucket.prod-aws-s3-bucket\n⬜ terraform state rm aws_s3_bucket.data-bucket\nWhich Terrafrom command only reads the state file and won\u0026rsquo;t cause any refresh or modification of state file? ⬜ terraform state list\n⬜ terraform plan\n⬜ terraform apply\n⬜ terraform destroy\nArea 5: Terraform Commands Before you can use a new backend or HCP Terraform/Terraform Cloud integration, you must first execute terraform init. ⬜ True\n⬜ False\nterraform init retrieves and caches the configuration for all remote modules. ⬜ True\n⬜ False\nWhen should you run terraform init? ⬜ Every time you run terraform apply\n⬜ Before you start coding a new Terraform project\n⬜ After you run terraform plan for the first time in a new Terraform project and before you run terraform apply\n⬜ After you start coding a new Terraform project and before you run terraform plan for the first time\nYou have just developed a new Terraform configuration for two virtual machines with a cloud provider. You would like to create the infrastructure for the first time. Which Terraform command should you run first? ⬜ terraform apply\n⬜ terraform show\n⬜ terraform plan\n⬜ terraform init\nWhat command does Terraform require the first time you run it within a configuration directory? ⬜ terraform import\n⬜ terraform init\n⬜ terraform plan\n⬜ terraform workspace\nWhich command need to run before Terraform plan or apply if you have added a new resource block in the configuration from different provider? ⬜ terraform refresh ⬜ terraform eval ⬜ terraform init ⬜ terraform validate\nHow can you check out the configuration from version control and initialize a directory? ⬜ terraform init -from-module={MODULE-SOURCE}\n⬜ terraform init -source={PATH}\n⬜ terraform init {PATH}\n⬜ terraform init -plugin-dir={PATH}\nAs a developer, you want to ensure your plugins are up-to-date with the latest versions. Which Terraform command should you use? ⬜ terraform refresh -upgrade\n⬜ terraform apply -upgrade\n⬜ terraform providers -upgrade\n⬜ terraform init -upgrade\nWhenever you add a new module to a configuration, Terraform must install the module before it can be used. Which two commands will install and update modules? (select two) ⬜ terraform plan ⬜ terraform refresh ⬜ terraform init ⬜ terraform get\nWhich command is used to create an execution plan in Terraform? ⬜ terraform plan\n⬜ terraform apply ⬜ terraform init ⬜ terraform validate\nBy default, when running terraform plan, what files are scanned? ⬜ All *.tf files in the current directory.\n⬜ Only files in the .terraform directory\n⬜ Only files you specify with the -file-path flag.\n⬜ All files on your hard drive.\nWhat happens when you execute terraform plan? ⬜ Imports all of your existing cloud provider resources to the state\n⬜ Compares your Terraform code and local state file to the remote state file in a cloud provider and determines if any changes need to be made\n⬜ Installs all providers and modules referenced by configuration\n⬜ Refreshes your state, then compares your state file to your Terraform configuration and creates an execution plan if changes need to be made\nHow can terraform plan aid in the development process? ⬜ Initializes your working directory containing your Terraform configuration files\n⬜ Validates your expectations against the execution plan without permanently modifying state\n⬜ Formats your Terraform configuration files\n⬜ Reconciles Terraform\u0026rsquo;s state against deployed resources and permanently modifies state using the current status of deployed resources\nWhich command is used to apply changes to infrastructure in Terraform? ⬜ terraform destroy\n⬜ terraform apply\n⬜ terraform plan\n⬜ terraform validate\nWhich command is used to destroy infrastructure in Terraform? ⬜ terraform destroy\n⬜ terraform apply\n⬜ terraform plan\n⬜ terraform validate\nWhich Terraform command can be used to delete all of your managed infrastructure? (select two) ⬜ terraform init -destroy\n⬜ terraform apply -destroy ⬜ terraform destroy ⬜ terraform plan -destroy\nIf different teams are working on the same configuration. How do you make files to have consistent formatting? ⬜ terraform fmt\n⬜ terraform apply\n⬜ terraform plan ⬜ terraform validate\nWhich of these commands makes your code more human-readable? ⬜ terraform validate\n⬜ terraform output\n⬜ terraform show\n⬜ ``terraform fmt`\nWhat Terraform command modifies a HashiCorp Configuration Language (HCL) file to adhere to the recommended spacing rules for HCL files? ⬜ terraform fmt\n⬜ terraform apply ⬜ terraform plan ⬜ terraform validate\nYour teammate is worried that if they run the terraform fmt command on their current directory, it will change their configuration files too much. What flag do you tell them to pass into the command such that they can see the differences? ⬜ -diff\n⬜ -check\n⬜ -refresh\n⬜ -list=True\nYour organization want to ensure that all Terraform configuration files follows the Terraform language format and style convention. You have main.tf in the current directory, which calls modules stored in module directory. Which command can be used to format all the configuration files in the current directory and all subdirectories? ⬜ terraform fmt -check -recursive ⬜ terraform fmt -diff\n⬜ terraform fmt -check\n⬜ terraform fmt -list=True\nWhich Terraform command checks that your configuration syntax is correct? ⬜ terraform validate ⬜ terraform show ⬜ terraform init ⬜ terraform fmt\nHow would you get the JSON output of the terraform validate command? ⬜ terraform validate -json\n⬜ terraform validate json\n⬜ terraform validate -output=json ⬜ terraform json validate\nDoes the terraform validate command connect to remote APIs and state when being ran? ⬜ No it does not.\n⬜ Only if configured to do so on the backend.\n⬜ If the -remote=True is set, yes it does.\n⬜ If there are providers set, it will attempt to.\nWhich command provides an interactive command-line console for evaluating and experimenting with expressions? ⬜ terraform show ⬜ terraform eval\n⬜ terraform console\n⬜ terraform exec\nWhich command is used to extract the value of an output variable from the state file? ⬜ terraform exec ⬜ terraform show ⬜ terraform output\n⬜ terraform state\nYou have defined the values for your variables in the file terraform.tfvars, and saved it in the same directory as your Terraform configuration. Which of the following commands will use those values when creating an execution plan? ⬜ terraform plan\n⬜ terraform plan -var-file=terraform.tfvars ⬜ All of the above ⬜ None of the above\nWhich two steps are required to provision new infrastructure in the Terraform workflow? (Choose Two.) ⬜ terraform init\n⬜ terraform import ⬜ terraform apply ⬜ terraform validate ⬜ terraform destroy\nIt is necessary to run terraform plan command before terraform apply in the Terraform Workflow. ⬜ True\n⬜ False\nWhich command(s) adds existing resources in a public cloud into Terraform state? ⬜ terraform init\n⬜ terraform plan\n⬜ terraform refresh\n⬜ terraform import\n⬜ All of these\nSomeone created few resources manually using the Azure console. You have company policy to manage all infrastructure using Terraform. How can you manage manually deployed resource using Terraform without impacting other resources? ⬜ run a terraform get to get the manually deployed resources that are not under Terraform management\n⬜ delete the resources created manually using the Azure console and add these resource in terraform configuration, then run terraform apply\n⬜ use terraform import to import existing resources under Terraform management\n⬜ resources created outside Terraform cannot be managed by Terraform\nHow does terraform import run? ⬜ As a part of terraform init\n⬜ As a part of terraform plan\n⬜ As a part of terraform refresh\n⬜ By an explicit call\n⬜ All of the above\nWhat must be provided with the terraform import command for Terraform to successfully import resources? ⬜ Resource ID, resource type, and the resource name.\n⬜ The resource name.\n⬜ The full resource ARN.\n⬜ Only resource Id\nYour team is currently using Terraform to provision the infrastructure in AWS. Someone in your team has manually created an EC2 instance in AWS. You now want to bring this EC2 instance under Terraform management. What must you do before running the terraform import to manage these resource using Terraform? ⬜ manually write a resource block in Terraform configuration file that match the resource you want to import ⬜ manually modify the Terraform state file to add the new resources so Terraform will have a record of the resources to be managed\n⬜ shut down or stop using the resources being imported so no changes are inadvertently missed\n⬜ run terraform apply -refresh-only to ensure that the state file has the latest information for existing resources.\nA user wants to list all resources which are deployed using Terraform. How can this be done? ⬜ terraform state show\n⬜ terraform state list\n⬜ terraform show\n⬜ terraform show list\nWhich terraform state subcommand will give you all of the resources in your state? ⬜ list\n⬜ show\n⬜ refresh\n⬜ apply\nWhich command does not cause Terraform to refresh its state? ⬜ terraform state list\n⬜ terraform plan\n⬜ terraform apply\n⬜ terraform destroy\nYou have deployed a new webapp with a public IP address on a cloud provider. However, you did not create any outputs for your code. What is the best method to quickly find the IP address of the resource you deployed? ⬜ Run terraform output ip_address to view the result\n⬜ In a new folder, use the terraform_remote_state data source to load in the state file, then write an output for each resource that you find the state file\n⬜ Run terraform state list to find the name of the resource, then terraform state show to find the attributes including public IP address\n⬜ Run terraform destroy then terraform apply and look for the IP address in stdout\nA user wants to see the resource block for resource aws_instance having name foo in state file. How can this be done? ⬜ terraform show aws_instance.foo\n⬜ terraform show aws_instance foo ⬜ terraform state show aws_instance.foo ⬜ terraform state show aws_instance foo\nWhich of the following command provides the JSON reprentation of the state? ⬜ terraform state -json\n⬜ terraform state show -json\n⬜ terraform show -json\n⬜ terraform show state -json\nWhy would you use the terraform taint command? ⬜ When you want to force Terraform to destroy a resource on the next apply\n⬜ When you want to force Terraform to destroy and recreate a resource on the next apply\n⬜ When you want Terraform to ignore a resource on the next apply\n⬜ When you want Terraform to destroy all the infrastructure in your workspace\nWhy would you use the -replace flag with terraform apply? ⬜ You want Terraform to ignore a resource on the next apply\n⬜ You want Terraform to destroy all the infrastructure in your workspace\n⬜ You want to force Terraform to destroy a resource on the next apply\n⬜ You want to force Terraform to destroy and recreate a resource on the next apply\nThe command terraform.taint is deprecated in v0.15.2, which command you should use intead? ⬜ terraform apply -replace\n⬜ terraform plan -replace\n⬜ terraform apply -taint\n⬜ terraform plan -taint\nYou want Terrafrorm to destory and recreate a particular AWS instance test_123 that was deployed with a bunch of other AWS instances. Which command can be used to accomplish this task without modifying the Terraform code? ⬜ terraform apply -replace=aws_instance.test_123 ⬜ terraform apply -destroy=aws_instance.test_123\n⬜ terraform state recreate aws_instance.test_123 ⬜ terraform state destroy aws_instance.test_123\nYou have an EC2 instance that is acting up in the cloud. It handles a relatively light ephemeral workload, so it can be restarted/destroyed with no repercussions. What full command would you use to target only this instance for recreation? ⬜ terraform apply -replace=aws_instance.{INSTANCE_NAME} ⬜ terraform apply -replace aws_instance\n⬜ terraform apply -replace {INSTANCE_NAME}\n⬜ terraform destroy --target=aws.instance{INSTANCE_NAME} and terraform apply\nWhat is not processed when running a terraform refresh? ⬜ State file\n⬜ Configuration file ⬜ Credentials ⬜ Cloud provider\nWhich of the following Terraform commands will automatically refresh the state unless supplied with additional flags or arguments? (Choose Two.) ⬜ terraform plan\n⬜ terraform state\n⬜ terraform apply\n⬜ terraform validate\n⬜ terraform output\nThe command terraform refresh is deprecated in v0.15.4, which command is recommended to use instead? (Choose Two.) ⬜ terraform apply -refresh-only ⬜ terraform plan -refresh-only ⬜ terraform apply -refresh\n⬜ terraform plan -refresh\nWhich of the following command will give you an opportunity to review the changes that Terraform has detected during refresh? (Choose Two.) ⬜ terraform apply -refresh-only -auto-approve ⬜ terraform apply -refresh-only ⬜ terraform refresh\n⬜ terraform plan -refresh-only\nWhat is terraform apply -refresh-only intended to detect? ⬜ Terraform configuration code changes\n⬜ Corrupt state files\n⬜ State file drift\n⬜ Empty state files\nWhat does Terraform not reference when running a terraform apply -refresh-only? ⬜ Credentials\n⬜ Terraform resource definitions in configuration files\n⬜ Cloud provider\n⬜ State file\nWhat happens when you apply Terraform configuration? (Choose Two.) ⬜ Terraform makes any infrastructure changes defined in your configuration.\n⬜ Terraform gets the plugins that the configuration requires.\n⬜ Terraform updates the state file with any configuration changes it made. ⬜ Terraform corrects formatting errors in your configuration.\n⬜ Terraform destroys and recreates all your infrastructure from scratch.\nWhich flag is used to find more information about a Terraform command? For example, you need additional information about how to use the plan command. You would type: terraform plan _____. Type your answer in the field provided. The text field is not case-sensitive and all variations of the correct answer are accepted.\n⬜ -h\n⬜ -help\n⬜ --help\nWhich flag would you add to terraform plan to save the execution plan to a file? You would type: terraform plan _____ Type your answer in the field provided. The text field is not case-sensitive and all variations of the correct answer are accepted.\n⬜ -out=FILENAME\nYou just added a new set of resources to your configuration and would only like to see them when you run your terraform plan command. What flag do you specify when running the terraform plan command to only see their plans? ⬜ -target={resources}\n⬜ -refresh=True\n⬜ -state={new_state_file}\n⬜ -lock=True\nYou have a simple Terraform configuration containing one virtual machine (VM) in a cloud provider. You run terraform apply and the VM is created successfully. What will happen if you delete the VM using the cloud provider console, and run terraform apply again without changing any Terraform code? ⬜ Terraform will remove the VM from state file\n⬜ Terraform will report an error\n⬜ Terraform will not make any changes\n⬜ Terraform will recreate the VM\nYou have a simple Terraform configuration containing one virtual machine (VM) in a cloud provider. You run terraform apply and the VM is created successfully. What will happen if you run terraform apply again immediately afterwards without changing any Terraform code? ⬜ Terraform will terminate and recreate the VM\n⬜ Terraform will create another duplicate VM\n⬜ Terraform will apply the VM to the state file\n⬜ Nothing\nYou have a Terraform configuration that defines a single virtual machine with no references to it. You have run terraform apply to create the resource, and then removed the resource definition from your Terraform configuration file. What will happen when you run terraform apply in the working directory again? ⬜ A. Nothing\n⬜ B. Terraform will destroy the virtual machine\n⬜ C. Terraform will error\n⬜ D. Terraform will remove the virtual machine from the state file, but the resource will still exist\nYou have multiple team members collaborating on infrastructure as code (IaC) using Terraform, and want to apply formatting standards for readability.How can you format Terraform HCL (HashiCorp Configuration Language) code according to standard Terraform style convention? ⬜ Run the terraform fmt command during the code linting phase of your CI/CD process\n⬜ Designate one person in each team to review and format everyone\u0026rsquo;s code\n⬜ Manually apply two spaces indentation and align equal sign \u0026ldquo;=\u0026rdquo; characters in every Terraform file ( .tf)\n⬜ Write a shell script to transform Terraform files using tools such as AWK, Python, and sed\nTerraform uses parallelism to walk through the dependency graph during terraform apply to provision the resources. What is the default value of parallelism? ⬜ 100\n⬜ 10\n⬜ 5\n⬜ 1\nSay you wanted to increase the number of operations that terraform is concurrently using to create your resources. Which command would you run, with what specific flag, to accomplish this? (Choose 2 answers) ⬜ terraform apply\n⬜ -parallelism={NUMBER-OF-OPERATIONS}\n⬜ terraform init\n⬜ -concurrent={NUMBER-OF-OPERATIONS}\nLately you noticed that your Terraform jobs are failing in your CI/CD pipeline. The error that is coming back mentions something about hitting a rate limit. Without altering the time that the builds are ran, what could you pass into the terraform apply command to slow your operations down? ⬜ -parallelism={NUMBER_OF_OPERATIONS}\n⬜ -concurrent={NUMBER_OF_OPERATIONS}\n⬜ -rate-limit={NUMBER_OF_OPERATIONS}\n⬜ -refresh=False\nWhich Terraform command is used to manually unlock the state, if unlocking failed? ⬜ terraform unlock ⬜ terraform force-unlock ⬜ terraform manual-unlock ⬜ terrafom state unlock\nWhen should you use the force-unlock command? ⬜ You see a status message that you cannot acquire the lock\n⬜ You have a high priority change\n⬜ Automatic unlocking failed\n⬜ You apply failed due to a state lock\nArea 6: Terraform Backend You can access state stored in a local backend by using the terraform_remote_state data source. ⬜ True\n⬜ False\nWhere in your Terraform configuration do you specify a state backend? ⬜ The provider block\n⬜ The terraform block\n⬜ The resource block\n⬜ The data source block\nWhich backend does the Terraform CLI use by default? ⬜ Terraform Cloud\n⬜ Consul\n⬜ Remote\n⬜ Local\nWhat does the default local Terraform backend store? ⬜ *.tfplan files\n⬜ Terraform binary\n⬜ Provider plugins\n⬜ terraform.tfstate file\nWhere does the Terraform local backend store its state? ⬜ In the terraform.tfstate file\n⬜ In the .terraform directory\n⬜ In the /tmp directory\n⬜ In the .terraform.lock.hcl file\nWhat two configuration variables are available to a default local backend? (Choose 2 answers) ⬜ path\n⬜ workspace_dir\n⬜ working_dir ⬜ path_dest\nWhat is NOT True about the Terraform backend? ⬜ A backend is where Terraform stores its state data files.\n⬜ By default, Terraform uses a backend called local, which stores state as a local file on disk.\n⬜ A terraform configuration can only provide one backend block. ⬜ A backend block can refer to named values (like input variables, locals, or data source attributes).\nHow do you supply remaining arguments to a partial backend configuration? (Choose 2 answers) ⬜ Specify file terraform init -backend-config=PATH ⬜ Specify key/value pairs terraform init -backend-config=\u0026quot;KEY=VALUE\u0026quot;\n⬜ Environment variable export TF_VAR_key=value ⬜ Set variable terraform init -var=\u0026quot;KEY=VALUE\u0026quot;\nHow is the Terraform remote backend different than other state backends such as S3, Consul, etc.? ⬜ It can execute Terraform runs on dedicated infrastructure on premises or in Terraform Cloud\n⬜ It doesn\u0026rsquo;t show the output of a terraform apply locally\n⬜ It is only available to enterprise customers\n⬜ All of the above\nYou are a part of a growing Cloud Infrastructure team. Your boss asks you to transition the team off of local backends, and onto remote backends. Within Terraform, what do you do to use the S3 buckets as a remote backend? (Choose 2 answers) terraform { backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;mybucket\u0026#34; key = \u0026#34;path/to/my/key\u0026#34; region = \u0026#34;us-east-1\u0026#34; } } ⬜ Specify the key to store state file inside the S3 bucket\n⬜ Make sure Terraform gets AWS IAM permission on target backend bucket and stored state file\n⬜ Export your AWS API key to TF_BACKEND_KEY\n⬜ Encrypt your AWS buckets with SSE.\nYour team has decided to move the Terraform state to remote backend for collaboration so that multiple people can access it. Your co-worker has decided to migrate Terraform state to a remote backend. Accessing remote state requires access credentials, where should you store these credentials? (select two) ⬜ in a variable\n⬜ in credentials file ⬜ hardcode in the configuration ⬜ in environment variables\nIf you want to migrate the backend configuration from \u0026ldquo;consul\u0026rdquo; to \u0026ldquo;s3\u0026rdquo;, you have to remove all the resources managed by \u0026ldquo;consul\u0026rdquo; remote backend first. ⬜ False\n⬜ True\nWhich of the following is a type of backend configurable in Terraform? ⬜ local ⬜ standard ⬜ enhanced ⬜ advanced\nYour DevOps team is currently using the local backend for your Terraform configuration. You would like to move to a remote backend to store the state file in a central location. Which of the following backends would not work? ⬜ Terraform Cloud ⬜ Git ⬜ Artifactory ⬜ Amazon S3\nAll standard backend types support remote state storage, state locking, and encryption at rest? ⬜ True ⬜ False\nWhich of the following Terraform backend type supports state locking? ⬜ consul\n⬜ kubernetes\n⬜ s3\n⬜ All of the above\nWhich of the following backend type doesn\u0026rsquo;t support state locking? ⬜ local\n⬜ s3\n⬜ remote\n⬜ artifactory\nWhich of the following backend type doesn\u0026rsquo;t support remote state storage? ⬜ remote\n⬜ Terraform Cloud\n⬜ github ⬜ artifactory\nYou have decided to migrate the Terraform state to a remote s3 backend. You have added the backend block in the Terraform configuration. Which command you should run to migrate the state? terraform { backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;terraform-s3-bucket-name\u0026#34; key = \u0026#34;s3 key path\u0026#34; region = \u0026#34;us-west-1\u0026#34; } } ⬜ terraform init\n⬜ terraform push\n⬜ terraform apply\n⬜ terraform plan\nOnce you configure a new Terraform backend with a terraform block, which command(s) should you use to migrate the state file? ⬜ terraform destroy, then terraform apply ⬜ terraform init ⬜ terraform push ⬜ terraform apply\nArea 7: Terraform Provisioners Provisioners add a considerable amount of complexity and uncertainty to Terraform usage and should be used as a last resort. True or False? ⬜ True\n⬜ False\nYou want to execute a script on the provisioned AWS instance. You can add a provisioner block inside which block type to achieve this? ⬜ terraform block\n⬜ data block\n⬜ provider block\n⬜ resource block\nWhich option will you use to run provisioners that are not associated with any resources? ⬜ null_resource\n⬜ file ⬜ local-exec\n⬜ remote-exec\nWhich provisioner copies files or directories from the machine running Terraform to the newly created resource? ⬜ null_resource ⬜ file ⬜ local-exec ⬜ remote-exec\nWhich type of connections supported by file provisioner? Select all valid options. ⬜ ssh ⬜ sftp\n⬜ winrm ⬜ rdc\nWhich provisioner invokes a process on the machine running Terraform, not on the resource? ⬜ null_resource ⬜ file ⬜ local-exec ⬜ remote-exec\nWhere does the \u0026rsquo;local-exec\u0026rsquo; provisioner execute its code provided in its block? ⬜ On the remote resource specified.\n⬜ On the local machine running terraform.\n⬜ On a spot-instance on your cloud provider.\n⬜ In a container on your machine provided by the Terraform binary.\nWhich provisioner invokes a process on the resource created by Terraform? ⬜ null_resource\n⬜ file\n⬜ local-exec\n⬜ remote-exec\nWhat are the two accepted values for provisioners that have the \u0026ldquo;on_failure\u0026rdquo; key specified? (Choose 2 answers) ⬜ continue\n⬜ fail\n⬜ abort\n⬜ retry\nWhat does the following provisioner block specify? provisioner \u0026#34;local-exec\u0026#34; { when = destroy command = \u0026#34;echo \u0026#39;Destroy-time provisioner\u0026#39;\u0026#34; } ⬜ Before the resource is destroyed, the provisioner will invoke \u0026ldquo;echo \u0026lsquo;Destroy-time provisioner\u0026rsquo;\u0026rdquo;\n⬜ If the resource receives a \u0026lsquo;destroy\u0026rsquo; command locally, it will echo \u0026lsquo;Destroy-time provisioner\u0026rsquo;\n⬜ After the resource is destroyed, it will invoke \u0026ldquo;echo \u0026lsquo;Destroy-time provisioner\u0026rsquo;\u0026rdquo;\n⬜ On the next \u0026rsquo;terraform apply\u0026rsquo; the resource will be destroyed\nArea 8: Terraform Providers Terraform providers are part of the Terraform core binary. ⬜ True\n⬜ False\nA provider configuration block is required in every Terraform configuration. provider \u0026#34;provider_name\u0026#34; { ... } ⬜ True\n⬜ False\nOfficial Terraform providers are owned and maintained by HashiCorp. ⬜ True\n⬜ False\nHow do you describe a Terraform provider? ⬜ A collection of resources that can be used to define a specific piece of infrastructure\n⬜ A plugin that allows Terraform to interact with a specific cloud provider or service\n⬜ A tool for managing Docker containers\n⬜ A set of variables used to configure Terraform resources\nWhich of the following is NOT True of Terraform providers? ⬜ Providers can be written by individuals\n⬜ Providers can be maintained by a community of users\n⬜ Some providers are maintained by HashiCorp\n⬜ Major cloud vendors and non-cloud vendors can write, maintain, or collaborate on Terraform providers\n⬜ None of the above\nYou add a new provider to your configuration and immediately run terraform apply in the CLI using the local backend. Why does the apply fail? ⬜ Terraform needs to install the necessary plugins first.\n⬜ Terraform requires you to manually run terraform plan first.\n⬜ The Terraform CLI needs you to log into HCP Terraform/Terraform Cloud first.\n⬜ Terraform needs you to format your code according to best practices first.\nWhich provider configuration can be used to define multiple AWS providers with different regions? ⬜ provider\n⬜ source\n⬜ region\n⬜ alias\nWhat is a provider block without an alias meta argument? ⬜ The default provider configuration.\n⬜ A broken provider configuration.\n⬜ A partial provider configuration.\n⬜ There must be an alias meta argument.\nYou need to deploy resources into two different regions in the same Terraform configuration. To do this, you declare multiple provider configurations as shown in the Exhibit. What meta-argument do you need to configure in a resource block to deploy the resource to the us-west-2 AWS region? provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; } provider \u0026#34;aws\u0026#34; { alias = \u0026#34;west\u0026#34; region = \u0026#34;us-west-2\u0026#34; } ⬜ provider = aws.west\n⬜ provider = west\n⬜ alias = west\n⬜ alias = aws.west\nHow do you select the alternate aws provider for us-west-2 region? # The default provider configuration provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; } # Additional provider configuration for west coast region provider \u0026#34;aws\u0026#34; { alias = \u0026#34;west\u0026#34; region = \u0026#34;us-west-2\u0026#34; } ⬜ resource \u0026quot;aws_instance\u0026quot; \u0026quot;foo\u0026quot; { provider = aws }\n⬜ resource \u0026quot;aws_instance\u0026quot; \u0026quot;foo\u0026quot; { provider = aws.west }\n⬜ resource \u0026quot;aws_instance\u0026quot; \u0026quot;foo\u0026quot; { provider = aws.us-west-2 }\n⬜ resource \u0026quot;aws_instance\u0026quot; \u0026quot;foo\u0026quot; { provider = west }\nTerraform uses a lock file to ensure predictable runs when using ambiguous provider version constraints. How do you update the lock file? ⬜ terraform providers lock\n⬜ terraform lock\n⬜ terraform apply lock\n⬜ terraform lock provider -provider={PROVIDER_NAME}\nTerraform is running in an isolated network without access to Terraform registry. How can you configure Terraform to consult only a local filesystem mirror to download plugins? ⬜ terraform providers mirror ⬜ terraform mirror ⬜ terraform providers local ⬜ terraform plugins mirror\nWhile creating a terraform module, which configuration under terraform block can be used to specify the requirement of a specific version of a provider? ⬜ required_providers\n⬜ required_provider\n⬜ required_versions\n⬜ required_version\nWhich of the following is True for installing provider when terraform init command runs? ⬜ If any acceptable versions are installed, Terraform uses the newest installed version that meets the constraint (even if the Terraform Registry has a newer acceptable version)\n⬜ If no acceptable versions are installed and the plugin is one of the providers distributed by HashiCorp, Terraform downloads the newest acceptable version from the Terraform Registry and saves it in a subdirectory under .terraform/providers/\n⬜ If no acceptable versions are installed and the plugin is not distributed in the Terraform Registry, initialization fails and the user must manually install an appropriate version.\n⬜ All of the above\nArea 9: Terraform Resources How does Terraform manage most dependencies between resources? ⬜ Terraform will automatically manage most resource dependencies\n⬜ Using the depends_on parameter\n⬜ By defining dependencies as modules and including them in a particular order\n⬜ The order that resources appear in Terraform configuration indicates dependencies\nWhat is the provider for the resource shown in the Exhibit? resource \u0026#34;aws_vpc\u0026#34; \u0026#34;main\u0026#34; { name = \u0026#34;test\u0026#34; } ⬜ VPC ⬜ main\n⬜ aws\n⬜ test\nA resource block is shown in the Exhibit space. What is the Terraform resource name of that resource block? resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;dev\u0026#34; { name = \u0026#34;test\u0026#34; location = \u0026#34;westus\u0026#34; } ⬜ azurerm_resource_group\n⬜ dev ⬜ test ⬜ azurerm\nA resource block is shown in the Exhibit space. What is the Terraform resource name of the resource block? resource \u0026#34;google_compute_instance\u0026#34; \u0026#34;main\u0026#34; { name = \u0026#34;test\u0026#34; } ⬜ test\n⬜ main\n⬜ compute_instance\n⬜ google\nWhich of the following is an implicit dependency in the below example? resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;financial_report\u0026#34; { } resource \u0026#34;aws_instance\u0026#34; \u0026#34;banking_service\u0026#34; { ami = data.aws_ami.amazon_linux.id instance_type = \u0026#34;t2.micro\u0026#34; depends_on = [aws_s3_bucket.financial_report] } ⬜ S3 Bucket financial_report ⬜ EC2 Instance Type t2.micro\n⬜ AMI amazon_linux\n⬜ EC2 Instance banking_service\nWhich type of dependency is created in the below example using depends_on argument? resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;financial_report\u0026#34; { } resource \u0026#34;aws_instance\u0026#34; \u0026#34;banking_service\u0026#34; { ami = data.aws_ami.amazon_linux.id instance_type = \u0026#34;t2.micro\u0026#34; depends_on = [aws_s3_bucket.financial_report] } module \u0026#34;example_sqs_queue\u0026#34; { source = \u0026#34;terraform-aws-modules/sqs/aws\u0026#34; version = \u0026#34;3.3.0\u0026#34; depends_on = [aws_s3_bucket.financial_report, aws_instance.banking_service] } ⬜ internal dependency ⬜ implicit dependency ⬜ external dependency ⬜ explicit dependency\nWhat is the syntax to correctly reference a data source? ⬜ data.\u0026lt;DATA TYPE\u0026gt;.\u0026lt;NAME\u0026gt; ⬜ data.\u0026lt;NAME\u0026gt; ⬜ data.\u0026lt;NAME\u0026gt;.\u0026lt;DATA TYPE\u0026gt;\n⬜ \u0026lt;DATA TYPE\u0026gt;.\u0026lt;NAME\u0026gt;.data\nExamine the following Terraform configuration, which uses the data source for an AWS AMI. What value should you enter for the ami argument in the AWS instance resource? data \u0026#34;aws_ami\u0026#34; \u0026#34;ubuntu\u0026#34; { ... } resource \u0026#34;aws_instance\u0026#34; \u0026#34;web\u0026#34; { ami = _____________ instance_type = \u0026#34;t3.micro\u0026#34; } ⬜ aws_ami.ubuntu\n⬜ data.aws_ami.ubuntu\n⬜ data.aws_ami.ubuntu.id\n⬜ aws_ami.ubuntu.id\nHow could you reference an attribute from the vsphere_datacenter data source for use with the datacenter_id argument within the vsphere_folder resource in the following configuration? data \u0026#34;vsphere_datacenter\u0026#34; \u0026#34;dc\u0026#34; {} resource \u0026#34;vsphere_folder\u0026#34; \u0026#34;parent\u0026#34; { path = \u0026#34;Production\u0026#34; type = \u0026#34;vm\u0026#34; datacenter_id = ____________ } ⬜ data.vsphere_datacenter.dc.id ⬜ vsphere_datacenter.dc.id\n⬜ data.dc.id\n⬜ data.vsphere_datacenter.dc\nWhat\u0026rsquo;s the correct syntax for referencing a resource within the configuration file? ⬜ \u0026lt;RESOURCE TYPE\u0026gt;.\u0026lt;NAME\u0026gt;\n⬜ \u0026lt;NAME\u0026gt;.\u0026lt;RESOURCE TYPE\u0026gt;\n⬜ \u0026lt;PROVIDER\u0026gt;.\u0026lt;RESOURCE TYPE\u0026gt;\n⬜ \u0026lt;LOCAL/REMOTE STATE\u0026gt;.\u0026lt;RESOURCE TYPE\u0026gt;\nA resource block is shown in the Exhibit. How would you reference the attribute name of this resource in HCL? resource \u0026#34;kubernetes_namespace\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;test\u0026#34; } ⬜ resource.kubernetes_namespace.example.name ⬜ kubernetes_namespace.test.name\n⬜ kubernetes_namespace.example.name\n⬜ data.kubernetes_namespace.name\nYou want to use the terraform state show to see the attributes of a single resource created by the for_each in below resource block. What resource address should be used for the instance related to vault? resource \u0026#34;aws_instance\u0026#34; \u0026#34;demo\u0026#34; { # ... for_each = { \u0026#34;terraform\u0026#34;: \u0026#34;infrastructure\u0026#34;, \u0026#34;vault\u0026#34;: \u0026#34;security\u0026#34;, \u0026#34;consul\u0026#34;: \u0026#34;connectivity\u0026#34;, \u0026#34;nomad\u0026#34;: \u0026#34;scheduler\u0026#34;, } } ⬜ aws_instance.demo[1]\n⬜ aws_instance.demo[\u0026quot;2\u0026quot;]\n⬜ aws_instance.demo.vault\n⬜ aws_instance.demo[\u0026quot;vault\u0026quot;]\nA configuration block is shown in the Exhibit. How would you reference the volume IDs associated with the ebs_block_device blocks in this configuration? resource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { ami = \u0026#34;ami-abc123\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; ebs_block_device { device_name = \u0026#34;sda2\u0026#34; volume_size = 16 } ebs_block_device { device_name = \u0026#34;sda3\u0026#34; volume_size = 20 } } ⬜ aws_instance.example.ebs_block_device.volume_ids ⬜ aws_instance.example.ebs_block_device[*].volume_id ⬜ aws_instance.example.ebs_block_device.*.volume_id ⬜ aws_instance.example.ebs_block_device[sda2, sda3].volume_id\nTerraform can only manage resource dependencies if you set them explicitly with the depends_on argument. ⬜ True\n⬜ False\nArea 10: Terraform Variables and Outputs How can you set the value to a variable \u0026ldquo;region\u0026rdquo; declared in the configuration file? ⬜ Using command line terraform apply -var=\u0026quot;region=us-east-1\u0026quot;\n⬜ Using variable file terraform apply -var-file=\u0026quot;variables.tfvars\u0026quot; where the file contains: region=us-east-1\n⬜ Using environment variable export TF_VAR_region=us-east-1\n⬜ All of the above\nWhich one of the following takes higher precedence in loading variable in Terraform? ⬜ Command line flag - terraform apply -var=\u0026quot;region=us-east-1\u0026quot;\n⬜ Configuration file - set in your terraform.tfvars file\n⬜ Environment variable - export TF_VAR_region=us-east-1\n⬜ Default Config - default value in variables.tf\nWhich of the following is an invalid argument for defining input variable in Terraform? ⬜ default\n⬜ type\n⬜ description\n⬜ validation\n⬜ sensitive\n⬜ nullable\n⬜ depends_on\nHow would you configure your input variable to fallback to a pre-declared value in your variable block? ⬜ By specifying the default meta-argument. ⬜ By specifying the fallback meta-argument. ⬜ Terraform has a list of fallbacks that it will always implement if nothing is specified. E.g. aws_instance will fall back to a t2.micro if the size is not specifed. ⬜ Terraform will ask you to set a fallback when you run the terraform apply command.\nYou defined a variable and would like to reference it in your terraform configuration file. What is the syntax required to do so? ⬜ var.\u0026lt;VARIABLE_NAME\u0026gt;\n⬜ \u0026lt;VARIABLE_NAME\u0026gt;.var\n⬜ var.\u0026lt;VARIABLE_NAME\u0026gt;.\u0026lt;RESOURCE_NAME\u0026gt;\n⬜ \u0026lt;RESOURCE_NAME\u0026gt;.var.\u0026lt;VARIABLE_NAME\u0026gt;\nConsider the following configuration snippet: How would you define the cidr_block for us-east-1 in the aws_vpc resource using a variable? variable \u0026#34;vpc_cidrs\u0026#34; { type = map default = { us-east-1 = \u0026#34;10.0.0.0/16\u0026#34; us-east-2 = \u0026#34;10.1.0.0/16\u0026#34; us-west-1 = \u0026#34;10.2.0.0/16\u0026#34; us-west-2 = \u0026#34;10.3.0.0/16\u0026#34; } } resource \u0026#34;aws_vpc\u0026#34; \u0026#34;shared\u0026#34; { cidr_block = _____________ } ⬜ var.vpc_cidrs[\u0026quot;us-east-1\u0026quot;]\n⬜ var.vpc_cidrs.0\n⬜ vpc_cidrs[\u0026quot;us-east-1\u0026quot;]\n⬜ var.vpc_cidrs[0]\nA new variable fruits has been created of type list as shown below. How would you reference banana in your configuration? variable \u0026#34;fruits\u0026#34; { type = list(string) default = [ \u0026#34;mango\u0026#34;, \u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;grapes\u0026#34; ] } ⬜ var.fruits[2] ⬜ var.fruits.banana ⬜ var.list.fruits[2] ⬜ var.fruits[3]\nA Terraform local value can reference other Terraform local values? ⬜ True\n⬜ False\nWhen are output variables ran and sent to stdout? ⬜ Only with terraform apply.\n⬜ Only on terraform plan or apply.\n⬜ With any terraform command.\n⬜ Only if you specify the -outputs flag on apply.\nWhich of the following arguments are required when declaring a Terraform output? ⬜ value\n⬜ description\n⬜ default\n⬜ sensitive\nWhile attempting to deploy resources into your cloud provider using Terraform, you begin to see some odd behavior and experience slow responses. In order to troubleshoot you decide to turn on Terraform debugging. Which environment variables must be configured to make Terraform\u0026rsquo;s logging more verbose? ⬜ TF_LOG ⬜ TF_LOG_PATH ⬜ TF_VAR_log_level ⬜ TF_VAR_log_path\nYou have encountered an issue with Terraform and want to enable logging to find the root cause of the error. You should set TF_LOG to which log level for the MOST verbose logging? ⬜ TRACE ⬜ DEBUG ⬜ WARN ⬜ INFO\nYou want to know from which paths Terraform is loading providers referenced in your Terraform configuration (*.tf files). You need to enable detailed logging to find this out. Which of the following would achieve this? ⬜ Set the environment variable TF_LOG=TRACE ⬜ Set the environment variable TF_INPUT=1 ⬜ Set the environment variable TF_VAR_LOG=TRACE ⬜ Set the environment variable TF_LOG_PATH=./terraform.log\nYou are required to set up Terraform logs. Your boss asks you to make sure they always end up in one location such that they can be collected, and that they be set to the informational level. How would you accomplish this? (Choose 2 answers) ⬜ Set and export the environment variable TF_LOG=INFO ⬜ Set and export the environment variable TF_LOG_PATH to the requested path location.\n⬜ Only invoke the terraform apply command in the location your boss wants the logs, because terraform automatically saves a .log file in the working directory. ⬜ Set and export the environment variable TF_PATH_LOG to the requested path location.\nEnvironment variable \u0026lt;?\u0026gt;_region=us-west-1 can be used to set the value of region input variable. What is \u0026lt;?\u0026gt; here? ⬜ TF_VAR ⬜ TF_ENV ⬜ TF_VAL ⬜ TF_VARIABLE\nYou have a Terraform variable that is declared as follows: variable \u0026#34;num\u0026#34; { default = 3 } You have also defined the following environment variables in your BASH shell:-\nexport TF_VAR_num=10 You also have a terraform.tfvars file with the following contents:-\nnum = 7 When you run the following apply command, what is the value assigned to the num variable?\nterraform apply -var num=4 ⬜ 4\n⬜ 7 ⬜ 3 ⬜ 10\nWhich of the following is a valid variable name in Terraform? ⬜ 1234\n⬜ 1_aws_vpc ⬜ invalid ⬜ count\nWhat are Data Sources in terraform? ⬜ Data to be fetched or computed for use elsewhere in terraform configuration.\n⬜ Similar to resources, they specify data to be created in the corresponding provider.\n⬜ A binary set of operators that tell resources how to behave with certain meta-arguments.\n⬜ Data sources are a way for terraform to keep track of all resources created in the provider\u0026rsquo;s infrastructure.\nArea 11: Terraform Module Which of the following best describes a Terraform module? ⬜ A collection of resources that make up a specific piece of infrastructure ⬜ A plugin that allows Terraform to interact with a specific cloud provider or service\n⬜ A set of variables used to configure Terraform resources\n⬜ A tool for managing Docker containers\nIn Terraform, what is a module? ⬜ A group of related resources ⬜ A singular, non-abstractive, resource. ⬜ Essentially a comment, it doesn\u0026rsquo;t do anything except to describe a set of resources. ⬜ Similar to programming functions, modules are used to write code in Golang for direct interaction with Terraform.\nIn Terraform, What are modules used for? ⬜ Organize configuration ⬜ Encapsulate configuration\n⬜ Re-use configuration\n⬜ All of the above\nWhen you initialize Terraform, where does it cache modules from the public Terraform Registry? ⬜ In the .terraform sub-directory\n⬜ In the /tmp directory\n⬜ In memory ⬜ They are not cached\nWhen you initialize Terraform from CLI terraform init, it starts downloading all the necessary modules referenced in the code and cache it to which directory? ⬜ in the /temp directory on the machine executing Terraform\n⬜ in a /modules directory in the current working directory\n⬜ in the /downloads directory for the user running the terraform init\n⬜ in the .terraform/modules subdirectory in the current working directory\nWhen you initialize a Terraform workspace, it installs all the provides and modules referred in the configuration in which directory? ⬜ directly in the current working directory ⬜ in the downloads directory in the current working directory ⬜ in the directory where terraform is installed ⬜ in the .terraform subdirectory in the current working directory\nWhat are three meta-arguments, along with source and version, that a module can use? (Choose 3 answers) ⬜ for_each ⬜ count ⬜ max ⬜ depends_on\nA module that has been called by another module is often referred to as a child module. Where is the child module stored in below module block? module \u0026#34;vpc\u0026#34; { source = \u0026#34;terraform-aws-modules/vpc/aws\u0026#34; name = \u0026#34;my-vpc\u0026#34; cidr = \u0026#34;10.0.0.0/16\u0026#34; azs = [\u0026#34;eu-west-1a\u0026#34;, \u0026#34;eu-west-1b\u0026#34;, \u0026#34;eu-west-1c\u0026#34;] private_subnets = [\u0026#34;10.0.1.0/24\u0026#34;, \u0026#34;10.0.2.0/24\u0026#34;, \u0026#34;10.0.3.0/24\u0026#34;] public_subnets = [\u0026#34;10.0.101.0/24\u0026#34;, \u0026#34;10.0.102.0/24\u0026#34;, \u0026#34;10.0.103.0/24\u0026#34;] } ⬜ in a local directory named .terraform/terraform-aws-modules/vpc/aws ⬜ in a remote code repository\n⬜ in terraform cloud private module registry\n⬜ in terraform public module registry\nWhich one of the following is a valid source type to download the source code of a module? ⬜ Local Paths\n⬜ Terraform Registry\n⬜ Github\n⬜ Bitbucket\n⬜ HTTP URLs\n⬜ S3 buckets\n⬜ All of the above\nWhich one of the following file extension recognized by terraform while fetching archived module over HTTP? ⬜ zip\n⬜ tar.bz2 and tbz2\n⬜ tar.gz and tgz\n⬜ tar.xz and txz\n⬜ All of the above\nHow do you download a module configured in your Terraform code? module \u0026#34;consul\u0026#34; { source = \u0026#34;hashicorp/consul/aws\u0026#34; version = \u0026#34;0.1.0\u0026#34; } ⬜ terraform get module consul\n⬜ terraform install modules consul\n⬜ terraform init\n⬜ terraform module init\nWhat feature of Terraform Cloud allows you to share Terraform providers and modules only with the members of the organization? ⬜ Remote Runs ⬜ Remote Terraform Execution ⬜ Private Registry ⬜ Private VCS Connection\nWhat value does the Terraform Cloud private registry provide over the public Terraform Module Registry? ⬜ The ability to share modules publicly with any user of Terraform\n⬜ The ability to restrict modules to members of Terraform Cloud or Enterprise organizations\n⬜ The ability to tag modules by version or release\n⬜ The ability to share modules with public Terraform users and members of Terraform Cloud Organizations\nHow do you correctly reference a private registry module source? ⬜ \u0026lt;HOSTNAME\u0026gt;/\u0026lt;NAMESPACE\u0026gt;/\u0026lt;NAME\u0026gt;/\u0026lt;PROVIDER\u0026gt;\n⬜ \u0026lt;NAMESPACE\u0026gt;/\u0026lt;NAME\u0026gt;/\u0026lt;PROVIDER\u0026gt;\n⬜ \u0026lt;HOSTNAME\u0026gt;/\u0026lt;NAMESPACE\u0026gt;/\u0026lt;PROVIDER\u0026gt;\n⬜ \u0026lt;NAMESPACE\u0026gt;/\u0026lt;NAME\u0026gt;/\u0026lt;PROVIDER\u0026gt;/\u0026lt;HOSTNAME\u0026gt;\nHow do you reference module source from public terraform registry? ⬜ \u0026lt;NAMESPACE\u0026gt;/\u0026lt;NAME\u0026gt;/\u0026lt;PROVIDER\u0026gt;\n⬜ \u0026lt;NAMESPACE\u0026gt;/\u0026lt;PROVIDER\u0026gt;/\u0026lt;NAME\u0026gt; ⬜ \u0026lt;NAMESPACE\u0026gt;/\u0026lt;PROVIDER\u0026gt;\n⬜ \u0026lt;HOSTNAME\u0026gt;/\u0026lt;NAMESPACE\u0026gt;/\u0026lt;NAME\u0026gt;/\u0026lt;PROVIDER\u0026gt;\nWhich of the following module source paths does NOT specify a remote module? ⬜ Source = './module/consul'\n⬜ Source = 'github.com/hasicorp/example'\n⬜ Source = 'git@github.com:hasicorp/example.git'\n⬜ Source = 'hasicorp/consul/aws'\nWhen specifying a module, what is the best practice for the implementation of the meta-argument version? ⬜ The best practice is to explicitly set the version argument as a version constraint string from the Terraform registry.\n⬜ The best practice is to use no version and accept the latest version.\n⬜ The best practice is to download the module, place it in your working directory, then source that module, and specify the version that was downloaded.\n⬜ The best practice is to always ensure you append beta to the end of the version. This allows you and your team to always be working on the latest and greatest features for that module.\nWhich argument can you use to prevent unexpected updates to a module\u0026rsquo;s configuration when calling Terraform Registry modules? ⬜ source\n⬜ count\n⬜ version\n⬜ depends_on\nWhen you include a module block in your configuration that references a module from the Terraform Registry, the version attribute is required. ⬜ True\n⬜ False\nWhen you use a module block to reference a module from the Terraform Registry such as the one in the example, how do you specify version 1.0.0 of the module? module \u0026#34;consul\u0026#34; { source = \u0026#34;hashicorp/consul/aws\u0026#34; } ⬜ Append ?ref=v1.0.0 argument to the source path.\n⬜ You cannot. Modules stored on the public Terraform Registry do not support versioning.\n⬜ Add a version = \u0026quot;1.0.0\u0026quot; attribute to the module block.\n⬜ Nothing. Modules stored on the public Terraform Registry always default to version 1.0.0.\nHow do you access module attributes? ⬜ Through the child module, by declaring an output value to selectively export certain values to be accessed by the calling module.\n⬜ Through the parent module, by declaring an output value to selectively export certain values to be accessed by the calling module.\n⬜ By specifying the outputs block.\n⬜ When apply is ran, you must pass in -resource-output={ATTRIBUTE.NAME}.\nWho can publish and share modules on the Terraform Registry? ⬜ Anyone ⬜ Only specific providers\n⬜ Those who have passed the Hashicorp Terraform Associate exam\n⬜ Only those who have contributed to Open Source Terraform\nWhat are the requirements to publish a module to the Terraform Public Module Registry? (select three) ⬜ Release tag is optional for initial module publishing and mandatory for subsequent releases ⬜ The module must be on GitHub and must be a public repo\n⬜ The module must adhere to the standard module structure ⬜ Module repositories must use this three-part name format, terraform-\u0026lt;PROVIDER\u0026gt;-\u0026lt;NAME\u0026gt; e.g. terraform-google-vault\nWhen developing a Terraform module, how would you specify the version when publishing it to the official Terraform Registry? ⬜ Configure it in the module’s Terraform code\n⬜ The Terraform Registry does not support versioning modules\n⬜ Tag a release in your module’s source control repository\n⬜ Mention it on the module’s configuration page on the Terraform Registry\nWhat are some of the requirements for publishing Private Modules to the Terraform Cloud Private Registry? (select three) ⬜ The module must be PCI/HIPPA compliant ⬜ The module must be on your configured VCS providers, and Terraform Cloud\u0026rsquo;s VCS user account must have admin access to the repository ⬜ The module must adhere to the standard module structure ⬜ Module repositories must use this three-part name format, terraform-\u0026lt;PROVIDER\u0026gt;-\u0026lt;NAME\u0026gt; e.g. terraform-google-vault\nTerraform configuration can only call modules from the public registry. ⬜ True\n⬜ False\nWhich one of the following is the required argument for calling a child module? ⬜ version\n⬜ source\n⬜ providers\n⬜ depends_on\nYou have written the below code snippet in main.tf file. Which of the following statements are True? (select two) module \u0026#34;servers\u0026#34; { source = \u0026#34;./app-cluster\u0026#34; servers = 5 } ⬜ app-cluster is a child module\n⬜ app-cluster is a calling module ⬜ main.tf is a child module ⬜ main.tf is the calling module\nIn a parent module, outputs of child modules are available in expressions as? ⬜ module.\u0026lt;MODULE NAME\u0026gt;.\u0026lt;OUTPUT NAME\u0026gt; ⬜ \u0026lt;MODULE NAME\u0026gt;.\u0026lt;OUTPUT NAME\u0026gt; ⬜ module.\u0026lt;OUTPUT NAME\u0026gt;\n⬜ output.\u0026lt;MODULE NAME\u0026gt;.\u0026lt;OUTPUT NAME\u0026gt;\nYou created a module named web_server that outputs the instance_ip_addr, which is the IP Address of the web server instance created by the module. How would you reference the IP Address when using it for an input of another module? ⬜ ip_addr = module.outputs.web_server.instance_ip_addr\n⬜ ip_addr = module.web_server.instance_ip_addr ⬜ ip_addr = web_server.instance_ip_addr\n⬜ ip_addr = web_server.outputs.instance_ip_addr\nHow would you output returned values from a child module in the Terraform CLI output? ⬜ Declare the output in the root configuration\n⬜ Declare the output in the child module\n⬜ Declare the output in both the root and child module\n⬜ None of the above\nYou are writing a child Terraform module that provisions an AWS instance. You want to reference the IP address returned by the child module in the root configuration. You name the instance resource \u0026ldquo;main\u0026rdquo;. Which of these is the correct way to define the output value? ⬜ output \u0026quot;instance_ip_addr\u0026quot; { value = aws_instance.main.private_ip }\n⬜ output \u0026quot;aws_instance.instance_ip_addr\u0026quot; { value = ${main.private_ip} }\n⬜ output \u0026quot;instance_ip_addr\u0026quot; { return aws_instance.main.private_ip }\n⬜ output \u0026quot;aws_instance.instance_ip_addr\u0026quot; { return aws_instance.main.private_ip }\nArea 12: Terraform Security You want to ensure that your S3 buckets provisioned by Terraform are securely encrypted. What is the best way to achieve this? ⬜ Create a Git hook that checks if the encryption parameter is enabled.\n⬜ Use AWS KMS to store a security key.\n⬜ Create a lambda function triggered on a “create bucket CloudTrail” event.\n⬜ Create a security policy using Sentinel policies.\nWhich Terraform feature can be used to apply policy as code to enforce compliance and governance policies before any infrastructure changes? ⬜ Resources\n⬜ Functions\n⬜ Sentinel\n⬜ Workspaces\nHashiCorp Sentinel is a(n) _____ framework. ⬜ platform as a service\n⬜ function as a service\n⬜ infrastructure as code\n⬜ policy as code\nTerraform Cloud provides imports to define Sentinel Policy Rules. Which of the following is NOT a valid import? ⬜ tfplan ⬜ tfconfig ⬜ tfstate ⬜ tfapply\nYou want to create a sentinel policy to ensure that naming convention is being followed in Terraform Configuration as per organization-wide standard. Which sentinel import can be used to access Terraform Configuration? ⬜ tfplan ⬜ tfconfig ⬜ tfstate ⬜ tflan\nWhich is NOT a valid sentinel policy enforcement level? ⬜ advisory\n⬜ soft mandatory\n⬜ warning ⬜ hard mandatory\nYou have enabled Sentinel Policy in Terraform Cloud. When Terraform Cloud evaluates policies? ⬜ On every Terraform Run ⬜ After successful terraform plan ⬜ During terraform plan ⬜ Before terraform apply\nYour security team scanned some Terraform workspaces and found secrets stored in a plaintext in state files. How can you protect sensitive data stored in Terraform state files? ⬜ Delete the state file every time you run Terraform\n⬜ Store the state in an encrypted backend\n⬜ Edit your state file to scrub out the sensitive data\n⬜ Always store your secrets in a secrets.tfvars file.\nIf you manage any sensitive data with Terraform (like database passwords, user passwords, or private keys), treat the state itself as sensitive data. What is recommended to protect the state file? (select two) ⬜ use the S3 bucket using the encrypt option to ensure state is encrypted ⬜ enable native encryption in Terraform as configured in the terraform block\n⬜ use Terraform Cloud which always encrypts state at rest\n⬜ replicate the state file to an encrypted storage device\nYou want to use API tokens and other secrets within your team\u0026rsquo;s Terraform workspaces. Where does HashiCorp recommend you store these sensitive values? (Pick 3 correct responses) ⬜ In an HCP Terraform/Terraform Cloud variable, with the sensitive option checked\n⬜ In HashiCorp Vault\n⬜ In a terraform.tfvars file, securely managed and shared with your team\n⬜ In a terraform.tfvars file, checked into your version control system\n⬜ In a plaintext document on a shared drive\nWhat are some benefits of using Sentinel with Terraform Cloud/Terraform Enterprise? (Choose three.) ⬜ You can restrict specific resource configurations, such as disallowing the use of CIDR=0.0.0.0/0.\n⬜ You can check out and check in cloud access keys\n⬜ Sentinel Policies can be written in HashiCorp Configuration Language (HCL)\n⬜ Policy-as-code can enforce security best practices\n⬜ You can enforce a list of approved AWS AMIs\nArea 13: Terraform Workspace Each Terraform CLI Workspace uses its own state file to manage the infrastructure associated with that particular workspace. ⬜ True\n⬜ False\nWhat Terraform feature is most applicable for managing small differences between different environments, for example development and production? ⬜ Workspaces\n⬜ States\n⬜ Repositories\n⬜ Versions\nWhere are Terraform Workspace local state files stored? ⬜ a directoy called terraform.tfstate.d\n⬜ a file called terraform.tfstate\n⬜ a temp directory called .tfstate ⬜ a directory called terraform.workspaces.tfstate\nYou would like to reuse the same Terraform configuration for your development and production environments with a different state file for each. Which command would you use? ⬜ terraform import\n⬜ terraform workspace\n⬜ terraform state ⬜ terraform init\nOne of your colleagues is new to Terraform and wants to add a new workspace named new-hire. What command he should execute from the following? ⬜ terraform workspace –new –new-hire\n⬜ terraform workspace new new-hire\n⬜ terraform workspace init new-hire\n⬜ terraform workspace new-hire\nAs a prestigious Sr. Cloud Engineer, your colleague comes up to you and asks for a new Development workspace. What\u0026rsquo;s the fastest way to accomplish this? ⬜ Through CLI terraform workspace new dev\n⬜ Head to the Terraform Enterprise console and create a new workspace there.\n⬜ Specify in the configuration block the new workspace to be created.\n⬜ Have them submit a Jira ticket and tell them you\u0026rsquo;ll get around to it in the next Sprint.\nYou have created prod and test workspaces from the command line. Which of the following commands you can run to switch to the test workspace? ⬜ terraform workspace switch test\n⬜ terraform workspace select test\n⬜ terraform workspace test\n⬜ terraform workspace -switch test\nWhat command should you run to display all workspaces for the current configuration? ⬜ terraform workspace\n⬜ terraform workspace show\n⬜ terraform workspace list\n⬜ terraform show workspace\nWhich is NOT True about Terraform Cloud and Terraform CLI Workspaces? ⬜ Each Terraform Cloud workspace has its own Terraform configuration, variables, state file, backup of previous state files, run history, credentials \u0026amp; secrets, and settings.\n⬜ Each Terraform CLI workspace is a persistent working directory, which may contains a configuration, state data, and variables.\n⬜ You cannot manage resources in Terraform Cloud without creating at least one workspace.\n⬜ You must create a local working directory using Terraform CLI to manage resources in local.\nArea 14: Terraform Version Constraint Which version constraint should use to set both a lower and upper bound on versions for each provider. Also known as pessimistic constraint operator? ⬜ \u0026gt;=\n⬜ ~\u0026gt;\n⬜ !=\n⬜ \u0026lt;\u0026gt;\nWhat does the specified contraint version = \u0026ldquo;~\u0026gt; 1.0.4\u0026rdquo; means in required_providers block? terraform { required_providers { mycloud = { source = \u0026#34;mycorp/mycloud\u0026#34; version = \u0026#34;~\u0026gt; 1.0.4\u0026#34; } } } ⬜ \u0026gt;= 1.0.4 and \u0026lt;= 1.1.0\n⬜ \u0026gt;= 1.0.4 and \u0026lt; 1.1.0\n⬜ \u0026gt; 1.0.4 and \u0026lt; 2.0.0\n⬜ \u0026gt;= 1.0.5 and \u0026lt; 1.1.0\nWhat does this symbol version = “~\u0026gt; 1.0” mean when defining versions? ⬜ \u0026gt; 1.0 and \u0026lt; 2.0 ⬜ \u0026gt;= 1.0 and \u0026lt; 2.0\n⬜ \u0026gt;= 1.0 and \u0026lt;= 2.0\n⬜ \u0026gt; 1.0.0 and \u0026lt; 2.0.0\nWhat is the provider version of Google Cloud being used in Terraform? Select all valid options. provider \u0026#34;google\u0026#34; { version = \u0026#34;~\u0026gt; 1.9.0\u0026#34; } ⬜ 1.9.1\n⬜ 1.10.0\n⬜ 1.8.0\n⬜ 1.9.9\nHow do you force users to use a particular version of required providers in your terraform code? ⬜ terraform { required_providers { aws = { source = “hashicorp/aws” version =”3.74.1″ } } } ⬜ terraform { aws = { source = “hashicorp/aws” version = “~\u0026gt;3.74.1” } } ⬜ aws = { source = “hashicorp/aws” version = “3.74.1” } ⬜ terraform { required_providers { aws = { source = “hashicorp/aws” version =”~\u0026gt;3.74.1″ } } }\nWhat is more accurate description of the below version constraint? terraform { required_version = \u0026#34;\u0026gt;= 1.4.7\u0026#34; } ⬜ All the terraform providers are should have a minimum 1.4.7 version ⬜ Terraform version older than 1.4.7 are not supported by Hashicorp ⬜ Terraform version older than 1.4.7 will produce an error running this configuration ⬜ The minimum version of the application using Terraform should be 1.4.7\nArea 15: Terraform Types and Functions In Terraform HCL, an object type of object({ name = string, age = number }) would match which of these values? ⬜ { name = \u0026quot;John\u0026quot; age = 52 }\n⬜ { name = John age = \u0026quot;52\u0026quot; } ⬜ { name = \u0026quot;John\u0026quot; age = fifty two}\n⬜ { name = John age = fifty two}\nYou are adding input variables in the Terraform module for customization. Which of the following is NOT a valid primitive variable type in Terraform? ⬜ string\n⬜ number\n⬜ float\n⬜ bool\nWhat are two complex types in terraform? (Choose 2 answers) ⬜ A Collection Type\n⬜ A Structural Type\n⬜ A String Type ⬜ A float64 type\nWhat are complex types in terraform? ⬜ A type that groups multiple values into a single value. ⬜ A variation of a string type.\n⬜ A variance of a data source.\n⬜ A type that derives its value from RegEx logic.\nIf an input variable has no type value set, what type does it accept? ⬜ Any type.\n⬜ None, it has to have a type value set.\n⬜ Terraform infers the type when it is referenced.\n⬜ Type string. As strings can be interpreted in a number of ways by Terraform.\nWhich of the following is not a valid Terraform Collection type? ⬜ list ⬜ map ⬜ tree ⬜ set\nWhich of the followings are valid Terraform Structural types? (Choose 2 answers) ⬜ optional ⬜ object ⬜ pair ⬜ tuple\nWhat are the similar kind of complex types in Terraform? (select three) ⬜ list ⬜ map ⬜ set ⬜ tuple\nYou want to define a single input variable to store information about servers mainly server-name of type string and memory-size of type number. Which variable type should you choose? ⬜ list ⬜ map\n⬜ object\n⬜ set\nWhich Terraform collection type should you use to store key/value pairs? ⬜ list ⬜ tuple ⬜ map ⬜ set\nWhich of the following is not a valid string function in Terraform? ⬜ split()\n⬜ join() ⬜ slice() ⬜ chomp()\nWhat are some built-in functions that terraform provides? (Choose 3 answers) ⬜ max()\n⬜ regex()\n⬜ alltrue()\n⬜ delete()\nYou\u0026rsquo;re writing a Terraform configuration that needs to read input from a local file called id_rsa.pub. Which built-in Terraform function can you use to import the file\u0026rsquo;s contents as a string? ⬜ file(\u0026quot;id_rsa.pub\u0026quot;)\n⬜ templaTefil(\u0026quot;id_rsa.pub\u0026quot;) ⬜ filebase64(\u0026quot;id_rsa.pub\u0026quot;) ⬜ fileset(\u0026quot;id_rsa.pub\u0026quot;)\nConsider buying the full set of questions with answers and explanation from below links:- Hashicorp Terraform Associate (003) Exam Questions with Answers and Explanation at a very reasonable price.\n","permalink":"https://codingnconcepts.com/post/terraform-associate-exam-questions/","tags":["Terraform","Certification"],"title":"Hashicorp Terraform Associate (003) Exam Questions"},{"categories":["Cloud"],"contents":"A complete study guide for Hashicorp Terraform Associate Certification Exam (003).\nAbout Exam Terraform Associate certification is a specialty certification in Terraform conducted by Hashicorp. This exam is recommended for Cloud, DevOps, and SRE engineers. The Terraform Associate 003 exam detail is available here You have to answer 57 questions within 60 minutes from your laptop under the supervision of an online proctor. You need to get \u0026ldquo;around\u0026rdquo; 70% to pass the exam so you can afford to get 17 questions wrong. The exam costs you 70.5 USD excluding taxes and there is no free retake. The exam can be taken in English language only. You should expect different types of question formats such as Multiple Choice, Multiple Answers, True or false, and Text Match where you have to type the answer to fill in the blank. You get the exam result instantly and receive an email with the same result. This certification is valid for 2 years. This exam is targeted for Terraform version 1.0 and higher. Recommended studying for 1-2 hours a day for 2-4 weeks depending upon your daily commitment. You can register for the exam here Exam Preparation To be honest, their documentation is more than enough for this exam. They have a very good Exam study guide and exam review guide. I would suggest you go through these links in order and would highly recommend you practice while going through these materials.\nExam Study Guide Sample Questions Exam Review Guide Free study material:-\nHashiCorp Terraform Associate Certification Course - Pass the Exam! https://www.examtopics.com/exams/hashicorp/terraform-associate/view/ https://www.itexams.com/exam/Terraform-Associate https://www.whizlabs.com/blog/terraform-certification-exam-questions/ https://www.pass4success.com/hashicorp/discussions Paid study materials:-\nHashiCorp Certified: Terraform Associate 2023 by Zeal Vora on Udemy HashiCorp Certified: Terraform Associate Practice Exam 2023 by Bryan Krausen on Udemy Exam Questions • Read 200+ Hashicorp Terraform Associate (003) Exam Questions for free\n• Buy 200+ Hashicorp Terraform Associate (003) Exam Questions with Answers and Explainations at very reasonable price\nExam Notes Infrastructure as Code Terraform is an Infrastructure as Code (IaC) tool that is Declarative and Cloud Agnostic Infrastructure lifecycle:- “Day 0” code provisions and configures your initial infrastructure. “Day 1” refers to OS and application configurations you apply after you’ve initially built your infrastructure. IaC enhances the Infrasture lifecycle: Reliability: IaC makes changes idempotent, consistent, repeatable, and predictable Manageability Sensibility Popular Infrastructure as Code (IaC) tools: Tool Supports ARM Templates, Azure Blueprints only Microsoft Azure CloudFormation only Amazon AWS Cloud Deployment Manager only Google Cloud Platform GCP Terraform AWS, Azure, GCP, and on-prem Terraform is written in HashiCorp Configuration Language (HCL). HCL is designed to strike a balance between human-readable and machine-parsable Terraform Workflow The core Terraform workflow consists of these stages:-\nWrite: Define infrastructure in a configuration file e.g. main.tf Initialize: Prepares the working directory so Terraform can run the configuration using terraform init command Plan: Review the changes Terraform will make to your infrastructure using terraform plan command Apply: Terraform provisions your infrastructure and updates the state file using terraform apply command Terraform Provider Provider is a plugin that allows Terraform to interact with a specific cloud provider or service Provider provides abstraction above the upstream API and is responsible for understanding API interactions and exposing resources. Major cloud vendors and non-cloud vendors can write, maintain, or collaborate on Terraform providers At least one provider block is required in terraform configuration. Supports multiple provider instances using alias e.g. multiple aws provides with a different region Input Variables The name of a variable can be any valid identifier except the following:- source, version, providers, count, for_each, lifecycle, depends_on, locals. The type of a variable can be string, number, bool, list, set, map, object, and tuple. The nullable argument in variable block can be set to false to not allow null values. default is true. The validation argument in variable block can be used to apply custom validation on the value The sensitive argument in variable block can be set to true to prevent from showing its value in the plan and apply output variable \u0026#34;image_id\u0026#34; { type = string description = \u0026#34;The id of the machine image (AMI) to use for the server.\u0026#34; default = ami-abc123 nullable = false sensitive = false validation { condition = length(var.image_id) \u0026gt; 4 \u0026amp;\u0026amp; substr(var.image_id, 0, 4) == \u0026#34;ami-\u0026#34; error_message = \u0026#34;The image_id value must be a valid AMI id, starting with \\\u0026#34;ami-\\\u0026#34;.\u0026#34; } } The loading of a variable goes from highest precedence (1) to lowest (5):-\nCommand line flag - specify in command using -var e.g. terraform plan -var=environment=\u0026quot;prod\u0026quot; Configuration file - set in your terraform.tfvars file Environment variable - part of your shell environment e.g. TF_VAR_environment=prod Default Config - default value in variables.tf User manual entry - if not specified, prompt the user for entry Terraform Backend A backend defines where Terraform stores its state data files. There are two backend types: local and remote. By default, Terraform uses a backend called local, which stores state as a local file on disk. Terraform v1.4.x supports the following backend types:- local, remote, azurerm, consul, cos, gcs, http, kubernetes, oss, pg, s3 Terraform v1.2.x also supports following backend types:- artifactory, etcd, etcdv3, manta, swift Backend types support state locking:- local, remote, azurerm, consul, cos, gcs, http, kubernetes, oss, pg, s3, etcdv3, manta, swift Backend types doesn\u0026rsquo;t support state locking:- artifactory, etcd A terraform configuration terraform block can only have one backend block. terraform { backend \u0026#34;remote\u0026#34; { organization = \u0026#34;example_corp\u0026#34; workspaces { name = \u0026#34;my-app-prod\u0026#34; } } } Terraform Cloud automatically manages state in the workspaces. If your configuration includes a cloud block, it cannot include a backend block. terraform { cloud { organization = \u0026#34;example_corp\u0026#34; ## Required for Terraform Enterprise; Defaults to app.terraform.io for Terraform Cloud hostname = \u0026#34;app.terraform.io\u0026#34; workspaces { tags = [\u0026#34;app\u0026#34;] } } } It is recommended to provide credentials and sensitive data using the credential file or environment variable supported by the remote backend type. You must run terraform init when you change backend configuration or backend type. Terraform auto-detects the changes and can migrate from existing state to new configuration. Terraform Type Constraints Category Type Example Reference Value Primitive string variable \u0026quot;image_id\u0026quot; { type=string default=\u0026quot;ami-abc123\u0026quot;} var.image_id Primitive number variable \u0026quot;threads\u0026quot; { type=number default=10 } var.threads Primitive bool variable \u0026quot;set_password\u0026quot; { type=bool default=false } var.set_password Collection list variable \u0026quot;az_names\u0026quot; { type=list(string) default=[\u0026quot;us-west-1a\u0026quot;, \u0026quot;us-west-2a\u0026quot;]} var.az_names[0] Collection set variable \u0026quot;az_names\u0026quot; { type=set(string) default=[\u0026quot;us-west-1a\u0026quot;, \u0026quot;us-west-2a\u0026quot;]} var.az_names[0] Collection map variable \u0026quot;amis\u0026quot; { type=\u0026quot;map\u0026quot; default={ \u0026quot;us-east-1\u0026quot;=\u0026quot;ami-b374d5a5\u0026quot; \u0026quot;us-west-2\u0026quot;=\u0026quot;ami-4b32be2b\u0026quot; }} var.amis[\u0026quot;us-east-1\u0026quot;] Structural object variable \u0026quot;user_info\u0026quot; { type=object({ name=string address=string })} var.user_info.name Structural tuple variable \u0026quot;var1\u0026quot; { type=tuple([string, number, bool]) default=[\u0026quot;a\u0026quot;, 15, true] } var.var1[0] Source: https://developer.hashicorp.com/terraform/language/expressions/type-constraints\nReferences to Named Values Type Expression Example Input Variable var.\u0026lt;NAME\u0026gt; var.ami Local Values local.\u0026lt;NAME\u0026gt; local.ami Resources \u0026lt;RESOURCE TYPE\u0026gt;.\u0026lt;NAME\u0026gt;.\u0026lt;ATTRIBUTE\u0026gt; aws_instance.example.ami Data Sources data.\u0026lt;DATA TYPE\u0026gt;.\u0026lt;NAME\u0026gt;.\u0026lt;ATTRIBUTE\u0026gt; data.aws_ami.ubuntu.id Child Module Outputs module.\u0026lt;MODULE NAME\u0026gt;.\u0026lt;OUTPUT NAME\u0026gt; module.prod_subnet.subnet_id Source: https://developer.hashicorp.com/terraform/language/expressions/references\nEnvironment Variables Environment Variable Example TF_LOG TF_LOG=trace|debug|info|warn|error to enable logs at different log level TF_LOG=off to disable logs TF_LOG=json to generate logs in JSON format TF_LOG_CORE to enable/disable logging separately for terraform core, values same as TF_LOG TF_LOG_PROVIDER to enable/disable logging separately for terraform providers, values same as TF_LOG TF_LOG_PATH to specify where the log should persist its output to e.g. TF_LOG_PATH=./terraform.log , by default logs appear on stderr TF_VAR_name to set the variables e.g. region variable TF_VAR_region=us-east1 Terraform Commands Command ▼ Description terraform init 1. First command to run. Safe to run it multiple times2. Initialize a working directory that contains terraform configuration files:- - create hidden .terraform directory - configure backend, download and install provider plugins into .terraform/providers - download modules into .terraform/modules - create dependency lock file.terraform.lock.hcl3. Use terraform init -from-module=MODULE-SOURCE to check out the configuration from VCS and initialize the current working directory.4. Use terraform init -backend-config=PATH for partial backend configuration, in situations where the backend settings are dynamic or sensitive and so cannot be statically specified in the configuration file. terraform get Use to download and update modules mentioned into a .terraform subdirectory of the current working directory. terraform plan 1. Creates an execution plan, which lets you preview the changes that Terraform plans to make to your infrastructure2. Reads the current state of any already-existing remote objects to make sure that the Terraform state is up-to-date.3. Compares the current configuration to the prior state and noting any differences.4. Proposes a set of change actions that should, if applied, make the remote objects match the configuration.5. You can use the optional -out=FILE option to save the generated plan to a file on disk6. Terraform has two alernative planning modes: -destroy and -refresh-only7. Terraform has following planning options:- -refresh=false, -replace=ADDRESS, -target=ADDRESS, -var 'NAME=VALUE', and -var-file=FILENAME8. Terraform planning mode and planning options are available for both terraform plan and terraform apply terraform apply 1. Apply the execution plan to provision the resources2. You can pass the -auto-approve option to instruct Terraform to apply the plan without asking for confirmation. terraform destroy 1. Destroy all remote objects managed by a particular Terraform configuration.2. This command is effectively an alias for terraform apply -destroy terraform login 1. terraform login [hostname] is used to automatically obtain and save an API token for Terraform Cloud, Terraform Enterprise, or any other host that offers Terraform services.2. If you don\u0026rsquo;t provide an explicit hostname, Terraform will assume you want to log in to Terraform Cloud at app.terraform.io terraform logout 1. terraform logout [hostname] is used to remove credentials stored by terraform login. These credentials are API tokens for Terraform Cloud, Terraform Enterprise, or any other host that offers Terraform services.2. If you don\u0026rsquo;t provide an explicit hostname, Terraform will assume you want to log out of Terraform Cloud at app.terraform.io terraform console Provides an interactive command-line console for evaluating and experimenting with expressions. terraform fmt 1. Format code in terrform style convention2. Indent two space for each nesting level and align equals signs at same nesting level 3. terraform fmt -diff displays diff of formatting changes terraform validate 1. Validates the configuration files in a directory, referring only to the configuration and not accessing any remote services such as remote state, provider APIs, etc.2. Verify whether a configuration is syntactically valid and internally consistent, regardless of any provided variables or existing state3. terraform validate -json produce validation result in JSON format terraform graph 1. Used to generate a visual representation of either a configuration or execution plan. The output is in the DOT format, which can be used by GraphViz to generate image or charts e.g. terraform graph | dot -Tsvg \u0026gt; graph.svg2. Use optional -plan tfplan option to render graph using specified plan file terraform output 1. Used to extract the value of an output variable from the state file.2. Use optional -json and -raw option for output to formatted as JSON and String format. Any sensitive values in Terraform state will be displayed in plain text. terraform show 1. Used to provide human-readable output from a state or plan file2. Use optional -json option for JSON representation of plan, configuration, and current state terraform state 1. The terraform state command and its subcommands can be used for various tasks related to the Terraform state.2. The terraform state pull and terraform state push subcommands can be used to retrieve and upload the Terraform state from and to a remote backend, respectively. This is useful when multiple users or systems are working with the same Terraform configuration. terraform state mv Used in the less common situation where you wish to retain an existing remote object but track it as a different resource instance address in Terraform, such as if you have renamed a resource block or you have moved it into a different module in your configuration. terraform state rm Used in the less common situation where you wish to remove a binding to an existing remote object without first destroying it, which will effectively make Terraform \u0026ldquo;forget\u0026rdquo; the object while it continues to exist in the remote system. terraform state replace-provider Used to replace the provider for resources in a Terraform state e.g. terraform state replace-provider hashicorp/aws registry.acme.corp/acme/aws replaces the hashicorp/aws provider by achme terraform state list Used to list resources within a Terraform state. terraform state show Used to show attributes of a single resource in the terraform state e.g. terraform state show aws_instance.foo terraform refresh 1. This command read the remote infrastructure and update the terraform state file to match with remote objects.2. This won\u0026rsquo;t modify your real remote objects, but it will modify the terraform state file. 3. This command is here primarily for backward compatibility, but we don\u0026rsquo;t recommend using it because it provides no opportunity to review the effects of the operation before updating the state.4. Equivalent command in Terraform v0.15.4 or later is terraform apply -refresh-only -auto-approve5. It is recommended to use terraform apply -refresh-only or terraform plan -refresh-only instead, which gives an opportunity to review the changes. terraform taint 1. Informs Terraform that a particular object has become degraded or damaged. Terraform represents this by marking the object as \u0026ldquo;tainted\u0026rdquo; in the Terraform state, and Terraform will propose to replace it in the next plan you create.2. Use the -replace option with terraform apply For Terraform v0.15.2 and later terraform force-unlock 1. Manually unlock the state for the defined configuration.2. This command removes the lock on the state for the current configuration. The behavior of this lock is dependent on the backend being used terraform workspace 1. Terraform workspace command is used to manage worsspaces 2. State files for each workspace are stored in the directory terraform.tfstate.d3. terraform workspace list to list all existing workspaces4. terraform workspace select prod to switch to existing workspace with name prod5. terraform workspace new uat to create and switch to new workspace with name uat6. terraform workspace delete dev to delete existing workspace with name dev and it must not be your current workspace7. terraform workspace show to show current workspace terraform import Used to import the existing remote resources into the Terraform state file. This allows Terraform to manage resources that were created outside of Terraform. terraform import command used using below three steps:-\n//1. first write empty resource block in config *.tf* file resource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { # ...instance configuration... } //2. then run command to import existing remote aws instance with id i-abcd1234 to state *.tfstate* file $ terraform import aws_instance.example i-abcd1234 //3. copy the required configuration manually from state .tfstate file to config *.tf* file Source: https://developer.hashicorp.com/terraform/cli\nTerraform Functions Function Type Function Name Numeric abs(-12.4) = 12.4ceil(5.1) = 6floor(4.9) = 4log(16, 2) = 4max(12, 54, 3) = 54min(12, 54, 3) = 3parseint(\u0026quot;100\u0026quot;, 10) = 100 and parseint(\u0026quot;FF\u0026quot;, 16) = 255pow(3, 2) = 9signum(-13) = -1, signum(0) = 0, and signum(344) = 1 String chomp(\u0026quot;hello\\r\\n\u0026quot;) = hellostartswith(\u0026quot;hello world\u0026quot;, \u0026quot;hello\u0026quot;) = trueendswith(\u0026quot;hello world\u0026quot;, \u0026quot;world\u0026quot;) = trueformat(\u0026quot;There are %d lights\u0026quot;, 4) = There are 4 lightsformatlist(\u0026quot;Hello, %s!\u0026quot;, [\u0026quot;Olivia\u0026quot;, \u0026quot;Sam\u0026quot;]) = [\u0026quot;Hello, Olivia!\u0026quot;, \u0026quot;Hello, Sam!\u0026quot;]indent(5, \u0026quot;hello\u0026quot;) = ・・・・・\u0026quot;hello\u0026quot;join(\u0026quot;-\u0026quot;, [\u0026quot;foo\u0026quot;, \u0026quot;bar\u0026quot;, \u0026quot;baz\u0026quot;]) = foo-bar-bazlower(\u0026quot;HELLO\u0026quot;) = helloupper(\u0026quot;hello\u0026quot;) = HELLOregex(\u0026quot;[a-z]+\u0026quot;, \u0026quot;53453453.345345aaabbbccc23454\u0026quot;) = aaabbbcccregexall(\u0026quot;[a-z]+\u0026quot;, \u0026quot;1234abcd5678efgh9\u0026quot;) = [\u0026quot;abcd\u0026quot;, \u0026quot;efgh\u0026quot;]replace(\u0026quot;1 + 2 + 3\u0026quot;, \u0026quot;+\u0026quot;, \u0026quot;-\u0026quot;) = 1-2-3split(\u0026quot;,\u0026quot;, \u0026quot;foo,bar,baz\u0026quot;) = [\u0026quot;foo\u0026quot;,\u0026quot;bar\u0026quot;,\u0026quot;baz\u0026quot;]strrev(\u0026quot;hello\u0026quot;) = ollehsubstr(\u0026quot;hello world\u0026quot;, 1, 4) = ellotitle(\u0026quot;hello world\u0026quot;) = Hello Worldtrim(\u0026quot;foobar\u0026quot;, \u0026quot;far\u0026quot;) = oobtrimprefix(\u0026quot;helloworld\u0026quot;, \u0026quot;hello\u0026quot;) = worldtrimsuffix(\u0026quot;helloworld\u0026quot;, \u0026quot;world\u0026quot;) = hellotrimspace(\u0026quot; hello\\n\\n\u0026quot;) = hello Collection alltrue([\u0026quot;true\u0026quot;, true]) = trueanytrue([true, false]) = truechunklistcoalesce(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;) = a and coalesce(\u0026quot;\u0026quot;, \u0026quot;b\u0026quot;) = bcoalescelist([\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;], [\u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;]) = [\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;] and coalescelist([], [\u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;]) = [\u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;]compact([\u0026quot;a\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;]) = [\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;]concat([\u0026quot;a\u0026quot;, \u0026quot;\u0026quot;], [\u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;]) = [\u0026quot;a\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;]contains([\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;], \u0026quot;a\u0026quot;) = truedistinct([\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;, \u0026quot;b\u0026quot;]) = [\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;]element([\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;], 1) = bflatten([[\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;], [], [\u0026quot;c\u0026quot;]]) = [\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;]index([\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;], \u0026quot;b\u0026quot;) = 1keys({a=1, c=2, d=3}) = [\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;d\u0026quot;]length([\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;]) = 2tolist([a, b, c]) converts to listlookuptomap({a = \u0026quot;b\u0026quot; c = \u0026quot;d\u0026quot;}) converts to mapmatchkeysmergeonerange(3) = [0, 1, 2] and range(1, 4) = [1, 2, 3]reverse([1, 2, 3]) = [3, 2, 1]setintersection([\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;], [\u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;], [\u0026quot;b\u0026quot;, \u0026quot;d\u0026quot;]) = [\u0026quot;b\u0026quot;]setproductsetsubtract([\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;], [\u0026quot;a\u0026quot;, \u0026quot;c\u0026quot;]) = [\u0026quot;b\u0026quot;]setunion([\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;], [\u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;], [\u0026quot;d\u0026quot;]) = [\u0026quot;d\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;a\u0026quot;]slice([\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;], 1, 3) = [\u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;]sort([\u0026quot;e\u0026quot;, \u0026quot;d\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;x\u0026quot;]) = [\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;e\u0026quot;, \u0026quot;x\u0026quot;]sum([10, 13, 6, 4.5]) = 33.5transposevalues({a=3, c=2, d=1}) = [3, 2, 1]zipmap([\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;], [1, 2]) = {\u0026quot;a\u0026quot;=1 \u0026quot;b\u0026quot;=2} Encoding base64encode(\u0026quot;Hello World\u0026quot;) = SGVsbG8gV29ybGQ=base64decode(\u0026quot;SGVsbG8gV29ybGQ=\u0026quot;) = Hello Worldbase64gziptextencodebase64(\u0026quot;Hello World\u0026quot;, \u0026quot;UTF-16LE\u0026quot;) = SABlAGwAbABvACAAVwBvAHIAbABkAA==textdecodebase64(\u0026quot;SABlAGwAbABvACAAVwBvAHIAbABkAA==\u0026quot;, \u0026quot;UTF-16LE\u0026quot;) = Hello Worldcsvdecodejsondecodejsonencodeurlencode(\u0026quot;Hello World!\u0026quot;) = Hello+World%21yamldecodeyamlencode Filesystem abspath(path.root) = /home/user/some/terraform/rootdirname(\u0026quot;foo/bar/baz.txt\u0026quot;) = foo/barpathexpand(\u0026quot;~/.ssh/id_rsa\u0026quot;) = /home/steve/.ssh/id_rsabasename(\u0026quot;foo/bar/baz.txt\u0026quot;) = baz.txtfile(\u0026quot;${path.module}/hello.txt\u0026quot;) = Hello Worldfileexists(\u0026quot;${path.module}/hello.txt\u0026quot;) = truefileset(path.module, \u0026quot;files/*.txt\u0026quot;) = [\u0026quot;files/hello.txt\u0026quot;, \u0026quot;files/world.txt\u0026quot;]filebase64(\u0026quot;${path.module}/hello.txt\u0026quot;) = SGVsbG8gV29ybGQ=templatefile(path, vars) Date and Time formatdate(\u0026quot;DD MMM YYYY hh:mm ZZZ\u0026quot;, \u0026quot;2018-01-02T23:12:01Z\u0026quot;) = 02 Jan 2018 23:12 UTCtimeadd(\u0026quot;2017-11-22T00:00:00Z\u0026quot;, \u0026quot;10m\u0026quot;) = 2017-11-22T00:10:00Ztimecmp(\u0026quot;2017-11-22T00:00:00Z\u0026quot;, \u0026quot;2017-11-22T01:00:00Z\u0026quot;) = -1timestamp() = 2023-05-13T07:44:12Z Hash and Crypto base64sha256(\u0026quot;hello world\u0026quot;) = uU0nuZNNPgilLlLX2n2r+sSE7+N6U4DukIj3rOLvzek=filebase64sha256()base64sha512(\u0026quot;hello world\u0026quot;) = MJ7MSJwS1utMxA9QyQLytNDtd+5RGnx6m808qG1M2G+YndNbxf9JlnDaNCVbRbDP2DDoH2Bdz33FVC6TrpzXbw==filebase64sha512()md5(\u0026quot;hello world\u0026quot;) = 5eb63bbbe01eeed093cb22bb8f5acdc3filemd5()sha1(\u0026quot;hello world\u0026quot;) = 2aae6c35c94fcfb415dbe95f408b9ce91ee846edfilesha1()sha256(\u0026quot;hello world\u0026quot;) = b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9filesha256()sha512(\u0026quot;hello world\u0026quot;) = 309ecc489c12d6eb4cc40f50c902f2b4d0ed77ee511a7c7a9bcd3ca86d4cd86f989dd35bc5ff499670da34255b45b0cfd830e81f60filesha512()bcrypt(\u0026quot;hello world\u0026quot;) = $2a$10$D5grTTzcsqyvAeIAnY/mYOIqliCoG7eAMX0/oFcuD.iErkksEbcAarsadecrypt(filebase64(\u0026quot;${path.module}/ciphertext\u0026quot;), file(\u0026quot;privatekey.pem\u0026quot;)) = hello worlduuid() = b5ee72a3-54dd-c4b8-551c-4bdc0204cedbuuidv5(\u0026quot;dns\u0026quot;, \u0026quot;www.terraform.io\u0026quot;) = a5008fae-b28c-5ba5-96cd-82b4c53552d6 IP Network cidrhost, cidrnetmask, cidrsubnet, cidrsubnets Type Conversion can, nonsensitive, sensitive, tobool, tolist, tomap, tonumber, toset, tostring, try, type Source: https://developer.hashicorp.com/terraform/language/functions\nTerraform Tiers Feature ▼ OSS (Open Source) Cloud Free Cloud Team \u0026amp; Governance Cloud Business Enterprise Infrastructure as code (HCL) ✅ ✅ ✅ ✅ ✅ Workspaces ✅ ✅ ✅ ✅ ✅ Variables ✅ ✅ ✅ ✅ ✅ Runs (separate plan and apply) ✅ ✅ ✅ ✅ ✅ Resource graph ✅ ✅ ✅ ✅ ✅ Providers ✅ ✅ ✅ ✅ ✅ Modules ✅ ✅ ✅ ✅ ✅ Public registry ✅ ✅ ✅ ✅ ✅ Remote state storage ⬜ ✅ ✅ ✅ ✅ Secure variable storage ⬜ ✅ ✅ ✅ ✅ Remote runs (plan \u0026amp; apply) ⬜ ✅ ✅ ✅ ✅ Private registry ⬜ ✅ ✅ ✅ ✅ Projects ⬜ ✅ ✅ ✅ ✅ Dynamic provider credentials ⬜ ✅ ✅ ✅ ✅ Run triggers ⬜ ✅ ✅ ✅ ✅ VCS connection ⬜ ✅ ✅ ✅ ✅ Workspace management ⬜ ✅ ✅ ✅ ✅ Team management ⬜ ⬜ ✅ ✅ ✅ Cost estimation ⬜ ⬜ ✅ ✅ ✅ Policy as code (Sentinel) ⬜ ⬜ ✅ ✅ ✅ Policy as code (OPA) ⬜ ⬜ ✅ ✅ ✅ Run tasks: Advisory enforcement ⬜ ⬜ ✅ ✅ ✅ Run tasks: Hard enforcement ⬜ ⬜ ✅ ✅ ✅ Policy enforcement level: Advisory ⬜ ⬜ ✅ ✅ ✅ Policy enforcement level: Soft/hard mandatory ⬜ ⬜ ✅ ✅ ✅ No-code provisioning ⬜ ⬜ ⬜ ✅ ⬜ Configuration designer ⬜ ⬜ ⬜ ✅ ✅ SSO (Single Sign On) ⬜ ⬜ ⬜ ✅ ✅ Support for ServiceNow integration ⬜ ⬜ ⬜ ✅ ✅ Drift detection ⬜ ⬜ ⬜ ✅ ✅ Audit logging ⬜ ⬜ ⬜ ✅ ✅ Concurrent Runs ⬜ 1 2(Add-on) Pay to customize ✅ Self-hosted agents ⬜ ⬜ ⬜ Pay to customize ✅ Cross-organization registry sharing ⬜ ⬜ ⬜ ⬜ ✅ Runtime metrics (Prometheus) ⬜ ⬜ ⬜ ⬜ ✅ Air gap network deployment ⬜ ⬜ ⬜ ⬜ ✅ Application-level logging ⬜ ⬜ ⬜ ⬜ ✅ Log forwarding ⬜ ⬜ ⬜ ⬜ ✅ Clustered Horizontal Scaling ⬜ ⬜ ⬜ ⬜ ✅ Source: https://www.hashicorp.com/products/terraform/pricing\n","permalink":"https://codingnconcepts.com/post/terraform-associate-exam-guide/","tags":["Terraform","Certification"],"title":"Hashicorp Terraform Associate (003) Exam Guide"},{"categories":["Java"],"contents":"In this article, we\u0026rsquo;ll learn how to validate IPv4 addresses using Java Regex\nIP Address Format Let\u0026rsquo;s first take a look at typical IPv4 address examples:-\n0.0.0.0 172.16.254.1 255.255.255.255 192.168.0.1 192.168.1.255 We have the following observations from the above examples:-\nA valid IPv4 Address is in the form of A.B.C.D The length of A, B, C, and D lies between 1 to 3 digits The value of A, B, C, and D lies between 0 to 255 Leading 0\u0026rsquo;s not allowed Regex to validate IP Address ^((\\\\d|[1-9]\\\\d|1\\\\d\\\\d|2[0-4]\\\\d|25[0-5])\\\\.?\\\\b){4}$ Where,\n\\\\d to match single-digit numbers between 0 to 9 [1-9]\\\\d to match numbers between 10 to 99 1\\\\d\\\\d to match numbers between 100 to 199 2[0-4]\\\\d to match numbers between 200 to 249 25[0-5] to match numbers between 250 to 255 (\\\\d|[1-9]\\\\d|1\\\\d\\\\d|2[0-4]\\\\d|25[0-5]) to match numbers between 0 to 255 \\\\.? to match the dot operator . after each number between 0 to 255 \\\\b is a word boundary {4} to match 4 sets of such numbers (i.e. between 0 to 255) We can further concise the above regex taking the common \\\\d out:-\n^(((|[1-9]|1\\\\d|2[0-4])\\\\d|25[0-5])\\\\.?\\\\b){4}$ Validate IP address in Java Let\u0026rsquo;s look at the Java method to validate the IPv4 addresses using the above regex:-\npublic static boolean validateIPv4Address(String ipv4){ String regex = \u0026#34;^(((|[1-9]|1\\\\d|2[0-4])\\\\d|25[0-5])\\\\.?\\\\b){4}$\u0026#34;; Pattern pattern = Pattern.compile(regex); Matcher matcher = pattern.matcher(ipv4); return matcher.matches(); } Let\u0026rsquo;s use the above method to run test cases:-\nTest valid IPv4 addresses:- @Test public void validateIPv4Address_validIPv4Addresses(){ assertTrue(validateIPv4Address(\u0026#34;0.0.0.0\u0026#34;)); // pass assertTrue(validateIPv4Address(\u0026#34;127.0.0.1\u0026#34;)); // pass assertTrue(validateIPv4Address(\u0026#34;192.168.10.1\u0026#34;)); // pass assertTrue(validateIPv4Address(\u0026#34;172.16.254.1\u0026#34;)); // pass assertTrue(validateIPv4Address(\u0026#34;192.168.1.255\u0026#34;)); // pass assertTrue(validateIPv4Address(\u0026#34;255.255.255.255\u0026#34;)); // pass assertTrue(validateIPv4Address(\u0026#34;10.98.30.56\u0026#34;)); // pass } Test invalid IPv4 addresses:- @Test public void validateIPv4Address_invalidIPv4Addresses(){ assertFalse(validateIPv4Address(\u0026#34;192.168.0.256\u0026#34;)); // value above 255 assertFalse(validateIPv4Address(\u0026#34;192.168.0.01\u0026#34;)); // leading 0 in 01 assertFalse(validateIPv4Address(\u0026#34;192.168.0\u0026#34;)); // only 3 sets, 4th missing assertFalse(validateIPv4Address(\u0026#34;.192.168.0.1\u0026#34;)); // starts with . assertFalse(validateIPv4Address(\u0026#34;.192.168.0.1.\u0026#34;)); // ends with . } ","permalink":"https://codingnconcepts.com/java/regex-validate-ipv4-address/","tags":["Core Java","Regex"],"title":"Java Regex to Validate IPv4 Address"},{"categories":["Java"],"contents":"In this article, we will learn how to assert Optional in Java using JUnit 5 and AssertJ assertion libraries.\nOverview We usually write a unit test to verify the expected output from a method. When a method returns an Optional object, we have to write test cases to check if the Optional is present, empty or has an expected value. We can do that using Junit 5 and AssertJ libraries.\nAssert Optional Let\u0026rsquo;s look at quick examples to write test cases for returning Optional Object using Junit 5 and AssertJ libraries.\nJUnit 5 JUnit 5 doesn\u0026rsquo;t provide great support for Optional like AssertJ library though we managed to do all basic Optional assertions in the below examples:-\nTest that a value is present in Optional:-\nOptional\u0026lt;String\u0026gt; optional = Optional.of(\u0026#34;foo\u0026#34;); assertTrue(optional.isPresent()); // pass Test that a value is not present or empty in Optional:-\nOptional\u0026lt;String\u0026gt; optional = Optional.empty(); assertFalse(optional.isPresent()); // pass Test an Optional String:-\nOptional\u0026lt;String\u0026gt; optional = Optional.of(\u0026#34;foo\u0026#34;); assertTrue(optional.isPresent()); assertEquals(\u0026#34;foo\u0026#34;, optional.get()); Test an Optional Object having multiple properties:-\nOptional\u0026lt;User\u0026gt; userOptional = Optional.of(new User(\u0026#34;Jack\u0026#34;, 21, true)); assertTrue(userOptional.isPresent()); userOptional.ifPresent(user -\u0026gt; { assertEquals(\u0026#34;Jack\u0026#34;, user.getFirstName()); assertTrue(user.getAge() \u0026gt; 18); assertTrue(user.getIsPremiumUser()); }); Note that Junit 5 doesn\u0026rsquo;t provide any assertion to test multiple properties of an Optional Object so we made use of Optional.ifPresent() to assert all the properties.\nAlso Read Unit Test with JUnit 5 in Java for more details.\nAssertJ AssertJ has good support for Optional objects and provides many fluent assertions specific to Optional. Let\u0026rsquo;s look at the below examples:-\nTest that a value is present in Optional:-\nOptional\u0026lt;String\u0026gt; optional = Optional.of(\u0026#34;foo\u0026#34;); assertThat(optional).isPresent(); // pass assertThat(optional).isNotEmpty(); // pass Note that isPresent() and isNotEmpty() are alias and can be used interchangeably.\nTest that a value is not present or empty in Optional:-\nOptional\u0026lt;String\u0026gt; optional = Optional.empty(); assertThat(optional).isEmpty(); // pass assertThat(optional).isNotPresent(); // pass Note that isEmpty() and isNotPresent() are alias and anyone can be used interchangeably.\nTest an Optional String:-\nOptional\u0026lt;String\u0026gt; optional = Optional.of(\u0026#34;foo\u0026#34;); assertThat(optional) .isPresent() .isNotEmpty() .containsInstanceOf(String.class) .hasValue(\u0026#34;foo\u0026#34;) .contains(\u0026#34;foo\u0026#34;); Note that hasValue() and contains() are alias and anyone can be used interchangeably.\nTest an Optional Object having multiple properties:-\nOptional\u0026lt;User\u0026gt; userOptional = Optional.of(new User(\u0026#34;Jack\u0026#34;, 21, true)); assertThat(userOptional) .isPresent() .isNotEmpty() .containsInstanceOf(User.class) .hasValueSatisfying(user -\u0026gt; { assertThat(user.getFirstName()).isEqualTo(\u0026#34;Jack\u0026#34;); assertThat(user.getAge()).isGreaterThan(18); assertThat(user.getIsPremiumUser()).isTrue(); }); We have tested all the properties of an Optional Object using hasValueSatisfying() in the above example.\nAlso Read Unit Test with AssertJ in Java for more details.\n","permalink":"https://codingnconcepts.com/java/java-test-assert-optional/","tags":["JUnit","Java Assertion"],"title":"Assert Optional Value in Java"},{"categories":["Java"],"contents":"In this article, we will learn how to assert thrown exceptions in Java using JUnit 5 and AssertJ assertion libraries.\nOverview We usually write a unit test to verify the expected output from a piece of code. We also expect that this piece of code can throw an exception in some situations and we also want to cover those cases in our unit test.\nIt is recommended to cover all those exceptions that are expected to be thrown by the code. Let\u0026rsquo;s see how we can assert thrown exceptions using JUnit 5 and AssertJ.\nAssert Thrown Exception Let\u0026rsquo;s say, we have a method doStuff in the class FooService. This method can throw a NullPointerException when a null flag is passed in the argument.\npublic class FooService { public void doStuff(Boolean flag) { try{ if(flag){ // do stuff } }catch (Exception e){ throw new RuntimeException(\u0026#34;Unexpected error occurred\u0026#34;, e); } } } Junit 5 - assertThrows In Junit 5, we can test that:-\nan exception of a specific type is expected to be thrown by the code using assertThrows() assertion. an exception is not expected to be thrown by the code using assertDoesNotThrow() assertion. assertThrows(ExpectedException.class, () -\u0026gt; methodCall) assertDoesNotThrow(() -\u0026gt; methodCall) Let\u0026rsquo;s look at the example usage of assertThrows and assertDoesNotThrow:-\nimport static org.junit.jupiter.api.Assertions.*; FooService fooService = new FooService(); @Test public void doStuff_testThrownException(){ // null is passed, expected NullPointerException Throwable exception = assertThrows(RuntimeException.class, () -\u0026gt; fooService.doStuff(null)); assertEquals(\u0026#34;Unexpected error occurred\u0026#34;, exception.getMessage()); assertEquals(NullPointerException.class, exception.getCause().getClass()); } @Test public void doStuff_shouldNotThrowException(){ // method is expected to work fine without exception assertDoesNotThrow(() -\u0026gt; fooService.doStuff(true)); } Please note that assertThrows() returns the Throwable exception, which we used to assert the exception message and exception cause in the above example.\nLet\u0026rsquo;s look at some more example usage:-\n// Assert IllegalArgumentException assertThrows(IllegalArgumentException.class, () -\u0026gt; Integer.valueOf(\u0026#34;foo\u0026#34;)); // Assert IndexOutOfBoundsException Exception e = assertThrows(IndexOutOfBoundsException.class, () -\u0026gt; Arrays.asList(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;).get(2)); assertEquals(\u0026#34;Index 2 out of bounds for length 2\u0026#34;, e.getMessage()); // Assert RuntimeException String str = null; assertThrows(RuntimeException.class, () -\u0026gt; str.equals(\u0026#34;foo\u0026#34;)); // Assert NullPointerException - exact match assertThrowsExactly(NullPointerException.class, () -\u0026gt; str.equals(\u0026#34;foo\u0026#34;)); Also Read Unit Test with JUnit 5 in Java for more details.\nAssertJ - assertThatExceptionOfType In AssertJ, we can test that:-\nany type of exception is expected to be thrown by the code using assertThatThrownBy() assertion. a specific type of exception is expected to be thrown by the code using assertThatExceptionOfType() assertion. an exception is not expected to be thrown by the code using assertThatNoException() assertion. assertThatThrownBy(() -\u0026gt; methodCall) assertThatExceptionOfType(ExpectedException.class).isThrownBy(() -\u0026gt; methodCall) assertThatNoException().isThrownBy(() -\u0026gt; methodCall) Let’s look at the example usage:-\nimport static org.assertj.core.api.Assertions.*; FooService fooService = new FooService(); @Test public void doStuff_testThrownException(){ // null is passed, expected NullPointerException // example 1: assert for any exception assertThatThrownBy(() -\u0026gt; fooService.doStuff(null)) .isInstanceOf(RuntimeException.class) .hasMessage(\u0026#34;Unexpected error occurred\u0026#34;) .hasCauseInstanceOf(NullPointerException.class); // example 2: assert for exception of type RuntimeException assertThatExceptionOfType(RuntimeException.class) .isThrownBy(() -\u0026gt; fooService.doStuff(null)) .withMessage(\u0026#34;Unexpected error occurred\u0026#34;) .withCauseInstanceOf(NullPointerException.class); // example 3: shortcut assert for RuntimeException assertThatRuntimeException().isThrownBy(() -\u0026gt; fooService.doStuff(null)) .withMessage(\u0026#34;Unexpected error occurred\u0026#34;) .withCauseInstanceOf(NullPointerException.class); } @Test public void doStuff_shouldNotThrowException(){ // method is expected to work fine without exception assertThatNoException().isThrownBy(() -\u0026gt; fooService.doStuff(true)); } We can use any assertion out of assertThatThrownBy, assertThatExceptionOfType, and assertThatRuntimeException to test RuntimeException, last one is more readable.\nShortcut Assertions AssertJ comes with shortcut assertions for popular exceptions e.g. IndexOutOfBoundsException, IllegalArgumentException, and NullPointerException. Let’s look at them:-\n@Test public void testSomeMoreThrownException() { assertThatIndexOutOfBoundsException().isThrownBy(() -\u0026gt; Arrays.asList(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;).get(2)) .withMessage(\u0026#34;Index 2 out of bounds for length 2\u0026#34;); assertThatIllegalArgumentException().isThrownBy(() -\u0026gt; Integer.valueOf(\u0026#34;foo\u0026#34;)) .isInstanceOf(NumberFormatException.class); assertThatNullPointerException().isThrownBy(() -\u0026gt; { String text = null; text.equals(\u0026#34;foo\u0026#34;); }); } Also Read Unit Test with AssertJ in Java for more details.\n","permalink":"https://codingnconcepts.com/java/java-test-assert-thrown-exception/","tags":["JUnit","Java Assertion"],"title":"Assert Thrown Exception in Java"},{"categories":["Java"],"contents":"In this article, we will learn how to assert Object\u0026rsquo;s multiple properties in a single assert in JUnit 5, Hamcrest, and AssertJ assertion libraries.\nOverview We usually write a unit test to verify the expected output from a piece of code. When the expected output is an Object, there are two ways to verify the properties of an Object:-\nWrite an assertion for each property of an object and verify it individually Group the assertion for all properties of an object together and verify them collectively The collective assertion of object properties is more human-readable and maintainable. Let\u0026rsquo;s see how we can group assertions using JUnit, Hamcrest and AssertJ.\nGroup the Assertions Let\u0026rsquo;s say, we want to verify Product Object, which has various types of properties, ranging from Long, String, Boolean, Integer, BigDecimal to List.\npublic class Product { private Long id; private String name; private Boolean onSale; private Integer stockQuantity; private BigDecimal price; private List\u0026lt;String\u0026gt; labels; } Junit 5 - assertAll We can collectively assert the Object\u0026rsquo;s properties using assertAll() in Junit 5. Let\u0026rsquo;s look at the example usage:-\nVerify the value of each Object\u0026rsquo;s property using assertEquals() Verify the various Object\u0026rsquo;s property conditions using assertNotNull() and assertTrue() assertAll(messge, () -\u0026gt; assertFn(expected, actual), () -\u0026gt; assertFn(...), () -\u0026gt; assertFn(...) ) import static org.junit.jupiter.api.Assertions.*; @Test public void givenObject_testAllProperties() { // Object to test Product product = new Product(1L, \u0026#34;Office Desk\u0026#34;, true, 50, new BigDecimal(\u0026#34;599.99\u0026#34;), Arrays.asList(\u0026#34;Wooden\u0026#34;, \u0026#34;Electric\u0026#34;)); // 1. Verify property values collectively assertAll(\u0026#34;Verify All Product Property Values\u0026#34;, () -\u0026gt; assertEquals(1L, product.getId()), () -\u0026gt; assertEquals(\u0026#34;Office Desk\u0026#34;, product.getName()), () -\u0026gt; assertEquals(true, product.getOnSale()), () -\u0026gt; assertEquals(50, product.getStockQuantity()), () -\u0026gt; assertEquals(\u0026#34;599.99\u0026#34;, product.getPrice().toString()), () -\u0026gt; assertEquals(Arrays.asList(\u0026#34;Wooden\u0026#34;, \u0026#34;Electric\u0026#34;), product.getLabels())); // 2. Test property conditions collectively assertAll(\u0026#34;Verify Product Property Conditions\u0026#34;, () -\u0026gt; assertNotNull(product), () -\u0026gt; assertNotNull(product.getId()), () -\u0026gt; assertTrue(product.getName().contains(\u0026#34;Desk\u0026#34;)), // Product name contains \u0026#39;Desk\u0026#39; () -\u0026gt; assertTrue(product.getOnSale()), // Product on sale () -\u0026gt; assertTrue(product.getStockQuantity() \u0026gt; 0), // Stock is available () -\u0026gt; assertTrue(product.getPrice().compareTo(new BigDecimal(1000.0)) \u0026lt; 0), // Price under 1000 () -\u0026gt; assertTrue(product.getLabels().contains(\u0026#34;Wooden\u0026#34;))); // Looking for \u0026#39;Wooden\u0026#39; Desk } Hamcrest - allOf We can combine the Hamcrest matchers using allOf() to collectively assert the Object\u0026rsquo;s properties.\nassertThat(message, object, allOf(matcher1, matcher2, ...)) Let\u0026rsquo;s look a the example usage:-\nimport static org.hamcrest.MatcherAssert.assertThat; import static org.hamcrest.Matchers.*; @Test public void givenObject_testAllProperties() { // Object to test Product product = new Product(1L, \u0026#34;Office Desk\u0026#34;, true, 50, new BigDecimal(\u0026#34;599.99\u0026#34;), Arrays.asList(\u0026#34;Wooden\u0026#34;, \u0026#34;Electric\u0026#34;)); // Verify property values collectively assertThat(\u0026#34;Verify All Product Property Values\u0026#34;, product, allOf( hasProperty(\u0026#34;id\u0026#34;, equalTo(1L)), hasProperty(\u0026#34;name\u0026#34;, equalTo(\u0026#34;Office Desk\u0026#34;)), hasProperty(\u0026#34;onSale\u0026#34;, is(true)), hasProperty(\u0026#34;stockQuantity\u0026#34;, equalTo(50)), hasProperty(\u0026#34;price\u0026#34;, is(new BigDecimal(\u0026#34;599.99\u0026#34;))), hasProperty(\u0026#34;labels\u0026#34;, equalTo(List.of(\u0026#34;Wooden\u0026#34;, \u0026#34;Electric\u0026#34;))))); // Test property conditions collectively assertThat(\u0026#34;Verify Product Property Conditions\u0026#34;, product, allOf( notNullValue(), hasProperty(\u0026#34;id\u0026#34;, notNullValue()), hasProperty(\u0026#34;name\u0026#34;, endsWith(\u0026#34;Desk\u0026#34;)), // Product name endsWith \u0026#39;Desk\u0026#39; hasProperty(\u0026#34;onSale\u0026#34;, is(true)), // Product on sale hasProperty(\u0026#34;stockQuantity\u0026#34;, greaterThan(0)), // Stock is available hasProperty(\u0026#34;price\u0026#34;, lessThan(new BigDecimal(1000.0))), // Price under 1000 hasProperty(\u0026#34;labels\u0026#34;, hasItem(\u0026#34;Electric\u0026#34;)))); // Looking for \u0026#39;Electric\u0026#39; Desk } AssertJ - extracting..containsExactly In AssertJ, we can collectively assert the Object\u0026rsquo;s properties by extracting them as a List using extracting() and then chain them with containsExactly() to verify the:-\nvalue of each Object\u0026rsquo;s property extracted from extracting() by passing field names result of each Object\u0026rsquo;s property condition extracted from extracting() by passing conditions assertThat(object) .describedAs(message) .extracting(fieldName1/condition1, fieldName2/condition2, ...) .containsExactly(value1/result1, value2/result2, ...) Let\u0026rsquo;s look at the example usage:-\nimport static org.assertj.core.api.Assertions.*; @Test public void givenObject_testAllProperties_extracting_containsExactly(){ // Object to test Product product = new Product(1L, \u0026#34;Office Desk\u0026#34;, true, 50, new BigDecimal(\u0026#34;599.99\u0026#34;), Arrays.asList(\u0026#34;Wooden\u0026#34;, \u0026#34;Electric\u0026#34;)); // 1. Verify property values collectively assertThat(product) .describedAs(\u0026#34;Verify All Product Property Values\u0026#34;) .extracting(\u0026#34;id\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;onSale\u0026#34;, \u0026#34;stockQuantity\u0026#34;, \u0026#34;price\u0026#34;, \u0026#34;labels\u0026#34;) .containsExactly(1L, \u0026#34;Office Desk\u0026#34;, true, 50, new BigDecimal(\u0026#34;599.99\u0026#34;), Arrays.asList(\u0026#34;Wooden\u0026#34;, \u0026#34;Electric\u0026#34;)); // 2. Test property conditions collectively assertThat(product) .describedAs(\u0026#34;Verify Product Property Conditions\u0026#34;) .extracting(Product::getId, Product::getName, Product::getOnSale, p -\u0026gt; p.getStockQuantity() \u0026gt; 0, p -\u0026gt; p.getPrice().compareTo(new BigDecimal(1000.0)) \u0026lt; 0, p -\u0026gt; p.getLabels().contains(\u0026#34;Wooden\u0026#34;)) .containsExactly(1L, \u0026#34;Office Desk\u0026#34;, true, true, true, true); } AssertJ - returns..from In AssertJ, we can collectively assert the Object\u0026rsquo;s property by chaining the returns() for each property together. We extract the:-\nvalue from Object\u0026rsquo;s property by passing the field name in from() result from Object\u0026rsquo;s property condition by passing the condition in from() assertThat(object) .describedAs(message) .returns(expected, from(fieldNameOrCondition)) .returns(...) .returns(...) Let\u0026rsquo;s look at the example usage:-\nimport static org.assertj.core.api.Assertions.*; @Test public void givenObject_testAllProperties_returns_from(){ // Object to test Product product = new Product(1L, \u0026#34;Office Desk\u0026#34;, true, 50, new BigDecimal(\u0026#34;599.99\u0026#34;), Arrays.asList(\u0026#34;Wooden\u0026#34;, \u0026#34;Electric\u0026#34;)); // 1. Verify property values collectively assertThat(product) .describedAs(\u0026#34;Verify All Product Property Values\u0026#34;) .returns(1L, from(Product::getId)) .returns(\u0026#34;Office Desk\u0026#34;, from(Product::getName)) .returns(true, from(Product::getOnSale)) .returns(50, from(Product::getStockQuantity)) .returns(new BigDecimal(\u0026#34;599.99\u0026#34;), from(Product::getPrice)) .returns(Arrays.asList(\u0026#34;Wooden\u0026#34;, \u0026#34;Electric\u0026#34;), from(Product::getLabels)); // 2. Test property conditions collectively assertThat(product) .describedAs(\u0026#34;Verify Product Property Conditions\u0026#34;) .isNotNull() .hasNoNullFieldsOrProperties() .returns(true, from(p -\u0026gt; p.getName().contains(\u0026#34;Desk\u0026#34;))) // Product name endsWith \u0026#39;Desk\u0026#39; .returns(true, from(Product::getOnSale)) // Product on sale .returns(true, from(p -\u0026gt; p.getStockQuantity() \u0026gt; 0)) // Stock is available .returns(true, from(p -\u0026gt; p.getPrice().compareTo(new BigDecimal(1000.0)) \u0026lt; 0)) // Price under 1000 .returns(true, from(p -\u0026gt; p.getLabels().contains(\u0026#34;Wooden\u0026#34;))); // Looking for \u0026#39;Wooden\u0026#39; Desk } ","permalink":"https://codingnconcepts.com/java/java-test-single-assert-multiple-properties/","tags":["JUnit","Java Assertion"],"title":"Test Object's multiple properties in Single Assert"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to read a PDF file in Java using different libraries.\nOverview Portable Document Format (PDF) is a popular and widely used file format for documents. PDF format is the first choice for electronic distribution (e.g. email attachments) and print media.\nUnlike text files, reading the data from PDF files is very complex and Java doesn\u0026rsquo;t provide native support for reading PDF files. The good news is that there are many open-source Java libraries available, that we can use. In this article, we\u0026rsquo;ll look at some of the popular libraries for reading PDF file in Java:-\nApache PDFBox Apache PDFBox library allows you to create new PDF documents, extract content from PDF, fill a PDF form, save PDF as an Image, split \u0026amp; merge, digital sign PDF files, print PDF files, and many more.\nApache PDFBox is quite easy to use if you\u0026rsquo;re doing basic text extraction, let\u0026rsquo;s look at the example:-\nLet\u0026rsquo;s first import the pdfbox dependency to the pom.xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.pdfbox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;pdfbox\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0.27\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; or build.gradle implementation group: \u0026#39;org.apache.pdfbox\u0026#39;, name: \u0026#39;pdfbox\u0026#39;, version: \u0026#39;2.0.27\u0026#39; Let\u0026rsquo;s look at the simple example of using Apache PDFBox to read text from a PDF file:- import org.apache.pdfbox.pdmodel.PDDocument; import org.apache.pdfbox.text.PDFTextStripper; import java.io.File; import java.io.IOException; public class PdfDocumentReader { public void readPdfFile(File file) throws IOException { try(PDDocument document = PDDocument.load(file)) { if (!document.isEncrypted()) { PDFTextStripper textStripper = new PDFTextStripper(); String text = textStripper.getText(document); System.out.println(\u0026#34;Text:\u0026#34; + text); } } } } In this example, we initialized the instance of PDDocument in the try block to auto-close the resources after reading the PDF file.\nPlease refer to apache pdfbox examples for more examples\niText 5 iText has open-source libraries to read and write complex PDF files. iText library provides low-level support to read a PDF file but is a bit complex to understand and work with.\nPlease note that iText 5 is EOL and transitioned to maintenance mode, meaning it only receives security-related releases and fixes to allow users who have developed their solutions using iText 5 to safely continue using it. No new features will be added. For new implementations, iText 7 is recommended.\nLet\u0026rsquo;s explore how to read PDF using iText 5:-\nLet\u0026rsquo;s first import the itextpdf dependency to the pom.xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.itextpdf\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;itextpdf\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.5.13.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; or build.gradle implementation group: \u0026#39;com.itextpdf\u0026#39;, name: \u0026#39;itextpdf\u0026#39;, version: \u0026#39;5.5.13.3\u0026#39; Let\u0026rsquo;s look at the simple example of using iText to read text from a PDF file:- import com.itextpdf.text.pdf.PdfReader; import com.itextpdf.text.pdf.parser.PdfTextExtractor; import java.io.IOException; public class PdfDocumentReader { public void readPdfFile(String fileName) throws IOException { PdfReader pdfReader = new PdfReader(fileName); int pages = pdfReader.getNumberOfPages(); StringBuilder text = new StringBuilder(); for (int i = 1; i \u0026lt;= pages; i++) { text.append(PdfTextExtractor.getTextFromPage(pdfReader, i)); } System.out.println(\u0026#34;Text:\u0026#34; + text); } } In this example, we initialized the instance of PdfReader to load a PDF file and then looped through the pages to extract the content from each page.\niText 7 iText 7 is recommended iText library to read a PDF file. Let\u0026rsquo;s explore that:-\nLet\u0026rsquo;s first import the itext7-core dependency to the pom.xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.itextpdf\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;itext7-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;7.2.5\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;/dependency\u0026gt; or build.gradle implementation group: \u0026#39;com.itextpdf\u0026#39;, name: \u0026#39;itext7-core\u0026#39;, version: \u0026#39;7.2.5\u0026#39;, ext: \u0026#39;pom\u0026#39; and write a code using iText to read text from a PDF file:- import com.itextpdf.kernel.pdf.PdfDocument; import com.itextpdf.kernel.pdf.PdfPage; import com.itextpdf.kernel.pdf.PdfReader; import com.itextpdf.kernel.pdf.canvas.parser.PdfTextExtractor; import java.io.File; import java.io.IOException; public class PdfDocumentReader { public void readPdfFileFromPdfBox7(File file) throws IOException { StringBuilder text = new StringBuilder(); try(PdfDocument document = new PdfDocument(new PdfReader(file))){ for (int i = 1; i \u0026lt;= document.getNumberOfPages(); ++i) { PdfPage page = document.getPage(i); text.append(PdfTextExtractor.getTextFromPage(page)); } } System.out.println(\u0026#34;Text:\u0026#34; + text); } } In this example, we initialized the instance of PdfDocument in the try block to auto-close the resources after reading the PDF file.\n","permalink":"https://codingnconcepts.com/java/read-pdf-file-java/","tags":["File","Advance Java"],"title":"Read PDF File in Java"},{"categories":["Java"],"contents":"Java has several assertion libraries that can help you with testing and debugging your code. In this article, we\u0026rsquo;ll look at various assertions libraries and compare them.\nOverview Test-driven development (TDD) has gained popularity in the last few years. TDD promotes writing tests before developing a functionality, which increases requirement understanding, reduces defects, and decreases bugs in the early phase of development. We write the unit tests for each piece of code and use assertion libraries to verify the expected output.\nThere are multiple libraries available in the Java ecosystem, some of the popular ones are: JUnit, Hamcrest, AssertJ, TestNG, and Truth. Let\u0026rsquo;s look at each one of them:-\nJunit Junit is the most widely used testing framework in Java. Junit comes bundled with many popular IDEs such as IntelliJ IDEA, Eclipse, NetBeans, and Visual Studio Code and build tools such as Maven, Gradle, and Ant Junit is the default testing module in Spring Boot Framework Junit serves as a foundation for developing and launching testing framework on the JVM Junit provides basic assertion methods. As a developer, you should be using Junit to write and run test cases in Java most of the time. Junit has limited assertion methods which are enough for writing basic test cases. You should consider using AssertJ or Hamcrest assertion methods along with Junit framework for writing complex test case conditions.\nAlso Read Write Efficient Test Cases with JUnit in Java\nimport org.junit.jupiter.api.Test; import static org.junit.jupiter.api.Assertions.*; public class MyClassTest { @Test public void testAddition() { int result = MyClass.add(2, 3); assertEquals(5, result); // pass assertNotEquals(0, result); // pass assertNotNull(result); // pass assertTrue(result \u0026gt; 0); // pass } } TestNG TestNG is a testing framework inspired by JUnit and NUnit but introduces some new functionalities that make it more powerful and easier to use TestNG provides more advanced features than JUnit, such as data-driven testing and parallel test execution TestNG is designed to cover all categories of tests: unit, functional, end-to-end, integration, etc\u0026hellip; TestNG is a popular library to write test cases in Selenium. import org.testng.Assert; public class MyClassTest { @Test public void testAddition() { int result = MyClass.add(2, 3); Assert.assertEquals(result, 5); } } Hamcrest Hamcrest is a widely used library that provides a set of matchers for testing conditions. It allows you to write more complex assertions by combining matchers using logical operators.\nHamcrest matchers are available in several languages - Java, Python, Ruby, Obj-C, PHP, Erlang, Swift, Rust, JavaScript (JsHamcrest), JavaScript (Hamjest), GO (Gocrest), and C# (NHamcrest) Hamcrest can be used with JUnit (all versions) and TestNG Hamcrest provides the ability to write custom matchers for testing custom classes Also Read Write Efficient Test Cases with Hamcrest in Java\nimport static org.hamcrest.MatcherAssert.assertThat; import static org.hamcrest.Matchers.equalTo; public class MyClassTest { @Test public void testAddition() { int result = MyClass.add(2, 3); assertThat(result, equalTo(5)); } } AssertJ AssertJ is a fluent assertion Java library that provides a rich set of assertions and truly helpful error messages. AssertJ is composed of several modules:- Core module to provide assertions for JDK types (String, Iterable, Stream, Path, File, Map…​) Guava module to provide assertions for Guava types (Multimap, Optional…​) Joda Time module to provide assertions for Joda Time types (DateTime, LocalDateTime) Neo4J module to provide assertions for Neo4J types (Path, Node, Relationship…​) DB module to provide assertions for relational database types (Table, Row, Column…​) Swing module provides a simple and intuitive API for functional testing of Swing user interfaces AssertJ has a very good starter guide and documentation to follow:- https://assertj.github.io/doc/ AssertJ assertions can be used with Junit framework to test complex conditions. AssertJ is designed to be super easy within your favorite IDE. Type assertThat followed by the object under test and a dot …​ and any Java IDE code completion will show you all available assertions like this:- AssertJ assertThat can be used like this:- import org.junit.jupiter.api.Test; import static org.assertj.core.api.Assertions.assertThat; public class MyClassTest { @Test public void testAddition() { int result = MyClass.add(2, 3); assertThat(result).isEqualTo(5); // pass assertThat(result).isNotZero(); // pass assertThat(result).isPositive(); // pass assertThat(result).isOdd(); // pass assertThat(result).isLessThan(6); // pass assertThat(result).isGreaterThan(0); // pass } } The Assertions class is the only class you need to start using AssertJ, it provides all the methods you need. One Assertions static import to rule them all …​ import static org.assertj.core.api.Assertions.*; \u0026hellip; or many if you prefer: import static org.assertj.core.api.Assertions.assertThat; // main one import static org.assertj.core.api.Assertions.atIndex; // for List assertions import static org.assertj.core.api.Assertions.entry; // for Map assertions import static org.assertj.core.api.Assertions.tuple; // when extracting several properties at once import static org.assertj.core.api.Assertions.fail; // use when writing exception tests import static org.assertj.core.api.Assertions.failBecauseExceptionWasNotThrown; // idem import static org.assertj.core.api.Assertions.filter; // for Iterable/Array assertions import static org.assertj.core.api.Assertions.offset; // for floating number assertions import static org.assertj.core.api.Assertions.anyOf; // use with Condition import static org.assertj.core.api.Assertions.contentOf; // use with File assertions Also Read Write Efficient Test Cases with AssertJ in Java\nTruth Truth is an assertion library developed by Google\u0026rsquo;s Guava team. It provides fluent assertions for Java and Android. Truth is used in the majority of the tests in Google’s codebase. Truth is inspired by AssertJ and you will find many similarities between Truth and AsserJ Truth provides fewer assertions as compared to AssertJ Similar to AsserJ, If you’re using an IDE with autocompletion, it will suggest a list of assertions you can make about the given type. import static com.google.common.truth.Truth.assertThat; public class MyClassTest { @Test public void testAddition() { int result = MyClass.add(2, 3); assertThat(result).isEqualTo(5); } } Comparison Let\u0026rsquo;s compare all the Java assertion libraries based on their usage, type of assertions available, customization support, etc.\nJunit vs TestNG vs Hamcrest vs AssertJ vs Truth Comparison Junit TestNG Hamcrest AssertJ Truth Available since 2000 since 2006 since 2006 since 2010 since 2011 Website junit.org testng.org hamcrest.org assertj.github.io truth.dev Source Code junit-team/junit5 cbeust/testng hamcrest/JavaHamcrest assertj/assertj google/truth Usage assertMethod (expected, actual) Assert.method (actual, expected) assertThat(actual, method(expected)) assertThat(actual) .method(expected) assertThat (actual) .method(expected) Soft Assertions* No Yes No Yes Yes Boolean checks Yes Yes Yes Yes Yes Number checks Yes Yes Yes Yes Yes String checks Yes Yes Yes Yes Yes Null checks Yes Yes Yes Yes Yes Array checks Yes Yes Yes Yes Yes Map checks No No Yes Yes Yes Date checks No No Yes Yes No Exception checks Yes Yes No Yes Yes Custom Message No Yes Yes Yes Yes Create Own Assertion No No Yes Yes Yes *Soft assertions do not throw an exception automatically on assertion failure but at the very end of the scenario and then show all failures within the scenario, not just the first one. Hard assertions throw an error during execution when the condition is not being met so test cases are marked as failed.\nHamcrest vs AssertJ Hamcrest and AssertJ are the two most popular open-source Java assertion libraries available in the market today. Let\u0026rsquo;s compare them:-\nComparison Hamcrest AssertJ First Release Back in March, 2006 Back in September, 2010 Website hamcrest.org assertj.github.io Source Code hamcrest/JavaHamcrest assertj/assertj Assertions Matchers Fluent assertions Contributors Less number of contributors and less commits More number of contributors with regular commits Popularity Declining Increasing Release Frequecy Latest version released 4 years ago Frequent releases every year and month Hamcrest is still the most used library and is available in many different languages other than Java. On the other hand, AssertJ is gaining more popularity over the past few years and matches up with Hamcrest as per google trends.\nConclusion JUnit and TestNG frameworks are used to automate the testing process, which also comes with basic assertions. Junit is more popular as compared to TestNG and enough to use for basic Projects. Hamcrest and AssertJ are used along with Junit to write more complex and readable assertions. Truth is similar to AssertJ but comparatively less popular and mostly used in Google\u0026rsquo;s codebase. Ultimately, the choice of assertion library depends on your personal preference and the needs of your project.\n","permalink":"https://codingnconcepts.com/java/java-test-assertion-libraries/","tags":["JUnit","Java Assertion"],"title":"Writing Effective Java Tests with Assertion Libraries"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to write efficient Junit test cases in Java using Junit 5 ( Jupiter ) assertions.\nJunit 5 Junit is the most popular unit-testing framework in Java. Junit 5 provides many different styles of testing and focuses on Java 8 and above.\nJUnit 5 = JUnit Platform + JUnit Jupiter + JUnit Vintage\nWhere,\nThe Junit Platform provides the foundation for running tests on the JVM.\nPopular IDEs like IntelliJ, Eclipse, and Visual Studio Code and build tools like Maven, Gradle provide first-hand support for the Junit platform. The Junit Jupiter provides annotations like @Test, @BeforeEach and assertions like assertEquals() to write test cases.\nJunit Jupiter is a default choice for writing test cases in Spring Framework and testing automation tools like Selenium. The Junit Vintage provides a TestEngine for running JUnit 3 and JUnit 4 based tests on the platform Setup Junit Jupiter Import the required junit-jupiter latest dependency for Java using Maven (pom.xml) or Gradle (build.gradle), whichever you are using:-\n\u0026lt;!-- add dependencies in pom.xml --\u0026gt; \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit-bom\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.9.1\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.junit.jupiter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit-jupiter\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; // add dependencies and test in build.gradle dependencies { testImplementation(platform(\u0026#39;org.junit:junit-bom:5.9.1\u0026#39;)) testImplementation(\u0026#39;org.junit.jupiter:junit-jupiter\u0026#39;) } test { useJUnitPlatform() testLogging { events \u0026#34;passed\u0026#34;, \u0026#34;skipped\u0026#34;, \u0026#34;failed\u0026#34; } } Junit dependency works well with Java, Spring Boot and Selenium.\nImport Junit Jupiter Once you add the Junit Jupiter dependency, you can import static assertions method in the unit test class like this:-\nimport static org.junit.jupiter.api.Assertions.*; The Junit Annotations reside in org.junit.jupiter.api package, which you can import like this:-\nimport org.junit.jupiter.api.Test; import org.junit.jupiter.api.BeforeEach; We are ready to write unit test cases with Junit 5.\nUsing Junit Assertions See the complete list of assertions available in org.junit.jupiter.api.Assertions Java documentation.\nassertNull The assertNull assertion can be used to test the null-ness of any type of Object.\nAssert that an Object is null:-\n@Test public void givenObject_whenNull_thenPass() { String text = null; Integer number = null; Double amount = null; Boolean flag = null; LocalDate date = null; List list = null; Map map = null; Character[] chars = null; assertNull(text); assertNull(number); assertNull(amount); assertNull(flag); assertNull(date); assertNull(list); assertNull(map); assertNull(chars); } assertNotNull The assertNotNull assertion can be used to test the non-null value of any type of Object.\nAssert that an Object is not null:-\n@Test public void givenObject_whenNotNull_thenPass() { String text = \u0026#34;foo\u0026#34;; Integer number = 1; Double amount = 100.5; Boolean flag = true; LocalDate date = LocalDate.now(); List list = Arrays.asList(); Map map = Collections.emptyMap(); Character[] chars = new Character[2]; assertNotNull(text); assertNotNull(number); assertNotNull(amount); assertNotNull(flag); assertNotNull(date); assertNotNull(list); assertNotNull(map); assertNotNull(chars); } assertEquals The assertEquals(expected, actual) assertion is used to test the expected value of any type e.g. Byte, Character, Double, Float, Integer, Long, Short, Object, and Collection like List and Map. The test is passed when expected.equals(actual) is true.\nAssert that given expected and actual Objects are equal:-\n@Test public void givenTwoObjects_whenEqual_thenPass() { String text = \u0026#34;foo\u0026#34;; Integer number = 1; Double amount = 100.5; Boolean flag = true; LocalDate date = LocalDate.now(); List list = Arrays.asList(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;); Map map = Map.of(\u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;); // assertEquals(expected, actual) assertEquals(\u0026#34;foo\u0026#34;, text); assertEquals(1, number); assertEquals(100.5, amount); assertEquals(true, flag); assertEquals(LocalDate.now(), date); assertEquals(Arrays.asList(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;), list); assertEquals(Map.of(\u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;), map); } 💡 Please note that assertEquals doesn\u0026rsquo;t work with Array and we should use assertArrayEquals to check the expected value of Array.\nassertArrayEquals The assertArrayEquals assertion is used to test that the Array contains the expected values in the given order. The Array can be of type boolean, byte, char, double, float, int, long, short, and Object.\nAssert that expected and actual Array items are equal:-\n@Test public void givenTwoArrays_whenEqual_thenPass() { Character[] chars = new Character[] {\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;}; String[] strings = new String[] {\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;}; Integer[] numbers = new Integer[] {1, 2}; Boolean[] flags = new Boolean[] {true, false}; // assertArrayEquals(expected, actual) assertArrayEquals(new Character[] {\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;}, chars); assertArrayEquals(new String[] { \u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;}, strings); assertArrayEquals(new Integer[] {1, 2}, numbers); assertArrayEquals(new Boolean[] {true, false}, flags); } assertIterableEquals The assertIterableEquals asserts the deep equality of any two iterables like ArrayList, LinkedList, and LinkedHashSet. Iterables are considered equal when their iterators return equal elements in the same order as each other.\nAssert that two iterables of type LinkedList and LinkedHashSet have all the given elements in the same order:-\n@Test void givenTwoIterables_whenItemsAreEqualAndSameOrder_thenPass() { List\u0026lt;String\u0026gt; list1 = new LinkedList\u0026lt;\u0026gt;(Arrays.asList(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;baz\u0026#34;)); Set\u0026lt;String\u0026gt; list2 = new LinkedHashSet\u0026lt;\u0026gt;(Arrays.asList(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;baz\u0026#34;)); assertIterableEquals(list1, list2); } assertLinesMatch The assertLinesMatch assert differs from other assertions that effectively only check String.equals(Object), in that it uses the following staged matching algorithm:-\nFor each pair of expected and actual lines do -\ncheck if expected.equals(actual) - if yes, continue with the next pair otherwise treat expected as a regular expression and check via String.matches(String) - if yes, continue with the next pair otherwise check if the expected line is a fast-forward marker, if yes then apply fast-forward and repeat the algorithm from step 1 The assertLineMatch assertion is quite useful to test the expected log lines, command-line output, and exception stack trace like this:-\n\u0026gt; ls -la total 1 drwxr-xr-x 0 root 512 Jan 1 1970 @Test void whenAssertingEqualityListOfStrings_thenEqual() { List\u0026lt;String\u0026gt; expected = Arrays.asList(\u0026#34;ls -la\u0026#34;, \u0026#34;total \\\\d+\u0026#34;, \u0026#34;drwxr-xr-x \\\\d+ root \\\\d+ Jan 1 1970\u0026#34;); List\u0026lt;String\u0026gt; actual = Arrays.asList(\u0026#34;ls -la\u0026#34;, \u0026#34;total 1\u0026#34;, \u0026#34;drwxr-xr-x 0 root 512 Jan 1 1970\u0026#34;); assertLinesMatch(expected, actual); } assertNotEquals The assertNotEquals(unexpected, actual) assertion is used to test the unexpected value of any type e.g. Byte, Character, Double, Float, Integer, Long, Short, Object, and Collection like List and Map. The test is passed when unexpected.equals(actual) is false.\nAssert that the given two Objects are not equal:-\n@Test public void givenTwoObjects_whenNotEqual_thenPass() { String text = \u0026#34;foo\u0026#34;; Integer number = 1; Double amount = 100.5; Boolean flag = true; LocalDate date = LocalDate.now(); List list = Arrays.asList(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;); Map map = Map.of(\u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;); // assertNotEquals(unexpected, actual) assertNotEquals(\u0026#34;bar\u0026#34;, text); assertNotEquals(2, number); assertNotEquals(1.5, amount); assertNotEquals(false, flag); assertNotEquals(LocalDate.now().plusDays(1), date); assertNotEquals(Arrays.asList(\u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;), list); assertNotEquals(Map.of(\u0026#34;key2\u0026#34;, \u0026#34;value2\u0026#34;), map); } assertSame The assertSame(expected, actual) assertion is used to test the expected and actual refer to the same object.\nAssert that the given two Objects refer to the same instance:-\n@Test public void givenObject_whenSame_thenPass() { String text = \u0026#34;foo\u0026#34;; Integer number = 1; Double amount = 100.5; Boolean flag = true; LocalDate date = LocalDate.now(); List list = Arrays.asList(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;); Map map = Map.of(\u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;); String sameAsText = text; Integer sameAsNumber = number; Double sameAsAmount = amount; Boolean sameAsFlag = flag; LocalDate sameAsDate = date; List sameAsList = list; Map sameAsMap = map; // assertSame(expected, actual) assertSame(sameAsText, text); assertSame(sameAsNumber, number); assertSame(sameAsAmount, amount); assertSame(sameAsFlag, flag); assertSame(sameAsDate, date); assertSame(sameAsList, list); assertSame(sameAsMap, map); } assertNotSame The assertNotSame(expected, actual) assertion is quite tricky.\nFor primitives like int, double, and boolean - assertNotSame is used to test that expected and actual values are not equal For objects like String, LocalDate, List, and Map - assertNotSame is used to test that expected and actual objects do not refer to same instance @Test public void givenObject_whenNotSame_thenPass() { Integer number = 1; Double amount = 100.5; Boolean flag = true; String text = \u0026#34;foo\u0026#34;; LocalDate date = LocalDate.now(); List list = Arrays.asList(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;); Map map = Map.of(\u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;); // assertNotSame(expected, actual) // assert that two primitives are not equals assertNotSame(2, number); assertNotSame(1.5, amount); assertNotSame(false, flag); // assert that two objects are equal but do not refer to same instance assertNotSame(new String(\u0026#34;foo\u0026#34;), text); assertNotSame(LocalDate.now(), date); assertNotSame(Arrays.asList(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;), list); assertNotSame(Map.of(\u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;), map); } assertTrue / assertFalse The assertTrue(condition) and assertFalse(condition) assertions can be used to test a variety of conditions that should be true or false. Let\u0026rsquo;s look at examples, where we can use these assertions:-\nAssert that two Strings are equal ignoring case:-\n@Test public void givenTwoStrings_whenEqualIgnoringCase_thenPass(){ String text1 = \u0026#34;foo\u0026#34;; String text2 = \u0026#34;FOO\u0026#34;; assertTrue(text1.equalsIgnoreCase(text2)); } Assert that two Arrays are equal:-\n@Test public void givenTwoArrays_whenEqual_thenPass(){ String[] array1 = new String[] {\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;}; String[] array2 = new String[] {\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;}; assertTrue(Arrays.equals(array1, array2)); } Assert that two Arrays are NOT equal:-\n@Test public void givenTwoArrays_whenNotEqual_thenPass(){ String[] array1 = new String[] {\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;}; String[] array2 = new String[] {\u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;}; assertFalse(Arrays.equals(array1, array2)); } Assert that a List contains the given value:-\n@Test public void givenList_whenContainsGivenValue_thenPass() { List\u0026lt;String\u0026gt; list = Arrays.asList(\u0026#34;lord\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;rings\u0026#34;); assertTrue(list.contains(\u0026#34;lord\u0026#34;)); } Assert that a Map contains the given key, value or key-value pair (Entry):-\n@Test public void givenMap_whenHasGivenKeyValueOrPair_thenPass() { Map\u0026lt;String, String\u0026gt; myMap = Map.of(\u0026#34;myKey\u0026#34;, \u0026#34;myValue\u0026#34;); //contains key assertTrue(myMap.containsKey(\u0026#34;myKey\u0026#34;)); //contains value assertTrue(myMap.containsValue(\u0026#34;myValue\u0026#34;)); //contains key-value pair assertTrue(myMap.containsKey(\u0026#34;myKey\u0026#34;) \u0026amp;\u0026amp; myMap.get(\u0026#34;myKey\u0026#34;).equals(\u0026#34;myValue\u0026#34;)); } Assert that a Number is positive:-\n@Test public void givenNumber_whenPositive_thenPass() { Integer number = 2; assertTrue(number \u0026gt;= 0); } Assert that an element contains text in Selenium using JUnit:-\n@Test public void givenElement_whenContainsText_thenPass() { String actualString = driver.findElement(By.xpath(\u0026#34;xpath\u0026#34;)).getText(); assertTrue(actualString.contains(\u0026#34;specific text\u0026#34;)); } assertInstanceOf The assertInstanceOf(expectedType, object) assertion is used to test that the given object is an instance of expected class type:-\n@Test public void givenObject_whenInstanceOfClassType_thenPass(){ User user = User.builder().build(); assertInstanceOf(User.class, user); //assertInstanceOf(expectedType, object); assertInstanceOf(String.class, \u0026#34;text\u0026#34;); assertInstanceOf(Integer.class, 1); assertInstanceOf(Boolean.class, true); assertInstanceOf(List.class, Arrays.asList()); assertInstanceOf(Collection.class, Arrays.asList()); } assertThrows The assertThrows assertion is used to test the exception, which is expected to be thrown by a Java program or method execution. Let\u0026rsquo;s look at some examples:-\nTest that when a given String can not be parsed to an Integer, then expect IllegalArgumentException:- @Test void parseInvalidInt_whenThrowIllegalArgumentException_thenPass() { String str = \u0026#34;foo\u0026#34;; assertThrows(IllegalArgumentException.class, () -\u0026gt; { Integer.valueOf(str); }); } Test that when trying to access the invalid index of an Array, then expect IndexOutOfBoundsException:- @Test public void getInvalidIndex_whenThrowIndexOutOfBoundsException_thenPass() { Exception e = assertThrows(IndexOutOfBoundsException.class, () -\u0026gt; { List\u0026lt;String\u0026gt; list = Arrays.asList(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;); String s = list.get(2); }); assertEquals(\u0026#34;Index 2 out of bounds for length 2\u0026#34;, e.getMessage()); } 💡The assertThrows assertion returns the thrown Exception, which can be further used to assert an error message like in the above example. assertThrowsExactly The assertThrowsExactly assertion is different from assertThrows where it matches the exact exception type.\nassertThrows vs assertThrowsExactly\nThe assertThrows assertion pass when the thrown exception is an instance of (child of) a given expected exception.\nFor example, assertThrows pass when code throws NullPointerException since it is a child of RuntimeException:-\n@Test public void accessNull_whenThrowRuntimeException_thenPass() { String str = null; Exception e = assertThrows(RuntimeException.class, () -\u0026gt; { str.equals(\u0026#34;foo\u0026#34;); }); } The assertThrowsExactly assertion pass when the thrown exception exactly match a given expected exception.\nFor example, assertThrowsExactly pass when code throws NullPointerException, since it is expecting the exact NullPointerException to be thrown:-\n@Test public void accessNull_whenThrowNullPointerException_thenPass() { String str = null; Exception e = assertThrowsExactly(NullPointerException.class, () -\u0026gt; { str.equals(\u0026#34;foo\u0026#34;); }); } assertDoesNotThrow The assertDoesNotThrow assertion is used to test that a supplier method executed without any exception. This method is also useful to write test cases for methods that return nothing i.e. void.\n@Test void voidMethod_whenRunWithoutException_thenPass() { assertDoesNotThrow(() -\u0026gt; {}); } assertTimeout The assertTimeout assertion is used to test that the execution of the supplied method completes before the given time. Let\u0026rsquo;s look at the examples:-\nThe test pass when the supplier method or network call finishes within the given duration:- @Test public void networkCall_whenFinishInGivenTime_thenPass(){ assertTimeout(Duration.ofSeconds(5), () -\u0026gt; networkCall()); } public void networkCall() throws InterruptedException { TimeUnit.SECONDS.sleep(2); } The test fails when the supplier method or network call takes longer than expected given duration:- @Test public void networkCall_whenTakesLongerThanExpected_thenFail(){ assertTimeout(Duration.ofSeconds(5), () -\u0026gt; delayedNetworkCall()); // fail - execution exceeded timeout of 5000 ms by 2000 ms } public void delayedNetworkCall() throws InterruptedException { TimeUnit.SECONDS.sleep(7); } 💡The assertTimeout completes the supplier method execution even if it takes longer than the expected timeout duration and also tells you by how much time it is exceeded in the failure message. assertTimeoutPreemptively The assertTimeoutPreemptively assertion is different from assertTimeout, where it aborts the supplier method execution if it is taking longer than the expected timeout duration.\nLet\u0026rsquo;s look at the example, where the test fails when the executable method takes longer than expected and aborts:-\n@Test public void networkCall_whenTakesLongerThanExpected_thenFail(){ assertTimeoutPreemptively(Duration.ofSeconds(5), () -\u0026gt; delayedNetworkCall()); // FAIL - execution timed out after 5000 ms } The failure message doesn\u0026rsquo;t show by how much time it exceeded. If you would like to print that in a failure message then use assertTimeout instead.\nassertAll The assertAll assertion should be used to group the assertions that belong together. The interesting fact about assertAll is that it tests all the grouped assertions, no matter if some of them failed and tells you the total number of failures in the test result.\nLet\u0026rsquo;s look at the example:-\n@Test void givenEmail_whenValid_thenPass() { String email = \u0026#34;admin@gmail.com\u0026#34;; assertAll(\u0026#34;Should be a valid email\u0026#34;, () -\u0026gt; assertNotNull(email), () -\u0026gt; assertTrue(email.contains(\u0026#34;@\u0026#34;)), () -\u0026gt; assertTrue(email.endsWith(\u0026#34;.com\u0026#34;))); } Assertion Failure Message When an assertion fails, Junit provides a default failure message with expected and actual parameters. Junit also provides additional methods for each assertion to provide the custom error message as a string or as a supplier method.\nFor example, the assertTrue assertion provides the following methods:-\nassertTrue(boolean condition) // Junit default error message assertTrue(boolean condition, String message) // Custom error message assertTrue(boolean condition, Supplier\u0026lt;String\u0026gt; messageSupplier) // Custom error messageSupplier Custom Error Message assertTrue(boolean condition, String message)\nWe can pass the custom error message as a String in the last parameter of the invoked assertion method.\n@Test void givenCondition_whenNotTrue_thenFailWithCustomErrorMessage(){ Boolean condition = false; assertTrue(condition, \u0026#34;Condition must be true\u0026#34;); } The above test fails with the following message:-\nCondition must be true ==\u0026gt; expected: \u0026lt;true\u0026gt; but was: \u0026lt;false\u0026gt; Expected :true Actual :false Custom Error Message Supplier assertTrue(boolean condition, Supplier messageSupplier)\nWe can also pass the supplier method in the last parameter of the invoked assertion method. This is a good choice to create complex error messages.\n@Test public void givenMap_whenNotContainsKey_thenFailWithCustomErrorMessageSupplier() { Map\u0026lt;String, String\u0026gt; myMap = Map.of(\u0026#34;myKey\u0026#34;, \u0026#34;myValue\u0026#34;); String KEY = \u0026#34;anotherKey\u0026#34;; assertTrue(myMap.containsKey(KEY), () -\u0026gt; String.format(\u0026#34;The map doesn\u0026#39;t contain the key: %s\u0026#34;, KEY)); } The above test fails with the following message:-\nThe map doesn\u0026#39;t contain the key: anotherKey ==\u0026gt; expected: \u0026lt;true\u0026gt; but was: \u0026lt;false\u0026gt; Expected :true Actual :false That\u0026rsquo;s all about assertions in Junit 5. Thanks for Reading!\n","permalink":"https://codingnconcepts.com/java/unit-test-with-junit5-assertions/","tags":["JUnit","Java Assertion"],"title":"Unit Test with JUnit 5 in Java"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to write efficient unit test cases in Java using AssertJ assertThat() assertions.\nAssertJ AssertJ is a Java library that provides a rich set of assertions similar to Hamcrest, which are very fluent to test the conditions on String, Number, Date, Time, Object, Collections, URI, File, etc.\nAssertJ also supports soft assertions and writing custom messages in assertions. We will learn all these using examples.\nYou write unit tests with AssertJ using assertThat() statement followed by one or more assertions applicable to that class type.\nSetup AssertJ Import the required assertj-core dependency for Java using Maven (pom.xml) or Gradle (build.gradle), whichever you are using:-\n\u0026lt;!-- add dependency in pom.xml --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.assertj\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;assertj-core\u0026lt;/artifactId\u0026gt; \u0026lt;!-- use 2.9.1 for Java 7 projects --\u0026gt; \u0026lt;version\u0026gt;3.23.1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; // add dependency in build.gradle dependencies { testImplementation \u0026#39;org.assertj:assertj-core:3.23.1\u0026#39; } Import AssertJ Once you add the AssertJ dependency, you can import static assertThat method in the unit test class like this:-\nimport static org.assertj.core.api.Assertions.*; We are ready to write unit test cases with AssertJ.\nUsing AssertJ Assertions AssertJ provides assertions for most of the data types in Java e.g. Text (String), Number (Integer, Double, BigDecimal), Collection (List, Array, Set, Map), File, etc. See the complete list of assertions available in AssertJ core package\nString assertions AssertJ provides a variety of fluent assertions for String. Let\u0026rsquo;s look at them:-\nString nullString = null; // Assert that a String is not null assertThat(\u0026#34;notnull\u0026#34;).isNotNull(); // Assert that a String is null or empty assertThat(nullString).isNullOrEmpty(); assertThat(\u0026#34;\u0026#34;).isNullOrEmpty(); // Assert that a String is empty assertThat(\u0026#34;\u0026#34;).isEmpty(); // Assert that a String is blank i.e. null, empty, or consists of one or more whitespace characters assertThat(nullString).isBlank(); assertThat(\u0026#34;\u0026#34;).isBlank(); assertThat(\u0026#34; \u0026#34;).isBlank(); assertThat(\u0026#34; \\t \u0026#34;).isBlank(); // Assert that a String has given size assertThat(\u0026#34;abc\u0026#34;).hasSize(3); assertThat(\u0026#34;abc\u0026#34;).hasSizeLessThan(4); assertThat(\u0026#34;abc\u0026#34;).hasSizeLessThanOrEqualTo(4); assertThat(\u0026#34;abc\u0026#34;).hasSizeGreaterThan(2); assertThat(\u0026#34;abc\u0026#34;).hasSizeGreaterThanOrEqualTo(2); // Assert that a String has same size as other String, Array or List assertThat(\u0026#34;abc\u0026#34;).hasSameSizeAs(\u0026#34;def\u0026#34;); assertThat(\u0026#34;abc\u0026#34;).hasSameSizeAs(new char[] { \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;f\u0026#39; }); assertThat(\u0026#34;abc\u0026#34;).hasSameSizeAs(Arrays.asList(1, 2, 3)); // Assert that a String has given number of lines String multiLine = \u0026#34;First line\\n\u0026#34; + \u0026#34;Last line\u0026#34;; assertThat(multiLine).hasLineCount(2); // Assert that two Strings are equal assertThat(\u0026#34;foo\u0026#34;).isEqualTo(\u0026#34;foo\u0026#34;); // Assert that two Strings are equal ignoring case assertThat(\u0026#34;FOO\u0026#34;).isEqualToIgnoringCase(\u0026#34;foo\u0026#34;); // Assert that a String is a number assertThat(\u0026#34;10\u0026#34;).containsOnlyDigits(); // Assert that a String contains the given one or more Substrings assertThat(\u0026#34;Gandalf the grey\u0026#34;).contains(\u0026#34;alf\u0026#34;); assertThat(\u0026#34;Gandalf the grey\u0026#34;).contains(\u0026#34;alf\u0026#34;, \u0026#34;grey\u0026#34;); assertThat(\u0026#34;Gandalf the grey\u0026#34;).contains(Arrays.asList(\u0026#34;alf\u0026#34;, \u0026#34;grey\u0026#34;)); assertThat(\u0026#34;Gandalf the grey\u0026#34;).containsIgnoringCase(\u0026#34;gandalf\u0026#34;); // Assert that a String starts with given Substring assertThat(\u0026#34;Lord of the Rings\u0026#34;).startsWith(\u0026#34;Lord\u0026#34;); assertThat(\u0026#34;Lord of the Rings\u0026#34;).startsWithIgnoringCase(\u0026#34;lord\u0026#34;); // Assert that a String ends with given Substring assertThat(\u0026#34;Lord of the Rings\u0026#34;).endsWith(\u0026#34;Rings\u0026#34;); assertThat(\u0026#34;Lord of the Rings\u0026#34;).endsWithIgnoringCase(\u0026#34;rings\u0026#34;); // Assert that a String is of given case assertThat(\u0026#34;abc\u0026#34;).isLowerCase(); assertThat(\u0026#34;camelCase\u0026#34;).isMixedCase(); assertThat(\u0026#34;ABC\u0026#34;).isUpperCase(); For the complete list of String\u0026rsquo;s assertions, see AssertJ\u0026rsquo;s documentation AbstractCharSequenceAssert\nInteger, Long, Double, Float, and BigDecimal assertions AssertJ provides a variety of fluent assertions for Numbers.\nTest some common conditions on Integer, Long, Double, Float and BigDecimals:-\n// Assert that two Numbers are equal assertThat(1).isEqualTo(1); // Assert that a Number is 0 assertThat(0).isZero(); ssertThat(0.0).isZero(); assertThat(BigDecimal.ZERO).isZero(); // Assert that a Number is not 0 assertThat(42).isNotZero(); assertThat(3.14).isNotZero(); assertThat(BigDecimal.ONE).isNotZero(); // Assert that a Number is positive assertThat(42).isPositive(); assertThat(3.14).isPositive(); // Assert that a Number is negative assertThat(-42).isNegative(); assertThat(-3.12).isNegative(); // Assert that a Number is even assertThat(12).isEven(); assertThat(-46).isEven(); // Assert that a Number is odd assertThat(3).isOdd(); assertThat(-17).isOdd(); // Assert that a number is less than (or equal to) given number assertThat(1).isLessThan(2); assertThat(-2).isLessThan(-1); assertThat(1).isLessThanOrEqualTo(1); // Assert that a number is greater than (or equal to) given number assertThat(1).isGreaterThan(0); assertThat(-1).isGreaterThan(-2); assertThat(1).isGreaterThanOrEqualTo(1); // Assert that a number is within given range (inclusive) assertThat(1).isBetween(1, 3); // Assert that a number is within given range (exclusive) assertThat(2).isStrictlyBetween(1, 3); Compare two Double values with given precision:-\nassertThat(3.141592653589793238).isEqualTo(3.14, withPrecision(0.01)); // pass Here withPrecision(0.01) means to check the equality upto 2 decimal places\nFor the complete list of Number\u0026rsquo;s assertions, see AssertJ\u0026rsquo;s documentation AbstractIntegerAssert, AbstractLongAssert, AbstractDoubleAssert, AbstractFloatAssert, and AbstractBigDecimalAssert\nLocalDate, LocalTime, and LocalDateTime assertions AssertJ provides a variety of assertions to check the date, time, or date-time. Let\u0026rsquo;s look at them:-\nAssert that a LocalDate is not null and is today\u0026rsquo;s date:-\nassertThat(LocalDate.now()).isNotNull().isToday(); Assert that a LocalDate has given Year, Month, and Day:-\n@Test public void givenLocalDate_whenHasGivenYearMonthAndDay_thenPass() { //ISO_LOCAL_DATE YYYY-MM-DD LocalDate date = LocalDate.parse(\u0026#34;2020-01-15\u0026#34;); assertThat(date).isEqualTo(\u0026#34;2020-01-15\u0026#34;); assertThat(date).hasYear(2020); assertThat(date).hasMonth(Month.JANUARY); assertThat(date).hasMonthValue(1); assertThat(date).hasDayOfMonth(15); } Assert that a LocalDate is before or after the given LocalDate:-\n@Test public void givenLocalDate_whenBeforeOrAfterGivenDate_thenPass(){ LocalDate date = LocalDate.parse(\u0026#34;2020-01-15\u0026#34;); LocalDate nextDate = LocalDate.parse(\u0026#34;2020-01-16\u0026#34;); assertThat(date).isNotEqualTo(nextDate); assertThat(date).isBefore(nextDate); assertThat(date).isBeforeOrEqualTo(nextDate); assertThat(nextDate).isAfter(date); assertThat(nextDate).isAfterOrEqualTo(date); } Assert that a LocalDate is in the given Date range (inclusive):-\n@Test public void givenLocalDate_whenInGivenDateRangeInclusive_thenPass() { LocalDate localDate = LocalDate.now(); assertThat(localDate).isBetween(localDate.minusDays(1), localDate.plusDays(1)) .isBetween(localDate, localDate.plusDays(1)) .isBetween(localDate.minusDays(1), localDate) .isBetween(localDate, localDate); LocalDate firstOfJanuary2000 = LocalDate.parse(\u0026#34;2000-01-01\u0026#34;); assertThat(firstOfJanuary2000).isBetween(\u0026#34;1999-01-01\u0026#34;, \u0026#34;2001-01-01\u0026#34;) .isBetween(\u0026#34;2000-01-01\u0026#34;, \u0026#34;2001-01-01\u0026#34;) .isBetween(\u0026#34;1999-01-01\u0026#34;, \u0026#34;2000-01-01\u0026#34;) .isBetween(\u0026#34;2000-01-01\u0026#34;, \u0026#34;2000-01-01\u0026#34;); } The start date and end date in .isBetween(startDate, endDate) are included in the range check.\nAssert that a LocalDate is in the given Date range (exclusive):-\n@Test public void givenLocalDate_whenInGivenDateRangeExclusive_thenPass() { LocalDate localDate = LocalDate.now(); assertThat(localDate).isStrictlyBetween(localDate.minusDays(1), localDate.plusDays(1)); LocalDate firstOfJanuary2000 = LocalDate.parse(\u0026#34;2000-01-01\u0026#34;); assertThat(firstOfJanuary2000).isStrictlyBetween(\u0026#34;1999-01-01\u0026#34;, \u0026#34;2001-01-01\u0026#34;); } The start date and end date in .isStrictlyBetween(startDate, endDate) are excluded from the range check.\nAssert that a LocalTime matches conditions:-\n@Test public void givenLocalTime_whenMatchesConditions_thenPass() { // ISO_LOCAL_TIME HH::MM::SS LocalTime localTime = LocalTime.parse(\u0026#34;10:00:00\u0026#34;); assertThat(localTime).isNotNull().isEqualTo(\u0026#34;10:00:00\u0026#34;); assertThat(localTime).isBefore(\u0026#34;14:00:00\u0026#34;); assertThat(localTime).isAfter(\u0026#34;09:00:00\u0026#34;); } Assert that a LocalDateTime matches conditions:-\n@Test public void givenLocalDateTime_whenMatchesConditions_thenPass() { // ISO_LOCAL_DATE_TIME YYYY-MM-DDTHH:MM:SS LocalDateTime localDateTime = LocalDateTime.parse(\u0026#34;2000-01-01T23:59:59\u0026#34;); assertThat(localDateTime).isNotNull().isEqualTo(\u0026#34;2000-01-01T23:59:59\u0026#34;); assertThat(localDateTime).isBefore(\u0026#34;2000-01-02T00:00:00\u0026#34;); assertThat(localDateTime).isAfter(\u0026#34;1999-01-01T00:00:00\u0026#34;); } Checkout AssertJ\u0026rsquo;s documentation for the full list of Date and Time assertions - AbstractLocalDateAssert, AbstractLocalTimeAssert, and AbstractLocalDateTimeAssert\nObject assertions Assert that two objects are equal:- @Test public void givenTwoObjects_whenEquals_thenPass() { User user1 = User.builder().firstName(\u0026#34;Adam\u0026#34;).build(); User user2 = User.builder().firstName(\u0026#34;Adam\u0026#34;).build(); assertThat(user1).isEqualTo(user2); } Assert that an Object is null:- @Test public void givenObject_whenNull_thenPass() { User user = null; assertThat(user).isNull(); } Assert that an Object is the same as the given instance @Test public void givenTwoObjects_whenSameInstance_thenPass() { User user = User.builder().firstName(\u0026#34;Adam\u0026#34;).build(); assertThat(user).isSameAs(user); } Assert that two Objects have the same property values:- @Test public void givenTwoObjects_whenSamePropertyValues_thenPass() { User user1 = User.builder().firstName(\u0026#34;Adam\u0026#34;).age(22).isPremiumUser(true).build(); User user2 = User.builder().firstName(\u0026#34;Adam\u0026#34;).age(22).isPremiumUser(true).build(); assertThat(user1).usingRecursiveComparison().isEqualTo(user2); } Assert that two Objects have the same property values ignoring one or more given properties:- @Test public void givenTwoObjects_whenSamePropertyValuesExcludingIgnoredProps_thenPass() { User user1 = User.builder().firstName(\u0026#34;Adam\u0026#34;).age(22).isPremiumUser(true).build(); User user2 = User.builder().firstName(\u0026#34;Adam\u0026#34;).age(18).isPremiumUser(false).build(); assertThat(user1).usingRecursiveComparison() .ignoringFields(\u0026#34;age\u0026#34;, \u0026#34;isPremiumUser\u0026#34;).isEqualTo(user2); } Assert that two Objects have the same property values ignoring one or more properties with null values:- @Test public void givenTwoObjects_whenSamePropertyValuesExcludingNullProps_thenPass() { User user1 = User.builder().firstName(\u0026#34;Adam\u0026#34;).age(22).isPremiumUser(true).build(); User user2 = User.builder().firstName(\u0026#34;Adam\u0026#34;).age(null).isPremiumUser(null).build(); assertThat(user1).usingRecursiveComparison() .ignoringExpectedNullFields() .isEqualTo(user2); } Assert that an Object has all the properties with non-null values:- @Test public void givenObject_whenHasNoNullProperties_thenPass() { User user = User.builder().firstName(\u0026#34;Adam\u0026#34;).age(22).isPremiumUser(true).build(); assertThat(user).hasNoNullFieldsOrProperties(); } Assert that an Object has all the properties with null values:- @Test public void givenObject_whenHasAllNullProperties_thenPass() { User user = User.builder().firstName(null).age(null).isPremiumUser(null).build(); assertThat(user).hasAllNullFieldsOrProperties(); } Assert that an Object has properties with the given names:- @Test public void givenObject_whenHasGivenProperties_thenPass() { User user = User.builder().build(); assertThat(user).hasFieldOrProperty(\u0026#34;firstName\u0026#34;); assertThat(user).hasFieldOrProperty(\u0026#34;age\u0026#34;); assertThat(user).hasFieldOrProperty(\u0026#34;isPremiumUser\u0026#34;); } Assert that an Object has properties with the given names and values:- @Test public void givenObject_whenHasPropertyWithGivenNameAndValue_thenPass() { User user = User.builder().firstName(\u0026#34;Adam\u0026#34;).age(22).isPremiumUser(true).build(); assertThat(user).hasFieldOrPropertyWithValue(\u0026#34;firstName\u0026#34;, \u0026#34;Adam\u0026#34;); assertThat(user).hasFieldOrPropertyWithValue(\u0026#34;age\u0026#34;, 22); assertThat(user).hasFieldOrPropertyWithValue(\u0026#34;isPremiumUser\u0026#34;, true); assertThat(user).extracting(\u0026#34;firstName\u0026#34;).isEqualTo(\u0026#34;Adam\u0026#34;); assertThat(user).extracting(\u0026#34;age\u0026#34;).isEqualTo(22); assertThat(user).extracting(\u0026#34;isPremiumUser\u0026#34;).isEqualTo(true); assertThat(user).extracting(\u0026#34;firstName\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;isPremiumUser\u0026#34;).containsExactly(\u0026#34;Adam\u0026#34;, 22, true); } For the complete list of Object\u0026rsquo;s assertions, see AssertJ\u0026rsquo;s documentation AbstractObjectAssert\nList/Array assertions AssertJ provides similar assertions for List and Array. Let\u0026rsquo;s look at the example where we are testing multiple conditions on List by chaining assertions one after another:-\n@Test public void givenList_whenMatchesConditions_thenPass(){ List\u0026lt;String\u0026gt; list = Arrays.asList(\u0026#34;lord\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;rings\u0026#34;); assertThat(list) .isNotNull() .isNotEmpty() .hasSize(4) .startsWith(\u0026#34;lord\u0026#34;) .contains(\u0026#34;of\u0026#34;) .contains(\u0026#34;the\u0026#34;, atIndex(2)) .endsWith(\u0026#34;rings\u0026#34;) .containsSequence(\u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;); assertThat(\u0026#34;lord\u0026#34;).isIn(list); } Checkout AssertJ\u0026rsquo;s documentation for the full list of List\u0026rsquo;s assertions - AbstractListAssert\nMap assertions AsssertJ provides assertion for Map to check keys, values, and entries. Let\u0026rsquo;s look at them:-\nAssert that a Map is null or empty:- assertThat(new HashMap()).isNullOrEmpty(); assertThat(new HashMap()).isEmpty(); Assert that a Map contains the given Key, Value, and Entry using multiple chained assertions:- @Test public void givenMap_whenMatchesConditions_thenPass(){ Map\u0026lt;String, String\u0026gt; myMap = new HashMap\u0026lt;\u0026gt;(); myMap.put(\u0026#34;myKey1\u0026#34;, \u0026#34;myValue1\u0026#34;); myMap.put(\u0026#34;myKey2\u0026#34;, \u0026#34;myValue2\u0026#34;); assertThat(myMap) .isNotNull() .isNotEmpty() .hasSize(2) .containsKey(\u0026#34;myKey1\u0026#34;) .containsValue(\u0026#34;myValue2\u0026#34;) .containsEntry(\u0026#34;myKey1\u0026#34;, \u0026#34;myValue1\u0026#34;) .contains(entry(\u0026#34;myKey2\u0026#34;, \u0026#34;myValue2\u0026#34;)); } Checkout AssertJ\u0026rsquo;s documentation for the full list of Map\u0026rsquo;s assertions - AbstractMapAssert\nURI assertions Let\u0026rsquo;s look at various assertions for http, https, mailto, and file URLs:-\n@Test public void givenURI_whenMatchesConditions_thenPass() throws URISyntaxException { assertThat(new URI(\u0026#34;http://localhost:8080\u0026#34;)) .hasScheme(\u0026#34;http\u0026#34;) .hasHost(\u0026#34;localhost\u0026#34;) .hasPort(8080) .hasPath(\u0026#34;\u0026#34;) .hasNoQuery() .hasNoParameters(); assertThat(new URI(\u0026#34;https://reqres.in/api/users?name=adam\u0026amp;page=1\u0026#34;)) .hasScheme(\u0026#34;https\u0026#34;) .hasHost(\u0026#34;reqres.in\u0026#34;) .hasNoPort() .hasPath(\u0026#34;/api/users\u0026#34;) .hasQuery(\u0026#34;name=adam\u0026amp;page=1\u0026#34;) .hasParameter(\u0026#34;name\u0026#34;) .hasParameter(\u0026#34;page\u0026#34;, \u0026#34;1\u0026#34;); assertThat(new URI(\u0026#34;mailto:java-net@java.sun.com\u0026#34;)) .hasScheme(\u0026#34;mailto\u0026#34;) .hasNoHost() .hasNoPort() .hasNoPath(); assertThat(new URI(\u0026#34;file:///home/user/Documents/hello-world.txt\u0026#34;)) .hasScheme(\u0026#34;file\u0026#34;) .hasNoPort() .hasNoHost() .hasPath(\u0026#34;/home/user/Documents/hello-world.txt\u0026#34;); } Checkout AssertJ\u0026rsquo;s documentation for the full list of Java URI\u0026rsquo;s assertions - AbstractUriAssert\nFile assertions AssertJ file provides a variety of assertions to check the File and Directory properties, size, path and it\u0026rsquo;s content. Let\u0026rsquo;s look at some examples:-\n@Test public void givenFile_whenMatchesConditions_thenPass() throws IOException { File tmpFile = File.createTempFile(\u0026#34;tmp\u0026#34;, \u0026#34;txt\u0026#34;); File tmpDir = Files.createTempDirectory(\u0026#34;tmpDir\u0026#34;).toFile(); assertThat(tmpFile) .exists() .isFile() .isReadable() .isWritable() .hasSize(0); assertThat(tmpDir) .exists() .isDirectory(); } AssertJ has a lot more useful Java File and Directory assertions, check them out - AbstractFileAssert and AbstractFileSizeAssert\nSoft assertions AssertJ is hard assertion by default AssertJ assertions are hard assertions by default, which means when you have multiple assertions in the test case and if one of them fails, it throws an exception and stops the execution there. Assertions after the failed assertion will not be executed. Let\u0026rsquo;s look at the below example of Hard Assertion:-\nimport static org.assertj.core.api.Assertions.*; @Test public void whenTestFail_hardAssert_stopExecution(){ String text = \u0026#34;abc\u0026#34;; assertThat(text).hasSize(3); // pass assertThat(text).contains(\u0026#34;z\u0026#34;); // fail - stop execution assertThat(text).startsWith(\u0026#34;a\u0026#34;); // not executed assertThat(text).isEqualTo(\u0026#34;def\u0026#34;); // not executed } An AssertionError is thrown upon execution like this:-\njava.lang.AssertionError: Expecting actual: \u0026#34;abc\u0026#34; to contain: \u0026#34;z\u0026#34; at com.example.assertion.AssertJAssertThatTests.whenTestFail_hardAssert_stopExecution(AssertJAssertThatTests.java:463) AssertJ soft assertion to the rescue There are cases where we want to execute all the assertions in a test case regardless of failure and generate a report of total passed and failed assertions. This is known as Soft Assertion. Luckily AssertJ provides Assertions.SoftAssertions for this. Let\u0026rsquo;s look at the below example of Soft Assertion:-\nimport static org.assertj.core.api.Assertions.*; @Test public void whenTestFail_softAssert_continueExecution(){ String text = \u0026#34;abc\u0026#34;; SoftAssertions softly = new SoftAssertions(); softly.assertThat(text).hasSize(3); // pass softly.assertThat(text).contains(\u0026#34;z\u0026#34;); // fail - continue execution softly.assertThat(text).startsWith(\u0026#34;a\u0026#34;); // pass softly.assertThat(text).isEqualTo(\u0026#34;def\u0026#34;); // fail softly.assertAll(); // must specify } Remember to specify softly.assertAll() after all your assertions, which tells AssertJ to execute all assertions regardless of failure.\nYou can also use the static method SoftAssertions.assertSoftly. The softly.assertAll() method will be called automatically after the lambda function completes. Check out the example below:-\n@Test public void whenTestFail_softAssert_continueExecution_lambda(){ String text = \u0026#34;abc\u0026#34;; SoftAssertions.assertSoftly(softly -\u0026gt; { softly.assertThat(text).hasSize(3); // pass softly.assertThat(text).contains(\u0026#34;z\u0026#34;); // fail - continue execution softly.assertThat(text).startsWith(\u0026#34;a\u0026#34;); // pass softly.assertThat(text).isEqualTo(\u0026#34;def\u0026#34;); // fail }); } Here is the sample test report after execution:-\norg.assertj.core.error.AssertJMultipleFailuresError: Multiple Failures (2 failures) -- failure 1 -- Expecting actual: \u0026#34;abc\u0026#34; to contain: \u0026#34;z\u0026#34; at AssertJAssertThatTests.whenTestFail_softAssert_continueExecution(AssertJAssertThatTests.java:474) -- failure 2 -- expected: \u0026#34;def\u0026#34; but was: \u0026#34;abc\u0026#34; at AssertJAssertThatTests.whenTestFail_softAssert_continueExecution(AssertJAssertThatTests.java:476) Custom Message in assertion You can also add a custom message in assertion using short-named as() method.\nassertThat(object).as(customMessage).assertion... Important note: You must call as() method before any actual assertion - otherwise it will not work, as assertion will fire first.\nLet\u0026rsquo;s look at the example:-\n@Test public void givenObject_testAgeWithCustomMessage(){ User user = new User(\u0026#34;Joe\u0026#34;, 22, true); assertThat(user.getAge()).as(\u0026#34;check %s\u0026#39;s age\u0026#34;, user.getFirstName()).isEqualTo(18); } It logs the custom message in case of assertion failure, which is more readable:-\norg.opentest4j.AssertionFailedError: [check Joe\u0026#39;s age] expected: 18 but was: 22 That\u0026rsquo;s all about AssertJ assertions. Thanks for Reading!\n","permalink":"https://codingnconcepts.com/java/unit-test-with-assertj-assertthat/","tags":["JUnit","Java Assertion"],"title":"Unit Test with AssertJ in java"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to write efficient Junit test cases in Java using Hamcrest assertThat() with Matchers.\nHamcrest Hamcrest is a widely used framework for writing unit test cases in Java. Sometimes it becomes difficult to write Junit test cases to test complex conditions, Hamcrest comes in handy in such cases, which provides pre-defined Matchers to specify conditions for Texts (String), Numbers (Integer, Long, Double, and Float), Collections (List, Array, and Map), Objects and many more.\nYou write unit tests with Hamcrest using assertThat() statement followed by one or more, or nested Matchers. We will look at all of Matcher\u0026rsquo;s examples in this article. Let\u0026rsquo;s do the setup first.\nSetup Hamcrest Import the required Hamcrest dependency for Java using Maven (pom.xml) or Gradle (build.gradle), whichever you are using:-\n\u0026lt;!-- add dependency in pom.xml --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.hamcrest\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hamcrest\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; // add dependency in build.gradle dependencies { testImplementation \u0026#39;org.hamcrest:hamcrest:2.2\u0026#39; } Import Hamcrest Once you add the Hamcrest dependency, you can import static Hamcrest assertThat() method and All Matchers in unit test class like this:-\nimport static org.hamcrest.MatcherAssert.assertThat; import static org.hamcrest.Matchers.*; We are ready to write unit test cases with Hamcrest.\nUsing Hamcrest Matchers Matcher to check null value These are quite often used Hamcrest Object Matchers to check the null and non-null value:-\nAssert that a given value is Null:- @Test public void givenValue_whenNull_thenPass() { String text = null; Integer number = null; Boolean flag = null; Object obj = null; // assertThat(null, is(null)); \u0026lt;- throw NullPointerException assertThat(null, is(nullValue())); // pass assertThat(text, is(nullValue())); // pass assertThat(number, is(nullValue())); // pass assertThat(flag, is(nullValue())); // pass assertThat(obj, is(nullValue())); // pass } Assert that a given value is Not Null:- @Test public void givenValue_whenNotNull_thenPass() { assertThat(\u0026#34;a\u0026#34;, is(notNullValue())); // pass assertThat(1, is(notNullValue())); // pass assertThat(false, is(notNullValue())); // pass assertThat(new Object(), is(notNullValue())); // pass } Text Matchers for String Hamcrest Text Matchers are used to write easier and neater assertions on String with various conditions. Let\u0026rsquo;s look at them:-\nAssert that a String is empty:-\n@Test public void givenString_whenEmpty_thenPass() { String text = \u0026#34;\u0026#34;; assertThat(text, is(emptyString())); } Assert that a String is empty or null:-\n@Test public void givenString_whenEmptyOrNull_thenPass() { String text = null; assertThat(text, is(emptyOrNullString())); } Assert that a String is not null:-\n@Test public void givenString_whenNotNull_thenPass() { String text = \u0026#34;notnull\u0026#34;; assertThat(text, notNullValue()); } Assert that two Strings are equal ignoring the case:-\n@Test public void givenTwoStrings_whenEqualToIgnoringCase_thenPass() { String text1 = \u0026#34;foo\u0026#34;; String text2 = \u0026#34;FOO\u0026#34;; assertThat(text1, equalToIgnoringCase(text2)); } Assert that two Strings are equal ignoring the white space:-\n@Test public void givenTwoStrings_whenEqualToIgnoringWhiteSpace_thenPass() { String text1 = \u0026#34; my\\tfoo bar \u0026#34;; String text2 = \u0026#34; my foo bar\u0026#34;; //assertThat(text1, equalToIgnoringWhiteSpace(text2)); // deprecated assertThat(text1, equalToCompressingWhiteSpace(text2)); // use this! } Please Note that equalToIgnoringWhiteSpace matcher is deprecated, use equalToCompressingWhiteSpace instead.\nAssert that a String contains the given Substring:-\n@Test public void givenString_whenContainsGivenSubstring_thenPass() { String text = \u0026#34;lordOfTheRings\u0026#34;; String subtext = \u0026#34;Ring\u0026#34;; assertThat(text, containsString(subtext)); } Assert that a String contains the given Substring ignoring the case:-\n@Test public void givenString_whenContainsGivenSubstringIgnoringCase_thenPass() { String text = \u0026#34;lordOfTheRings\u0026#34;; String subtext = \u0026#34;RING\u0026#34;; assertThat(text, containsStringIgnoringCase(subtext)); } Assert that a String contains one or more Substrings in a given order:-\n@Test public void givenString_whenContainsOneOrMoreSubstringsInGivenOrder_thenPass() { String text = \u0026#34;lordOfTheRings\u0026#34;; assertThat(text, stringContainsInOrder(Arrays.asList(\u0026#34;lord\u0026#34;, \u0026#34;Ring\u0026#34;))); } Assert that a String starts with a given Substring:-\n@Test public void givenString_whenStartsWithGivenSubstring_thenPass() { String text = \u0026#34;lordOfTheRings\u0026#34;; String subtext = \u0026#34;lord\u0026#34;; assertThat(text, startsWith(subtext)); } Assert that a String starts with a given Substring ignoring the case:-\n@Test public void givenString_whenStartsWithGivenSubstringIgnoringCase_thenPass() { String text = \u0026#34;lordOfTheRings\u0026#34;; String subtext = \u0026#34;LORD\u0026#34;; assertThat(text, startsWithIgnoringCase(subtext)); } Assert that a String ends with a given Substring:-\n@Test public void givenString_whenEndsWithGivenSubstring_thenPass() { String text = \u0026#34;lordOfTheRings\u0026#34;; String subtext = \u0026#34;Rings\u0026#34;; assertThat(text, endsWith(subtext)); } Assert that a String ends with a given Substring ignoring the case:-\n@Test public void givenString_whenEndsWithGivenSubstringIgnoringCase_thenPass() { String text = \u0026#34;lordOfTheRings\u0026#34;; String subtext = \u0026#34;RINGS\u0026#34;; assertThat(text, endsWithIgnoringCase(subtext)); } Number Matchers for Integer, Long, Double, Float, BigDecimal Hamcrest Number Matchers are used to write assertions on Numbers (Integer, Long, Double, Float, and BigDecimal) with various conditions. Let\u0026rsquo;s look at them:-\nAssert that an Integer is greater than a given Integer:- @Test public void givenInt_whenGreaterThanGivenInt_thenPass() { assertThat(2, greaterThan(1)); } Assert that an Integer is greater than or equal to a given Integer:- @Test public void givenInt_whenGreaterThanOrEqualToGivenInt_thenPass() { assertThat(1, greaterThanOrEqualTo(1)); } Assert that an Integer is less than a given Integer:- @Test public void givenInt_whenLessThanGivenInt_thenPass() { assertThat(-1, lessThan(1)); } Assert that an Integer is less than or equal to a given Integer:- @Test public void givenInt_whenLessThanOrEqualToGivenInt_thenPass() { assertThat(1, lessThanOrEqualTo(1)); } Assert that a Double is within the given range specified by the operand and (+/-) error arguments:- @Test public void givenDouble_whenWithinRange_thenPass() { // 0.8 is within range of 1.0 (+/-) 0.2 = 0.8 to 1.2 assertThat(0.8, is(closeTo(1.0 /*operand*/, 0.2 /*error*/))); } Assert that a BigDecimal is within the given range specified by the operand and (+/-) error arguments:- @Test public void givenBigDecimal_whenWithinRange_thenPass() { // 1.8 is within range of 2.0 (+/-) 0.5 = 1.5 to 2.5 assertThat(new BigDecimal(\u0026#34;1.8\u0026#34;), is(closeTo(new BigDecimal(\u0026#34;2.0\u0026#34;) /*operand*/, new BigDecimal(\u0026#34;0.5\u0026#34;) /*error*/))); } Object Matchers for Java Object Hamcrest Object Matchers are used to write assertions on Java Objects to check various conditions. Let\u0026rsquo;s look at them:-\nAssert that two Objects are equal using the Object\u0026rsquo;s equals() method:- @Test public void givenTwoObjects_whenEquals_thenPass() { User user1 = User.builder().firstName(\u0026#34;Adam\u0026#34;).build(); User user2 = User.builder().firstName(\u0026#34;Adam\u0026#34;).build(); assertThat(user1, equalTo(user2)); } Assert that two objects are referring to the same instance:- @Test public void givenTwoObjects_whenSameInstance_thenPass() { User user = User.builder().firstName(\u0026#34;Adam\u0026#34;).build(); assertThat(user, sameInstance(user)); } Assert that an Object is an instance of the given Class:- @Test public void givenObject_whenInstanceOfGivenClass_thenPass() { User user = User.builder().firstName(\u0026#34;Adam\u0026#34;).lastName(\u0026#34;Smith\u0026#34;).build(); assertThat(user, instanceOf(User.class)); } Assert that toString() method of an Object returns the given String:- @Test public void givenObject_whenToStringMethodReturnsGivenString_thenPass() { User user = User.builder().firstName(\u0026#34;Adam\u0026#34;).lastName(\u0026#34;Smith\u0026#34;).build(); String str = user.toString(); assertThat(user, hasToString(str)); } Assert that a Class is a subclass of given another class:- @Test public void given2Classes_whenFirstClassChildOfSecondClass_thenCorrect(){ assertThat(Integer.class, typeCompatibleWith(Number.class)); } Bean Matchers for Java Object\u0026rsquo;s instance Hamcrest Bean Matchers are used to test the properties of a Java Object instance.\nLet\u0026rsquo;s create an instance of User class and assert them:-\n@Data @Builder public class User { private String firstName; private Integer age; private Boolean isPremiumUser; } Assert that an Object\u0026rsquo;s instance has a property with the given name:- @Test public void givenObject_whenHasGivenProperties_thenPass() { User user = User.builder().build(); assertThat(user, hasProperty(\u0026#34;firstName\u0026#34;)); assertThat(user, hasProperty(\u0026#34;age\u0026#34;)); assertThat(user, hasProperty(\u0026#34;isPremiumUser\u0026#34;)); } Assert that an Object\u0026rsquo;s instance has a property with the given name and value:- @Test public void givenObject_whenHasPropertyWithGivenNameAndValue_thenPass() { User user = User.builder().firstName(\u0026#34;Adam\u0026#34;).age(22).isPremiumUser(true).build(); assertThat(user, hasProperty(\u0026#34;firstName\u0026#34;, equalTo(\u0026#34;Adam\u0026#34;))); assertThat(user, hasProperty(\u0026#34;age\u0026#34;, equalTo(22))); assertThat(user, hasProperty(\u0026#34;isPremiumUser\u0026#34;, equalTo(true))); } Assert that two Object\u0026rsquo;s instances have all properties with the same values:- @Test public void givenTwoObjects_whenSamePropertyValues_thenPass() { User user1 = User.builder().firstName(\u0026#34;Adam\u0026#34;).age(22).isPremiumUser(true).build(); User user2 = User.builder().firstName(\u0026#34;Adam\u0026#34;).age(22).isPremiumUser(true).build(); assertThat(user1, samePropertyValuesAs(user2)); } Assert that two Object\u0026rsquo;s instances have properties with the same values excluding one or more ignore properties specified:- @Test public void givenTwoObjects_whenSamePropertyValuesExcludingIgnoredProps_thenPass() { User user1 = User.builder().firstName(\u0026#34;Adam\u0026#34;).age(22).isPremiumUser(true).build(); User user2 = User.builder().firstName(\u0026#34;Adam\u0026#34;).age(18).isPremiumUser(false).build(); assertThat(user1, samePropertyValuesAs(user2, \u0026#34;age\u0026#34;, \u0026#34;isPremiumUser\u0026#34;)); } The above two instances have the same firstName value and we ignored the age and isPremiumUser properties from comparison. Collection Matchers for List Hamcrest Collection Matchers can be used for List assertions. Let\u0026rsquo;s look at examples:-\nAssert that a List is empty:-\n@Test public void givenList_whenEmpty_thenPass() { List\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); assertThat(list, empty()); } Assert that a List is of a given size:-\n@Test public void givenList_whenSizeMatches_thenPass() { List\u0026lt;String\u0026gt; list = Arrays.asList(\u0026#34;lord\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;rings\u0026#34;); assertThat(list, hasSize(4)); assertThat(list, iterableWithSize(4)); } Both matchers hasSize and iterableWithSize have the same results and can be used alternatively.\nAssert that a List contains all the given values in the same order:-\n@Test public void givenList_whenContainsAllValuesInSameOrder_thenPass() { List\u0026lt;String\u0026gt; list = Arrays.asList(\u0026#34;lord\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;rings\u0026#34;); assertThat(list, contains(\u0026#34;lord\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;rings\u0026#34;)); } The contains matcher should have the same number of specified items as the length of the list for a positive match.\nAssert that a List contains all the given values in any order:-\n@Test public void givenList_whenContainsAllValuesInAnyOrder_thenPass() { List\u0026lt;String\u0026gt; list = Arrays.asList(\u0026#34;lord\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;rings\u0026#34;); assertThat(list, containsInAnyOrder(\u0026#34;rings\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;lord\u0026#34;)); } Same as contains, the containsInAnyOrder matcher should have the same number of specified items as the length of the list for a positive match. Though they can be in any order.\nAssert that a List contains one or more given values in the relative order:-\n@Test public void givenList_whenContainsValuesInRelativeOrder_thenPass() { List\u0026lt;String\u0026gt; list = Arrays.asList(\u0026#34;lord\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;rings\u0026#34;); assertThat(list, containsInRelativeOrder(\u0026#34;of\u0026#34;, \u0026#34;rings\u0026#34;)); } Assert that a List contains the given value:-\n@Test public void givenList_whenContainsGivenValue_thenPass() { List\u0026lt;String\u0026gt; list = Arrays.asList(\u0026#34;lord\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;rings\u0026#34;); assertThat(list, hasItem(\u0026#34;lord\u0026#34;)); assertThat(\u0026#34;of\u0026#34;, is(in(list))); } Both the matchers in the example have the same result and can be used alternatively.\nAssert that a List contains the given value with the specified condition:-\n@Test public void givenList_whenContainsGivenValueWithCondition_thenPass() { List\u0026lt;String\u0026gt; list = Arrays.asList(\u0026#34;lord\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;rings\u0026#34;); assertThat(list, hasItem(equalTo(\u0026#34;of\u0026#34;))); assertThat(list, hasItem(startsWith(\u0026#34;th\u0026#34;))); assertThat(list, hasItem(endsWith(\u0026#34;ngs\u0026#34;))); } Matchers can be nested together for more complex conditions e.g. hasItem is nested with equalsTo, startsWith, and endsWith in the above example.\nAssert that List contains one or more given values with the specified condition:-\n@Test public void givenList_whenContainsOneOrMoreGivenValuesWithCondition_thenPass() { List\u0026lt;String\u0026gt; list = Arrays.asList(\u0026#34;lord\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;rings\u0026#34;); assertThat(list, hasItems(startsWith(\u0026#34;ring\u0026#34;), endsWith(\u0026#34;ord\u0026#34;), equalTo(\u0026#34;of\u0026#34;))); } Assert that every item in the List matches the given condition:-\n@Test public void givenList_whenEveryItemMatchesCondition_thenPass() { List\u0026lt;String\u0026gt; list = Arrays.asList(\u0026#34;bar\u0026#34;, \u0026#34;baz\u0026#34;); assertThat(list, everyItem(startsWith(\u0026#34;ba\u0026#34;))); } Collection Matchers for Array Hamcrest Collection Matchers can also be used for Array assertions. Let\u0026rsquo;s look at examples:-\nAssert that Array is of the given size:-\n@Test public void givenArray_whenGivenSize_thenPass() { String[] arrayItems = { \u0026#34;lord\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;rings\u0026#34; }; assertThat(arrayItems, arrayWithSize(4)); } Assert that an Array has the given item:-\n@Test public void givenArray_whenHasItemInArray_thenPass() { String[] arrayItems = { \u0026#34;lord\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;rings\u0026#34; }; assertThat(arrayItems, hasItemInArray(\u0026#34;lord\u0026#34;)); assertThat(\u0026#34;of\u0026#34;, oneOf(arrayItems)); assertThat(\u0026#34;the\u0026#34;, is(in(arrayItems))); } All matchers in the example have the same result and can be used alternatively.\nAssert that an Array contains all the given items in the same order:-\n@Test public void givenArray_whenContainsAllItemsInSameOrder_thenPass() { String[] arrayItems = { \u0026#34;lord\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;rings\u0026#34; }; assertThat(arrayItems, arrayContaining(\u0026#34;lord\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;rings\u0026#34;)); } The arrayContaining matcher should have the same number of specified items as the length of the array for a positive match.\nAssert that an Array contains all the given values in any order:-\n@Test public void givenArray_whenContainsAllValuesInAnyOrder_thenPass() { String[] arrayItems = { \u0026#34;lord\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;rings\u0026#34; }; assertThat(arrayItems, arrayContainingInAnyOrder(\u0026#34;rings\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;lord\u0026#34;)); } Same as arrayContaining, the arrayContainingInAnyOrder matcher should have the same number of specified items as the length of the array for a positive match. Though they can be in any order.\nCollection Matchers for Map Hamcrest Collection Matchers provide assertions for Map to check key, value and entry. Let\u0026rsquo;s look at examples:-\nAssert that a Map is empty:- @Test public void givenMap_whenEmpty_thenPass() { Map\u0026lt;String, String\u0026gt; myMap = new HashMap\u0026lt;\u0026gt;(); assertThat(myMap, is(anEmptyMap())); } Assert that the Map has given size:- @Test public void givenMap_whenSizeMatched_thenPass() { Map\u0026lt;String, String\u0026gt; myMap = Map.of(\u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;, \u0026#34;key2\u0026#34;, \u0026#34;value2\u0026#34;); assertThat(myMap, is(aMapWithSize(equalTo(2)))); } Assert that the Map has a given Key:- @Test public void givenMap_whenHasGivenKey_thenPass() { Map\u0026lt;String, String\u0026gt; myMap = Map.of(\u0026#34;myKey\u0026#34;, \u0026#34;myValue\u0026#34;); assertThat(myMap, hasKey(startsWith(\u0026#34;my\u0026#34;))); assertThat(myMap, hasKey(endsWith(\u0026#34;Key\u0026#34;))); } Assert that the Map has a given Key with the specified condition:- @Test public void givenMap_whenHasGivenKeyWithCondition_thenPass() { Map\u0026lt;String, String\u0026gt; myMap = Map.of(\u0026#34;myKey\u0026#34;, \u0026#34;myValue\u0026#34;); assertThat(myMap, hasKey(startsWith(\u0026#34;my\u0026#34;))); assertThat(myMap, hasKey(endsWith(\u0026#34;Key\u0026#34;))); } Assert that the Map has a given Value:- @Test public void givenMap_whenHasGivenValue_thenPass() { Map\u0026lt;String, String\u0026gt; myMap = Map.of(\u0026#34;myKey\u0026#34;, \u0026#34;myValue\u0026#34;); assertThat(myMap, hasValue(\u0026#34;myValue\u0026#34;)); } Assert that the Map has a given Value with the specified condition:- @Test public void givenMap_whenHasGivenValueWithCondition_thenPass() { Map\u0026lt;String, String\u0026gt; myMap = Map.of(\u0026#34;myKey\u0026#34;, \u0026#34;myValue\u0026#34;); assertThat(myMap, hasValue(startsWith(\u0026#34;my\u0026#34;))); assertThat(myMap, hasValue(endsWith(\u0026#34;Value\u0026#34;))); } Assert that the Map has a given Entry:- @Test public void givenMap_whenHasGivenEntry_thenPass() { Map\u0026lt;String, String\u0026gt; myMap = Map.of(\u0026#34;myKey\u0026#34;, \u0026#34;myValue\u0026#34;); assertThat(myMap, hasEntry(\u0026#34;myKey\u0026#34;, \u0026#34;myValue\u0026#34;)); assertThat(myMap, hasEntry(endsWith(\u0026#34;Key\u0026#34;), endsWith(\u0026#34;Value\u0026#34;))); } Assert that the Map has a given Entry with the specified condition:- @Test public void givenMap_whenHasGivenEntryWithCondition_thenPass() { Map\u0026lt;String, String\u0026gt; myMap = Map.of(\u0026#34;myKey\u0026#34;, \u0026#34;myValue\u0026#34;); assertThat(myMap, hasEntry(\u0026#34;myKey\u0026#34;, \u0026#34;myValue\u0026#34;)); assertThat(myMap, hasEntry(endsWith(\u0026#34;Key\u0026#34;), endsWith(\u0026#34;Value\u0026#34;))); } Logical Matchers - NOT, AND, OR Hamcrest Logical Matchers can be used for chaining the matcher conditions with logical operators.\nMatcher Logical Operator not NOT anyOf OR allOf AND anyOf(allOf(), not()) OR (AND, NOT) either...or OR both...and AND Let\u0026rsquo;s look at the examples:-\nAssert that two Strings are not equal:- @Test public void givenTwoStrings_whenNotEquals_thenPass() { String text1 = \u0026#34;text1\u0026#34;; String text2 = \u0026#34;text2\u0026#34;; assertThat(text1, not(equalTo(text2))); } Assert that a String matches with any of the given conditions:- @Test public void givenString_whenAnyOfGivenConditionsMatch_thenPass() { String text = \u0026#34;lord of the rings\u0026#34;; assertThat(text, anyOf(startsWith(\u0026#34;lord\u0026#34;), containsString(\u0026#34;power\u0026#34;))); } Assert that a String matches with all of the given conditions:- @Test public void givenString_whenAllOfGivenConditionsMatch_thenPass() { String text = \u0026#34;lord of the rings\u0026#34;; assertThat(text, allOf(startsWith(\u0026#34;lord\u0026#34;), endsWith(\u0026#34;rings\u0026#34;))); } Assert that a String matches the nested condition using AND and OR operators:- @Test public void givenString_whenComplexConditionMatch_thenPass() { String text = \u0026#34;lord of the rings\u0026#34;; assertThat(text, anyOf(allOf(startsWith(\u0026#34;lord\u0026#34;), endsWith(\u0026#34;rings\u0026#34;)), endsWith(\u0026#34;power\u0026#34;))); } Assert that a String matches either of the two given conditions:- @Test public void givenString_whenEitherConditionMatch_thenPass() { String text = \u0026#34;lord of the rings\u0026#34;; assertThat(text, either(startsWith(\u0026#34;lord\u0026#34;)).or(endsWith(\u0026#34;power\u0026#34;))); } Assert that a String matches both of the given conditions:- @Test public void givenString_whenBothConditionMatch_thenPass() { String text = \u0026#34;lord of the rings\u0026#34;; assertThat(text, both(startsWith(\u0026#34;lord\u0026#34;)).and(endsWith(\u0026#34;rings\u0026#34;))); } Writing Custom Matcher Hamcrest comes with lots of useful matchers, but also provides you the ability to write your custom matcher to fit your testing needs.\nLet’s write a custom matcher for testing if a given number is a prime number.\npackage com.example.assertion; import org.hamcrest.Description; import org.hamcrest.Matcher; import org.hamcrest.TypeSafeMatcher; import java.util.stream.IntStream; public class IsPrimeNumber extends TypeSafeMatcher\u0026lt;Integer\u0026gt; { @Override protected boolean matchesSafely(Integer number) { return number \u0026gt; 1 \u0026amp;\u0026amp; IntStream.rangeClosed(2, (int) Math.sqrt(number)) .noneMatch(n -\u0026gt; (number % n == 0)); } @Override public void describeTo(Description description) { description.appendText(\u0026#34;a prime number\u0026#34;); } public static Matcher isPrimeNumber() { return new IsPrimeNumber(); } } We create a custom matcher by extending the TypeSafeMatcher abstract class. We need to implement the matchSaely method which provides a predicate if the number is a prime number. We also implement describeTo method which produces a failure message in case a test fails.\nLet\u0026rsquo;s test if a number is prime using our custom matcher isPrimeNumber:-\nimport static com.example.assertion.IsPrimeNumber.isPrimeNumber; @Test public void givenNumber_whenPrimeNumber_thenPass() { Integer num = 7; assertThat(num, new isPrimeNumber()); } and here is a failure message when test a non-prime number:-\njava.lang.AssertionError: Expected: a prime number but: was \u0026lt;8\u0026gt; That\u0026rsquo;s all about the Hamcrest matchers. Thanks for Reading!\n","permalink":"https://codingnconcepts.com/java/unit-test-with-hamcrest-assertthat/","tags":["JUnit","Java Assertion"],"title":"Unit Test with Hamcrest in java"},{"categories":["Spring Boot"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to customize the default Jackson JSON Mapper in Spring Boot Web Application with various Jackson configurations.\nOverview Spring MVC uses the Jackson JSON ObjectMapper as the default HttpMessageConverters in your Spring Boot web application, which does mainly two things:-\nMap the Java Object to JSON Response when you return the Object from GET request method like this:-\n@GetMapping public List\u0026lt;User\u0026gt; getAllUsers() The process of converting the Java Object to JSON is known as Marshalling or Serialization\nMap the JSON to Java Object when you add a @RequestBody argument in POST request method like this:-\n@PostMapping public Long createUser(@RequestBody User user) The process of converting the JSON to Java Object is known as Unmarshalling or Deserialization\nCustomize Default ObjectMapper In this section, we\u0026rsquo;ll see how to customize the auto-configured default ObjectMapper implicitly used by Spring Boot.\nUsing Application Properties Spring Boot auto-configures the default ObjectMapper based on the configurations provided in property files i.e. application.yml or application.properties file. Spring Boot also auto-registers the Jackson modules in ObjectMapper, if they are available in the classpath.\nIt is recommended to use the auto-configured ObjectMapper and customize the behavior by turning on/off the Jackson serialization and deserialization features using the property file. Set the Jackson property to true or false to enable or disable the feature respectively.\nMost of the customization use cases can be achieved with this approach. Here is the complete list of Jackson serialization, deserialization, and mapper configuration properties, which you can use:-\napplication.yml spring: jackson: default-property-inclusion: use_defaults/always/non-null/non-empty/non-absent/non-default serialization: CLOSE_CLOSEABLE: true/false EAGER_SERIALIZER_FETCH: true/false FAIL_ON_EMPTY_BEANS: true/false FAIL_ON_SELF_REFERENCES: true/false FAIL_ON_UNWRAPPED_TYPE_IDENTIFIERS: true/false FLUSH_AFTER_WRITE_VALUE: true/false INDENT_OUTPUT: true/false ORDER_MAP_ENTRIES_BY_KEYS: true/false USE_EQUALITY_FOR_OBJECT_ID: true/false WRAP_EXCEPTIONS: true/false WRAP_ROOT_VALUE: true/false WRITE_BIGDECIMAL_AS_PLAIN: true/false WRITE_CHAR_ARRAYS_AS_JSON_ARRAYS: true/false WRITE_DATES_AS_TIMESTAMPS: true/false WRITE_DATES_WITH_ZONE_ID: true/false WRITE_DATE_KEYS_AS_TIMESTAMPS: true/false WRITE_DATE_TIMESTAMPS_AS_NANOSECONDS: true/false WRITE_DURATIONS_AS_TIMESTAMPS: true/false WRITE_EMPTY_JSON_ARRAYS: true/false WRITE_ENUMS_USING_INDEX: true/false WRITE_ENUMS_USING_TO_STRING: true/false WRITE_ENUM_KEYS_USING_INDEX: true/false WRITE_NULL_MAP_VALUES: true/false WRITE_SELF_REFERENCES_AS_NULL: true/false WRITE_SINGLE_ELEM_ARRAYS_UNWRAPPED: true/false deserialization: ACCEPT_EMPTY_ARRAY_AS_NULL_OBJECT: true/false ACCEPT_EMPTY_STRING_AS_NULL_OBJECT: true/false ACCEPT_FLOAT_AS_INT: true/false ACCEPT_SINGLE_VALUE_AS_ARRAY: true/false ADJUST_DATES_TO_CONTEXT_TIME_ZONE: true/false EAGER_DESERIALIZER_FETCH: true/false FAIL_ON_IGNORED_PROPERTIES: true/false FAIL_ON_INVALID_SUBTYPE: true/false FAIL_ON_MISSING_CREATOR_PROPERTIES: true/false FAIL_ON_MISSING_EXTERNAL_TYPE_ID_PROPERTY: true/false FAIL_ON_NULL_CREATOR_PROPERTIES: true/false FAIL_ON_NULL_FOR_PRIMITIVES: true/false FAIL_ON_NUMBERS_FOR_ENUMS: true/false FAIL_ON_READING_DUP_TREE_KEY: true/false FAIL_ON_TRAILING_TOKENS: true/false FAIL_ON_UNKNOWN_PROPERTIES: true/false FAIL_ON_UNRESOLVED_OBJECT_IDS: true/false READ_DATE_TIMESTAMPS_AS_NANOSECONDS: true/false READ_ENUMS_USING_TO_STRING: true/false READ_UNKNOWN_ENUM_VALUES_AS_NULL: true/false READ_UNKNOWN_ENUM_VALUES_USING_DEFAULT_VALUE: true/false UNWRAP_ROOT_VALUE: true/false UNWRAP_SINGLE_VALUE_ARRAYS: true/false USE_BIG_DECIMAL_FOR_FLOATS: true/false USE_BIG_INTEGER_FOR_INTS: true/false USE_JAVA_ARRAY_FOR_JSON_ARRAY: true/false USE_LONG_FOR_INTS: true/false WRAP_EXCEPTIONS: true/false mapper: ACCEPT_CASE_INSENSITIVE_PROPERTIES: true/false ACCEPT_CASE_INSENSITIVE_ENUMS: true/false ACCEPT_CASE_INSENSITIVE_VALUES: true/false ALLOW_COERCION_OF_SCALARS: true/false ALLOW_EXPLICIT_PROPERTY_RENAMING: true/false ALLOW_FINAL_FIELDS_AS_MUTATORS: true/false ALLOW_VOID_VALUED_PROPERTIES: true/false AUTO_DETECT_CREATORS: true/false AUTO_DETECT_FIELDS: true/false AUTO_DETECT_GETTERS: true/false AUTO_DETECT_IS_GETTERS: true/false AUTO_DETECT_SETTERS: true/false BLOCK_UNSAFE_POLYMORPHIC_BASE_TYPES: true/false CAN_OVERRIDE_ACCESS_MODIFIERS: true/false DEFAULT_VIEW_INCLUSION: true/false IGNORE_DUPLICATE_MODULE_REGISTRATIONS: true/false IGNORE_MERGE_FOR_UNMERGEABLE: true/false INFER_PROPERTY_MUTATORS: true/false INFER_CREATOR_FROM_CONSTRUCTOR_PROPERTIES: true/false INFER_BUILDER_TYPE_BINDINGS: true/false OVERRIDE_PUBLIC_ACCESS_MODIFIERS: true/false PROPAGATE_TRANSIENT_MARKER: true/false REQUIRE_SETTERS_FOR_GETTERS: true/false SORT_PROPERTIES_ALPHABETICALLY: true/false SORT_CREATOR_PROPERTIES_FIRST: true/false USE_STATIC_TYPING: true/false USE_BASE_TYPE_AS_DEFAULT_IMPL: true/false USE_GETTERS_AS_SETTERS: true/false USE_WRAPPER_NAME_AS_PROPERTY_NAME: true/false USE_STD_BEAN_NAMING: true/false You can also refer to the Spring Boot official documentation for Jackson configurations.\nPlease note that the spring boot configuration supports Relaxed Binding which means properties can be in uppercase or lowercase i.e. spring.jackson.serialization.INDENT_OUTPUT and spring.jackson.serialization.indent_output both are valid.\nAlso Read JSON API Request and Response customization using Jackson\nUsing Jackson2ObjectMapperBuilderCustomizer By default, Spring boot auto-configure Jackson2ObjectMapperBuilder bean and use this builder to auto-configure ObjectMapper (or XmlMapper for Jackson XML converter) instance.\nYou can customize the default behavior of Jackson2ObjectMapperBuilder by defining a custom Jackson2ObjectMapperBuilderCustomizer bean. Spring boot will use this custom builder to build ObjectMapper and XmlMapper.\n@Configuration public class CustomJacksonConfig { @Bean public Jackson2ObjectMapperBuilderCustomizer jackson2ObjectMapperBuilderCustomizer() { return builder -\u0026gt; builder.serializationInclusion(JsonInclude.Include.NON_NULL) .featuresToEnable(MapperFeature.SORT_PROPERTIES_ALPHABETICALLY) .featuresToEnable(MapperFeature.ACCEPT_CASE_INSENSITIVE_PROPERTIES) .failOnUnknownProperties(false) .featuresToDisable(DeserializationFeature.FAIL_ON_IGNORED_PROPERTIES, false) .featuresToDisable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS) .indentOutput(true) .modules(new JavaTimeModule()); } } Using Custom ObjectMapper If you just want to customize the default ObjectMapper and not all the Jackson mappers created by Jackson2ObjectMapperBuilder builder then follow this approach:-\n@Configuration public class CustomJacksonConfig { @Bean @Primary public ObjectMapper objectMapper(Jackson2ObjectMapperBuilder builder) { return builder.build().setSerializationInclusion(JsonInclude.Include.NON_NULL) .configure(MapperFeature.SORT_PROPERTIES_ALPHABETICALLY, true) .configure(MapperFeature.ACCEPT_CASE_INSENSITIVE_PROPERTIES, true) .configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false) .configure(DeserializationFeature.FAIL_ON_IGNORED_PROPERTIES, false) .configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false) .configure(SerializationFeature.INDENT_OUTPUT, true) .registerModule(new JavaTimeModule()); } } Overwrite Default ObjectMapper If you don\u0026rsquo;t want to rely on spring boot auto configuration and would like to have complete control over the Jackson ObjectMapper instance then overwrite the default ObjectMapper by providing a custom bean.\nUsing Jackson2ObjectMapperBuilder When you define a new custom Jackson2ObjectMapperBuilder bean, it disables all auto-configurations of ObjectMapper and XmlMapper and uses your custom builder to build them. An example of a custom object builder is as follows:-\n@Configuration public class CustomJacksonConfig { @Bean public Jackson2ObjectMapperBuilder jackson2ObjectMapperBuilder() { return new Jackson2ObjectMapperBuilder() .serializationInclusion(JsonInclude.Include.NON_NULL) .featuresToEnable(MapperFeature.ACCEPT_CASE_INSENSITIVE_PROPERTIES) .failOnUnknownProperties(false) .featuresToDisable(DeserializationFeature.FAIL_ON_IGNORED_PROPERTIES, false) .featuresToDisable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS) .indentOutput(true) .modules(new JavaTimeModule()); } } Using Custom ObjectMapper If you just want to overwrite the default ObjectMapper and not all the Jackson mappers created by Jackson2ObjectMapperBuilder builder then follow this approach:-\n@Configuration public class CustomJacksonConfig { @Bean @Primary public ObjectMapper objectMapper() { return new ObjectMapper() .setSerializationInclusion(JsonInclude.Include.NON_NULL) .configure(MapperFeature.SORT_PROPERTIES_ALPHABETICALLY, true) .configure(MapperFeature.ACCEPT_CASE_INSENSITIVE_PROPERTIES, true) .configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false) .configure(DeserializationFeature.FAIL_ON_IGNORED_PROPERTIES, false) .configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false) .configure(SerializationFeature.INDENT_OUTPUT, true); } } Conclusion In this article, we learned how to -\nOverride the default behavior of Jackson JSON Mapper using application properties, defining a custom bean of Jackson2ObjectMapperBuilder or ObjectMapper. Overwrite the default behavior of Jackson JSON Mapper by defining a custom bean of Jackson2ObjectMapperBuilder or ObjectMapper. If we want to customize all Jackson mappers i.e. ObjectMapper and XmlMapper, then define a custom Jackson2ObjectMapperBuilder bean. In case, we only want to customize Jackson ObjectMapper for JSON, then define a custom ObjectMapper bean.\nWe can use Jackson MapperFeature, DeserializationFeature, and SerializationFeature properties to customize mapper, deserialization, and serialization behavior respectively.\nFind the source code for the examples in this article on GitHub\nThanks for Reading!\n","permalink":"https://codingnconcepts.com/spring-boot/customize-jackson-json-mapper/","tags":["Spring Boot API","Jackson"],"title":"Customize Jackson JSON Mapper in Spring Boot"},{"categories":["Spring Boot"],"contents":"In this quick article, we learn how to change the servlet context path in the Spring Boot application.\nOverview Spring Boot web application, by default, serves the content at the root context path (\u0026quot;/\u0026quot;) i.e. localhost:port/\nSome application requires serving the content from a custom servlet context path e.g. localhost:port/myapp and require all the API request mappings should be appended to this context path e.g. localhost:port/myapp/users, localhost:port/myapp/dashboard, etc.\nSpring Boot provides a property to change the context path which can be configured from application.properties, application.yml, or command-line argument.\nUsing Application Property Set the property server.servlet.context-path to your custom context path for Spring Boot 2.x and above version like this:-\n# Using application.properties server.servlet.context-path=/myapp # Using application.yml server: servlet: context-path: /myapp # Using command-line $ java -jar -Dserver.servlet.context-path=/myapp spring-boot-app-1.0.jar For Spring Boot 1.x, use this property instead:-\nserver.context-path=/myapp When you start your application on a local machine. Your home page content is served at localhost:8080/myapp now.\nFor External Tomcat Please note that this property only works with Spring Boot embedded containers e.g. Tomcat, Undertow, Jetty, etc. This property doesn\u0026rsquo;t work when you deploy the Spring boot web application to an external container like Tomcat using WAR.\nTo change the context path in external container, your WAR file name should match the context path e.g. myapp.war. You can change the WAR file name from the pom.xml like this:-\npom.xml \u0026lt;build\u0026gt; ... \u0026lt;finalName\u0026gt;myapp\u0026lt;/finalName\u0026gt; \u0026lt;/build\u0026gt; Using Java Code You have to provide a bean of WebServerFactoryCustomizer for Spring Boot 2.x and above version:-\n@Bean public WebServerFactoryCustomizer\u0026lt;ConfigurableServletWebServerFactory\u0026gt; webServerFactoryCustomizer() { return factory -\u0026gt; factory.setContextPath(\u0026#34;/myapp\u0026#34;); } For Spring Boot 1.x, provide a bean of EmbeddedServletContainerCustomizer like this:-\n@Bean public EmbeddedServletContainerCustomizer\u0026lt;ConfigurableEmbeddedServletContainer\u0026gt; embeddedServletContainerCustomizer() { return container -\u0026gt; container.setContextPath(\u0026#34;/myapp\u0026#34;); } Using Environment Variable You can also use OS environment variable to set the servlet context path for Spring Boot Application.\nUse the following OS environment variable for Spring Boot 2.x and above version:-\nUNIX $ export SERVER_SERVLET_CONTEXT_PATH=/myapp WINDOWS \u0026gt; set SERVER_SERVLET_CONTEXT_PATH=/myapp For Spring Boot 1.x, use:-\nUNIX $ export SERVER_CONTEXT_PATH=/myapp WINDOWS \u0026gt; set SERVER_CONTEXT_PATH=/myapp That is all about this article. Thanks for Reading!\n","permalink":"https://codingnconcepts.com/spring-boot/change-context-path/","tags":["Spring Boot Basics"],"title":"How to Change Context Path in Spring Boot"},{"categories":["Spring Boot"],"contents":"In this article, we will learn how to build a Spring Boot JAR as a library and use it as a dependency in other applications.\nOverview By default, Spring Boot use repackage goal to build an executable standalone Fat JAR:-\n💼springboot-demo-0.0.1-SNAPSHOT.jar 📁 BOOT-INF 📂 classes 📄 ...compiled classes 📄 application.yml 📂 lib 📄 spring-boot-2.5.0.jar 📄 spring-core-5.3.7.jar 📄 ...other dependencies 📁 META-INF 📁 org This FAT JAR has compiled classes in BOOT-INF/classes and the dependent libraries in BOOT-INF/lib location, which is very specific to Spring boot to execute it as a standalone application.\nWhen you add this Fat JAR as a dependency in other applications and try to use its classes and methods, then it throws ClassNotFoundException. It is because dependent modules are not able to load classes from these specific locations. You need a traditional library JAR for that.\nBuild Spring Boot JAR as a dependency Maven If you want to use Spring Boot JAR as a library and using Maven as dependency management then you need to overwrite the default Spring boot repackage goal behavior by adding a classifier in the spring-boot-maven-plugin execution configuration in pom.xml as below:-\nSpring Boot 1.x \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.5.20.RELEASE\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;repackage\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;classifier\u0026gt;exec\u0026lt;/classifier\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; ... \u0026lt;/plugin\u0026gt; Spring Boot 2.x If you are using Spring Boot 2.x and spring-boot-starter-parent, then it is essential to add execution id i.e. \u0026lt;id\u0026gt;repackage\u0026lt;/id\u0026gt; to overwrite the repackage goal default behavior in pom.xml as below:-\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;repackage\u0026lt;/id\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;classifier\u0026gt;exec\u0026lt;/classifier\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; ... \u0026lt;/plugin\u0026gt; The above configurations build two JARs in the maven .m2 repository as follows:-\n📁 .m2\\repository\\com\\example\\springboot-demo\\0.0.1-SNAPSHOT 📄_remote.repositories 📄maven-metadata-local.xml 💼springboot-demo-0.0.1-SNAPSHOT.jar 📄springboot-demo-0.0.1-SNAPSHOT.pom 💼springboot-demo-0.0.1-SNAPSHOT-exec.jar Where,\nspringboot-demo-0.0.1-SNAPSHOT.jar is a Standard JAR that can be used as a dependent library springboot-demo-0.0.1-SNAPSHOT-exec.jar is a Fat JAR that can be used as a standalone executable Please note the -exec suffix appended in the executable JAR name, which comes from \u0026lt;classifier\u0026gt;exec\u0026lt;/classifier\u0026gt;. You can provide any name in the classifier.\nOnly Dependent JAR If the Spring Boot project is meant to be used as a dependent library only and it doesn\u0026rsquo;t have any Main class to be used as an application then it makes more sense to generate the standard JAR only and skip the repackaging to FAT JAR like this:-\nSpring Boot 2.x (pom.xml) \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;repackage\u0026lt;/id\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;skip\u0026gt;true\u0026lt;/skip\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; The above configuration builds only dependency JAR as follows:-\n📁 .m2\\repository\\com\\example\\springboot-demo\\0.0.1-SNAPSHOT 📄_remote.repositories 📄maven-metadata-local.xml 💼springboot-demo-0.0.1-SNAPSHOT.jar 📄springboot-demo-0.0.1-SNAPSHOT.pom Gradle If you want to use Spring Boot JAR as a library and using Gradle as dependency management then follow this:-\nSpring Boot 1.x If you are using Spring Boot 1.x then use following Gradle configuration in build.gradle to build both standard and executable JARs:-\nbootRepackage { classifier = \u0026#39;exec\u0026#39; } Disable bootRepackage task to generate only Standard JAR without executable Fat JAR:-\nbootRepackage { enabled = false } Spring Boot 2.x If you are using Spring Boot 2.x then you need to apply java plugin with,\nGradle bootJar task to build the executable Fat JAR Gradle jar task to build the Standard JAR Add following Gradle plugin configuration in build.gradle to generate both JARs:-\napply plugin: \u0026#39;java\u0026#39; tasks.named(\u0026#34;bootJar\u0026#34;) { archiveClassifier = \u0026#39;exec\u0026#39; enabled = true } tasks.named(\u0026#34;jar\u0026#34;) { archiveClassifier = \u0026#39;\u0026#39; enabled = true } Gradle build generate below two JARs with above configuration:-\n📁 springboot-demo\\build\\libs 💼springboot-demo-0.0.1-SNAPSHOT.jar 💼springboot-demo-0.0.1-SNAPSHOT-exec.jar Only Dependent JAR Use following Gradle plugin configuration if you want to generate only Standard JAR without executable Fat JAR:-\napply plugin: \u0026#39;java\u0026#39; tasks.named(\u0026#34;bootJar\u0026#34;) { enabled = false } tasks.named(\u0026#34;jar\u0026#34;) { enabled = true } Only Executable JAR Use following Gradle plugin configuration if you want to generate only Executable Fat JAR without standard JAR:-\napply plugin: \u0026#39;java\u0026#39; tasks.named(\u0026#34;bootJar\u0026#34;) { enabled = true } tasks.named(\u0026#34;jar\u0026#34;) { enabled = false } Conclusion We learned how to build a plain standard JAR from Spring Boot Project, which can be used as a library and can be added as a dependency in other applications. We also learned the internal structure of the standalone executable JAR, which is built by the Spring Boot project by default, whether you use Maven or Gradle as dependency management. We can either generate both Standard and Executable JAR or disable one of them from the Maven pom.xml and Gradle build.gradle configurations.\nThanks for reading!\n","permalink":"https://codingnconcepts.com/spring-boot/spring-boot-project-as-dependency/","tags":["Spring Boot Basics"],"title":"Use Spring Boot Project as a Dependency"},{"categories":["Spring Boot"],"contents":"In this article, we will learn how to log SQL statements in the Spring Boot application for debugging.\nOverview Spring Data JDBC and JPA modules provide the abstraction over the data access layer by writing the boilerplate code and using Hibernate as the default JPA provider. Developers generally write the Repository interfaces and Spring boot JPA auto-generates the SQL statements for CRUD operations for those repositories.\nThere are times when we encounter internal server/database errors, which require us to debug the data access layer code and SQL statements executed then. We will learn how to print SQL statements in the console (standard output) and application logs for debugging and production support.\n(Don\u0026rsquo;t Use) Print SQL Statements in the Console The easiest but not recommended way to show the hibernate-generated SQL statements in Console (standard output) is by setting the following property in the property file:-\nspring.jpa.show-sql=true Please note that the above configuration is equivalent to:\nspring.jpa.properties.hibernate.show_sql=true This approach writes SQL queries to the standard output (same as System.out.println). You see these SQL queries in the console when using an IDE during development or in stdout logs generated by the underlying application server e.g. tomcat in /tomcat/logs/catalina.out file\nIt is not a recommended approach for production-grade applications and we should write the SQL queries in application logs for better log management and debugging.\nWrite SQL Statements in Application Logs We can use the following properties to write the SQL statements in the application log file:-\nlogging.level.org.hibernate.SQL=DEBUG logging.level.org.hibernate.type.descriptor.sql.BasicBinder=TRACE The first property logs the hibernate-generated SQL queries and the second property logs the prepared statement parameters with values.\nLogs: -2022-12-11 23:09:39.293 -DEBUG 4620 --- [nio-8080-exec-1] org.hibernate.SQL : insert into user (age, name, id) values (?, ?, ?) -2022-12-11 23:09:39.307 -TRACE 4620 --- [nio-8080-exec-1] o.h.type.descriptor.sql.BasicBinder : binding parameter [1] as [INTEGER] - [36] -2022-12-11 23:09:39.308 -TRACE 4620 --- [nio-8080-exec-1] o.h.type.descriptor.sql.BasicBinder : binding parameter [2] as [VARCHAR] - [ashish] -2022-12-11 23:09:39.309 -TRACE 4620 --- [nio-8080-exec-1] o.h.type.descriptor.sql.BasicBinder : binding parameter [3] as [BIGINT] - [1] Pretty Print SQL Queries You can also pretty-print the SQL queries, which is quite useful to analyze long and complex queries. Use the below property along with the above two properties:-\nspring.jpa.properties.hibernate.format_sql=true The formatted SQL query in the logs looks like this:-\nLogs: -2022-12-11 23:26:10.193 -DEBUG 10272 --- [nio-8080-exec-2] org.hibernate.SQL : insert into user (age, name, id) values (?, ?, ?) -2022-12-11 23:26:10.203 -TRACE 10272 --- [nio-8080-exec-2] o.h.type.descriptor.sql.BasicBinder : binding parameter [1] as [INTEGER] - [36] -2022-12-11 23:26:10.205 -TRACE 10272 --- [nio-8080-exec-2] o.h.type.descriptor.sql.BasicBinder : binding parameter [2] as [VARCHAR] - [ashish] -2022-12-11 23:26:10.207 -TRACE 10272 --- [nio-8080-exec-2] o.h.type.descriptor.sql.BasicBinder : binding parameter [3] as [BIGINT] - [1] Print SQL Queries when using JdbcTemplate If you are using JdbcTemplate in Spring boot to query database then use the following properties to print the SQL queries:-\nlogging.level.org.springframework.jdbc.core.JdbcTemplate=DEBUG logging.level.org.springframework.jdbc.core.StatementCreatorUtils=TRACE Similar to JPA logging, the first property prints the SQL queries and the second prints the parameters of the prepared statements.\nThat is all about logging SQL queries. Thanks for reading!\n","permalink":"https://codingnconcepts.com/spring-boot/log-sql-queries-spring-boot/","tags":["Spring Boot JPA"],"title":"Log Hibernate/JPA SQL Queries in Spring Boot"},{"categories":["Java"],"contents":"Quick example to format any type of number - int, long, float, and double to 2 decimal places in Java\nDecimalFormat(\u0026ldquo;0.00\u0026rdquo;) import java.text.DecimalFormat; public class FormatAnyNumber { private static final DecimalFormat df = new DecimalFormat(\u0026#34;0.00\u0026#34;); public static void main(String[] args) { int num1 = 12345; long num2 = 12345; float num3 = 12345.6f; double num4 = 12345.6789; System.out.println(\u0026#34;int: \u0026#34; + df.format(num1)); System.out.println(\u0026#34;long: \u0026#34; + df.format(num2)); System.out.println(\u0026#34;float: \u0026#34; + df.format(num3)); System.out.println(\u0026#34;double: \u0026#34; + df.format(num4)); } } Output int: 12345.00 long: 12345.00 float: 12345.60 double: 12345.68 Points to note\u0026hellip;\nint and long number has no decimal places, appended \u0026ldquo;00\u0026rdquo; at the end for 2 decimal places float number has only one decimal place, appended \u0026ldquo;0\u0026rdquo; at the end for 2 decimal places double number has 4 decimal places, rounded to 2 decimal places. RoundingMode By default, DecimalFormat use RoundingMode.HALF_EVEN for rounding. You can use other rounding modes as follows:-\nimport java.math.RoundingMode; import java.text.DecimalFormat; public class FormatAnyNumber { private static final DecimalFormat df = new DecimalFormat(\u0026#34;0.00\u0026#34;); public static void main(String[] args) { double num = 12345.6789; df.setRoundingMode(RoundingMode.UP); System.out.println(\u0026#34;double rounding up : \u0026#34; + df.format(num)); df.setRoundingMode(RoundingMode.DOWN); System.out.println(\u0026#34;double rounding down: \u0026#34; + df.format(num)); } } Output double rounding up : 12345.68 double rounding down: 12345.67 ","permalink":"https://codingnconcepts.com/java/format-number-to-2-decimal-places/","tags":["Core Java","Numbers"],"title":"Format number to 2 decimal places in Java"},{"categories":["Spring Boot"],"contents":"In this article, we\u0026rsquo;ll learn how to customize default Whitelabel Error Page in Spring Boot using Thymeleaf HTML templates\nWhitelabel Error Page Whitelabel error page is default error page generated by Spring Boot when no custom error page is provided.\nIf you don\u0026rsquo;t like this default behavior and looking for custom HTML pages to be displayed for different type of HTTP errors then Thymeleaf error templates are simplest approach to follow.\nThymeleaf Error Templates You create Thymeleaf HTML error template under resources/templates directory:-\nGeneric error template required with name error.html Specific error templates are optional under errors directory with name \u0026lt;error_code\u0026gt;.html for e.g. 404.html for Not found error, 500.html for Internal server error, etc. Generic Error Template If specific error template not found then fallback to generic template for all types of HTTP error codes.\nresources/templates/error.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html xmlns:th=\u0026#34;http://www.thymeleaf.org\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Something went wrong!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Generic Error - \u0026lt;span th:text=\u0026#34;${status}\u0026#34;\u0026gt;Status\u0026lt;/span\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Sorry for the inconvenience. Please contact the administrator.\u0026lt;/p\u0026gt; \u0026lt;br/\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;Timestamp: \u0026lt;span th:text=\u0026#34;${timestamp}\u0026#34;\u0026gt;Timestamp\u0026lt;/span\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Path: \u0026lt;span th:text=\u0026#34;${path}\u0026#34;\u0026gt;Path\u0026lt;/span\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Error: \u0026lt;span th:text=\u0026#34;${error}\u0026#34;\u0026gt;Error\u0026lt;/span\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Specific error template 500.html doesn\u0026rsquo;t exist, display error.html Specific Error Template If specific error template found for e.g. 404.html, display it for 404 error!\nWe imported bootstrap css in the 404 error page to look more elegant.\nresources/templates/errors/404.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html xmlns:th=\u0026#34;http://www.thymeleaf.org\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;IE=edge\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1\u0026#34;\u0026gt; \u0026lt;!-- Bootstrap --\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css\u0026#34; integrity=\u0026#34;sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; \u0026lt;title\u0026gt;404 - resource not found\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;span class=\u0026#34;label label-default\u0026#34;\u0026gt;Resource Not Found Error - 404\u0026lt;/span\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Requested resource was not found. Please contact the administrator.\u0026lt;/p\u0026gt; \u0026lt;table class=\u0026#34;table table-bordered table-striped\u0026#34;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;Timestamp\u0026lt;/td\u0026gt; \u0026lt;td th:text=\u0026#34;${timestamp}\u0026#34;\u0026gt;Timestamp\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;Path\u0026lt;/td\u0026gt; \u0026lt;td th:text=\u0026#34;${path}\u0026#34;\u0026gt;Path\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;Status\u0026lt;/td\u0026gt; \u0026lt;td th:text=\u0026#34;${status}\u0026#34;\u0026gt;Status\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;Error\u0026lt;/td\u0026gt; \u0026lt;td th:text=\u0026#34;${error}\u0026#34;\u0026gt;Error\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;Message\u0026lt;/td\u0026gt; \u0026lt;td th:text=\u0026#34;${message}\u0026#34;\u0026gt;Message\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Specific error template 400.html exist, display it! Thymeleaf Dependency We need to add Thymeleaf dependency in the project for these error templates to work.\nbuild.gradle dependencies { implementation \u0026#39;org.springframework.boot:spring-boot-starter-thymeleaf\u0026#39; } pom.xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-thymeleaf\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Thats it! Once you add Thymeleaf dependency, it does all the magic.\n","permalink":"https://codingnconcepts.com/spring-boot/custom-html-error-page-thymeleaf/","tags":["Spring Boot API","REST"],"title":"Custom HTML Error Page using Thymeleaf"},{"categories":["Spring Boot"],"contents":"In this article, we\u0026rsquo;ll learn how to disable default Whitelabel Error Page in Spring Boot.\nWhitelabel Error Page Whitelabel error page appears when you hit a url which result into HTTP error. Whitelabel error page is generated by Spring Boot as a default mechanism when no custom error page is found.\nLet\u0026rsquo;s hit a wrong or unknown url, which result into HTTP 404 error:-\nIf you don\u0026rsquo;t want to see this default Whitelabel error page then it can be disabled using any one of these two properties:-\nDisable Whitelabel Error Server Property server.error.whitelabel.enabled = false Exclude ErrorMvcAutoConfiguration spring.autoconfigure.exclude = org.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration ## For Spring Boot \u0026lt; 2.0 spring.autoconfigure.exclude = org.springframework.boot.autoconfigure.web.ErrorMvcAutoConfiguration When Whitelabel error page is disabled and no custom error page is provided, you will see the error page generated by underlying web application server such as Tomcat, Undertow, Jetty etc.\nFollow the post for Customize Whitelabel HTML Error Page using Thymeleaf\nDisable Whitelabel Error Server Property You can disable the Whitelabel error page by setting the server.error.whitelabel.enabled property to false. This can be done from .properties file, .yml file, or through command-line parameters as follows:-\n## application.properties server.error.whitelabel.enabled = false ## application.yml server: error: whitelabel: enabled: false $ java -jar -Dserver.error.whitelabel.enabled=false spring-boot-app-1.0.jar $ java -jar spring-boot-app-1.0.jar --server.error.whitelabel.enabled=false Exclude ErrorMvcAutoConfiguration Another way to disable it via excluding the ErrorMvcAutoConfiguration from auto configuration. This can be done easily from .properties file or .yml file\n## application.properties spring.autoconfigure.exclude = org.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration ## application.yml spring: autoconfigure: exclude: org.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration Alternatively, you can also use @EnableAutoConfiguration annotation to exclude it.\nimport org.springframework.context.annotation.Configuration; import org.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration; ... @Configuration @EnableAutoConfiguration(exclude = {ErrorMvcAutoConfiguration.class}) public static MainApp { ... } For Spring Boot \u0026lt; 2.0 Please note that for Spring Boot \u0026lt; 2.0, the ErrorMvcAutoConfiguration class is located in package org.springframework.boot.autoconfigure.web.\n## application.properties spring.autoconfigure.exclude = org.springframework.boot.autoconfigure.web.ErrorMvcAutoConfiguration ## application.yml spring: autoconfigure: exclude: org.springframework.boot.autoconfigure.web.ErrorMvcAutoConfiguration ","permalink":"https://codingnconcepts.com/spring-boot/disable-whitelabel-error-page-spring-boot/","tags":["Spring Boot API","REST"],"title":"Disable Whitelabel Error Page in Spring Boot"},{"categories":["Java"],"contents":"In this tutorial, we will learn how to auto generate class files from XSD schema using JAXB task and use them to convert to Json.\nOverview Consider a use case, where you are given an XML Schema .xsd file, based on which you should convert an XML to JSON format. There is no straightforward solution for this. We need to do following:-\nFirst we need to generate Java class files from XSD schema .xsd file Use Jackson for deserialization (XML to Java Object) and serialization (Java Object to JSON), which result into XML to JSON conversion. We want to automate this as much as possible so that if there is any update in XML schema, it can be adapted with minimal change. We will create a JAXB task (gradle or maven) here, which is tied with project build phase and responsible for generating Java class files from given schema .xsd file.\nWhen we converted the XML to JSON file using the generated Java class files, we find mainly two issues which were not meeting our requirement and solved them:-\nDate and Time in XSD xs:date and xs:time are converted to timestamp format instead of date and time format Enum value in XSD \u0026lt;xs:enumeration value = \u0026quot;half down\u0026quot;/\u0026gt; having space is converted to \u0026ldquo;HALF_UP\u0026rdquo; instead of \u0026ldquo;half up\u0026rdquo; Follow the steps to automate generation of class files and solve above two issues:-\nSteps 1. Add Gradle Task to generate classes Add a jaxb gradle task in build.gradle where you specify following:-\nTarget destDir and package name to generate classes, for e.g. JAXB generate the classed in directory src/main/generated-sources and package com.example.jaxb schema .xsd file location, for e.g. JAXB generates the java classes from schema file src/main/resources/schema/schema.xsd. binding .xjb file location, for e.g. JAXB use the binding file src/main/resources/jaxb/bindings.xjb build.gradle configurations { jaxb } // Dependencies to be used by \u0026#34;jaxb\u0026#34; task dependencies { jaxb( \u0026#39;com.sun.xml.bind:jaxb-xjc:2.3.1\u0026#39;, \u0026#39;com.sun.xml.bind:jaxb-impl:2.3.1\u0026#39;, \u0026#39;org.glassfish.jaxb:jaxb-runtime:2.3.1\u0026#39;, \u0026#39;org.jvnet.jaxb2_commons:jaxb2-basics:0.12.0\u0026#39; ) } // JAXB task definition task jaxb { def generatedResouces = \u0026#34;src/main/generated-sources\u0026#34; def jaxbTargetDir = file(generatedResouces) jaxbTargetDir.deleteDir() doLast { jaxbTargetDir.mkdirs() ant.taskdef(name: \u0026#39;xjc\u0026#39;, classname: \u0026#39;com.sun.tools.xjc.XJCTask\u0026#39;, classpath: configurations.jaxb.asPath) ant.jaxbTargetDir = jaxbTargetDir ant.xjc(destDir: \u0026#39;${jaxbTargetDir}\u0026#39;, package: \u0026#39;com.example.jaxb\u0026#39;, extension: true){ schema(dir: \u0026#34;src/main/resources/schema\u0026#34;, includes: \u0026#34;schema.xsd\u0026#34;) binding(dir: \u0026#34;src/main/resources/jaxb\u0026#34;, includes: \u0026#34;bindings.xjb\u0026#34;) arg(line: \u0026#39;-XenumValue\u0026#39;) } } } // Add generated classes directory to source sourceSets.main.java.srcDirs += \u0026#39;src/main/generated-sources\u0026#39; // Run jaxb task before compile Java classes compileJava.dependsOn jaxb Generate classes from multiple XSD schema files If you require generating classes from multiple XML schema .xsd files in different packages, you can add multiple ant.xjc like this:-\nant.xjc(destDir: \u0026#39;${jaxbTargetDir}\u0026#39;, package: \u0026#39;com.example.jaxb.schema1\u0026#39;, extension: true){ schema(dir: \u0026#34;src/main/resources/schema\u0026#34;, includes: \u0026#34;schema1.xsd\u0026#34;) binding(dir: \u0026#34;src/main/resources/jaxb\u0026#34;, includes: \u0026#34;bindings.xjb\u0026#34;) arg(line: \u0026#39;-XenumValue\u0026#39;) } ant.xjc(destDir: \u0026#39;${jaxbTargetDir}\u0026#39;, package: \u0026#39;com.example.jaxb.schema2\u0026#39;, extension: true){ schema(dir: \u0026#34;src/main/resources/schema\u0026#34;, includes: \u0026#34;schema2.xsd\u0026#34;) binding(dir: \u0026#34;src/main/resources/jaxb\u0026#34;, includes: \u0026#34;bindings.xjb\u0026#34;) arg(line: \u0026#39;-XenumValue\u0026#39;) } It will generate classes from schema1.xsd in package com.example.jaxb.schema1 and schema2.xsd in package com.example.jaxb.schema2\n2. Fix date and time format issue JAXB maps xs:time, xs:date, and xs:dateTime to javax.xml.datatype.XMLGregorianCalendar by default.\nXMLGregorianCalendar lacks semantics of what the underlying data type really is:\nit lacks the information on whether this is a time, date or dateTime it lacks the information on whether the value is a local date/time versus one tied to a specific timezone offset. it is mutable To avoid this, we want JAXB to map:-\nxs:time to java.time.LocalTime xs:date to java.time.LocalDate xs:dateTime to java.time.LocalDateTime We need to do two things for this, first create adapter classes for e.g. com.example.xml.adapter.TimeAdapter and then tell JAXB to use these classes through binding bindings.xjb file.\nTimeAdapter.java package com.example.xml.adapter; public class TimeAdapter extends XmlAdapter\u0026lt;String, LocalTime\u0026gt; { @Override public LocalTime unmarshal(String v) { if (Objects.nonNull(v)) { try { return LocalTime.parse(v); } catch (DateTimeParseException e) { throw new RuntimeException(\u0026#34;Failed to parse time: \u0026#34; + v, e); } } return null; } @Override public String marshal(LocalTime v) { if (Objects.nonNull(v)) { return v.format(DateTimeFormatter.ISO_TIME); } return null; } } bindings.xjb \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; standalone=\u0026#34;yes\u0026#34;?\u0026gt; \u0026lt;jaxb:bindings version=\u0026#34;2.1\u0026#34; xmlns:jaxb=\u0026#34;http://java.sun.com/xml/ns/jaxb\u0026#34; xmlns:xjc=\u0026#34;http://java.sun.com/xml/ns/jaxb/xjc\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://java.sun.com/xml/ns/jaxb http://java.sun.com/xml/ns/jaxb/bindingschema_2_0.xsd\u0026#34;\u0026gt; \u0026lt;jaxb:globalBindings typesafeEnumMaxMembers=\u0026#34;2000\u0026#34;\u0026gt; \u0026lt;xjc:serializable uid=\u0026#34;-1\u0026#34;/\u0026gt; \u0026lt;xjc:javaType xmlType=\u0026#34;xs:date\u0026#34; name=\u0026#34;java.time.LocalDate\u0026#34; adapter=\u0026#34;com.example.xml.adapter.DateAdapter\u0026#34;/\u0026gt; \u0026lt;xjc:javaType xmlType=\u0026#34;xs:time\u0026#34; name=\u0026#34;java.time.LocalTime\u0026#34; adapter=\u0026#34;com.example.xml.adapter.TimeAdapter\u0026#34;/\u0026gt; \u0026lt;xjc:javaType xmlType=\u0026#34;xs:dateTime\u0026#34; name=\u0026#34;java.time.LocalDateTime\u0026#34; adapter=\u0026#34;com.example.xml.adapter.DateTimeAdapter\u0026#34;/\u0026gt; \u0026lt;/jaxb:globalBindings\u0026gt; \u0026lt;/jaxb:bindings\u0026gt; That\u0026rsquo;s it! Now the generated classes will have time, date, and dateTime mapped to Java time package.\n3. Fix Enum value issue First Look at the problem statement, below is the example of enumeration in xsd schema:-\nschema.xsd \u0026lt;xs:simpleType name = \u0026#34;roundingDirection\u0026#34;\u0026gt; \u0026lt;xs:restriction base = \u0026#34;xs:string\u0026#34;\u0026gt; \u0026lt;xs:enumeration value = \u0026#34;up\u0026#34;/\u0026gt; \u0026lt;xs:enumeration value = \u0026#34;half up\u0026#34;/\u0026gt; \u0026lt;xs:enumeration value = \u0026#34;down\u0026#34;/\u0026gt; \u0026lt;xs:enumeration value = \u0026#34;half down\u0026#34;/\u0026gt; \u0026lt;xs:enumeration value = \u0026#34;nearest\u0026#34;/\u0026gt; \u0026lt;/xs:restriction\u0026gt; \u0026lt;/xs:simpleType\u0026gt; JAXB generate following enum class from schema.xsd file:-\nRoundingDirection.java public enum RoundingDirection { @XmlEnumValue(\u0026#34;up\u0026#34;) UP(\u0026#34;up\u0026#34;), @XmlEnumValue(\u0026#34;half up\u0026#34;) HALF_UP(\u0026#34;half up\u0026#34;), @XmlEnumValue(\u0026#34;down\u0026#34;) DOWN(\u0026#34;down\u0026#34;), @XmlEnumValue(\u0026#34;half down\u0026#34;) HALF_DOWN(\u0026#34;half down\u0026#34;), @XmlEnumValue(\u0026#34;nearest\u0026#34;) NEAREST(\u0026#34;nearest\u0026#34;); } Using the above generated enum class, Jackson by default convert following XML:-\nAccountSummary.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;accountSummary\u0026gt; \u0026lt;interest rounding = \u0026#34;half up\u0026#34;\u0026gt;27.55\u0026lt;/interest\u0026gt; \u0026lt;/accountSummary\u0026gt; to following json:-\nAccountSummary.json { \u0026#34;interest\u0026#34; : { \u0026#34;value\u0026#34; : 27.55, \u0026#34;rounding\u0026#34; : \u0026#34;HALF_UP\u0026#34; } } Jackson use the name() method of enum classes by default during conversion. If we want enum value to be used in the conversion, we need custom deserializer.\nWe need to do two things to solve this for all generated enum classes:-\nFirst we will use library org.jvnet.jaxb2_commons:jaxb2-basics and pass argument arg(line: '-XenumValue') in jaxb gradle task. All generated enum classes implements EnumValue class. Second we write custom deserializer for EnumValue to use enumValue() instead of default name() while deserializing to json. Register this serializer in ObjectMapper. public class EnumValueDeserializer extends JsonSerializer\u0026lt;EnumValue\u0026gt; { @Override public void serialize(EnumValue value, JsonGenerator gen, SerializerProvider serializers) throws IOException { gen.writeString(value.enumValue().toString()); } } ObjectMapper objectMapper = new ObjectMapper(); SimpleModule module = new SimpleModule(); module.addSerializer(EnumValue.class, new EnumValueDeserializer()); objectMapper.registerModule(module); objectMapper.registerModule(new JavaTimeModule()); objectMapper.disable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS); Thats it! This will result our Json will have \u0026quot;rounding\u0026quot; : \u0026quot;half up\u0026quot; instead of \u0026quot;rounding\u0026quot; : \u0026quot;HALF_UP\u0026quot;\nConclusion Download the complete source code for the examples in this post from github/springboot-xml\n","permalink":"https://codingnconcepts.com/java/convert-xsd-to-json-using-jaxb/","tags":["XML","Advance Java"],"title":"Convert XSD Schema to JSON using JAXB"},{"categories":["Puzzles"],"contents":"This is my favorite weight puzzle which have been asked from me in many interviews over the past few years.\nPuzzle You have 9 balls identical in size and appearance. One of them is defective and weighs heavy than the others. You have a weighing scale with no measurements so you can just compare weight of balls against each other. How would you find the defective ball given only 2 chances to use the scale?\nSolution First of all we will give a number to each ball i.e. 1, 2, 3, 4, 5, 6, 7, 8, and 9\nThe trick to solve these kind of weight problem is to divide them in groups. We will divide these 8 balls in 3 groups:\nGroup Ball Numbers group 1 1, 2, 3 group 2 4, 5, 6 group 3 7, 8, 9 Now we keep the group 3 aside and put group 1 balls on one side of scale and group 2 balls on another side of scale [1st chance] with three possible outcomes:-\n1. Scale is balanced That means each ball in group 1 and group 2 are identical in weight and defective one is from group 3 i.e. either 7 or 8 or 9.\nLet\u0026rsquo;s keep number 9 aside and put balls 7 and 8 on each side of scale [2nd chance] with three possible outcomes:-\nIf 7 and 8 balances, then 9 is defective one If 7 is heavy then it is defective one If 8 is heavy then it is defective one 2. group 1 is heavier then group 2 That means defective balls is from group 1 i.e. either 1 or 2 or 3.\nLet\u0026rsquo;s keep number 3 aside and put balls 1 and 2 on each side of scale [2nd chance] with three possible outcomes:-\nIf 1 and 2 balances, then 3 is defective one If 1 is heavy then it is defective one If 2 is heavy then it is defective one 3. group 2 is heavier then group 1 That means defective balls is from group 2 i.e. either 4 or 5 or 6.\nLet\u0026rsquo;s keep number 6 aside and put balls 4 and 5 on each side of scale [2nd chance] with three possible outcomes:-\nIf 4 and 5 balances, then 6 is defective one If 4 is heavy then it is defective one If 5 is heavy then it is defective one Conclusion That\u0026rsquo;s it mates. We have found the defective balls out of 9 balls in only 2 weighing.\nIf by now you understood the trick of dividing balls in group and keeping some balls aside then you can solve weight puzzle with any number of balls. Here is the cheat sheet:-\nCheat Sheet N Balls Weight Puzzle Groups Min Weighing (Best Case) Min Weighing (Worst Case) N = 2 [1] [2] 1 1 N = 3 [1] [2] [3] 1 1 N = 4 [1] [2] [3,4] 1 2 N = 5 [1,2] [3,4] [5] 1 2 N = 6 [1,2] [3,4] [5,6] 2 2 N = 6 [1,2,3] [4,5,6] [7] 1 2 N = 8 [1,2,3] [4,5,6] [7,8] 2 2 N = 9 [1,2,3] [4,5,6] [7,8,9] 2 2 N = 10 [1,2,3,4] [5,6,7,8] [9,10] 2 3 N = 11 [1,2,3,4] [5,6,7,8] [9,10,11] 2 3 N = 12 [1,2,3,4] [5,6,7,8] [9,10,11,12] 3 3 If you are interested in how to solve 8 or 12 balls weight puzzle then check out these posts:-\nHow to solve 8 balls weight puzzle How to solve 12 balls weight puzzle ","permalink":"https://codingnconcepts.com/puzzle/9-balls-weight-puzzle/","tags":["Interview Puzzle","Weight Puzzle"],"title":"9 Balls Weight Puzzle (Solved)"},{"categories":["Agile"],"contents":"Some of the cool ideas for informal business meetings, trainings, or knowledge sharing sessions, which are effective:-\nBrown Bag Meeting A \u0026ldquo;Brown Bag Meeting\u0026rdquo; or \u0026ldquo;Brown Bag Session\u0026rdquo; refers to an informal meeting, session, training, or presentation usually held in a workplace where attendees bring their own food.\nAs the name \u0026ldquo;Brown Bag\u0026rdquo; suggest, it refers to the plan brown paper sack in which many people bring their breakfast, lunch, or evening snack to work. For the same reason it can happen at breakfast, lunch, or snack hour, whichever works best for everyone. Lunch hours are more popular as sessions can last an hour or long.\nGenerally these sessions are optional for participants to attend.\nBrown Bag Meeting\nLunch \u0026amp; Learn A \u0026ldquo;Lunch \u0026amp; Learn\u0026rdquo; sessions are also quite popular where a learning session usually happen over lunch. Each attendee bring their own lunch, or in some cases provided by the meeting host.\nIt works well from office, hybrid, or work from home. Its a good way to engage staff for some lunch time talk.\nFishbowl Discussion A Fishbowl method is suitable for discussion among large group of 25 to 150 participants. This allows entire group to participate in the discussion.\nThe seating arrangement of a fishbowl is circular consist of an inner circle of speakers and an outer circle of listeners. Upto 5 participants can sit in the inner circle and join the discussion. Rest participants sit in the outer circle as audience.\nParticipants from outer circle are free to come forward and become part of the inner circle at any time during the discussion if they wish to contribute. Participant can simply grab an empty chair or switch with anyone in the inner circle by raising hand.\nOne or more chairs in the discussion group can be variable. For example, one variable chair means participant can only be switched on one chair, other four inner circle participants are fixed.\nModerator should decide the maximum number of topic for discussion and maximum time per topic discussion round. Moderator can also be a fixed participant of inner circle.\nFishbowl Wiki\nFishbowl Discussion\nKnowledge Bytes A \u0026ldquo;Knowledge Bytes\u0026rdquo; or \u0026ldquo;Tech Bytes\u0026rdquo; refers to a byte-sized knowledge sharing session of shorter duration of 15 to 30 min. A session usually held multiple byte-sized talks from multiple speakers.\nThe other cool names for Knowledge Sharing sessions could be:-\nKnowledge Cafe Knowledge Forum Gather Town Gather Town is a multiverse platform for online meetings, sessions, or presentation\nGather Town\nUnconference An Unconference is a participant-driven meeting. The term \u0026ldquo;unconference\u0026rdquo; has been applied, or self-applied, to a wide range of gatherings that try to avoid hierarchical aspects of a conventional conference, such as sponsored presentations and top-down organization.\nCoding Night A \u0026ldquo;Coding Night\u0026rdquo; refers to a coding session after working hours at workplace or online. Its an event where team comes together to solve a technical problem through individual contribution or pair programming, along with some beer and dinner :)\nCoding Night\n","permalink":"https://codingnconcepts.com/agile/cool-ideas-for-informal-meeting-session-training/","tags":null,"title":"Cool ideas for informal business meetings, sessions, and trainings"},{"categories":["Kotlin"],"contents":"In this post, we\u0026rsquo;ll see quick examples of creating ranges using various range expressions and iterating over ranges in Kotlin.\nCreating Ranges We can create ranges in Kotlin using .. operator, rangeTo() and downTo() functions. Few points to note:-\n.. operator is simplest way to create ranges. Ranges are inclusive by default. For example, range 1..9 includes 1 and 9 Elements in range are incremented/decremented by one step by default. For example, numbers in range 1..9 are incremented by 1 by default and numbers in range 1..9 step 3 are incremented by 3. Use until to exclude last element in range. For example, range 1 until 10 exclude 10 Create reverse range using downTo operator, downTo() function or reversed() function. For example, range 10 downTo 1 is 10 to 1 Let\u0026rsquo;s look at various examples of creating ranges for numbers:-\n1..9 // 123456789 1.rangeTo(5) // 12345 5.downTo(1) // 54321 5 downTo 1 // 54321 (1..5).reversed() // 54321 1..9 step 2 // 13579 1..9 step 3 // 147 1 until 10 // 123456789 Similarly, we can create ranges for characters:-\n\u0026#39;a\u0026#39;..\u0026#39;z\u0026#39; // abcdefghijklmnopqrstuvwxyz \u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39; // ABCDEFGHIJKLMNOPQRSTUVWXYZ \u0026#39;a\u0026#39;.rangeTo(\u0026#39;e\u0026#39;) // abcde \u0026#39;E\u0026#39;.downTo(\u0026#39;A\u0026#39;) // EDCBA \u0026#39;e\u0026#39; downTo \u0026#39;a\u0026#39; // edcba (\u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39;).reversed() // ZYXWVUTSRQPONMLKJIHGFEDCBA \u0026#39;a\u0026#39;..\u0026#39;i\u0026#39; step 2 // acegi \u0026#39;A\u0026#39;..\u0026#39;I\u0026#39; step 3 // ADG \u0026#39;a\u0026#39; until \u0026#39;e\u0026#39; // abcd Iterating Ranges Now we know how to create ranges, let\u0026rsquo;s look at various examples to iterate over the ranges using for, forEach and iterator\nUsing for loop for loop is well known to developers to iterate over the ranges:-\nfor (i in 1..9) print(i) //Prints 123456789 for (i in 5 downTo 1) print(i) //Prints 54321 for (ch in \u0026#39;a\u0026#39;..\u0026#39;e\u0026#39;) print(ch) //Prints abcde for (ch in \u0026#39;E\u0026#39; downTo \u0026#39;A\u0026#39;) print(ch) //Prints EDCBA We can also use variables while creating and iterating ranges:-\nval start = 1 val end = 9 val rangeOneToNine = start..end for (i in rangeOneToNine) print(i) //Prints 123456789 Using foreach() function foreach function can be used directly on ranges or chained with other functions to iterate the result:-\n(1..5).forEach(::print) //Prints 12345 (1..5).reversed().forEach { i -\u0026gt; print(\u0026#34;$i \u0026#34;) } //Prints 5 4 3 2 1 \u0026#39;a\u0026#39;.rangeTo(\u0026#39;z\u0026#39;).step(5).forEach { print(it) } //Prints AFKPUZ Using iterator() function Range also provide iterator function to iterate over the elements of a range:-\nval chars = (\u0026#39;a\u0026#39;..\u0026#39;e\u0026#39;) val it = chars.iterator() while (it.hasNext()) { print(it.next()) } //Prints abcde Filtering Ranges We can apply filter() function on range to filter the elements by given predicate:-\nvar rangeOneToTen = 1..10; rangeOneToTen.filter { it % 2 == 0 }.forEach(::print) //Prints 246810 rangeOneToTen.filter { it % 2 != 0 }.forEach(::print) //Prints 13579 We can also apply map and reduce functions on ranges:-\nvar rangeOneToFive = 1..5; rangeOneToFive.map { it * it }.forEach{ print(\u0026#34;$it, \u0026#34;)} //Prints 1, 4, 9, 16, 25, println(rangeOneToFive.reduce{a, b -\u0026gt; a + b}) //Prints 15 Ranges Math Functions There are many math functions are available to use with ranges like min, max, count, sum, average, and random\nvar r = 1..10; print(r.minOrNull()) //Prints 1 print(r.maxOrNull()) //Prints 10 print(r.count()) //Prints 10 print(r.sum()) //Prints 55 print(r.average()) //Prints 5.5 print(r.random()) //Prints random number from 1 to 10 The first, last, and step We can get first, last, and step element from the ranges\nval r = 18..31 step 3 r.forEach(::println) //Prints 18 21 24 27 30 println(r.first) //Prints 18 println(r.last) //Prints 30 println(r.step) //Prints 3 ","permalink":"https://codingnconcepts.com/kotlin/ranges-in-kotlin/","tags":["Kotlin"],"title":"Ranges in Kotlin"},{"categories":["Kotlin"],"contents":"In this post, we\u0026rsquo;ll see quick examples of splitting a string by delimiter to list in Kotlin.\nsplit() to list Split a string by colon (:) delimiter and print the list\nval str = \u0026#34;A:B:C\u0026#34; val delim = \u0026#34;:\u0026#34; val list = str.split(delim) println(list) // [A, B, C] split() and trim() Split urls separated by comma (,) delimiter, trim the spaces, and print the list\nval brokerUrls = \u0026#34;http://broker1.com , http://broker2.com , http://broker3.com\u0026#34; val delim = \u0026#34;,\u0026#34; val list = brokerUrls.split(delim).map { it.trim() } println(list) // [http://broker1.com, http://broker2.com, http://broker3.com] split() by multiple delimiters Split pairs by semicolon (;) and equal (=) delimiters, and print the extracted list of keys and values\nval pairs = \u0026#34;key1=value1;key2=value2;key3=value3\u0026#34; val list = pairs.split(\u0026#34;;\u0026#34;, \u0026#34;=\u0026#34;).map { it.trim() } println(list) // [key1, value1, key2, value2, key3, value3] split() to array Split a pair by equal (=) delimiter and print the array elements by index\nval pair = \u0026#34;key=value\u0026#34; val delim = \u0026#34;=\u0026#34; val array = pair.split(delim).toTypedArray() println(array[0]) // key println(array[1]) // value removeSurrounding(), split(), and trim() Given the json as a string, first remove the prefix and suffix from the string using removeSurrounding(), then split the string by multiple delimiters, and finally trim the spaces before returning the list\nval str = \u0026#34;{ key1: value1, key2: value2, key3: value3 }\u0026#34; val list = str.removeSurrounding(\u0026#34;{\u0026#34;, \u0026#34;}\u0026#34;).split(\u0026#34;:\u0026#34;, \u0026#34;,\u0026#34;).map { it.trim() } println(list) // [key1, value1, key2, value2, key3, value3] split string to equal parts by n letters We use chunked() function to split the string into equal parts, each part contains 3 letters of currency code\nval str = \u0026#34;USDAUDINRSGDEUR\u0026#34; val list = str.chunked(3) println(list) // [USD, AUD, INR, SGD, EUR] split string by new line We use lines() function to split the string into a list of lines delimited by any of the following character sequences: CRLF, LF or CR.\nval str = \u0026#34;codingnconcepts.com\\nKotlin Tutorial\\rSplit String\u0026#34; val list = str.lines() println(list) // [codingnconcepts.com, Kotlin Tutorial, Split String] ","permalink":"https://codingnconcepts.com/kotlin/split-string-in-kotlin/","tags":["Kotlin"],"title":"Split String by delimiter in Kotlin"},{"categories":["Interview Questions"],"contents":"List of frequently asked hibernate interview questions. Keep following this post for regular updates.\nWhat are the key classes and interfaces in Hibernate? Configuration\nConfiguration configuration = new Configuration(); // Hibernate settings equivalent to hibernate.cfg.xml\u0026#39;s properties Properties settings = new Properties(); settings.put(Environment.DRIVER, \u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;); settings.put(Environment.URL, \u0026#34;jdbc:mysql://localhost:3306/hibernate_db?useSSL=false\u0026#34;); settings.put(Environment.USER, \u0026#34;root\u0026#34;); settings.put(Environment.PASS, \u0026#34;root\u0026#34;); settings.put(Environment.DIALECT, \u0026#34;org.hibernate.dialect.MySQL5Dialect\u0026#34;); settings.put(Environment.SHOW_SQL, \u0026#34;true\u0026#34;); settings.put(Environment.HBM2DDL_AUTO, \u0026#34;create-drop\u0026#34;); configuration.setProperties(settings); ServiceRegistry\nServiceRegistry serviceRegistry = new StandardServiceRegistryBuilder() .applySettings(configuration.getProperties()).build(); SessionFactory provides an instance of Session. It is a factory class that gives the Session objects based on the configuration parameters in order to establish the connection to the database. As a good practice, the application generally has a single instance of SessionFactory. The internal state of a SessionFactory which includes metadata about ORM is immutable, i.e once the instance is created, it cannot be changed.\nSessionFactory sessionFactory = configuration.buildSessionFactory(serviceRegistry); Session is an object that maintains the connection between Java object application and database. Session also has methods for storing, retrieving, modifying or deleting data from database using methods like persist(), load(), get(), update(), delete(), etc. Additionally, It has factory methods to return Query, Criteria, and Transaction objects.\nSession session = sessionFactory.openSession(); Query\nString hql = \u0026#34;FROM Employee\u0026#34;; Query query = session.createQuery(hql); List results = query.list(); Criteria\nCriteria criteria = session.createCriteria(Employee.class); // To get records having salary more than 2000 criteria.add(Restrictions.gt(\u0026#34;salary\u0026#34;, 2000)); List results = criteria.list(); Transaction\nSession session = sessionFactory.openSession(); Transaction transaction; try { transaction = session.beginTransaction(); //do some work ... transaction.commit(); } catch (Exception e) { if (transaction!=null) transaction.rollback(); throw e; } finally { session.close(); } Difference between first and second level cache? Hibernate has 2 cache types. First level and second level cache for which the difference is given below:\nFirst Level Cache Second Level Cache This is local to the Session object and cannot be shared between multiple sessions. This cache is maintained at the SessionFactory level and shared among all sessions in Hibernate. This cache is enabled by default and there is no way to disable it. This is disabled by default, but we can enable it through configuration. The first level cache is available only until the session is open, once the session is closed, the first level cache is destroyed. The second-level cache is available through the application’s life cycle, it is only destroyed and recreated when an application is restarted. If an entity or object is loaded by calling the get() method then Hibernate first checked the first level cache, if it doesn’t find the object then it goes to the second level cache if configured. If the object is not found then it finally goes to the database and returns the object, if there is no corresponding row in the table then it returns null.\nDifference between save() and saveOrUpdate() methods in hibernate session? Both the methods save records to the table in the database in case there are no records with the primary key in the table. However, the main differences between these two are listed below:\nsave() saveOrUpdate() Session.save() generates a new identifier and INSERT record into a database Session.saveOrUpdate() can either INSERT or UPDATE based upon existence of a record. The insertion fails if the primary key already exists in the table. In case the primary key already exists, then the record is updated. The return type is Serializable which is the newly generated identifier id value as a Serializable object. The return type of the saveOrUpdate() method is void. This method is used to bring only a transient object to a persistent state. This method can bring both transient (new) and detached (existing) objects into a persistent state. It is often used to re-attach a detached object into a Session Clearly, saveOrUpdate() is more flexible in terms of use but it involves extra processing to find out whether a record already exists in the table or not.\nDifference between get() and load() in hibernate session? These are the methods to get data from the database. The primary differences between get and load in Hibernate are given below:\nget() load() This method gets the data from the database as soon as it is called. This method returns a proxy object and loads the data only when it is required. The database is hit every time the method is called. The database is hit only when it is really needed and this is called Lazy Loading which makes the method better. The method returns null if the object is not found. The method throws ObjectNotFoundException if the object is not found. This method should be used if we are unsure about the existence of data in the database. This method is to be used when we know for sure that the data is present in the database. What is N+1 problem in Hibernate and how to solve? The N+1 query problem is said to occur when an ORM, like hibernate, executes 1 query to retrieve the parent entity and N queries to retrieve the child entities. It happens due the lazy loading and on-demand fetching strategy of Hibernate.\nLet\u0026rsquo;s take an example where one user can have many addresses.\n@Entity @Table(name = \u0026#34;user\u0026#34;) public class User { @Id private String id; @OneToMany(fetch = FetchType.LAZY, cascade = CascadeType.ALL, mappedBy = \u0026#34;user\u0026#34;) private List\u0026lt;Address\u0026gt; addresses; } @Entity @Table(name = \u0026#34;address\u0026#34;) public class Address { @Id private String id; @ManyToOne(cascade = CascadeType.ALL, fetch = FetchType.LAZY) @JoinColumn(name = \u0026#34;user_id\u0026#34;, referencedColumnName = \u0026#34;id\u0026#34;) private User User; } String hql = \u0026#34;FROM User\u0026#34;; Query query = session.createQuery(hql); List\u0026lt;User\u0026gt; users = query.list(); To fetch a list of users, it will fire 1 query to load all users from user table and then subsequently fire N queries to load list of addresses for each user from address table, ends up executing N+1 queries\nSolve N+1 Some of the strategies followed for solving the N+1 SELECT problem are:-\n@EntityGraph provides a way to formulate better performing queries by defining which entities need to be retrieved from the database using SQL JOINS. Another way to use @BatchSize on lazy association to fetch records in batches, which helps us to reduce the problem of N+1 to (N/K) + 1 where K refers to the size of the batch. As last resort, try to avoid or disable lazy loading altogether. ","permalink":"https://codingnconcepts.com/hibernate-interview-questions/","tags":["Interview Q\u0026A","Database Q\u0026A"],"title":"Hibernate Interview Questions"},{"categories":["Java"],"contents":"Fetch Data from multiple sources asynchronously using Java executor fixed thread pool and combine them in Java\nUse case Let\u0026rsquo;s consider a use case where user profile information comes from multiple sources:-\nUser Bio information from http API endpoint User profile pictures from S3 bucket User documents from FTP server We want to use Java multi-threading to fetch the profile of 5 users from these 3 different sources asynchronously and return the combined result.\nProgram We will do following things to achieve this:-\nWe create 3 functions to fetch bio, pictures and documents asynchronously using CompletableFuture.supplyAsync, which return a CompletableFuture means some result in future. These 3 functions are:-\nfetchBioOverHttpAsync\nfetchPicturesFromS3BucketAsync\nfetchDocumentsFromFtpServerAsync Next, we create a function to fetch user profile fetchUserProfileAsync asynchronously, which combines the result using CompletableFuture::join from above 3 functions as they arrive and return the CompletableFuture Last, we create a function to fetch multiple user profiles fetchUserProfiles, which combines the result using CompletableFuture::join for multiple user profile as they arrive and return the list of user profiles. We also create a fixed thread pool of size 10 using Java executor service Executors.newFixedThreadPool(10) and use this to spawn the threads for all our asynchronous operations. We use the suffix Async in function names which returns CompletableFuture which is a good practice to identify asynchronous functions.\npackage com.example.thread; import java.time.Duration; import java.time.Instant; import java.util.Arrays; import java.util.Collections; import java.util.List; import java.util.Random; import java.util.concurrent.CompletableFuture; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.stream.Collectors; import java.util.stream.Stream; public class FetchProfileDataFromNSourcesAsync { ExecutorService executor = Executors.newFixedThreadPool(6); public static void main(String[] args) { Instant start = Instant.now(); FetchProfileDataFromNSourcesAsync async = new FetchProfileDataFromNSourcesAsync(); List\u0026lt;Profile\u0026gt; profiles = async.fetchUserProfiles(Arrays.asList(\u0026#34;andrew\u0026#34;, \u0026#34;billy\u0026#34;, \u0026#34;charlie\u0026#34;, \u0026#34;david\u0026#34;, \u0026#34;emma\u0026#34;)); Instant finish = Instant.now(); long timeElapsed = Duration.between(start, finish).toMillis(); System.out.println(\u0026#34;Time elapsed \u0026#34; + timeElapsed); System.out.println(\u0026#34;Profiles \u0026#34; + profiles); System.exit(0); } List\u0026lt;Profile\u0026gt; fetchUserProfiles(List\u0026lt;String\u0026gt; profileIds){ List\u0026lt;CompletableFuture\u0026lt;Profile\u0026gt;\u0026gt; future = profileIds.stream() .map((s) -\u0026gt; fetchUserProfileAsync(s)) .collect(Collectors.toList()); return future.stream().map(CompletableFuture::join) .collect(Collectors.toList()); } CompletableFuture\u0026lt;Profile\u0026gt; fetchUserProfileAsync(String profileId) { return CompletableFuture.supplyAsync(() -\u0026gt; { System.out.println(\u0026#34;fetchUserProfile \u0026#34; + Thread.currentThread().getName()); List\u0026lt;Object\u0026gt; result = Stream.of(fetchBioOverHttpAsync(profileId), fetchPicturesFromS3BucketAsync(profileId), fetchDocumentsFromFtpServerAsync(profileId)) .map(CompletableFuture::join) .collect(Collectors.toList()); return new Profile((Bio)result.get(0), (List\u0026lt;String\u0026gt;) result.get(1), (List\u0026lt;String\u0026gt;) result.get(2)); }, executor); } CompletableFuture\u0026lt;Bio\u0026gt; fetchBioOverHttpAsync(String profileId) { return CompletableFuture.supplyAsync(() -\u0026gt; { System.out.println(\u0026#34;fetchBioOverHttpAsync \u0026#34; + Thread.currentThread().getName()); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } // Thread.sleep to simulate 1 sec to fetch bio // Here write code to fetch bio from API Random random = new Random(); List\u0026lt;String\u0026gt; gender = Arrays.asList(\u0026#34;male\u0026#34;, \u0026#34;female\u0026#34;, \u0026#34;na\u0026#34;); char[] alphabet = \u0026#34;abcdefghijklmnopqrstuvwxyz\u0026#34;.toCharArray(); Collections.shuffle(gender); return new Bio(profileId, random.nextInt(100), gender.get(0), \u0026#34;location \u0026#34; + alphabet[random.nextInt(25)+1]); }, executor); } CompletableFuture\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; fetchPicturesFromS3BucketAsync(String profileId) { return CompletableFuture.supplyAsync(() -\u0026gt; { System.out.println(\u0026#34;fetchPicturesFromS3BucketAsync \u0026#34; + Thread.currentThread().getName()); try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } // Thread.sleep to simulate 2 sec to fetch pictures // Here write code to fetch pictures from S3 bucket return Arrays.asList(\u0026#34;picture of \u0026#34; + profileId); }, executor); } CompletableFuture\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; fetchDocumentsFromFtpServerAsync(String profileId) { return CompletableFuture.supplyAsync(() -\u0026gt; { System.out.println(\u0026#34;fetchDocumentsFromFtpServerAsync \u0026#34; + Thread.currentThread().getName()); try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } // Thread.sleep to simulate 5 sec to fetch documents // Here write code to fetch documents from FTP server return Arrays.asList(\u0026#34;document of \u0026#34; + profileId); }, executor); } } class Profile { Bio bio; List\u0026lt;String\u0026gt; pictures; List\u0026lt;String\u0026gt; documents; public Profile(Bio bio, List\u0026lt;String\u0026gt; pictures, List\u0026lt;String\u0026gt; documents) { this.bio = bio; this.pictures = pictures; this.documents = documents; } @Override public String toString() { return \u0026#34;Profile{\u0026#34; + \u0026#34;bio=\u0026#34; + bio + \u0026#34;, pictures=\u0026#34; + pictures + \u0026#34;, documents=\u0026#34; + documents + \u0026#39;}\u0026#39;; } } class Bio { String name; Integer age; String gender; String location; public Bio(String name, Integer age, String gender, String location) { this.name = name; this.age = age; this.gender = gender; this.location = location; } @Override public String toString() { return \u0026#34;Bio{\u0026#34; + \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, age=\u0026#34; + age + \u0026#34;, gender=\u0026#39;\u0026#34; + gender + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, location=\u0026#39;\u0026#34; + location + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#39;}\u0026#39;; } } Run /jdk-11.0.10.jdk/Contents/Home/bin/java com.example.thread.FetchProfileDataFromNSourcesAsync fetchUserProfile pool-1-thread-5 fetchUserProfile pool-1-thread-4 fetchUserProfile pool-1-thread-1 fetchUserProfile pool-1-thread-2 fetchUserProfile pool-1-thread-3 fetchBioOverHttpAsync pool-1-thread-6 fetchBioOverHttpAsync pool-1-thread-9 fetchBioOverHttpAsync pool-1-thread-10 fetchBioOverHttpAsync pool-1-thread-8 fetchBioOverHttpAsync pool-1-thread-7 fetchPicturesFromS3BucketAsync pool-1-thread-9 fetchPicturesFromS3BucketAsync pool-1-thread-6 fetchPicturesFromS3BucketAsync pool-1-thread-8 fetchPicturesFromS3BucketAsync pool-1-thread-10 fetchPicturesFromS3BucketAsync pool-1-thread-7 fetchDocumentsFromFtpServerAsync pool-1-thread-9 fetchDocumentsFromFtpServerAsync pool-1-thread-6 fetchDocumentsFromFtpServerAsync pool-1-thread-10 fetchDocumentsFromFtpServerAsync pool-1-thread-8 fetchDocumentsFromFtpServerAsync pool-1-thread-7 Time elapsed 8058 Profiles [Profile{bio=Bio{name=\u0026#39;andrew\u0026#39;, age=50, gender=\u0026#39;male\u0026#39;, location=\u0026#39;location c\u0026#39;}, pictures=[picture of andrew], documents=[document of andrew]}, Profile{bio=Bio{name=\u0026#39;billy\u0026#39;, age=33, gender=\u0026#39;female\u0026#39;, location=\u0026#39;location x\u0026#39;}, pictures=[picture of billy], documents=[document of billy]}, Profile{bio=Bio{name=\u0026#39;charlie\u0026#39;, age=27, gender=\u0026#39;na\u0026#39;, location=\u0026#39;location n\u0026#39;}, pictures=[picture of charlie], documents=[document of charlie]}, Profile{bio=Bio{name=\u0026#39;david\u0026#39;, age=38, gender=\u0026#39;male\u0026#39;, location=\u0026#39;location y\u0026#39;}, pictures=[picture of david], documents=[document of david]}, Profile{bio=Bio{name=\u0026#39;emma\u0026#39;, age=10, gender=\u0026#39;male\u0026#39;, location=\u0026#39;location d\u0026#39;}, pictures=[picture of emma], documents=[document of emma]}] Process finished with exit code 0 We simulated our program to fetch user bio, pictures and documents in 1s, 2s, and 5s respectively, a total of 8s to fetch single user profile. With the help of asynchronous programming, we were able to fetch 5 user profiles in the same time.\n","permalink":"https://codingnconcepts.com/java/fetch-data-from-sources-async/","tags":["Core Java","Java Threads","Async"],"title":"Async Fetch Data from N Sources and Combine In Java"},{"categories":["Kotlin"],"contents":"Fetch Data from multiple sources asynchronously using Kotlin Coroutines and combine them in Kotlin\nUse case Let\u0026rsquo;s consider a use case where user profile information comes from multiple sources:-\nUser Bio information from http API endpoint User profile pictures from S3 bucket User documents from FTP server We want to use Kotlin coroutines to fetch the profile of 10 users from these 3 different sources asynchronously and return the combined result.\nProgram We will do following things to achieve this:-\nWe create 3 functions to fetch bio, pictures and documents asynchronously using GlobalScope.async, which return a Deferred result sometime in future. These 3 functions are:-\nfetchBioOverHttpAsync\nfetchPicturesFromS3BucketAsync\nfetchDocumentsFromFtpServerAsync Next, we create a function to fetch user profile fetchUserProfileAsync asynchronously, which wait for the results using await() from above 3 sources as they arrive, combine them, and return the Deferred result Last, we create a function to fetch multiple user profiles fetchUserProfiles, which map the result using await() for multiple user profile as they arrive and return the list of user profiles. We also create a Coroutine dispatcher fixed thread pool of size 10 using Java executor service Executors.newFixedThreadPool(10).asCoroutineDispatcher() and use this to spawn the threads for all our asynchronous operations. We use the suffix Async in function names which returns Deferred result, which is a good practice to identify asynchronous functions.\npackage com.example.concurrency import kotlinx.coroutines.* import java.util.concurrent.Executors import kotlin.system.measureTimeMillis val executor = Executors.newFixedThreadPool(10).asCoroutineDispatcher() data class Profile(val bio: Bio, val picture: List\u0026lt;String\u0026gt;, val documents: List\u0026lt;String\u0026gt;) data class Bio(val name: String, val age: Int, val gender: String, val location: String) fun main() { runBlocking { val timeElapsed = measureTimeMillis { // fetch profile for 10 users fetchUserProfiles(listOf(\u0026#34;andrew\u0026#34;, \u0026#34;billy\u0026#34;, \u0026#34;charlie\u0026#34;, \u0026#34;david\u0026#34;, \u0026#34;emma\u0026#34;, \u0026#34;flora\u0026#34;, \u0026#34;gavin\u0026#34;, \u0026#34;harry\u0026#34;, \u0026#34;idris\u0026#34;, \u0026#34;jack\u0026#34;)) .forEach(::println) } println(\u0026#34;Time elapsed: $timeElapsed\u0026#34;) } } suspend fun fetchUserProfiles(profileIds: List\u0026lt;String\u0026gt;): List\u0026lt;Profile\u0026gt; = profileIds.map { fetchUserProfileAsync(it) }.map { it.await() } fun fetchUserProfileAsync(profileId: String): Deferred\u0026lt;Profile\u0026gt; = GlobalScope.async(executor) { val bio = fetchBioOverHttpAsync(profileId) val picture = fetchPicturesFromS3BucketAsync(profileId) val documents = fetchDocumentsFromFtpServerAsync(profileId) Profile(bio.await(), picture.await(), documents.await()) } fun fetchBioOverHttpAsync(id: String): Deferred\u0026lt;Bio\u0026gt; = GlobalScope.async(executor) { println(\u0026#34;fetchBioOverHttpAsync ${Thread.currentThread().name}\u0026#34;) delay(1000) // delay to simulate 1 sec to fetch bio // Here write code to fetch bio from API Bio(id, (1..100).random(), listOf(\u0026#34;male\u0026#34;, \u0026#34;female\u0026#34;, \u0026#34;na\u0026#34;).random(), \u0026#34;location ${(\u0026#39;a\u0026#39;..\u0026#39;z\u0026#39;).random()}\u0026#34;) } fun fetchPicturesFromS3BucketAsync(id: String): Deferred\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; = GlobalScope.async(executor) { println(\u0026#34;fetchPictureFromDBAsync ${Thread.currentThread().name}\u0026#34;) delay(2000) // delay to simulate 2 sec to fetch pictures // Here write code to fetch pictures from S3 bucket listOf(\u0026#34;picture of $id\u0026#34;) } fun fetchDocumentsFromFtpServerAsync(id: String): Deferred\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; = GlobalScope.async(executor) { println(\u0026#34;fetchDocumentsFromFtpAsync ${Thread.currentThread().name}\u0026#34;) delay(5000) // delay to simulate 5 sec to fetch documents // Here write code to fetch documents from FTP server listOf(\u0026#34;document for $id\u0026#34;) } Run /jdk-11.0.10.jdk/Contents/Home/bin/java com.example.concurrency.FetchProfileCoroutineAsyncKt fetchBioOverHttpAsync pool-1-thread-6 fetchBioOverHttpAsync pool-1-thread-8 fetchBioOverHttpAsync pool-1-thread-9 fetchBioOverHttpAsync pool-1-thread-10 fetchBioOverHttpAsync pool-1-thread-2 fetchBioOverHttpAsync pool-1-thread-4 fetchPictureFromDBAsync pool-1-thread-5 fetchPictureFromDBAsync pool-1-thread-3 fetchPictureFromDBAsync pool-1-thread-7 fetchPictureFromDBAsync pool-1-thread-3 fetchPictureFromDBAsync pool-1-thread-5 fetchPictureFromDBAsync pool-1-thread-7 fetchDocumentsFromFtpAsync pool-1-thread-3 fetchDocumentsFromFtpAsync pool-1-thread-7 fetchDocumentsFromFtpAsync pool-1-thread-5 fetchDocumentsFromFtpAsync pool-1-thread-7 fetchDocumentsFromFtpAsync pool-1-thread-3 fetchDocumentsFromFtpAsync pool-1-thread-5 fetchBioOverHttpAsync pool-1-thread-7 fetchBioOverHttpAsync pool-1-thread-3 fetchBioOverHttpAsync pool-1-thread-5 fetchBioOverHttpAsync pool-1-thread-8 fetchDocumentsFromFtpAsync pool-1-thread-5 fetchPictureFromDBAsync pool-1-thread-9 fetchPictureFromDBAsync pool-1-thread-2 fetchDocumentsFromFtpAsync pool-1-thread-10 fetchDocumentsFromFtpAsync pool-1-thread-3 fetchDocumentsFromFtpAsync pool-1-thread-4 fetchPictureFromDBAsync pool-1-thread-7 fetchPictureFromDBAsync pool-1-thread-1 Profile(bio=Bio(name=andrew, age=46, gender=male, location=location o), picture=[picture of andrew], documents=[document for andrew]) Profile(bio=Bio(name=billy, age=78, gender=na, location=location b), picture=[picture of billy], documents=[document for billy]) Profile(bio=Bio(name=charlie, age=69, gender=male, location=location n), picture=[picture of charlie], documents=[document for charlie]) Profile(bio=Bio(name=david, age=18, gender=female, location=location u), picture=[picture of david], documents=[document for david]) Profile(bio=Bio(name=emma, age=66, gender=female, location=location p), picture=[picture of emma], documents=[document for emma]) Profile(bio=Bio(name=flora, age=65, gender=female, location=location v), picture=[picture of flora], documents=[document for flora]) Profile(bio=Bio(name=gavin, age=76, gender=na, location=location v), picture=[picture of gavin], documents=[document for gavin]) Profile(bio=Bio(name=harry, age=32, gender=female, location=location i), picture=[picture of harry], documents=[document for harry]) Profile(bio=Bio(name=idris, age=18, gender=female, location=location b), picture=[picture of idris], documents=[document for idris]) Profile(bio=Bio(name=jack, age=42, gender=na, location=location p), picture=[picture of jack], documents=[document for jack]) Time elapsed 5038 We simulated our program to fetch user bio, pictures and documents in 1s, 2s, and 5s respectively, a total of 8s to fetch single user profile. With the help of asynchronous programming using Coroutines, we were able to fetch 10 user profiles in around 5s.\nYou see the total time taken (~5s) is almost equal to max delayed function i.e. fetch documents with delay (5s). Isn\u0026rsquo;t it amazing?\nIn Kotlin, each async operation GlobalScope.async {} is a coroutine, which returns a Deferred result. Unlike threads, coroutines are not bound to any particular thread. A coroutine can start executing in one thread, suspend execution, and resume on a different thread.\nThread vs Coroutines Kotlin coroutines are lightweight version of Java threads. Coroutines also use the Java thread pool behind the scene though there is a difference how coroutines perform tasks.\nThread When a thread is performing a task which block for some time (IO operation or network call), thread needs to wait. Other threads in the thread pool can take turn to perform some other tasks while the previous thread is waiting. Operating system determines the scheduling of threads and context-switching, which is an extra overhead.\nCoroutines When a coroutine is performing a task under a thread which block for some time (IO operation or network call), coroutine is suspended, same thread can perform some other coroutines while the previous coroutine is waiting. Programming language (Kotlin) determines when to switch coroutines, which is lightweight.\nIn our program, while coroutine fetchDocumentsFromFtpAsync wait for 5s, it get suspended, and the current thread can perform some other coroutines in that period say fetchBioOverHttpAsync and fetchPictureFromDBAsync in 1s and 2s (total 3s), then it resumes back fetchDocumentsFromFtpAsync. In this way, coroutines able to fetch bio, pictures, and document all in 5s using a single thread. This is just an example, in practical, these coroutines might have performed by different threads, but you got the gist. Coroutine provide a very high level of concurrency because of its non blocking nature and less overhead of switching threads.\nIf you use threads without coroutines, like Java, it would take 8s to fetch bio, pictures and document by a single thread.\n","permalink":"https://codingnconcepts.com/kotlin/fetch-data-from-sources-async/","tags":["Coroutines"],"title":"Async Fetch Data from N Sources and Combine In Kotlin"},{"categories":["Kotlin"],"contents":"In this post, we\u0026rsquo;ll learn how to write clean code in Kotlin with some practical examples and make our code more concise and readable\nUse when Use when expression, when you have more than two conditions. You can replace any code snippet having if-else condition with more concise when expression. It is similar to switch operator in Java.\n// Don\u0026#39;t fun getCategoryByEatableName(eatable: String): String { if (eatable == \u0026#34;mango\u0026#34; || eatable == \u0026#34;banana\u0026#34; || eatable == \u0026#34;apple\u0026#34;) { return \u0026#34;FRUIT\u0026#34; }else if (eatable == \u0026#34;potato\u0026#34; || eatable == \u0026#34;tomato\u0026#34; || eatable == \u0026#34;onion\u0026#34;) { return \u0026#34;VEGETABLE\u0026#34; }else if(eatable == \u0026#34;milk\u0026#34; || eatable == \u0026#34;curd\u0026#34;) { return \u0026#34;DAIRY\u0026#34; }else { return \u0026#34;UNKNOWN\u0026#34; } } // Do fun getCategoryByEatableName(eatable: String) = when(eatable) { \u0026#34;mango\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;apple\u0026#34; -\u0026gt; \u0026#34;FRUIT\u0026#34; \u0026#34;potato\u0026#34;, \u0026#34;tomato\u0026#34;, \u0026#34;onion\u0026#34; -\u0026gt; \u0026#34;VEGETABLE\u0026#34; \u0026#34;milk\u0026#34;, \u0026#34;curd\u0026#34; -\u0026gt; \u0026#34;DAIRY\u0026#34; else -\u0026gt; \u0026#34;UNKNOWN\u0026#34; } Use also The also function doesn\u0026rsquo;t mutate the returned output from the previous function and it comes handy in printing the output for debugging.\nExample 1 We want to multiply and print the returned output, which make our method verbose like this:-\nfun multiplyAndPrint(a: Int, b: Int): Int { val c = a * b println(c) return c } var out = multiplyAndPrint(2, 3) // out = 6 Let\u0026rsquo;s improve above function where multiply only do the multiplication and also take care of printing the returned output\nfun multiply(a: Int, b: Int) = a*b var out = multiply(2, 3).also { println(it)} // out 6 Even if you call another function add from also expression, it doesn\u0026rsquo;t mutate the returned output of multiply\nfun multiply(a: Int, b: Int) = a*b fun add(a: Int, b: Int) = a+b var out = multiply(2, 3).also { add(it, 4)} // out is still 6, not 10 Example 2 The also function is quite useful in the streams as well, where it doesn\u0026rsquo;t mutate the collection. It comes quite handy where you want to debug the output in the chain of stream operations\nval l = (1..10).toList() // Don\u0026#39;t l.filter{ it % 2 == 0 } .map { println(it) it * it } // Do l.filter{ it % 2 == 0 } // Prints, but doesn\u0026#39;t mutate the collection .also { println(it) } .map { it*it} Use let Use let along with ?. null-safe (or elvis) operator, when you want to do a null check\n// Don\u0026#39;t val user: User? = findUser() if (user != null){ println(user.name) } // Do findUser()?.let { println(it.name) } //or findUser()?.name?.let(::println) Use apply You can use apply to initialize a mutable object. It returns the same object it operates on and comes handy in initializing the properties.\nclass Server { lateinit var host: String var port: Int = 0 var numOfInstance: Int = 1 var isCloudInstance: Boolean = true } // Don\u0026#39;t var server = Server() server.host = \u0026#34;sg1234\u0026#34; server.port = 8080 server.numOfInstance = 3 // Do var server = Server().apply { host = \u0026#34;sg1234\u0026#34; port = 8080 numOfInstance = 3 } Initialize Map with values Immutable map is initialized using mapOf and mutable map is initialized using mutableMapOf in Kotlin.\nExample 1 Use shorthand to operator to map \u0026ldquo;key\u0026rdquo; to \u0026ldquo;value\u0026rdquo; instead of initializing Pair each time\n// Don\u0026#39;t val immutableMap = mapOf( Pair(\u0026#34;A\u0026#34;, 1), Pair(\u0026#34;B\u0026#34;, 2), Pair(\u0026#34;C\u0026#34;, 3)) val mutableMap = mutableMapOf( Pair(\u0026#34;A\u0026#34;, 1), Pair(\u0026#34;B\u0026#34;, 2), Pair(\u0026#34;C\u0026#34;, 3)) // Do val immutableMap = mapOf( \u0026#34;A\u0026#34; to 1, \u0026#34;B\u0026#34; to 2, \u0026#34;C\u0026#34; to 3 ) val mutableMap = mutableMapOf( \u0026#34;A\u0026#34; to 1, \u0026#34;B\u0026#34; to 2, \u0026#34;C\u0026#34; to 3 ) Example 2 Map is also quite useful in Kotlin logging\nvar url = \u0026#34;http://localhost:8080/users\u0026#34; var method = \u0026#34;GET\u0026#34; var status = 200 logger.debug(mapOf(\u0026#34;url\u0026#34; to url, \u0026#34;method\u0026#34; to method, \u0026#34;status\u0026#34; to status)) Multiline String An easy way to create a multiline String is to wrap it in \u0026quot;\u0026quot;\u0026quot;.\n// Don\u0026#39;t println(\u0026#34;Twinkle, Twinkle Little Bat\\n\u0026#34; + \u0026#34;How I wonder what you\u0026#39;re at!\\n\u0026#34; + \u0026#34;Up above the world you fly,\\n\u0026#34; + \u0026#34;Like a tea tray in the sky.\\n\u0026#34; + \u0026#34;Twinkle, twinkle, little bat!\\n\u0026#34; + \u0026#34;How I wonder what you\u0026#39;re at!\u0026#34;) // Do println(\u0026#34;\u0026#34;\u0026#34; Twinkle, Twinkle Little Bat How I wonder what you\u0026#39;re at! Up above the world you fly, Like a tea tray in the sky. Twinkle, twinkle, little bat! How I wonder what you\u0026#39;re at!\u0026#34;\u0026#34;\u0026#34;) Extension Function Extension functions is very powerful feature in Kotlin to add new functionality in existing class such as String without even inheriting it. We cannot do such things in Java and generally write a StringUtil class to do that.\n// Don\u0026#39;t object StringUtil { fun capitalize(string: String): String{ return string.replaceFirstChar { it.uppercaseChar() } } } println(StringUtil.capitalize(\u0026#34;ashish\u0026#34;)) // Ashish // Do fun String.capitalize(): String { return replaceFirstChar { it.uppercaseChar() } } println(\u0026#34;ashish\u0026#34;.capitalize()) // Ashish ","permalink":"https://codingnconcepts.com/kotlin/clean-code-in-kotlin/","tags":["Kotlin"],"title":"Clean Code in Kotlin"},{"categories":["Agile"],"contents":"If you are planning or preparing for Leading SAFe Agilist 6.0 (Scaled Agile Framework) certification then this article is for you to get started.\nOverview Prepare well for the exam. Understand all SAFe concepts and you can crack it like me! Requires 1 to 3 weeks of preparation depending upon your commitment per day. You need to solve 45 questions (multiple choice = 1 answer and multiple select = 2-3 answers) in 90 mins from your laptop without any supervision. It is an open book online exam where you can search for the answers. Passing score is 36/45 (80%) means you should answer at least 36 (out of 45) questions correctly. No negative scoring so answer all the questions! You get the result (Pass or Fail) once you submit the exam. First attempt included in the course registration fee if taken within 30 days of course completion. Each retake or attempt past the 30-day window is $50 You can download the Leading SAFe 6.0 Workbook after the course registration from https://community.scaledagile.com/ Refer to the official Exam Details for more information. Refer to the official SAFe Website for exam material. Exam Questions Read Leading SAFe Agilist 6.0 (Scaled Agile) Exam Questions for free\nBuy Leading SAFe Agilist 6.0 Questions with Answers and Explaination at a very reasonable price.\nPractice Leading SAFe Agilist 6.0 Practice Exams with Answers and Explaination at Exam Topics Lesson 1 - Digital Age and Business Agility (12-14%) Thriving in the digital age SAFe© as an Operating System for Business Agility Core competencies of Business Agility Lesson 2 - Lean-Agile Leaders (29-33%) Lean-Agile Mindset SAFe Core Values SAFe Lean-Agile Principles Lesson 3 - Team and Technical Agility (6-8%) Cross-functional Agile Teams Built-in Quality Organizing around value with ARTs Lesson 4 - Agile Product Delivery (29-33%) Customer-centric culture Design Thinking ART and Solution Train Backlogs WSJF PI Planning Develop on Cadence; Release on Demand Continuous Delivery Pipelines with DevOps Lesson 5 - Lean Portfolio Management (12-14%) SAFe Portfolio Strategic Themes Portfolio canvas Epic hypothesis statements Traditional and Lean budgeting approaches Portfolio Kanban Lesson 6 - Leading the Change (6-8%) Lead by example Lead the change SAFe Implementation Roadmap Other Links to Refer:-\nSAFe Big Picture Extended SAFe Guidance What’s New in SAFe 6.0 Exam Notes Lesson 1: Digital Age and Business Agility What are the different stages of the technological revolution? Technological Revolution Installation Period Turning Point Deployment Period Industrial Revolution 1771 1793-1801 Age of Steam and Railways 1829 1948-1850 Age of Steal and Heavy Engineering 1875 1890-1895 Age of Oil and Mass Production 1908 1929-1943 Age of Software and Digital 1971 2000-2010 We are here Installation Period – New technology and financial capital combine to create a ‘Cambrian explosion’ of new market entrants, disrupting entire industries from the previous age Turning Point – Existing businesses either master the new technology or decline and become relics of the last age Deployment Period – The Production capital of the new technological giants starts to take over We are in the midst of one of those ages now, the deployment period of the age of software and digital.\nReference: https://scaledagileframework.com/business-agility/\nWhat is the Dual Operating System for Business Agility? The primary goal of SAFe is to achieve Business Agility to succeed in the age of software and digital.\nAchieving business agility using SAFe requires a dual operating system. This means embracing two seemingly contradictory approaches:-\nFunctional Hierarchy: This represents the traditional organizational structure with established roles, departments, and processes. It provides stability, governance, and compliance. Value Stream Network: This refers to a more flexible and customer-centric network of teams and individuals who collaborate to rapidly deliver value. They focus on innovation, experimentation, and adaptation to market changes. SAFe is a second operating system around the value of streams, without disrupting the existing functional hierarchy.\nReference: https://scaledagileframework.com/business-agility/ Reference: https://scaledagileframework.com/advanced-topic-balancing-the-dual-operating-system/\nWhat are the four SAFe configurations that provide the right configuration for each Enterprise? SAFe Configuration Provide Flows Provide Competencies Essential SAFe Team Flow + ART Flow four competencies: Lean-Agile Leadership, Team and Technical Agility, Agile Product Delivery, and Continuous Learning Culture competencies Large Solution SAFe Team Flow + ART Flow + Solution Train Flow Enterprise Solution Delivery and four essential competencies Portfolio SAFe Team Flow + ART Flow + Portfolio Flow Organizational Agility, Lean Portfolio Management and four essential competencies Full SAFe Team Flow + ART Flow + Solution Train Flow + Portfolio Flow All seven core competencies Reference: https://scaledagileframework.com/safe/\nWhat are the 7 SAFe core competencies to achieve business agility? Team and Technical Agility has 3 Dimensions:-\n-Agile Teams\n-Teams of Agile Teams (ART)\n-Built-In Quality Agile Product Delivery has 3 Dimensions:-\n-Customer Centricity and Design Thinking\n-Develop on cadence and release on demand\n-DevOps and the Continuous Delivery Pipeline Enterprise Solution Delivery has 3 Dimensions:-\n-Lean System Engineering -Coordinating Trains and Suppliers\n-Continually Evolve Live Systems Lean Portfolio Management has 3 Dimensions:-\n-Strategy \u0026amp; Investment Funding\n-Agile Portfolio Operations\n-Lean Governance Organizational Agility has 3 Dimensions:-\n-Lean-thinking People and Agile Teams -Lean Business Operations\n-Strategy Agility Continuous Learning Culture has 3 Dimensions:-\n-Learning Organization\n-Innovation Culture\n-Relentless Improvement - Inspect \u0026amp; Adapt (I\u0026amp;A) - Plan Do Check Adjust Lean-Agile Leadership has 3 Dimensions:-\n-Lean-Agile Mindset, Core Values, and SAFe Principles\n-Leading by Example -Leading Change Reference: https://scaledagileframework.com/safe/\nHow to evaluate the progress toward business agility? Measure and Grow is an approach SAFe enterprises use to evaluate progress towards Business Agility and determine improvement actions.\nThe three measurement to evaluate the progress of Business Agility are defined as follows:-\nOutcomes: Do our solutions meet the needs of our customers and the business? Flow: How efficient is the organization at delivering value to the customer? Competency: How proficient is the organization in the practices that enable business agility? Reference: https://scaledagileframework.com/measure-and-grow/\nLesson 2: Lean-Agile Leaders What is the Agile Manifesto? Agile Manifesto uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value:\nIndividuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan\nThat is, while there is a value in the items on the right, we value the items on the left more Reference: https://scaledagileframework.com/lean-agile-mindset/\nReference: https://agilemanifesto.org/\nWhat are the 12 Agile Manifesto Principles? Our highest priority is to satisfy the customer through early and continuous delivery of valuable software. Welcome changing requirements, even late in development. Agile processes harness change for the customer’s competitive advantage. Deliver working software frequently, from a couple of weeks to a couple of months, with a preference for the shorter timescale. Business people and developers must work together daily throughout the project. Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done. The most efficient and effective method of conveying information to and within a development team is face-to-face conversation. Working software is the primary measure of progress. Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely. Continuous attention to technical excellence and good design enhances agility. Simplicity – the art of maximizing the amount of work not done – is essential. The best architectures, requirements, and designs emerge from self-organizing teams. At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly. Reference: https://scaledagileframework.com/lean-agile-mindset/\nReference: https://agilemanifesto.org/principles.html\nWhat is Lean Thinking? Lean Thinking is to deliver the maximum value (a solution) to the customer in the shortest sustainable lead time from the trigger (the identification of the need or opportunity) to the point at which the customer receives the value. The five principles of Lean thinking are:-\nPrecisely specify value by product Identify the Value Stream for each product Make value flow without interruptions Let the Customer pull value from the producer Pursue perfection Reference: https://scaledagileframework.com/lean-agile-mindset/\nWhat are the four SAFe core values? SAFe has the following four core values:-\nAlignment Transparency Respect for people Relentless improvement Reference: https://scaledagileframework.com/safe-core-values/\nWhat are the 10 SAFe Lean-Agile Principles? Take an economic view Deliver Early and Often Apply a Comprehensive Economic Framework:- -Operate Within Lean Budgets and Guardrails\n-Understand Solution Economic Trade-Offs: Development expense, Lead time, Product cost, Value, and Risk\n-Leverage Suppliers\n-Sequencing Jobs for Maximum Benefit: Weighted Shortest Job First (MSJF) Apply systems thinking The Solution Is a System\n-Optimizing a component does not optimize the whole system -For the system to behave well, teams must understand the intended behavior and architecture\n-The value of a system passes through its interconnections -A system can evolve no faster than its slowest integration point The Enterprise Building the System Is a System, Too Understand and Optimize the Full Development Value Stream Only Management Can Change the System Assume variability; preserve options\n-Flexible requirements and design, the Cone of uncertainty, set-based over point-based approach Build incrementally with fast, integrated learning cycles\n-PDCA = Plan – Do – Check – Adjust, The shorted the cycles, the faster the learning\n-Integration points control product development and reduce risk Base milestones on objective evaluation of working systems\n-Phase-gate milestones force design decisions too early, false-positive feasibility, they assume a point Solution exists, huge batches and long queues, centralized requirements and design.\n-Use Objective milestones instead, PI System Demos, continuous, cost-effective adjustments towards an optimum Solution) Make value flow without interruptions\n-Reduce batch size for higher predictability. Total cost = Holding cost + Transaction cost. Reducing transaction costs increases predictability, accelerates feedback, reduces rework, and lowers cost.\n-Little’s Law: Wq = Lq / Lambda, Average wait time = Average queue length / Average processing rate Apply cadence, synchronize with cross-domain planning Cadence – converts unpredictable events into predictable occurrences and lowers cost, makes waiting times for new work predictable, supports regular planning and cross-functional coordination, limits batch sizes to a single interval, controls injection of new work, provides scheduled integration points;\nSynchronization – causes multiple events to happen simultaneously, facilitates cross-functional trade-offs, provides routine dependency management, supports full system integration and assessment, provides multiple feedback perspectives Unlock the intrinsic motivation of knowledge workers -Workers are most qualified to make decisions about how to perform their work\n-The workers must be heard and respected for management to lead effectively\n-Knowledge workers must manage themselves. They need autonomy -Continuing innovation must be part of the work, the tasks, and the responsibilities of knowledge workers. -Unlocking intrinsic motivation with autonomy, mastery, and purpose Decentralize decision-making Centralize – Infrequent, Long-lasting, Significant economies of scale\nDecentralize – Frequent, Time critical, Requires local information Organize around value -Value doesn’t follow silos\n-Organize around Development Value Streams. Reference: https://scaledagileframework.com/safe-lean-agile-principles/\nWhat are the common properties of a flow-based system in SAFe? As per #6 SAFe principle, Flow occurs when there is a smooth, linear, and fast movement of work product from step to step in a relevant value. All flow systems have 8 common properties:-\n1. Work in process: There is always some work in process in the system; if there weren’t, there could be no flow of value.\n2. Bottlenecks: In every flow system, one or more bottlenecks effectively limit the flow through the entire system.\n3. Handoffs: Handoffs wouldn’t be necessary if one person could do all the work. But in any material flow system, different individuals and teams will have different skills and responsibilities. Each plays its part in moving a work item through the system.\n4. Feedback: Customer and stakeholder feedback is integral to efficient and effective outcomes. Ideally, feedback happens throughout the entire process. 5. Batch: As any system has a finite capacity, all the work can’t be done at once. Therefore, work through the system occurs in batches designed to be as efficient as possible.\n6. Queue: It all starts with a set of work items to be done. In addition, each value stream needs a prioritizing mechanism to sequence the work for the best value.\n7. Worker: People do the critical work of moving work items from one state to another.\n8. Policies: Policies are integral to flow. They may be local policies — like team-based policies that determine how a work item moves from step to step— or global policies like those that govern how work is performed within the company.\nReference: https://scaledagileframework.com/make-value-flow-without-interruptions/\nWhat are the eight flow accelerators for making value flow without interruptions? As per #6 SAFe principle, Making value flow without interruptions can best be achieved by adopting the eight ‘flow accelerators’ described:-\n#1 Visualize and Limit WIP: Kanban boards make excessive work-in-process (WIP) visible #2 Address Bottlenecks: Bottlenecks reduce the flow of value through the value stream #3 Minimize Handoffs and Dependencies: Excessive handoffs and dependencies made visible on the ART planning board #4 Get Faster Feedback: Fast feedback is generally achieved by applying the basic Plan-Do-Check-Adjust (PDCA) learning cycle #5 Work in Smaller Batches: smaller batches reduce WIP by limiting the number of requirements, designs, code, tests, and other work items moving through the system at any point. Smaller batches go through the system faster and with less variability, fostering faster learning. Reducing batch size typically involves investment in automating the Continuous Delivery Pipeline #6 Reduce Queue Length: Longer queue creates longer wait times, increased risk, more variability, lower quality, and less motivation. Reducing queue length decreases delays, reduces waste, increases flow, and improves predictability. It’s a requisite for faster service and a more consistent flow of value. #7 Optimize Time ‘In the Zone’: Being ‘in the zone’ (also described as being in a ‘flow state’) is an engaged mental state of extreme focus on an activity where the work feels effortless and time passes quickly. #8 Remediate Legacy Policies and Practices: such as Extraneous meetings, extra reporting, timesheet reporting, waterfall mindset, legacy compensation, obsolete standard, and mindset \u0026ldquo;we’ve always done it this way,” even when they are no longer fit for purpose.\nReference: https://scaledagileframework.com/make-value-flow-without-interruptions/\nWhat is centralized and decentralized decision-making? As per #9 SAFe principle, It is important to balance centralized (take) and decentralized (delegate) decision-making. Simply put, the decision-maker can take a ‘centralized decision’ on behalf of those below them in the chain or ‘decentralize the decision’ by delegating to a level below.\nCentralized decisions:- Infrequent, long-lasting, and provide significant economies of scale\nDecentralized decisions:- Frequent, time-critical, and require local knowledge\nReference: https://scaledagileframework.com/decentralize-decision-making/\nLesson 3: Team and Technical Agility What is an Agile team? An Agile Team is a cross-functional group of typically ten or fewer individuals with all the skills necessary to define, build, test, and deploy increments of value to their customers. Agile teams are optimized for communication and the continuous delivery of value to the customer. Agile Teams visualize flow with SAFe Scrum or SAFe Kanban Agile Team Events are:- Team Sync, Backlog Refinement, Iteration Review, Iteration Retro, Iteration Planning The Agile Team\u0026rsquo;s responsibilities are:-\nConnecting with the Customer (led by PO) Planning the Work: ART Planning (PI Planning), Team planning using SAFe Scrum or SAFe Team Kanban, and refining the Team Backlog. Delivering Value: Frequently integrate and test, Sync with other teams in ART through ART Sync (includes Coach Sync and PO Sync), build continuous delivery pipeline, release frequently. Getting Feedback: with the help of PO and through System Demos Improving relentlessly: participate in ART\u0026rsquo;s joint Inspect \u0026amp; Adapt, address the problems as they occur Reference: https://scaledagileframework.com/agile-teams/\nWhat are the two specialty roles in Agile Teams? The Agile Team contains two specialty roles:- Product Owner (PO) and Scrum Master/Team Coach (SM/TC).\nProduct Owner (PO) responsibilities are:- Connect with the customer Contribute to the Vision and Roadmap Manage and prioritize the Team Backlog Support the team in delivering value Get and apply fast feedback Scrum Master/Team Coach (SM/TC) responsibilities are:- Facilitate SAFe Scrum (or SAFe Kanban) and PI planning Supports Iteration Execution Improves Flow Build a high-performing team Optimizes and improves the team and ART performance Reference: https://scaledagileframework.com/agile-teams/\nWhat are the four Team Topologies to Organize Agile Teams? The four team topologies to organize Agile teams are as follows:-\nStream-aligned team is aligned to a single, valuable stream of work, empowered to build and deliver customer or user value as quickly, safely, and independently as possible without requiring handoffs to other teams to perform parts of the work. Complicated subsystem team – is responsible for building and maintaining a part of the system that depends heavily on specialist knowledge. Most team members must be specialists in that area of expertise to understand and make changes to the subsystem. Platform team – provide the underlying internal services required by stream-aligned teams to deliver higher-level services or functionalities, thus reducing their cognitive load. Enabling team – helps stream-aligned teams acquire missing capabilities, usually around a specific technical or product management area. Reference: https://scaledagileframework.com/organizing-agile-teams-and-arts-team-topologies-at-scale/\nWhat is Agile Release Train (ART)? ART is a team of cross-functional Agile Teams and has the capabilities to define, build, validate, and release to deliver a continuous flow of value. ART is a virtual organization of 5-12 teams (50-125+ individuals) All the teams in ART are synchronized on a common cadence - a Program Increment (PI), aligned to a common mission via a single Program Backlog (ART Backlog). Critical Roles in the ART are:-\nRelease Train Engineer (RTE) is a servant leader (chief scrum master) who facilitates ART execution, impediment removal, risk and dependency management, and continuous improvement. Product Management is largely responsible for ‘what gets built,’ as defined by the Vision, Roadmap, and new Features in the ART Backlog. They work with customers, teams, and Product Owners to understand and communicate their needs and participate in solution validation. System Architect is an individual or team that defines the system’s overall architecture. They work at a level of abstraction above the teams and components and typically define Non-functional Requirements (NFRs), major system elements, subsystems, and interfaces. Business Owners are key stakeholders of the ART, with final responsibility for the business outcomes of the train. Customers are the ultimate economic buyers or value users of the solution. Other essential roles in the ART are:-\nSystem Teams typically assist in building and maintaining development, continuous integration, and test environments. Shared Services are specialists necessary for the success of an ART but cannot be dedicated to a specific train. They often include data security, information architects, site reliability engineering (SRE), database administrators (DBAs), and many more. Three Sync events to keep ART on track:-\nCoach Sync: focuses on executing the current PI, including risk, dependencies, progress, and impediments PO Sync: manages the PI’s scope, reviews progress, adjusts priorities, and prepares for the following PI ART Sync: usually replaces the Coach Sync and PO Sync for a particular iteration to reduce overhead. The ROAM board created during PI planning can be reviewed during the ART Sync. ART Planning board is used during ART sync to track and manage dependencies, ensuring they do not block other teams. Other events in ART:-\nPI Planning Each ART begins with the PI Planning using the PI planning board, the outcome is PI Objectives to be completed in the PI iteration. System Demos Occur at the end of Iteration to receive feedback from stakeholders, business owners, and customers. Inspect \u0026amp; Adapt Each PI concludes with I\u0026amp;A event for restrospection. Reference: https://scaledagileframework.com/agile-release-train/ Reference: https://scaledagileframework.com/planning-interval/\nWhat is Solution Train? The Solution Train is the organizational construct used to build large solutions that require the coordination of multiple ARTs and Suppliers. Critical Roles in the ART are:-\nSolution Management defines and supports building desirable, feasible, viable, and sustainable large-scale business solutions that meet customer needs over the solution’s significant lifespan. They represent the customer and business needs of the ARTs. Solution Architects define and communicate a shared technical and architectural vision across the Solution Train to help ensure the solution under development is fit for its intended purpose. They work with the ART’s System Architects to help guide their portion of the solution’s design. Solution Train Engineer (STE) is the coach for the Solution Train, facilitating and guiding the work of all ARTs and suppliers. The STE works with Release Train Engineers (RTEs) to facilitate ART execution and coordinate delivery. Suppliers are internal or external organizations that develop and deliver components, subsystems, or services, which help Solution Trains deliver solutions to customers e.g. contracting individuals and vendors or fixed contracts on development efforts. Business Owners are key stakeholders of the Solution Train, with final responsibility for the business outcomes. They, along with Solution Train leaders, may also serve as business owners for the Solution Train’s ARTs. Customers are the buyers of the solution and ultimately determine value. When delivering in a supply chain, customers work closely with Solution Management and other key stakeholders to define and adjust the solution’s vision, intent, and delivery roadmap. Reference: https://scaledagileframework.com/solution-train/ Reference: https://scaledagileframework.com/supplier/\nLesson 4: Agile Product Delivery What is Design Thinking? Design Thinking is a customer-centric development process that creates desirable products that are profitable and sustainable over their lifecycle.\nUnderstand the problem (activities: Discover, Define) - Use Personas to understand customers and Empathy Maps to identify with customers Design the right solution (activities: Develop, Deliver) - Use Customer Journey Maps to design end-to-end customer experience, Story Maps to capture user workflows, build Prototypes for faster feedback. Measure the success from these new ways: Desirable, Viable, Feasible, Sustainable Reference: https://scaledagileframework.com/design-thinking/\nWhat is ART, Solution Train, and Portfolio Backlog? ART Backlog is a Kanban system to capture and manage Features by Product Management ART Backlog also known as Program Backlog is holiding are of upcoming Features for a single ART Solution Train Backlog is a Kanban system to capture and manage Capabilities by Solution Management Portfolio Backlog is a Kanban system to capture and manage Portfolio Epics by Epic Owner and Lean Portfolio Management (LPM) Portfolio Epics splits into Features or Capabilities Capabilities split into Features ART, Solution Train and Portfolio Backlogs are prioritized using Weighted Shortest Job First (WSJF) ART and Solution Train Kanban Flow: Funnel, Analyzing, Ready, Implementing, Validating on Staging, Deploying to Production, Releasing, Done Portfolio Kanban Flow: Funnel, Reviewing, Analyzing, Ready, Implementing, Done Reference: https://scaledagileframework.com/art-and-solution-train-backlogs/\nReference: https://scaledagileframework.com/epic/\nWhat is a Feature? Features are maintained in the ART Backlog Feature are sized to fit in a Program Increment (PI) and delivered by a single Agile Release Train (ART) Features are split into Stories and fit in one Iteration for one team Features include a definition of Minimum Marketable Feature (MMF), a benefit hypothesis (to justify development cost) and Acceptance criteria (defined during program backlog refinement). Features are prioritized using WSJF and the top 10 features are presented to the team during PI planning Typically Product Management creates business features and System Architect creates enabler features Reference: https://scaledagileframework.com/features-and-capabilities/\nWhat is a Story? Features are implemented by Stories Stories are small increments of value that can be developed in days and are relatively easy to estimate Features fits in one PI for one ART; Stories fits in one iteration for one team. A Story Point is a relative number that represents: Volume, Complexity, Knowledge, and Uncertaity. Reference: https://scaledagileframework.com/story/\nWhat is Capability? Capabilities are maintained in the Solution Train Backlog Capabilities are sized to fit in a Program Increment (PI) and delivered by multiple Agile Release Trains (ARTs) Capabilities include a Phrase, a benefit hypothesis and Acceptance criteria Reference: https://scaledagileframework.com/features-and-capabilities/\nHow to prioritize Program Backlog for optimal ROI? Using Weighted Shortest Job First (WSJF). Give preference to jobs with a shorter duration and higher CoD.\nCost of Delay (CoD) WSJF = ----------------------- Job Duration (Job Size) CoD = User-business + Time + Risk reduction and/or value criticality opportunity enablement Reference: https://scaledagileframework.com/wsjf/\nWhat is PI Planning and its events? PI Planning stands for Program Increment Planning. PI Planning sessions are regularly scheduled events held throughout the year where multiple teams within the same Agile Release Train (ART) meet to align to a shared vision, discuss features, plan the roadmap, and identify cross-team dependencies. PI Planning is a 2 full day event that typically runs every 8-12 weeks (10 weeks typical). The two-day agenda is as follows:- Day 1 08:00 - 09:00 Business Context 09:00 - 10:30 Product/Solution Vision 10:30 - 11:30 Architecture Vision and Development Practicies 11:30 - 01:00 Planning Context and Lunch 01:00 - 04:00 Team breakouts 04:00 - 05:00 Draft Plan Review 05:00 - 06:00 Management review and problem solving Day 2 08:00 - 09:00 Planning Adjustment 09:00 - 11:00 Team breakouts 11:00 - 01:00 Final Plan Review and Lunch 01:00 - 02:00 ART Risks 02:00 - 02:15 Confidence Vote 02:15 - ?? Plan Rework (if needed) When ready Planning Retrospective and moving forward Primary Inputs to the PI Planning include: 1. Business context, 2. Roadmap \u0026amp; vision, and 3. Highest priority Features (typically top 10) of the ART backlog Primary Outputs of the PI Planning include: 1. Committed PI objectives, and 2. ART planning board Product Management provides the vision and backlog (typically represented by the top ten or so upcoming features) and owns the Feature priorities Business Owner provides the business context and assigns business value (BV) to each PI Objective on a scale from 1 to 10 Development Teams own Story planning and high-level estimates ART Planning Board is used for PI Planning showing: 1. Features, 2. Significant Dependency, and 3. Milestone or Event Reference: https://scaledagileframework.com/pi-planning/\nWhat are PI Uncommitted Objectives? Uncommitted objectives are used to identify work that can be variable within the scope of a PI. The work is planned, but the outcome is simply not certain. Teams can apply uncommitted objectives whenever there is low confidence in meeting the objective. This can be due to many circumstances:\nDependencies with another team or supplier that cannot be guaranteed. The team has little to no experience with functionality of this type. In this case the teams may plan ‘Spikes’ early in the PI to reduce uncertainty. There are a large number of fairly critical objectives that the business is depending on and the team is already loaded close to full capacity. What is Innovation and Planning (IP) Iteration? The Innovation and Planning (IP) Iteration is a unique iteration that occurs every PI, which provides dedicated time for Innvoation and Planning where:-\n– Innovation includes opportunity for innovation, hackathons, infrastructure improvements, continuing education, certifications, etc.\n– Planning includes PI Planning Readiness, Inspect and Adapt (I\u0026amp;A), and PI Planning events, etc. It provides an estimating buffer for meeting PI Objectives and sufficient capacity margin to enable cadence Without the IP Iteration\n– Lack of delivery capacity buffer impacts predictability\n– Little innovation; the tyranny of the urgent\n– Technical debt grows uncontrollably\n– People burn out\n– No time for teams to plan, demo, or improve together Reference: https://scaledagileframework.com/innovation-and-planning-iteration/\nWhat is Inspect and Adapt (I\u0026amp;A) Event? The Inspect and Adapt (I\u0026amp;A) is a significant event (Timebox: 3-4 hours) held at the end of each PI, where All ART stakeholders and Agile Team participate. The I\u0026amp;A event consists of three parts:- – PI System Demo (Timebox: 45-40 mins) - team demonstrate the current state of the solution\n– Quantitative and qualitative measurement - Team PI performance report is created which includes team\u0026rsquo;s planned vs actual business value. Individual team totals are rolled up into the ART predictibility report.\n– Retrospective and problem-solving workshop Reference: https://scaledagileframework.com/inspect-and-adapt/\nWhat is DevOps? DevOps is a mindset, culture, and set of technical practices that supports the integration, automation, and collaboration needed to effectively develop and operate a solution. It is a combination of Dev (Development) and Ops (Operations). DevOps enable the Continuous Delivery Pipeline (CDP) to release on demand and deliver value whenever there is a business need Reference: https://scaledagileframework.com/devops/\nWhat is the CALMR approach to DevOps? CALMR is a DevOps mindset that guides the ART toward achieving continuous value delivery by enhancing culture, automation, lean flow, measurement, and recovery.\nCulture - Establish a culture of shared responsibility for development, deployment, and operations. Automation - Automate the Continuous Delivery Pipeline. Lean flow - Keep batch sizes small, limit WIP, and provide extreme visibility. Measurement - Measure the flow through the pipeline. Implement full-stack telemetry. Recovery - Architect and enable low-risk releases. Establish fast recovery, fast reversion, and fast fix-forward. Reference: https://scaledagileframework.com/calmr/\nHow to build a Continuous Delivery Pipeline with DevOps? Continuous Exploration - Understand Customer needs - Hypothesize, Collaborate \u0026amp; Research, Architect, Synthesize Continuous Integration – A critical technical practice of the ART - Develop, Build, Test End-to-End, Stage Continuous Deployment – Getting to production early - Deploy, Verify, Monitor, Respond - Deploy to Staging every Iteration, Automate deployment, Automate testing of features and NFRs, Decouple deployment from release Release on Demand - Release, Stabilize, Measure, Learn What is Architectural Runway? Architectural Runway is the existing code, hardware components, marketing branding guidelines, and other variables that enable near-term business Features. Enablers build up the runway to support Features e.g. A single sign-on mechanism will enable sign-on in multiple applications. Use capacity allocation (a percentage of the train\u0026rsquo;s overall capacity in a PI) for Enablers that extend the runway Lesson 5: Lean Portfolio Management What is SAFe Portfolio? SAFe Portfolio is a collection of Development Value Streams for a specific business domain in an Enterprise. An Enterprise may have a single portfolio or multiple portfolios Each value stream can have multiple Solution trains and Agile release trains. The portfolio canvas is a template for identifying a specific SAFe portfolio. One of the primary uses of the canvas is to record the current state of the portfolio Reference: https://scaledagileframework.com/portfolio/\nWhat is Lean Portfolio Management? LPM provides an alignment and governance model for a specific portfolio, which contains a set of Development Value Streams (DVS) for a business domain in an Enterprise.\nThe three dimensions of LPM:-\nStrategy \u0026amp; Investment Funding ensures the entire portfolio is aligned and funded to create and maintain the solutions needed to meet business targets.\nParticipants: Enterprise Executives, Business Owners, and Enterprise Architects Agile Portfolio Operations coordinates and supports decentralized ART execution and fosters operational excellence.\nParticipants: Value Management Office (VMO), Lean-Agile Center of Excellence (LACE), Release Train Engineer (RTE), and Scrum Master/Team Coach CoP Lean Governance supports oversight of spending, audit, compliance, expenditure, measurement, and reporting. Participants: Enterprise Executives, Business Owners, Value Management Office (VMO), and Lean-Agile Center of Excellence (LACE) The effective operation of the LPM function relies on three significant events:-\nStategic Portfolio Review event provides ongoing strategy, implementation, and budget alignment. This event focuses on achieving and advancing the portfolio vision. It’s typically held on a quarterly cadence, at least one month before the next PI Planning event, to enable value streams to prepare and respond to any changes, Portfolio Sync event to review portfolio operational progress such as epic implementation, status of KPIs, addressing dependencies, and removing impediments. The portfolio sync is generally held monthly and may be replaced with the strategic portfolio review on a given month. Participatory Budgeting (PB) is an LPM event in which a group of stakeholders decides how to invest the portfolio budget across solutions and epics. Reference: https://scaledagileframework.com/lean-portfolio-management/\nWhat is Portfolio Epic? Epics are defined at portfolio level, they are typically cross-cutting and spanning multiple Value Streams and PIs. There are two types: Business Epics directly deliver business value Enabler Epics support the Architectural Runway and future business functionality Epics need a Lean business case, the definition of a minimum viable product (MVP), an Epic Owner, and approval by LPM. Epics are described with four major fields:- Epic Hypothesis Statement Business Outcomes Leading Indicators Nonfunctional requirements (NFRs) Reference: https://scaledagileframework.com/epic/\nWhat is Portfolio Backlog? The Portfolio Backlog is a Kanban system that is used to capture and manage the business and enabler epics intended to create and evolve the portfolio’s products, services, and solutions. Lean Portfolio Management (LPM) is responsible for developing, maintaining, and prioritizing the Portfolio backlog. Portfolio Epics are large (and typically cross-cutting initiatives) managed through the Portfolio Kanban. Epic Owners take responsibility for the essential collaborations needed for this task Enterprise Architects typically guide the enabler epics that support the technical considerations for business epics The Epic Owner works with various stakeholders to split epics into Features and Capabilities. Epic flows through the Portfolio Kanban from funnel to done as follows: Funnel, Reviewing, Analyzing, Ready, Implementing, Done LPM uses the Lean Business Case during the portfolio sync to make a ‘Go/No-go’ decision. ‘Go’ confirms the epic is approved for implementation and sequenced using WSJF. ‘No-go’ moves the epic to done. Epics in the analyzing state with the highest WSJF are pulled into the next state, Ready, as soon as space is available. Reference: https://scaledagileframework.com/portfolio-backlog/\nWhat are Strategic Themes? Strategic themes provide a mechanism to align the business objectives of an enterprise to SAFe portfolio. Strategic themes influence portfolio strategy and provide business context for portfolio decision-making. Strategic themes are direct inputs to the portfolio vision. Strategic themes can be defined by a phrase or by using the Objectives and Key Results (OKRs) template. Reference: https://scaledagileframework.com/strategic-themes/\nWhat is Lean Budget? Funding Value Streams, not projects. Expenses across a PI are fixed and easy to forecast\nReference: https://scaledagileframework.com/lean-budgets/\nWhat is Participatory Budgeting? Participatory Budgeting (PB) is the process that Lean Portfolio Management (LPM) uses to allocate the total portfolio budget to its value streams.\nThe Enterprise provides a portion of its total budget to each portfolio. In turn, Lean Portfolio Management (LPM) allocates the portfolio Budget to individual Value Streams. The value streams fund the people and resources needed to achieve the current Portfolio Vision and Roadmap. Empowered Agile Release Trains (ART) advance Solutions and implement Epics approved by LPM.\nReference: https://scaledagileframework.com/participatory-budgeting/\nLesson 6: Leading the Change How a leader lead by example? Leader sets an example for others to follow Insatiable learning: self-learning and self-motivated Authenticity: models the desired professional and ethical behaviors Emotional competence: idetify and manage their emotions and others Courage: to guide and lead in rapidly changing dynamics of digital age Growing others: by providing personal, professional, and technical guidance and resources Decentralize decision-making: moves the authority for decisions to where the information is Reference: https://scaledagileframework.com/lean-agile-leadership/\nKeys to Leading successful change? Create a sense of urgency Build a guiding coalition Form a strategic vision Enlist a volunteer army Enable action by removing barrier Generate short-term wins Sustain acceleration Institute change Reference: https://scaledagileframework.com/lean-agile-leadership/\nWhat are the 13 steps of SAFe Implementation Roadmap? Roadmap is a script of critical moves, which gives the best results when followed in same sequence to implement SAFe:-\nReaching the Tipping Point Train Lean-Agile Change Agents Create a Lean-Agile Center of Excellence Train Executives, Managers, and Leaders Lead in the Digital Age Organize Around Value Create the Implementation Plan Prepare for ART Launch Train Teams and Launch ART Coach ART Execution Launch More ARTs and Value Streams Enhance the Portfolio Accelerate Reference: https://scaledagileframework.com/implementation-roadmap/\nOther SAFe Certification Exam Notes Read SAFe Scrum Master SSM 6.0 Exam Notes\nRead SAFe Product Owner/Producer Manager POPM 6.0 Exam Notes\nComparisons Let\u0026rsquo;s do some comparisons within SAFe:-\nCompare SAFe vs Scrum SAFe Scrum Iteration Sprint Iteration Planning Sprint Planning Iteration Review Sprint Review Iteration Retrospective Sprint Retrospective Program Increment (PI) Typically 5 Sprints Program Increment Planning Planning for typically 5 Sprints Cadence and Synchronization Velocity Agile Team Scrum Team Agile Release Train (ART) - Teams of Agile teams Teams of Scrum teams Release Train Engineer Chief scrum Master ART Sync Scrum of Scrum Compare Agile Team vs Program vs Solution vs Portfolio Comparison Agile Team Program Solution Portfolio SAFe Configuration Essential Essential Large Solution Portfolio Cycle Iteration Program Increment \u0026amp; Agile Release Train Solution Train has multiple ARTs and suppliers Value Stream has multiple STs and ARTs Time Box typically 2-weeks long typically 5 iteration long typically 5 iteration long - Planning Iteration Planning PI Planning - - Coordination Daily Standup ART Sync - - Retro Iteration Retrospective Inspect \u0026amp; Adapt - - Backlog Team Backlog Program Backlog Solution Backlog Portfolio Backlog Backlog contains User Stories Features Capabilities Epics Backlog provided by Product Owner Product Management Solution Management Epic Owners Coordinator Scrum Master Release Train Engineer Solution Train Engineer Architect - System Architect Solution Architect Enterprise Architect Other Members Development \u0026amp; Testing Team Business Owners - - Agile Methodology Kanban or Scrum/XP Kanban Kanban Kanban Economic View Economic Framework Economic Framework Economic Framework Lean Budgets ","permalink":"https://codingnconcepts.com/agile/leading-safe/","tags":["SAFe","Certification"],"title":"Leading SAFe Agilist SA 6.0 Exam Notes"},{"categories":["Spring Boot"],"contents":"In this article, we\u0026rsquo;ll learn how to compose emails using Thymeleaf template and send emails using spring framework\u0026rsquo;s JavaMailSender.\nOverview Thymeleaf is a modern sever-side Java template engine. Thymeleaf is mostly used to build elegant HTML templates. Spring boot web MVC provides good integration with Thymeleaf for server-side view development.\nWe will first use Thymeleaf templates to compose HTML email messages and then we will use spring framework\u0026rsquo;s JavaMailSender to send email.\nProject Setup For initial setup of your Spring Boot project, you should use Spring Initializr. Choose the Java Mail Sender and Thymeleaf as dependencies.\nYou can use either Maven or Gradle build tool of your choice:-\nMaven Project Click on the below link to generate a Maven project with pre-selected required dependencies:-\nhttps://start.spring.io/#!type=maven-project\u0026amp;language=java\u0026amp;platformVersion=2.6.1\u0026amp;packaging=jar\u0026amp;jvmVersion=11\u0026amp;groupId=com.example\u0026amp;artifactId=email\u0026amp;name=email\u0026amp;description=Send%20Email%20with%20Thymeleaf%20Templates\u0026amp;packageName=com.example.email\u0026amp;dependencies=mail,thymeleaf\npom.xml \u0026lt;!-- to send email using java Mail and spring framework\u0026#39;s JavaMailSender --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-mail\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- to create email templates using Thymeleaf server-side Java template engine --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-thymeleaf\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- to write test class using junit jupiter --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Gradle Project Click on the below link to generate a Gradle project with pre-selected required dependencies:-\nhttps://start.spring.io/#!type=gradle-project\u0026amp;language=java\u0026amp;platformVersion=2.6.1\u0026amp;packaging=jar\u0026amp;jvmVersion=11\u0026amp;groupId=com.example\u0026amp;artifactId=email\u0026amp;name=email\u0026amp;description=Send%20Email%20with%20Thymeleaf%20Templates\u0026amp;packageName=com.example.email\u0026amp;dependencies=mail,thymeleaf\nbuild.gradle dependencies { // to send email using java Mail and spring framework\u0026#39;s JavaMailSender implementation \u0026#39;org.springframework.boot:spring-boot-starter-mail\u0026#39; // to create email templates using Thymeleaf server-side Java template engine implementation \u0026#39;org.springframework.boot:spring-boot-starter-thymeleaf\u0026#39; // to write test class using junit jupiter testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; } Project Structure A typical project structure for writing an email service looks like this:-\n📖springboot-email 📁src 📁main 📁java 📦com.example.email 📦config 📄ThymeleafTemplateConfig 📦model 📄Email 📦service 📄EmailSenderService EmailApplication 📁resources 📁templates 📄welcome-email.html ⚙application.yml pom.xml build.gradle Thymeleaf Email Template Configuration In order to process our email templates, we will configure a SpringTemplateEngine:-\npackage com.example.email.config; @Configuration public class ThymeleafTemplateConfig { @Bean public SpringTemplateEngine springTemplateEngine() { SpringTemplateEngine springTemplateEngine = new SpringTemplateEngine(); springTemplateEngine.addTemplateResolver(emailTemplateResolver()); return springTemplateEngine; } public ClassLoaderTemplateResolver emailTemplateResolver() { ClassLoaderTemplateResolver emailTemplateResolver = new ClassLoaderTemplateResolver(); emailTemplateResolver.setPrefix(\u0026#34;/templates/\u0026#34;); emailTemplateResolver.setSuffix(\u0026#34;.html\u0026#34;); emailTemplateResolver.setTemplateMode(TemplateMode.HTML); emailTemplateResolver.setCharacterEncoding(StandardCharsets.UTF_8.name()); emailTemplateResolver.setCacheable(false); return emailTemplateResolver; } } This template configuration will look for templates in src/main/resources/templates/ folder with .html extension.\nLet\u0026rsquo;s create our welcome email HTML template with Thymeleaf:-\nsrc/main/resources/templates/welcome-email.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html xmlns:th=\u0026#34;http://www.thymeleaf.org\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title th:remove=\u0026#34;all\u0026#34;\u0026gt;Template for HTML email\u0026lt;/title\u0026gt; \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=UTF-8\u0026#34;/\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt; Hello, \u0026lt;span th:text=\u0026#34;${name}\u0026#34;\u0026gt;Peter Static\u0026lt;/span\u0026gt;! \u0026lt;/p\u0026gt; \u0026lt;p th:if=\u0026#34;${name.length() \u0026gt; 10}\u0026#34;\u0026gt; Wow! You\u0026#39;ve got a long name (more than 10 chars)! \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; You have been successfully subscribed to the \u0026lt;b\u0026gt;CodingNConcepts\u0026lt;/b\u0026gt; on \u0026lt;span th:text=\u0026#34;${subscriptionDate}\u0026#34;\u0026gt;28-12-2012\u0026lt;/span\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;We write on following technologies:-\u0026lt;/p\u0026gt; \u0026lt;ul th:remove=\u0026#34;all-but-first\u0026#34;\u0026gt; \u0026lt;li th:each=\u0026#34;tech : ${technologies}\u0026#34; th:text=\u0026#34;${tech}\u0026#34;\u0026gt;Java\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;JavaScript\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;CSS\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;p\u0026gt; Regards, \u0026lt;br/\u0026gt; \u0026lt;em\u0026gt;The CodingNConcepts Team\u0026lt;/em\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; JavaMailSender Configuration Provide the SMTP (Mail) server configuration in application.yml or application.properties file to initialize JavaMailSender, which we will use to send emails.\napplication.yml spring: mail: default-encoding: UTF-8 host: smtp.gmail.com port: 587 username: lahoti.ashish20@gmail.com password: #gmail_app_password properties: mail: smtp: auth: true starttls: enable: true debug: true protocol: smtp test-connection: false Some of the popular SMTP (Mail) server details are as follows:-\nEmail Host Port Authentication Username Password Gmail smtp.gmail.com 587 (TLS) true example@gmail.com Generate Gmail App Password Yahoo smtp.mail.yahoo.com 465 (SSL) or 587 (TLS) true example@yahoo.com Generate Yahoo App Password If you want to use Gmail or Yahoo server to send email then you need a valid username and password for authentication. You can use your existing email address for e.g. lahoti.ashish20@gmail.com as username but password isn\u0026rsquo;t the same as your email password. You need to generate an app password from your mail account to use with SMTP (Mail) server.\nIf you are working in an organization then you might need your organization SMTP server details to configure.\nService to Send Email Let\u0026rsquo;s use SpringTemplateEngine to compose email and JavaMailSender to send email in our service class:-\npackage com.example.email.service; @Service @RequiredArgsConstructor @Slf4j public class EmailSenderService { private final JavaMailSender emailSender; private final SpringTemplateEngine templateEngine; public void sendHtmlMessage(Email email) throws MessagingException { MimeMessage message = emailSender.createMimeMessage(); MimeMessageHelper helper = new MimeMessageHelper(message, MimeMessageHelper.MULTIPART_MODE_MIXED_RELATED, StandardCharsets.UTF_8.name()); Context context = new Context(); context.setVariables(email.getProperties()); helper.setFrom(email.getFrom()); helper.setTo(email.getTo()); helper.setSubject(email.getSubject()); String html = templateEngine.process(email.getTemplate(), context); helper.setText(html, true); log.info(\u0026#34;Sending email: {} with html body: {}\u0026#34;, email, html); emailSender.send(message); } } Test Service to Send Email Now its time to test sending email message composed using welcome-email.html thymeleaf template\n@SpringBootTest public class EmailSenderServiceTest { @Autowired private EmailSenderService emailSenderService; @Test public void sendHtmlMessageTest() throws MessagingException { Email email = new Email(); email.setTo(\u0026#34;lahoti.ashish20@gmail.com\u0026#34;); email.setFrom(\u0026#34;lahoti.ashish20@gmail.com\u0026#34;); email.setSubject(\u0026#34;Welcome Email from CodingNConcepts\u0026#34;); email.setTemplate(\u0026#34;welcome-email.html\u0026#34;); Map\u0026lt;String, Object\u0026gt; properties = new HashMap\u0026lt;\u0026gt;(); properties.put(\u0026#34;name\u0026#34;, \u0026#34;Ashish\u0026#34;); properties.put(\u0026#34;subscriptionDate\u0026#34;, LocalDate.now().toString()); properties.put(\u0026#34;technologies\u0026#34;, Arrays.asList(\u0026#34;Python\u0026#34;, \u0026#34;Go\u0026#34;, \u0026#34;C#\u0026#34;)); email.setProperties(properties); Assertions.assertDoesNotThrow(() -\u0026gt; emailSenderService.sendHtmlMessage(email)); } } Voilà! Message arrived in my mailbox:-\nDownload the complete source code for the examples in this post from github/springboot-email\n","permalink":"https://codingnconcepts.com/spring-boot/send-email-with-thymeleaf-template/","tags":["Spring Boot Email"],"title":"Send Email with Thymeleaf template in Spring Boot"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to zip a file or directory into an archive and how to unzip the archive into a directory using Java core libraries.\nZip a File Let\u0026rsquo;s look at the example where we want to zip a single file into an archive:-\npublic static void zipFile(Path fileToZip, Path zipFile) throws IOException { Files.createDirectories(zipFile.getParent()); try (ZipOutputStream zipOutputStream = new ZipOutputStream(Files.newOutputStream(zipFile))) { ZipEntry zipEntry = new ZipEntry(fileToZip.toFile().getName()); zipOutputStream.putNextEntry(zipEntry); if (Files.isRegularFile(fileToZip)) { Files.copy(fileToZip, zipOutputStream); } } } The below method call will zip a file /cnc/toZip/picture1.png into an archive /cnc/zip/singleFileArchive.zip:-\nzipFile(Path.of(\u0026#34;/cnc/toZip/picture1.png\u0026#34;), Path.of(\u0026#34;/cnc/zip/singleFileArchive.zip\u0026#34;)); Make note of following in this method:-\nUsing Java nio method Files.createDirectories() to create all directories of zip file if doesn\u0026rsquo;t exist. Unlike the Files.createDirectory() method, an exception is not thrown if it already exists. Initializing the ZipOutputStream in the try block so that closing the streams will be taken care by JVM. Using Java nio method Files.copy() to copy the file to ZipOutputStream, makes your code concise. Zip Multiple files Let\u0026rsquo;s look at the example where we want to zip multiple files into an archive:-\npublic static void zipMultipleFiles(List\u0026lt;Path\u0026gt; filesToZip, Path zipFile) throws IOException { Files.createDirectories(zipFile.getParent()); try (ZipOutputStream zipOutputStream = new ZipOutputStream(Files.newOutputStream(zipFile))) { for(Path fileToZip: filesToZip){ ZipEntry zipEntry = new ZipEntry(fileToZip.toFile().getName()); zipOutputStream.putNextEntry(zipEntry); if (Files.isRegularFile(fileToZip)) { Files.copy(fileToZip, zipOutputStream); } } } } The below method call will zip these two files picture1.png \u0026amp; picture2.png into an archive /cnc/zip/multiFileArchive.zip:-\nList\u0026lt;Path\u0026gt; filesToZip = Arrays.asList(Path.of(\u0026#34;/cnc/toZip/picture1.png\u0026#34;), Path.of(\u0026#34;/cnc/toZip/picture2.png\u0026#34;)); zipMultipleFiles(filesToZip, Path.of(\u0026#34;/cnc/zip/multiFileArchive.zip\u0026#34;)); Make note that we put each file entry in ZipEntry which represents the archive file.\nZip All Files and Folders in a Directory Let\u0026rsquo;s look at the example where we want to zip all the files and folders inside a directory into an archive:-\npublic static void zipDirectory(Path directoryToZip, Path zipFile) throws IOException { Files.deleteIfExists(zipFile); Files.createDirectories(zipFile.getParent()); try (ZipOutputStream zipOutputStream = new ZipOutputStream(Files.newOutputStream(zipFile))) { if (Files.isDirectory(directoryToZip)) { Files.walk(directoryToZip).filter(path -\u0026gt; !Files.isDirectory(path)).forEach(path -\u0026gt; { ZipEntry zipEntry = new ZipEntry(directoryToZip.relativize(path).toString()); try { zipOutputStream.putNextEntry(zipEntry); if (Files.isRegularFile(path)) { Files.copy(path, zipOutputStream); } zipOutputStream.closeEntry(); } catch (IOException e) { System.err.println(e); } }); } } } The below method call will zip all the files and folders inside /cnc/toZip recursively into an archive /cnc/zip/fullDirectoryArchive.zip:-\nzipDirectory(Path.of(\u0026#34;/cnc/toZip\u0026#34;), Path.of(\u0026#34;/cnc/zip/fullDirectoryArchive.zip\u0026#34;)); Make note of following in this method:-\nUsing Java NIO file utility methods Files.deleteIfExists() and Files.createDirectories(), which are safe to use and doesn\u0026rsquo;t throw an exception if file or directory doesn\u0026rsquo;t exist. Initializing the ZipOutputStream in the try block so that closing the streams will be taken care by JVM. Using Java NIO Files.walk() to iterate through all files and folders recursively in a directory using Java Streams. Put each file or directory entry in the ZipEntry, which represents the archive file. Unzip an Archive file Let\u0026rsquo;s look at the example where we want to unzip an archive file into target directory:-\npublic static void unzipDirectory(Path zipFile, Path targetDirectory) throws IOException { if (!Files.exists(zipFile)) { return; } deleteDirectoryRecursively(targetDirectory); Files.createDirectory(targetDirectory); try (ZipInputStream zipInputStream = new ZipInputStream(Files.newInputStream(zipFile))) { ZipEntry entry; while ((entry = zipInputStream.getNextEntry()) != null) { final Path toPath = targetDirectory.resolve(entry.getName()); if (entry.isDirectory()) { Files.createDirectory(toPath); } else { if (!Files.exists(toPath.getParent())) { Files.createDirectories(toPath.getParent()); } Files.copy(zipInputStream, toPath); } } } } private static void deleteDirectoryRecursively(Path dir) throws IOException { if (Files.exists(dir)) { Files.walk(dir).sorted(Comparator.reverseOrder()).map(Path::toFile).forEach(File::delete); } } The below method call will unzip all the files and folders inside fullDirectoryArchive.zip recursively into the directory /cnc/toUnzip:-\nunzipDirectory(Path.of(\u0026#34;/cnc/zip/fullDirectoryArchive.zip\u0026#34;), Path.of(\u0026#34;/cnc/toUnzip\u0026#34;)); Make note of following in this method:-\nDoing initial validations for e.g. return and do nothing if zip file does\u0026rsquo;nt exist, delete the target directory recursively if it already exist. Initializing the ZipInputStream in the try block so that closing the streams will be taken care by JVM. Iterate through ZipEntry, which is an archive file representation and have all archive information:-\n- If it is a directory then create that directory\n- If it is a file then create all its parent directories and then copy that file Thats it for this tutorial. Thanks for reading and happy learning!\nDownload the ZipFileUtil.java which have all these methods and start using in your code.\n","permalink":"https://codingnconcepts.com/java/zip-unzip-files-in-java/","tags":["File","Advance Java"],"title":"Zip and Unzip Files in Java"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn different methods to convert long primitive or Long object to String in Java\nConvert long primitive to String String s1 = Long.toString(1L); // \u0026#34;1\u0026#34; String s2 = String.valueOf(1L); // \u0026#34;1\u0026#34; String s3 = \u0026#34;\u0026#34; + 1L; // \u0026#34;1\u0026#34; String s4 = new StringBuilder().append(1L).toString(); // \u0026#34;1\u0026#34; String s5 = String.format(\u0026#34;%d\u0026#34;, 1L); // \u0026#34;1\u0026#34; String s6 = new DecimalFormat(\u0026#34;#\u0026#34;).format(1L); // \u0026#34;1\u0026#34; We see that first three methods i.e. Long.toString(), String.valueOf, and + are concise and good to use.\nConvert Long object to String Long longObj = 1L; String s1 = longObj.toString(); // \u0026#34;1\u0026#34; String s2 = Long.toString(longObj); // \u0026#34;1\u0026#34; String s3 = String.valueOf(longObj); // \u0026#34;1\u0026#34; String s4 = \u0026#34;\u0026#34; + longObj; // \u0026#34;1\u0026#34; String s5 = new StringBuilder().append(longObj).toString(); // \u0026#34;1\u0026#34; String s6 = String.format(\u0026#34;%d\u0026#34;, longObj); // \u0026#34;1\u0026#34; String s7 = new DecimalFormat(\u0026#34;#\u0026#34;).format(longObj); // \u0026#34;1\u0026#34; We see that first method i.e. longObj.toString() is the quickest and concise way to convert a Long object to String, followed by the next three methods.\nException handling - Long to String Let\u0026rsquo;s see how these methods handle the null value:-\nLong longObj = null; String s1 = longObj.toString(); // throw \u0026#34;NullPointerException\u0026#34; String s2 = Long.toString(longObj); // throw \u0026#34;NullPointerException\u0026#34; String s3 = String.valueOf(longObj); // throw \u0026#34;IllegalArgumentException\u0026#34; String s4 = \u0026#34;\u0026#34; + longObj; // \u0026#34;null\u0026#34; String s5 = new StringBuilder().append(longObj).toString(); // \u0026#34;null\u0026#34; String s6 = String.format(\u0026#34;%d\u0026#34;, longObj); // \u0026#34;null\u0026#34; String s7 = new DecimalFormat(\u0026#34;#\u0026#34;).format(longObj); // throw \u0026#34;IllegalArgumentException\u0026#34; We see that highlighted methods return the \u0026ldquo;null\u0026rdquo; string for null value while others throw NullPointerException or IllegalArgumentException so you can use the method wisely.\n","permalink":"https://codingnconcepts.com/java/convert-long-to-string-in-java/","tags":["Core Java","String"],"title":"Convert Long to String in Java"},{"categories":["Spring Boot"],"contents":"In this article, we\u0026rsquo;ll learn how to configure a FeignClient with SSL and Proxy Connection using ApacheHttp5 in your Spring Boot project.\nProject Setup For initial setup of your Spring Boot project, you should use Spring Initializr. Choose the OpenFeign and Spring Web as dependencies and Contract Stub Runner as test dependency.\nMaven Project You can click the below link to generate a Maven project with pre-selected dependencies:-\nhttps://start.spring.io/#!type=maven-project\u0026amp;language=java\u0026amp;platformVersion=2.5.1.RELEASE\u0026amp;packaging=jar\u0026amp;jvmVersion=1.8\u0026amp;groupId=com.example\u0026amp;artifactId=api\u0026amp;name=api\u0026amp;description=Create%20Feign%20Client%20to%20consume%20RESTFul%20APIs\u0026amp;packageName=com.example.api\u0026amp;dependencies=cloud-feign,web,cloud-contract-stub-runner\nand then add io.github.openfeign:feign-hc5 dependency.\n\u0026lt;!-- to write web layer --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- to write web client using OpenFeign --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-openfeign\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- to write test class using junit jupiter --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- to write integration test and mock stub using WireMock --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-contract-stub-runner\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- to add ApacheHttp5 openfeign client --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.github.openfeign\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;feign-hc5\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Gradle Project Similarly, You can click the below link to generate a Gradle project with pre-selected dependencies:-\nhttps://start.spring.io/#!type=gradle-project\u0026amp;language=java\u0026amp;platformVersion=2.5.1.RELEASE\u0026amp;packaging=jar\u0026amp;jvmVersion=1.8\u0026amp;groupId=com.example\u0026amp;artifactId=api\u0026amp;name=api\u0026amp;description=Create%20Feign%20Client%20to%20consume%20RESTFul%20APIs\u0026amp;packageName=com.example.api\u0026amp;dependencies=cloud-feign,web,cloud-contract-stub-runner\nand then add io.github.openfeign:feign-hc5 dependency.\ndependencies { // to write web layer implementation \u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39; // to write web client using OpenFeign implementation \u0026#39;org.springframework.cloud:spring-cloud-starter-openfeign\u0026#39; // to write test class using junit jupiter testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; // to write integration test and mock stub using WireMock testImplementation \u0026#39;org.springframework.cloud:spring-cloud-starter-contract-stub-runner\u0026#39; // to add ApacheHttp5 openfeign client implementation \u0026#39;io.github.openfeign:feign-hc5\u0026#39; } Enable ApacheHttp5 Feign Client When you are working with spring boot project, you have nothing much to do to enable ApacheHttp5 FeignClient for your project. Make sure:-\nYou have spring-boot-starter-web, spring-cloud-starter-openfeign, and io.github.openfeign:feign-hc5 dependencies in your pom.xml or build.gradle You are using @SpringBootApplication and @EnableFeignClients annotations at your application starter class file ApiApplication. package com.example.api; @SpringBootApplication @EnableFeignClients public class ApiApplication { public static void main(String[] args) { SpringApplication.run(ApiApplication.class, args); } } You set feign.httpclient.enabled property to true in application.properties or application.yml file feign.httpclient.enabled: true Create ApacheHttp5 Feign Client with SSL and Proxy Connection Next, we are going to create a FeignClient to consume secured APIs using ApacheHttp5 SSL and Proxy Connection.\nLet\u0026rsquo;s create a UserFeignClient interface -\nAnnotated with @FeignClient which auto scan by spring boot application to generate feign client Consumes the APIs from this URL: https://reqres.in Get the ApacheHttp5 SSL and Proxy connection from configuration class ApacheHttp5FeignSslClientConfig.class Get the properties value such as baseUrl, ssl and proxy connection from application.yml file application.yml client: api: baseUrl: https://reqres.in ssl: protocol: TLS key-store-type: JKS key-store: classpath:KeyStore.jks key-store-password: changeit key-password: changeit trust-store: classpath:TrustStore.jks trust-store-password: changeit proxy-host: 10.20.30.40 proxy-port: 443 Feign Client @FeignClient(name = \u0026#34;userFeignClient\u0026#34;, url = \u0026#34;${client.api.baseUrl}\u0026#34;, configuration = ApacheHttp5FeignSslClientConfig.class) public interface UserFeignClient { @GetMapping(\u0026#34;/api/users\u0026#34;) ListUserResponse getUserList(@RequestParam(\u0026#34;page\u0026#34;) Integer page); @GetMapping(\u0026#34;/api/users/{userId}\u0026#34;) SingleUserResponse getUserById(@PathVariable(\u0026#34;userId\u0026#34;) Long userId); @PostMapping(\u0026#34;/api/users\u0026#34;) User createUser(@RequestBody UserRequest userRequest); @PutMapping(\u0026#34;/api/users\u0026#34;) User updateUser(@RequestBody UserRequest userRequest); @DeleteMapping(\u0026#34;/api/users/{userId}\u0026#34;) void deleteUserById(@PathVariable(\u0026#34;userId\u0026#34;) Long userId); } ApacheHttp5 Feign Client Configuration import feign.Feign; import feign.Retryer; import feign.hc5.ApacheHttp5Client; import lombok.extern.slf4j.Slf4j; import org.apache.commons.lang3.StringUtils; import org.apache.hc.client5.http.impl.classic.HttpClients; import org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManagerBuilder; import org.apache.hc.client5.http.io.HttpClientConnectionManager; import org.apache.hc.client5.http.ssl.SSLConnectionSocketFactory; import org.apache.hc.client5.http.ssl.SSLConnectionSocketFactoryBuilder; import org.apache.hc.core5.http.HttpHost; import org.apache.hc.core5.ssl.SSLContexts; import org.apache.hc.core5.ssl.TrustStrategy; import org.springframework.beans.factory.annotation.Value; import org.springframework.context.annotation.Bean; import org.springframework.util.ResourceUtils; import javax.net.ssl.SSLContext; import java.io.IOException; import java.security.KeyManagementException; import java.security.KeyStoreException; import java.security.NoSuchAlgorithmException; import java.security.UnrecoverableKeyException; import java.security.cert.CertificateException; @Slf4j public class ApacheHttp5FeignSslClientConfig { @Bean public Feign.Builder feignBuilder( @Value(\u0026#34;${client.api.ssl.protocol}\u0026#34;) String protocol, @Value(\u0026#34;${client.api.ssl.key-store-type}\u0026#34;) String keyStoreType, @Value(\u0026#34;${client.api.ssl.key-store}\u0026#34;) String keyStore, @Value(\u0026#34;${client.api.ssl.key-store-password}\u0026#34;) String keyStorePassword, @Value(\u0026#34;${client.api.ssl.key-password}\u0026#34;) String keyPassword, @Value(\u0026#34;${client.api.ssl.trust-store}\u0026#34;) String trustStore, @Value(\u0026#34;${client.api.ssl.trust-store-password}\u0026#34;) String trustStorePassword, @Value(\u0026#34;${client.api.ssl.proxy-host}\u0026#34;) String proxyHost, @Value(\u0026#34;${client.api.ssl.proxy-port}\u0026#34;) String proxyPort ) { SSLContext sslContext = getSSLContext(protocol, keyStoreType, keyStore, keyStorePassword, keyPassword, trustStore, trustStorePassword); SSLConnectionSocketFactory sslConnectionSocketFactory = SSLConnectionSocketFactoryBuilder.create().setSslContext(sslContext).build(); HttpClientConnectionManager connectionManager = PoolingHttpClientConnectionManagerBuilder.create().setSSLSocketFactory(sslConnectionSocketFactory).build(); return Feign.builder() .retryer(Retryer.NEVER_RETRY) .client(new ApacheHttp5Client(HttpClients.custom() .setConnectionManager(connectionManager) .setProxy(StringUtils.isNotEmpty(proxyHost) ? new HttpHost(proxyHost, proxyPort) : null) .build())); } private SSLContext getSSLContext(String protocol, String keyStoreType, String keyStore, String keyStorePassword, String keyPassword, String trustStore, String trustStorePassword) { try { TrustStrategy acceptingTrustStrategy = (chain, authType) -\u0026gt; true; return SSLContexts.custom() .setProtocol(protocol) .setKeyStoreType(keyStoreType) .loadKeyMaterial(ResourceUtils.getFile(keyStore), keyStorePassword.toCharArray(), keyPassword.toCharArray()) .loadTrustMaterial(ResourceUtils.getFile(trustStore), trustStorePassword.toCharArray(), acceptingTrustStrategy) .build(); } catch (IOException | UnrecoverableKeyException | CertificateException | NoSuchAlgorithmException | KeyStoreException | KeyManagementException e) { log.error(\u0026#34;Error while building SSLContext for ApacheHttp5FeignSslClient\u0026#34;, e); throw new ExceptionInInitializerError(\u0026#34;Error while building SSLContext for ApacheHttp5FeignSslClient\u0026#34;); } } } Please note that if you don\u0026rsquo;t want to set the proxy in the above configuration, then keep the proxy-host and proxy-port values as blank in property file.\nConclusion We learned how to configure the latest version of Apache Http Client i.e. ApacheHttp5 to use with OpenFeign in Spring Boot Application. We also learned how to load SSL certificates and set Proxy in ApacheHttp5 client.\nDownload the complete source code for the examples in this post from github/springboot-openfeign\n","permalink":"https://codingnconcepts.com/spring-boot/feign-apachehttp5-ssl-proxy-connection/","tags":["Spring Boot API","REST","Feign"],"title":"ApacheHttp5 FeignClient SSL and Proxy Connection in Spring Boot"},{"categories":["Java"],"contents":"Java 8 streams API is a widely used feature to write code in a functional programming way. In this tutorial, we\u0026rsquo;ll discuss how to use Streams API for Map creation, iteration and sorting.\nLet\u0026rsquo;s create a User class and List of users, which we will use in the examples of this tutorial:-\nclass User { Long id; String name; Integer age; // constructor, getters, setters, toString } List\u0026lt;User\u0026gt; users = List.of(new User(1L, \u0026#34;Andrew\u0026#34;, 23), new User(2L, \u0026#34;Billy\u0026#34;, 42), new User(3L, \u0026#34;David\u0026#34;, 29), new User(4L, \u0026#34;Charlie\u0026#34;, 30), new User(5L, \u0026#34;Andrew\u0026#34;, 18), new User(6L, \u0026#34;Charlie\u0026#34;, 19)); Please note that id is unique but name is not unique. You can see multiple users having similar names i.e. Andrew and Charlie. We have kept them intentionally to handle duplicate scenarios in the examples.\nCreate a Map A Map is created when you collect a stream of elements using either Collectors.toMap() or Collectors.groupingBy().\nusing Collectors.toMap() Example 1: Map from streams having unique keys Let\u0026rsquo;s stream the List and collect it to a Map using Collectors.toMap(keyMapper, valueMapper). We used id as key and name as value in the collector. The generated map has all the unique keys but value may contain duplicates, which is perfectly fine in the case of Map.\nMap\u0026lt;Long, String\u0026gt; map = users.stream() .collect(Collectors.toMap(User::getId, User::getName)); // {1=Andrew, 2=Billy, 3=David, 4=Charlie, 5=Andrew, 6=Charlie} Another example of creating a Map using a unique id as key and user object as value:-\nMap\u0026lt;Long, User\u0026gt; map = users.stream() .collect(Collectors.toMap(User::getId, Function.identity())); //{1=User{id=1, name=\u0026#39;Andrew\u0026#39;, age=23}, // 2=User{id=2, name=\u0026#39;Billy\u0026#39;, age=42}, // 3=User{id=3, name=\u0026#39;David\u0026#39;, age=29}, // 4=User{id=4, name=\u0026#39;Charlie\u0026#39;, age=30}, // 5=User{id=5, name=\u0026#39;Andrew\u0026#39;, age=18}, // 6=User{id=6, name=\u0026#39;Charlie\u0026#39;, age=19}} Notice the use of Function.identity() method to collect the object itself.\nExample 2: Map from streams having a duplicate key In previous examples, we used the id as a key which perfectly works because the key of a Map should be unique.\nduplicate key results error! Let\u0026rsquo;s see what happens when we use the user\u0026rsquo;s name as a key which is not unique and the user\u0026rsquo;s age as a value:-\nMap\u0026lt;String, Integer\u0026gt; map = users.stream() .collect(Collectors.toMap(User::getName, User::getAge)); It throws IllegalStateException which is expected since the key of a Map should be unique\njava.lang.IllegalStateException: Duplicate key Andrew (attempted merging values 23 and 18) mergeFunction to the rescue! Java 8 Streams provide Collectors.toMap(keyMapper, valueMapper, mergeFunction) overloaded method where you can specify which value to consider when duplicate key issues occur.\nLet\u0026rsquo;s collect a Map having user name as a key, The merge function indicates that keep the old value for the same key:-\nMap\u0026lt;String, Integer\u0026gt; idValueMap = users.stream() .collect(Collectors.toMap(User::getName, User::getAge, (oldValue, newValue) -\u0026gt; oldValue)); // {Billy=42, Andrew=23, Charlie=30, David=29} We don\u0026rsquo;t see any error this time and a Map is created with unique user names. Duplicate user names are merged having age value whichever comes first in the list.\nExample 3: ConcurrentHashMap, LinkedHashMap, and TreeMap from streams Java 8 Streams provide Collectors.toMap(keyMapper, valueMapper, mergeFunction, mapFactory) overloaded method where you can specify the type using mapFactory to return ConcurrentHashMap, LinkedHashMap or TreeMap.\nMap\u0026lt;String, Integer\u0026gt; concurrentHashMap = users.stream() .collect(Collectors.toMap(User::getName, User::getAge, (o1, o2) -\u0026gt; o1, ConcurrentHashMap::new)); Map\u0026lt;String, Integer\u0026gt; linkedHashMap = users.stream() .collect(Collectors.toMap(User::getName, User::getAge, (o1, o2) -\u0026gt; o1, LinkedHashMap::new)); Map\u0026lt;String, Integer\u0026gt; treeMap = users.stream() .collect(Collectors.toMap(User::getName, User::getAge, (o1, o2) -\u0026gt; o1, TreeMap::new)); using Collectors.groupingBy() A Map is returned when you group a stream of objects using Collectors.groupingBy(keyMapper, valueMapper). You can specify the key and value mapping function. Specifying the value mapping function is optional and it returns a List by default.\nExample 1: Group the stream by Key Let\u0026rsquo;s group the stream of user objects by name using Collectors.groupingBy(keyMapper) which returns a Map where key is user name and value is List of objects of the users having the same name:-\nMap\u0026lt;String, List\u0026lt;User\u0026gt;\u0026gt; groupByName = users.stream() .collect(Collectors.groupingBy(User::getName)); // {Billy=[User{id=2, name=\u0026#39;Billy\u0026#39;, age=42}], // Andrew=[User{id=1, name=\u0026#39;Andrew\u0026#39;, age=23}, User{id=5, name=\u0026#39;Andrew\u0026#39;, age=18}], // Charlie=[User{id=4, name=\u0026#39;Charlie\u0026#39;, age=30}, User{id=6, name=\u0026#39;Charlie\u0026#39;, age=19}], // David=[User{id=3, name=\u0026#39;David\u0026#39;, age=29}]} Example2: Group the stream by key and value This time we will specify both key and value mapping functions in Collectors.groupingBy(keyMapper, valueMapper). For example:-\nCreate a map where key is user name and value is count of the users having the same name:-\nMap\u0026lt;String, Long\u0026gt; countByName = users.stream() .collect(Collectors.groupingBy(User::getName, Collectors.counting())); // {Billy=1, Andrew=2, Charlie=2, David=1} Create a map where key is user name and value is sum of age of users having the same name:-\nMap\u0026lt;String, Integer\u0026gt; sumAgeByName = users.stream() .collect(Collectors.groupingBy(User::getName, Collectors.summingInt(User::getAge))); // {Billy=42, Andrew=41, Charlie=49, David=29} Iterate through Map There are three ways to iterate through a Map:-\nUsing keySet() The method keySet() is applied on Map\u0026lt;K,V\u0026gt; which returns Set\u0026lt;K\u0026gt; and can be streamed to iterate through keys:-\nusers.stream() .collect(Collectors.toMap(User::getId, User::getName)) .keySet() .stream() .forEach(System.out::print); // Prints \u0026#34;1 2 3 4 5\u0026#34; Using values() The method values() is applied on Map\u0026lt;K,V\u0026gt; which returns Collection\u0026lt;V\u0026gt; and can be streamed to iterate through values:-\nusers.stream() .collect(Collectors.toMap(User::getId, User::getName)) .values() .stream() .forEach(System.out::print); // Prints \u0026#34;Andrew Billy David Charlie Andrew Charlie\u0026#34; Using entrySet() The method entrySet() is applied on Map\u0026lt;K,V\u0026gt; which returns Set\u0026lt;Map.Entry\u0026lt;K, V\u0026gt;\u0026gt; and can be streamed to iterate through entries (keys \u0026amp; values):-\nusers.stream() .collect(Collectors.toMap(User::getId, User::getName)) .entrySet() .stream() .forEach(System.out::print); // Prints \u0026#34;1=Andrew 2=Billy 3=David 4=Charlie 5=Andrew 6=Charlie\u0026#34; Sort the Map By Key We can sort the Map by key using streams with built-in comparator Map.Entry.comparingByKey()\nSort the Map by key in alphabetical order and print it:-\nusers.stream() .collect(Collectors.toMap(User::getName, User::getAge, (o1,o2) -\u0026gt; o1)) .entrySet() .stream() .sorted(Map.Entry.comparingByKey()) .forEach(System.out::println); // Andrew=23 // Billy=42 // Charlie=30 // David=29 Sort the Map by key in reverse alphabetical order and collect to LinkedHashMap:-\nMap\u0026lt;String, Integer\u0026gt; sortByKeyReverse = users.stream() .collect(Collectors.toMap(User::getName, User::getAge, (o1,o2) -\u0026gt; o1)) .entrySet() .stream() .sorted(Map.Entry.comparingByKey(Comparator.reverseOrder())) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (o1,o2) -\u0026gt; o1, LinkedHashMap::new)); // {David=29, Charlie=30, Billy=42, Andrew=23} By Value We can sort the Map by value using streams with built-in comparator Map.Entry.comparingByValue()\nSort the Map by value in ascending order and print it:-\nusers.stream() .collect(Collectors.toMap(User::getName, User::getAge, (o1,o2) -\u0026gt; o1)) .entrySet() .stream() .sorted(Map.Entry.comparingByValue()).forEach(System.out::println); // Andrew=23 // David=29 // Charlie=30 // Billy=42 Sort the Map by value in descending order and collect to LinkedHashMap:-\nMap\u0026lt;String, Integer\u0026gt; sortByValueReverse = users.stream() .collect(Collectors.toMap(User::getName, User::getAge, (o1,o2) -\u0026gt; o1)) .entrySet() .stream() .sorted(Map.Entry.comparingByValue(Comparator.reverseOrder())) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (o1,o2) -\u0026gt; o1, LinkedHashMap::new)); // {Billy=42, Charlie=30, David=29, Andrew=23} By Both Key and Value We can sort the Map by using both key and value one after another using thenComparing()\nSort the Map by value in alphabetical order and then sort by key in descending order and collect to LinkedHashMap:-\nComparator\u0026lt;Map.Entry\u0026lt;Long, String\u0026gt;\u0026gt; valueComparator = Map.Entry.comparingByValue(); Comparator\u0026lt;Map.Entry\u0026lt;Long, String\u0026gt;\u0026gt; keyComparator = Map.Entry.comparingByKey(Comparator.reverseOrder()); Map\u0026lt;Long, String\u0026gt; sortByValueThenKey = users.stream() .collect(Collectors.toMap(User::getId, User::getName)) .entrySet() .stream() .sorted(valueComparator.thenComparing(keyComparator)) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (o1, o2) -\u0026gt; o1, LinkedHashMap::new)); // {5=Andrew, 1=Andrew, 2=Billy, 6=Charlie, 4=Charlie, 3=David} We got a sorted Map having user names sorted in alphabetical order first and then keys are sorted in descending order of user id.\n","permalink":"https://codingnconcepts.com/java/streams-with-map-java-8/","tags":["Java Streams"],"title":"Using Streams API with Map in Java 8"},{"categories":["Spring Boot"],"contents":"In this article, we\u0026rsquo;ll learn how to automate the build \u0026amp; deployment of Spring boot based microservices using Docker, Jenkins, Gradle and Git.\nOverview It is essential in microservice environment to automate the build, test, containerize, deploy and run phases of a Spring boot based applications.\nCI/CD (Continuous integration and continuous deployment) is an important aspect of microservices, which can be achieved using DevOps tools such as Gradle, Docker, Jenkins, and Git.\nGradle provides gradle task such as build, test, docker, and dockerRun for spring boot microservice Docker provides ability to containerize (docker image) our spring boot microservice Jenkins build the CI/CD pipeline and stages to build and deploy our spring boot microservice Git triggers the Jenkins job when any source-code is committed to the Git repository. Sounds Complicated? Don\u0026rsquo;t worry, We\u0026rsquo;ll learn everything step by step by using all these tools in our local machine.\nSpring Boot Microservices We\u0026rsquo;re created our two Spring boot based microservices in a single GitHub repository github/springboot-microservices as modules also called as Git Monorepo. You can clone or download this repository and setup in your favourite IDE to follow along.\nIt is recommended to use Git Monorepo for smaller applications having 5 to 10 microservices for faster development and collaboration.\nProject structure for Spring Boot based Microservices\nAbout our project structure in the monorepo,\nreview-service microservice provide APIs and execute database CRUD operations on review details of a specific product. product-service microservice provide APIs, fetch product details from thirdparty service, and fetch review details from review-service common-library is used by both microservices and provides common configuration for api logging, security, documentation, exception handling, etc. You can ignore this project from automation perspective. build.gradle is used for dependency management, to build and run individual microservices. settings.gradle is used to manage modules in Monorepo. Dockerfile is set of instructions to build docker image using docker command Jenkinsfile define deployment pipeline and used by jenkins to trigger deployment job Assume that our spring boot based microservices are running on following server port:\nMicroservice Running on.. product-service http://localhost:8081 review-service http://localhost:8082 We can configure the port for each microservice using server.port property in application.yml file.\nLet\u0026rsquo;s deep dive into the automation of build and deployment of these microservices.\nDockerfile Dockerfile is essentially a set of instruction describing how to build a Docker image.\nWe\u0026rsquo;ve created a Dockerfile in our product-service project to dockerize our microservice:-\n$springboot-microservices/product-service/Dockerfile FROM adoptopenjdk/openjdk11:alpine-jre ARG APP_NAME=\u0026#34;product-service\u0026#34; ARG APP_VERSION=\u0026#34;0.0.1\u0026#34; ARG JAR_FILE=\u0026#34;/build/libs/${APP_NAME}-${APP_VERSION}.jar\u0026#34; COPY ${JAR_FILE} app.jar ENTRYPOINT [\u0026#34;java\u0026#34;,\u0026#34;-jar\u0026#34;, \u0026#34;app.jar\u0026#34;] Let\u0026rsquo;s understand the meaning of each instruction:-\nFROM instructs to build Docker image on top of base image, in this case use OpenJDK 11 image ARG is used to define variables, which can be passed as arguments at runtime to build docker image, in this case defined three variables - APP_NAME, APP_VERSION, and JAR_FILE.\nThese variables can be used in other variables and instructions, in this case JAR_FILE is used to provide Spring boot jar file path and used in COPY instruction COPY allows us to copy a file into the docker image, in this case the application JAR file ENTRYPOINT describes execution command to start a docker container, in this case execute the JAR to run a spring boot application How to run this Dockerfile? Keep Reading\u0026hellip;\nInstall Docker Desktop You need docker command tool to build and run docker image from Dockerfile and for this, you need to install Docker Desktop.\nTo install Docker Desktop on macOS using Homebrew package manager:-\nbrew install --cask docker Once the Docker Desktop is installed, you are ready to execute docker commands. Let\u0026rsquo;s test it:-\n$any-path % docker --version Docker version 20.10.8, build 3967b7d Note: Make sure that Docker Desktop is running in the background in your local machine otherwise docker commands result into error\nBuild and Run Docker Image To build a docker image for microservice product-service, Go to the location, where you have Dockerfile and run following command:- $springboot-microservices/product-service % docker build -t com.example/product-service . [+] Building 15.4s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile =\u0026gt; =\u0026gt; transferring dockerfile: 37B =\u0026gt; [internal] load .dockerignore =\u0026gt; =\u0026gt; transferring context: 2B =\u0026gt; [internal] load metadata for docker.io/adoptopenjdk/openjdk11:alpine-jre =\u0026gt; [auth] adoptopenjdk/openjdk11:pull token for registry-1.docker.io =\u0026gt; [internal] load build context =\u0026gt; =\u0026gt; transferring context: 115B =\u0026gt; [1/2] FROM docker.io/adoptopenjdk/openjdk11:alpine-jre@sha256:f66b966de21575a3d2b589605c8284c8ad8e06c0b9ed47d24fa1754885f461ad =\u0026gt; [2/2] COPY /build/libs/product-service-0.0.1.jar app.jar =\u0026gt; exporting to image =\u0026gt; =\u0026gt; exporting layers =\u0026gt; =\u0026gt; writing image sha256:6da82c22c380fd4849949a8cf79b3708b5c1925ac9937e81344e30451626faef =\u0026gt; =\u0026gt; naming to com.example/product-service To run the docker container from image created in step 1, run following command:- $any-path % docker run -d -p 8081:8081 -e \u0026#34;SPRING_PROFILES_ACTIVE=dev\u0026#34; com.example/product-service:latest Similar steps can be performed for review-service microservice as well to build and run com.example/review-service image.\nPush Docker Image to Docker Hub Though we can build and run the Docker image from our local machine. It is a good practice to maintain all our Docker images in centralized repository like DockerHub similar to how we manage our source code in GitHub or BitBucket repository.\nNext we are going to push the docker image to remote DockerHub repository by following these steps:-\nSign up and create a free DockerHub account for personal use, if you don\u0026rsquo;t have one. You will get a unique DockerId for your account for e.g. aklahoti Next, you login to your DockerHub account and create a repository with name and description for e.g. product-service At this stage, our account and repository is created. We\u0026rsquo;re going to push the images to this remote repository using docker command that means our Docker Desktop should be running in our local machine. How to authenticate before pushing to remote repository? Go to Docker Desktop -\u0026gt; Images -\u0026gt; Remote Repositories, and Sign in with DockerId and password of your DockerHub account. Now all docker push commands will be authenticated automatically. Alternatively you can use following command to login, it will prompt for username and password:- $any-path % docker login All set! Run the command to tag your local docker image to remote docker image:- $any-path % docker tag com.example/product-service:latest aklahoti/product-service/0.0.1 Now run the command to push to remote docker image to DockerHub Repository:- $any-path % docker image push aklahoti/product-service:0.0.1 The push refers to repository [docker.io/aklahoti/product-service] 8feb5df7f49a: Layer already exists 1a94ead4570f: Layer already exists b59055dc22e8: Layer already exists e2eb06d8af82: Layer already exists 0.0.1: digest: sha256:de5d73282c0ed327f1c054be90fd5e52938f65b6cb84e4e2d696b94e1a89f102 size: 1163 Please note that at this point, we have two docker images of our product-service microservice. one is com.example/product-service:latest in our local machine and another one is aklahoti/product-service/0.0.1 in remote DockerHub repository. We\u0026rsquo;ve kept different names for these two images to differentiate between local vs remote.\nDocker Desktop running in your local machine in well aware of these two docker images so you can use them alternatively. If you use remote docker image, Docker Desktop pull the image from DockerHub automatically. Similar steps can be performed for review-service microservice as well.\ndocker-compose.yaml Docker Compose is quite useful to build and run docker images of multiple microservices using single command.\nLet\u0026rsquo;s create a Docker Compose configuration file docker-compose.yaml in the root directory $springboot-microservices to build and run our both microservices product-service and review-service using a single command:-\n$springboot-microservices/docker-compose.yaml version: \u0026#34;3.8\u0026#34; services: product-service: image: com.example/product-service:latest container_name: product-service build: context: ./product-service args: - APP_NAME=product-service - APP_VERSION=0.0.1 environment: SPRING_PROFILES_ACTIVE: dev expose: - 8081 ports: - 8081:8081 review-service: image: com.example/review-service:latest container_name: review-service build: context: ./review-service args: - APP_NAME=review-service - APP_VERSION=0.0.1 environment: SPRING_PROFILES_ACTIVE: dev expose: - 8082 ports: - 8082:8082 You can use remote image aklahoti/product-service/0.0.1 instead of local image com.example/product-service:latest in docker-compose.yaml as well.\nOnce Docker Compose configuration is in place, You can build and run the docker images of both the microservices using single command:-\n$springboot-microservices % docker-compose up You will see that images for both the microservices are created with name com.example/product-service and com.example/review-service and docker containers are up and running (showing IN USE) for both the microservices at port 8081 and 8082 respectively in Docker Desktop\u0026rsquo;s images menu like this:-\nDocker Desktop\nYou can access the running Docker containers from Docker Desktop\u0026rsquo;s Containers/Apps menu.\nTo bring all running docker containers of microservices down:-\n$springboot-microservices % docker-compose down build.gradle Next, we are going the build and run the docker image by executing a Gradle task.\nGradle Plugin for Docker For this, we are going to use Palantir Gradle Docker plugin which expose useful gradle tasks like docker to build docker image and dockerRun to run it.\nplugins { id \u0026#39;com.palantir.docker\u0026#39; version \u0026#39;0.26.0\u0026#39; id \u0026#39;com.palantir.docker-run\u0026#39; version \u0026#39;0.26.0\u0026#39; } Build Docker image using Gradle We provide additional configuration to docker gradle task to describe, how to build docker image:-\ngroup = \u0026#39;com.example\u0026#39; String imageName = \u0026#34;${project.group}/${project.name}\u0026#34; docker { dockerfile project.file(\u0026#39;Dockerfile\u0026#39;) name imageName files bootJar.archiveFile.get() buildArgs([\u0026#39;JAR_FILE\u0026#39;: \u0026#34;${bootJar.archiveFileName.get()}\u0026#34;]) } Let\u0026rsquo;s understand the configuration:-\nimageName is a variable to be used at multiple places, in this case define the name of the docker image i.e. \u0026ldquo;com.example/product-service\u0026rdquo; docker is the gradle task name, for which we are proving configuration dockerfile is the Dockerfile to use for building the image; defaults to project.file(\u0026lsquo;Dockerfile\u0026rsquo;) and must be a file object files is a list of files to be included in the Docker build context. The specified files are used in COPY instructions of Dockerfile. We are referencing bootJar.archiveFile.get() to get the build jar file. buildArgs are arguments to be passed to Dockerfile. We are passing JAR_FILE argument to be used in COPY instruction of Dockerfile Execute the gradle task to build the image:-\n$springboot-microservices/product-service % gradle docker Note: Make sure that Docker Desktop is running in the background in your local machine otherwise it will complain that docker daemon is not running.\nRun Docker image using Gradle We provide additional configuration to dockerRun gradle task to describe, how to run docker container from image:-\nString imageName = \u0026#34;${project.group}/${project.name}\u0026#34; dockerRun { name \u0026#34;${project.name}-container\u0026#34; image imageName ports \u0026#39;8081:8081\u0026#39; env \u0026#39;SPRING_PROFILES_ACTIVE\u0026#39;: \u0026#39;dev\u0026#39; daemonize true clean true } Let\u0026rsquo;s understand the configuration:-\ndockerRun is the gradle task name, for which we are proving configuration name is the name to use for the docker container, in this case \u0026ldquo;product-service-container\u0026rdquo; image is the docker image to use to build the docker container, in this case \u0026ldquo;com.example/product-service\u0026rdquo; ports configuration is local:container, for e.g. 8081:8081 means docker container will run on port 8081 and it binds to port 8081 of your local machine. In simple words, you will able to access docker container http://product-service:8081 from your local machine http://localhost:8081 env is environment variables to be passed to docker container daemonize defaults to true to daemonize the container after starting clean default to false. It is good to set clean to true while testing in local machine, which automatically delete the container when you stop it. Execute the gradle task to run docker container from the image:-\n$springboot-microservices/product-service % gradle dockerRun Note: Make sure that Docker Desktop is running in the background in your local machine otherwise it will complain that docker daemon is not running.\nbuild.gradle A typical build.gradle with docker plugins and configurations looks like this:-\nplugins { id \u0026#39;org.springframework.boot\u0026#39; version \u0026#39;2.5.0\u0026#39; id \u0026#39;io.spring.dependency-management\u0026#39; version \u0026#39;1.0.11.RELEASE\u0026#39; id \u0026#39;java\u0026#39; id \u0026#39;com.palantir.docker\u0026#39; version \u0026#39;0.26.0\u0026#39; id \u0026#39;com.palantir.docker-run\u0026#39; version \u0026#39;0.26.0\u0026#39; } group = \u0026#39;com.example\u0026#39; version = \u0026#39;0.0.1\u0026#39; sourceCompatibility = \u0026#39;11\u0026#39; repositories { mavenCentral() } ext { set(\u0026#39;springCloudVersion\u0026#39;, \u0026#34;2020.0.3\u0026#34;) } dependencies { implementation \u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39; implementation \u0026#39;org.springframework.cloud:spring-cloud-starter-openfeign\u0026#39; implementation \u0026#39;org.springdoc:springdoc-openapi-ui:latest.release\u0026#39; implementation \u0026#39;commons-io:commons-io:2.6\u0026#39; implementation project(\u0026#39;:common-library\u0026#39;) testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; testImplementation \u0026#39;org.springframework.cloud:spring-cloud-starter-contract-stub-runner\u0026#39; compileOnly \u0026#39;org.projectlombok:lombok:1.18.20\u0026#39; annotationProcessor \u0026#39;org.projectlombok:lombok:1.18.20\u0026#39; } dependencyManagement { imports { mavenBom \u0026#34;org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\u0026#34; } } test { useJUnitPlatform() } String imageName = \u0026#34;${project.group}/${project.name}\u0026#34; docker { dockerfile file(\u0026#39;Dockerfile\u0026#39;) name imageName files bootJar.archiveFile.get() buildArgs([\u0026#39;JAR_FILE\u0026#39;: \u0026#34;${bootJar.archiveFileName.get()}\u0026#34;]) } dockerRun { name \u0026#34;${project.name}-container\u0026#34; image imageName ports \u0026#39;8081:8081\u0026#39; env \u0026#39;SPRING_PROFILES_ACTIVE\u0026#39;: \u0026#39;dev\u0026#39; daemonize true clean false } Jenkinsfile Jenkinsfile is essentially defines the automation pipeline with various stages such as build, test, dockerize, run docker container etc. It is used by Jenkins to trigger a job as per the defined pipeline.\nLet\u0026rsquo;s create Jenkinsfile in the root of product-service project:-\n$springboot-microservices/product-service/Jenkinsfile pipeline { agent any triggers { pollSCM \u0026#39;* * * * *\u0026#39; } stages { stage(\u0026#39;Build\u0026#39;) { steps { sh \u0026#39;gradle assemble\u0026#39; } } stage(\u0026#39;Test\u0026#39;) { steps { sh \u0026#39;gradle test\u0026#39; } } stage(\u0026#39;Build Docker Image\u0026#39;) { steps { sh \u0026#39;gradle docker\u0026#39; } } stage(\u0026#39;Run Docker Image\u0026#39;) { steps { sh \u0026#39;gradle dockerRun\u0026#39; } } } } Please note that this is a Jenkinsfile to run jenkins job in your local machine. The production version may have few more and complex stages for e.g. you would be pushing the docker images to docker hub of your organization.\nInstall Jenkins You should install Jenkins to run Jenkinsfile in your local machine.\nTo install Jenkins on macOS using Homebrew package manager:-\nbrew install jenkins Start \u0026amp; Stop Jenkins Using Homebrew To start, stop, restart, and upgrade Jenkins using Homebrew:-\nbrew services start jenkins brew services stop jenkins brew services restart jenkins brew upgrade jenkins Using Command Line To start jenkins from command line:-\njenkins To stop jenkins running from command line, press CTRL+C\nUsing browser URL By default, jenkins runs at http://localhost:8080\nTo stop, restart and reload jenkins using URL http://localhost:8080/[command] where [command] can be:-\nexit restart reload Create Jenkins pipeline from Jenkinsfile Once you install and start Jenkins successfully. You should be able to access Jenkins from browser http://localhost:8080. It will ask you to provide admin username and password. It will also ask you to install recommended plugins. Just finish all these steps.\nYou source code should be checked into an accessible source code repository such as github. Please create a free account and check in your code to a repository if you don\u0026rsquo;t have one. This is a prerequisite to create a Jenkins pipeline.\nAll set! Now is the time to create Jenkins pipeline. Follow these steps:-\nGo to the Jenkins Dashboard Click on New Item from menu Enter an item name for e.g. microservice-pipeline and select Pipeline from options. Click OK. You will see a dialogue with four tabs: General, Build Triggers, Advance Project Options, and Pipeline. You can skip General, Build Triggers and Advance Project Options tabs. No configuration required. Pipeline tab: Choose Definition: Pipeline Script from SCM SCM: Git Repositories/Repository URL: Give repository name for e.g. https://github.com/ashishlahoti/springboot-microservices Repositories/Credentials: Give username and password to access the repository Branched to build/Branch Specifier: Give branch name from where you want jenkins to fetch Jenkinsfile for e.g. */main Script path: Give path of Jenkinsfile in the repository for e.g. product-discovery/Jenkinsfile Click on Save. Congrats your pipeline is created. Create Jenkins pipeline from Jenkinsfile\nBuild Jenkins pipeline Once you create a pipeline. You see it on Jenkins dashboard. Click on it. Alternatively go to http://localhost:8080/job/microservice-pipeline/\nClick on the Build Now option to run Jenkins pipeline. You will see your build pipeline and stages like this:-\nNote: Make sure that Docker Desktop is running in the background in your local machine otherwise build will fail and say that docker daemon is not running.\nBuild stages of Jenkins pipeline\nYou will see that once the build is successful, docker container for microservices will be running in your local machine. You can verify them using Docker Desktop.\nConclusion We looked at the typical project setup of spring boot based microservices with best practices. Each microservice project has their own set of build.gradle, Dockerfile, and Jenkinsfile for DevOps CI/CD pipelines. We learned how to automate and test the deployment in our local machine using Gradle, Git, Docker and Jenkins.\nDownload the complete source code for this example from github/springboot-microservices\n","permalink":"https://codingnconcepts.com/spring-boot/deployment-of-microservices-using-docker-and-jenkins/","tags":["Spring Boot API","Docker","Jenkins"],"title":"Deployment of Spring Boot Microservices using Docker and Jenkins"},{"categories":["Spring Boot"],"contents":"Documentation is essential when you are building RESTFul APIs in spring boot application. In this tutorial, we\u0026rsquo;ll learn how to configure the documentation using OpenAPI in Spring Boot.\nOverview SpringDoc OpenAPI generates the API documentation for your Spring Boot APIs by examining the application at runtime to infer API semantics based on spring configurations, class structure and various annotations.\nSpring Boot automatically generates the API documentation based on OpenAPI specification, when it finds springdoc-openapi-ui dependency in the classpath. It also use swagger-ui library internally to generate Swagger UI.\nOnce the configuration is successful, You should be able to see the documentation at following URL:-\nDocumentation URL Swagger UI Page http://\u0026lt;server\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;context-path\u0026gt;/swagger-ui/index.html?configUrl=/v3/api-docs/swagger-config OpenAPI Json Docs http://\u0026lt;server\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;context-path\u0026gt;/v3/api-docs Where,\n\u0026lt;server\u0026gt;: The server name or IP \u0026lt;port\u0026gt;: The server port \u0026lt;context-path\u0026gt;: The context path of the application Project Setup For initial setup of your Spring Boot project, you should use Spring Initializr. Choose the Spring Web dependency.\nMaven Project You can click the below link to generate a maven project with pre-selected dependencies:-\nhttps://start.spring.io/#!type=maven-project\u0026amp;language=java\u0026amp;platformVersion=2.5.1.RELEASE\u0026amp;packaging=jar\u0026amp;jvmVersion=11\u0026amp;groupId=com.example\u0026amp;artifactId=demo\u0026amp;name=demo\u0026amp;description=Demo%20project%20for%20Spring%20Boot\u0026amp;packageName=com.example.demo\u0026amp;dependencies=web\nthen add the springdoc-openapi-ui maven dependency to pom.xml which looks like this:-\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.1\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;demo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;demo\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;Demo project for Spring Boot\u0026lt;/description\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;11\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springdoc\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springdoc-openapi-ui\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; Gradle Project You can click the below link to generate a gradle project with pre-selected dependencies:-\nhttps://start.spring.io/#!type=gradle-project\u0026amp;language=java\u0026amp;platformVersion=2.5.1.RELEASE\u0026amp;packaging=jar\u0026amp;jvmVersion=11\u0026amp;groupId=com.example\u0026amp;artifactId=demo\u0026amp;name=demo\u0026amp;description=Demo%20project%20for%20Spring%20Boot\u0026amp;packageName=com.example.demo\u0026amp;dependencies=web\nthen add the org.springdoc:springdoc-openapi-ui gradle dependency to build.gradle which looks like this:-\nplugins { id \u0026#39;org.springframework.boot\u0026#39; version \u0026#39;2.5.1\u0026#39; id \u0026#39;io.spring.dependency-management\u0026#39; version \u0026#39;1.0.11.RELEASE\u0026#39; id \u0026#39;java\u0026#39; } group = \u0026#39;com.example\u0026#39; version = \u0026#39;0.0.1-SNAPSHOT\u0026#39; sourceCompatibility = \u0026#39;11\u0026#39; repositories { mavenCentral() } dependencies { implementation \u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39; implementation \u0026#39;org.springdoc:springdoc-openapi-ui:latest.release\u0026#39; testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; } test { useJUnitPlatform() } Define Documentation Properties Instead of hard coding values in OpenAPI configuration we are going to define some documentation properties in configuration file. We\u0026rsquo;re using application.yml here. You may also use application.properties file.\napplication.yml api: info: title: Review Service API description: API to fetch review details for product version: 1.0.0 terms-of-service: http://example.com/terms/ contact: name: Review Service API Team email: apiteam@example.com url: http://example.com/team license: name: Apache 2.0 url: http://www.apache.org/licenses/LICENSE-2.0.html springdoc: swagger-ui: enabled: true Note that we\u0026rsquo;ve set the springdoc.swagger-ui.enabled property to true to enable the Swagger UI in our sprint boot project. If you don\u0026rsquo;t specify the property then default value of this property is true.\nLet\u0026rsquo;s use these properties to define OpenAPIConfig class file.\nDefine OpenAPIConfig Let\u0026rsquo;s create OpenApiConfig configuration class file to customize the OpenAPI and Swagger UI configuration in our Spring boot project.\n1@Configuration 2@ConditionalOnProperty(name = \u0026#34;springdoc.swagger-ui.enabled\u0026#34;, havingValue = \u0026#34;true\u0026#34;, matchIfMissing = true) 3public class OpenApiConfig { 4 5 private static final String BEARER_FORMAT = \u0026#34;JWT\u0026#34;; 6 private static final String SCHEME = \u0026#34;Bearer\u0026#34;; 7 private static final String SECURITY_SCHEME_NAME = \u0026#34;Security Scheme\u0026#34;; 8 9 @Value(\u0026#34;${api.info.title: api.info.title}\u0026#34;) 10 private String title; 11 12 @Value(\u0026#34;${api.info.description: api.info.description}\u0026#34;) 13 private String description; 14 15 @Value(\u0026#34;${api.info.version: api.info.version}\u0026#34;) 16 private String version; 17 18 @Value(\u0026#34;${api.info.term-of-service: api.info.terms-of-service}\u0026#34;) 19 private String termOfService; 20 21 @Value(\u0026#34;${api.info.contact.name: api.info.contact.name}\u0026#34;) 22 private String contactName; 23 24 @Value(\u0026#34;${api.info.contact.email: api.info.contact.email}\u0026#34;) 25 private String contactEmail; 26 27 @Value(\u0026#34;${api.info.contact.url: api.info.contact.url}\u0026#34;) 28 private String contactUrl; 29 30 @Value(\u0026#34;${api.info.license.name: api.info.license.name}\u0026#34;) 31 private String licenseName; 32 33 @Value(\u0026#34;${api.info.license.url: api.info.license.url}\u0026#34;) 34 private String licenseUrl; 35 36 @Bean 37 public OpenAPI api() { 38 return new OpenAPI() 39 .schemaRequirement(SECURITY_SCHEME_NAME, getSecurityScheme()) 40 .security(getSecurityRequirement()) 41 .info(info()); 42 } 43 44 private Info info() { 45 return new Info() 46 .title(title) 47 .description(description) 48 .version(version) 49 .contact(new Contact().name(contactName).email(contactEmail).url(contactUrl)) 50 .license(new License().name(licenseName).url(licenseUrl)); 51 } 52 53 private List\u0026lt;SecurityRequirement\u0026gt; getSecurityRequirement() { 54 SecurityRequirement securityRequirement = new SecurityRequirement(); 55 securityRequirement.addList(SECURITY_SCHEME_NAME); 56 return List.of(securityRequirement); 57 } 58 59 private SecurityScheme getSecurityScheme() { 60 SecurityScheme securityScheme = new SecurityScheme(); 61 securityScheme.bearerFormat(BEARER_FORMAT); 62 securityScheme.type(SecurityScheme.Type.HTTP); 63 securityScheme.in(SecurityScheme.In.HEADER); 64 securityScheme.scheme(SCHEME); 65 return securityScheme; 66 } 67} Note some important points of our OpenAPIConfig class file:\n@Configuration annotation is used to auto scan this class file. @ConditionalOnProperty annotation is used to load the OpenAPIConfig bean in spring context based on springdoc.swagger-ui.enabled property flag. Security Scheme OpenAPIConfig Line 39: .schemaRequirement(SECURITY_SCHEME_NAME, getSecurityScheme()) is used to facilitate security requirement.\n(You can comment this line if you don\u0026rsquo;t have such requirement.)\nThis creates an button in Swagger UI which opens a dialog Available authorizations on click, where you can input access-token for e.g. JWT Token and click Authorize. Once submit, token is passed in Authorization header of each API request, which you execute from Swagger UI.\nVerify Swagger UI That\u0026rsquo;s it. Now define some REST Controllers in your spring boot project to expose API endpoints and start your spring boot application. You will be able to see Swagger UI something like this:-\nSwagger UI at landing page By default, Swagger UI is located at http://\u0026lt;server\u0026gt;:\u0026lt;port\u0026gt;/swagger-ui/index.html?configUrl=/v3/api-docs/swagger-config if no server.servlet.context-path is specified.\nIf you want to load Swagger UI at landing page of your microservice i.e. http://\u0026lt;server\u0026gt;:\u0026lt;port\u0026gt; then use this property:-\nspringdoc: swagger-ui: path: / # Redirect http://localhost:8080 to http://localhost:8080/swagger-ui/index.html?configUrl=/v3/api-docs/swagger-config Turn off Swagger UI in Production Swagger UI is very convenient for development purpose. However we generally turn it off in production environment due to security concerns.\nYou can disable the Swagger UI in production by setting property springdoc.swagger-ui.enabled to false.\napplication-prod.properties springdoc.swagger-ui.enabled = true application-prod.yml springdoc: swagger-ui: enabled: true command-line parameter $ java -jar -Dspringdoc.swagger-ui.enabled=false spring-boot-app-1.0.jar or $ java -jar spring-boot-app-1.0.jar --springdoc.swagger-ui.enabled=false Download the complete source code from github\n","permalink":"https://codingnconcepts.com/spring-boot/configure-springdoc-openapi/","tags":["Spring Boot API","Swagger"],"title":"RESTful API documentation using OpenAPI"},{"categories":["Spring Boot"],"contents":"In this quick tutorial, we\u0026rsquo;ll learn how to change default Feign Client implementation to ApacheHttpClient or OkHttpClient in Spring Boot application.\nWhen you configure a Feign Client in Spring Boot application to execute outbound API calls, then it uses HttpClient under the covers by default which can be changed to ApacheHttpClient or OkHttpClient\nChange to ApacheHttpClient You can change the default Feign Client implementation to ApacheHttpClient by:-\nSetting feign.httpclient.enabled property to true, and Adding io.github.openfeign:feign-httpclient dependency in the project classpath Property application.yml feign.httpclient.enabled: true Maven pom.xml Click on the below link to get initial pom.xml which includes web and cloud-feign dependencies:-\nhttps://start.spring.io/#!type=maven-project\u0026amp;language=java\u0026amp;platformVersion=2.5.1.RELEASE\u0026amp;packaging=jar\u0026amp;jvmVersion=1.8\u0026amp;groupId=com.example\u0026amp;artifactId=api\u0026amp;name=api\u0026amp;description=Create%20Feign%20Client%20to%20consume%20RESTFul%20APIs\u0026amp;packageName=com.example.api\u0026amp;dependencies=cloud-feign,web\nand then add the feign-httpclient dependency:-\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.github.openfeign\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;feign-httpclient\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Gradle build.gradle plugins { id \u0026#39;org.springframework.boot\u0026#39; version \u0026#39;2.5.1\u0026#39; id \u0026#39;io.spring.dependency-management\u0026#39; version \u0026#39;1.0.11.RELEASE\u0026#39; id \u0026#39;java\u0026#39; } group = \u0026#39;com.example\u0026#39; version = \u0026#39;0.0.1-SNAPSHOT\u0026#39; sourceCompatibility = \u0026#39;11\u0026#39; repositories { mavenCentral() } ext { set(\u0026#39;springCloudVersion\u0026#39;, \u0026#34;2020.0.3\u0026#34;) } dependencies { implementation \u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39; implementation \u0026#39;org.springframework.cloud:spring-cloud-starter-openfeign\u0026#39; implementation \u0026#39;io.github.openfeign:feign-httpclient\u0026#39; testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; } dependencyManagement { imports { mavenBom \u0026#34;org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\u0026#34; } } test { useJUnitPlatform() } Spring Boot takes care of the dependency version of feign-httpclient. You can also get it from maven\nYou can further customize the Apache HTTP Client by providing a bean of type org.apache.http.impl.client.CloseableHttpClient\n@Configuration public class FeignClientConfig { @Bean public CloseableHttpClient feignClient() { return HttpClients.createDefault(); } } Please note the @Configuration annotation on the FeignClientConfig, which makes it global configuration and applied to all FeignClient in your spring boot application.\nDo not annotate with @Configuration if you want to use it for specific FeignClient, Custom feignClient bean doesn\u0026rsquo;t work without @Configuration, You need to provide custom Feign.Builder bean instead\npublic class FeignClientConfig { @Bean public Feign.Builder feignBuilder() { return Feign.builder() .retryer(Retryer.NEVER_RETRY) .client(new ApacheHttpClient()); } } Change to ApacheHttp5Client You can change the default Feign Client implementation to ApacheHttp5Client (latest version of ApacheHttpClient) by:-\nSetting feign.httpclient.enabled property to true, and Adding io.github.openfeign:feign-hc5 dependency in the project classpath Property application.yml feign.httpclient.enabled: true Maven pom.xml Click on the below link to get initial pom.xml which includes web and cloud-feign dependencies:-\nhttps://start.spring.io/#!type=maven-project\u0026amp;language=java\u0026amp;platformVersion=2.5.1.RELEASE\u0026amp;packaging=jar\u0026amp;jvmVersion=1.8\u0026amp;groupId=com.example\u0026amp;artifactId=api\u0026amp;name=api\u0026amp;description=Create%20Feign%20Client%20to%20consume%20RESTFul%20APIs\u0026amp;packageName=com.example.api\u0026amp;dependencies=cloud-feign,web\nand then add the feign-hc5 dependency:-\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.github.openfeign\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;feign-hc5\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Gradle build.gradle plugins { id \u0026#39;org.springframework.boot\u0026#39; version \u0026#39;2.5.1\u0026#39; id \u0026#39;io.spring.dependency-management\u0026#39; version \u0026#39;1.0.11.RELEASE\u0026#39; id \u0026#39;java\u0026#39; } group = \u0026#39;com.example\u0026#39; version = \u0026#39;0.0.1-SNAPSHOT\u0026#39; sourceCompatibility = \u0026#39;11\u0026#39; repositories { mavenCentral() } ext { set(\u0026#39;springCloudVersion\u0026#39;, \u0026#34;2020.0.3\u0026#34;) } dependencies { implementation \u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39; implementation \u0026#39;org.springframework.cloud:spring-cloud-starter-openfeign\u0026#39; implementation \u0026#39;io.github.openfeign:feign-hc5\u0026#39; testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; } dependencyManagement { imports { mavenBom \u0026#34;org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\u0026#34; } } test { useJUnitPlatform() } Spring Boot takes care of the dependency version of feign-hc5. You can also get it from maven\nYou can further customize the Apache HTTP Client by providing a bean of type org.apache.hc.client5.http.impl.classic.CloseableHttpClient\n@Configuration public class FeignClientConfig { @Bean public CloseableHttpClient feignClient() { return HttpClients.createDefault(); } } Please note the @Configuration annotation on the FeignClientConfig, which makes it global configuration and applied to all FeignClient in your spring boot application.\nDo not annotate with @Configuration if you want to use it for specific FeignClient, Custom feignClient bean doesn\u0026rsquo;t work without @Configuration, You need to provide custom Feign.Builder bean instead\npublic class FeignClientConfig { @Bean public Feign.Builder feignBuilder() { return Feign.builder() .retryer(Retryer.NEVER_RETRY) .client(new ApacheHttp5Client()); } } Change to OkHttpClient You can change the default Feign Client implementation to OkHttpClient by:-\nSetting feign.okhttp.enabled property to true, and Adding io.github.openfeign:feign-okhttp dependency in the project classpath Property application.yml feign.okhttp.enabled: true Maven pom.xml Click on the below link to get initial pom.xml which includes web and cloud-feign dependencies:-\nhttps://start.spring.io/#!type=maven-project\u0026amp;language=java\u0026amp;platformVersion=2.5.1.RELEASE\u0026amp;packaging=jar\u0026amp;jvmVersion=1.8\u0026amp;groupId=com.example\u0026amp;artifactId=api\u0026amp;name=api\u0026amp;description=Create%20Feign%20Client%20to%20consume%20RESTFul%20APIs\u0026amp;packageName=com.example.api\u0026amp;dependencies=cloud-feign,web\nand then add the feign-okhttp dependency:-\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.github.openfeign\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;feign-okhttp\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Gradle build.gradle plugins { id \u0026#39;org.springframework.boot\u0026#39; version \u0026#39;2.5.1\u0026#39; id \u0026#39;io.spring.dependency-management\u0026#39; version \u0026#39;1.0.11.RELEASE\u0026#39; id \u0026#39;java\u0026#39; } group = \u0026#39;com.example\u0026#39; version = \u0026#39;0.0.1-SNAPSHOT\u0026#39; sourceCompatibility = \u0026#39;11\u0026#39; repositories { mavenCentral() } ext { set(\u0026#39;springCloudVersion\u0026#39;, \u0026#34;2020.0.3\u0026#34;) } dependencies { implementation \u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39; implementation \u0026#39;org.springframework.cloud:spring-cloud-starter-openfeign\u0026#39; implementation \u0026#39;io.github.openfeign:feign-okhttp\u0026#39; testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; } dependencyManagement { imports { mavenBom \u0026#34;org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\u0026#34; } } test { useJUnitPlatform() } Spring Boot takes care of the dependency version of feign-okhttp. You can also get it from maven\nYou can further customize the Ok HTTP Client by providing a bean of type okhttp3.OkHttpClient\n@Configuration public class FeignClientConfiguration { @Bean public OkHttpClient feignClient() { return new OkHttpClient(); } } Conclusion If we create both @Configuration bean and configuration properties, configuration properties will win. It will override @Configuration values. But if you want to change the priority to @Configuration, you can change feign.client.default-to-properties to false.\nPlease refer to Spring Cloud OpenFeign official documentation for more details.\n","permalink":"https://codingnconcepts.com/spring-boot/change-default-feign-client-implementation/","tags":["Spring Boot API"],"title":"Change Default Feign Client implementation in Spring Boot"},{"categories":["Spring Boot"],"contents":"In this quick tutorial, we\u0026rsquo;ll configure embedded Undertow server by replacing it with default Tomcat server in Spring Boot web application.\nAdd Undertow Dependency We need to do two things here:-\nExclude default dependency spring-boot-starter-tomcat added in spring-boot-start-web Add spring-boot-starter-undertow dependency. pom.xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-tomcat\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-undertow\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; build.gradle dependencies { implementation(\u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39;) { exclude group: \u0026#39;org.springframework.boot\u0026#39;, module:\u0026#39;spring-boot-starter-tomcat\u0026#39; } implementation \u0026#39;org.springframework.boot:spring-boot-starter-undertow\u0026#39; } That\u0026rsquo;s it. You have replaced tomcat with Undertow server.\nApplication Startup Logs When you start spring boot application, You will in the logs that Undertow is serving your web application now:-\nINFO c.e.demo.SpringBootDemoApplication : Starting SpringBootDemoApplication using Java 11.0.10 on Ashishs-MBP with PID 5166 (/Users/ashl/IdeaProjects/springboot-examples/springboot-config/build/classes/java/main started by ashl in /Users/ashl/IdeaProjects/springboot-examples/springboot-config) DEBUG c.e.demo.SpringBootDemoApplication : Running with Spring Boot v2.5.0, Spring v5.3.7 INFO c.e.demo.SpringBootDemoApplication : No active profile set, falling back to default profiles: default WARN io.undertow.websockets.jsr : UT026010: Buffer pool was not set on WebSocketDeploymentInfo, the default pool will be used INFO io.undertow.servlet : Initializing Spring embedded WebApplicationContext INFO w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 753 ms INFO io.undertow : starting server: Undertow - 2.2.7.Final INFO org.xnio : XNIO version 3.8.0.Final INFO org.xnio.nio : XNIO NIO Implementation Version 3.8.0.Final INFO org.jboss.threads : JBoss Threads version 3.1.0.Final INFO o.s.b.w.e.undertow.UndertowWebServer : Undertow started on port(s) 8080 (http) INFO c.e.demo.SpringBootDemoApplication : Started SpringBootDemoApplication in 1.858 seconds (JVM running for 2.21) Add Spring Boot Undertow Configuration Spring boot also provides Undertow specific configuration which you can configure from application.yml or application.properties file.\napplication.yml server: undertow: allow-encoded-slash: false # Whether the server should decode percent encoded slash characters. Enabling encoded slashes can have security implications due to different servers interpreting the slash differently. Only enable this if you have a legacy application that requires it. always-set-keep-alive: true # Whether the \u0026#39;Connection: keep-alive\u0026#39; header should be added to all responses, even if not required by the HTTP specification. buffer-size: # Size of each buffer. The default is derived from the maximum amount of memory that is available to the JVM. decode-url: true # Whether the URL should be decoded. When disabled, percent-encoded characters in the URL will be left as-is. direct-buffers: # Whether to allocate buffers outside the Java heap. The default is derived from the maximum amount of memory that is available to the JVM. eager-filter-init: true # Whether servlet filters should be initialized on startup. max-cookies: 200 # Maximum number of cookies that are allowed. This limit exists to prevent hash collision based DOS attacks. max-headers: 12 # Maximum number of headers that are allowed. This limit exists to prevent hash collision based DOS attacks. max-parameters: # Maximum number of query or path parameters that are allowed. This limit exists to prevent hash collision based DOS attacks. max-http-post-size: -1B # Maximum size of the HTTP post content. When the value is -1, the default, the size is unlimited. no-request-timeout: # Amount of time a connection can sit idle without processing a request, before it is closed by the server. preserve-path-on-forward: false # Whether to preserve the path of a request when it is forwarded. url-charset: UTF-8 # Charset used to decode URLs accesslog: enabled: false # Whether to enable the access log dir: # Undertow access log directory pattern: common # Format pattern for access logs rotate: true # Whether to enable access log rotation prefix: access_log. # Log file name prefix. suffix: log # Log file name suffix threads: io: # Number of I/O threads to create for the worker. The default is derived from the number of available processors. worker: # Number of worker threads. The default is 8 times the number of I/O threads. You can refer to the Spring Boot official documentation for full list of configuration.\n","permalink":"https://codingnconcepts.com/spring-boot/configure-embedded-undertow-server/","tags":["Spring Boot Basics","Undertow"],"title":"Configure embedded Undertow server in Spring Boot"},{"categories":["Spring Boot"],"contents":"In this tutorial, we\u0026rsquo;ll take a look at Conditional Annotations in Spring Boot with examples.\nOverview Spring Boot is opinionated. Spring Boot provides default (auto) configuration for a module when it finds related dependency in the classpath.\nFor Example, Spring Boot provides:-\ndefault embedded Tomcat server when it doesn\u0026rsquo;t see any server dependency or configuration in the classpath. You can change the default embedded tomcat server to Jetty or Undertow by just changing the dependency in the classpath. default HttpClient configuration when it find spring-cloud-starter-openfeign dependency in the classpath. You can change the default client to OkHttpClient or ApacheHttpClient by just changing the dependency in the classpath. default Jackson request and response mapping when it find spring-boot-starter-web dependency in the classpath default DataSource configuration when it find spring-boot-starter-data-jpa dependency in the classpath How does Spring Boot make this happen? Spring Boot does this magic using @Conditional annotation. Spring Boot heavily use @Conditional annotation to load default configurations and beans based on conditions.\nThese conditions can be anything like:-\navailability of dependency, resource, or class in the classpath property defined in application.yml or application.properties file, system property, environment variable java version, operating system, cloud platform, web application etc. How it is useful for us? Similar to how Spring Boot magically loads default configuration, we also sometime want to load beans and modules into Spring application context based on custom conditions. We can now define and apply these custom conditions using @Conditional annotation.\n@Conditional @Conditional annotation indicate that a component is only eligible for registration in spring context, when all the specified conditions are matched.\nThe @Conditional annotation may be used in any of the following three ways:\n1. As a Method Level Annotation We can use @Conditional on any method annotated with @Bean annotation to load that bean in Spring context if the condition is met.\n@Configuration class ConditionalBeanConfiguration { @Bean @Conditional(CustomCondition.class) //\u0026lt;- method level condition ConditionalBean conditionalBean(){ return new ConditionalBean(); }; } 2. As a Type (Class) Level Annotation We can use @Conditional on any class annotated with @Component, @Service, @Repository, @Controller, @RestController, or @Configuration annotation to load that class bean in Spring context if the condition is met.\n@Component @Conditional(CustomCondition.class) //\u0026lt;- class level condition class ConditionalComponent { } @Configuration with @Conditional If a @Configuration class is annotated with @Conditional, all the @Bean methods, @Import annotations, and @ComponentScan annotations associated with that class will be loaded only when the class level condition is met.\n@Configuration @Conditional(CustomCondition.class) //\u0026lt;- class level condition class ConditionalConfiguration { @Bean //\u0026lt;- will be loaded only if class level condition is met Bean bean(){ // Code for bean definition }; } 3. As a Meta Annotation We can use @Conditional as a meta annotation to create custom conditional annotations.\n@Target({ElementType.TYPE, ElementType.METHOD}) @Retention(RetentionPolicy.RUNTIME) @Conditional(CustomCondition.class) //\u0026lt;- meta annotation condition public @interface CustomConditionAnnotation { } Now we can use our custom @CustomConditionAnnotation annotation at method and class level instead of @Conditional annotation like this:-\n@Configuration class ConditionalBeanConfiguration { @Bean @CustomConditionAnnotation //\u0026lt;- custom annotation at method level ConditionalBean conditionalBean(){ return new ConditionalBean(); }; } @Component @CustomConditionAnnotation //\u0026lt;- custom annotation at class level class ConditionalComponent { } Custom Condition You see that in previous examples, we have used CustomCondition.class in @Conditional annotation which provides the logic for condition matching.\nLet\u0026rsquo;s create this custom condition by implementing Spring\u0026rsquo;s Condition class and provide the matching logic in matches method:-\npublic class CustomCondition implements Condition { @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) { return context.getEnvironment().getProperty(\u0026#34;custom.condition.enabled\u0026#34;, Boolean.class, false); } } application.yml custom.condition.enabled: true We see that CustomCondition matches when property custom.condition.enabled is set to true.\nCombine Conditions with Any Match We can combine multiple conditions such that it is matched if any one of the underlying condition is matched. In another words, combining the conditions with \u0026ldquo;OR\u0026rdquo; logical operator.\nCreate one more condition to combine We have already created one custom condition i.e. CustomCondition. Let\u0026rsquo;s quickly create one more custom condition i.e. AnotherCustomCondition:-\npublic class AnotherCustomCondition implements Condition { @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) { return context.getEnvironment().getProperty(\u0026#34;another-custom.condition.enabled\u0026#34;, Boolean.class, false); } } application.yml another-custom.condition.enabled: true We see that AnotherCustomCondition matches when property another-custom.condition.enabled is set to true.\nAnyNestedCondition Now let\u0026rsquo;s combine these two conditions CustomCondition and AnotherCustomCondition with any match by extending Spring\u0026rsquo;s AnyNestedCondition class:-\npublic class CombinedConditionsWithAnyMatch extends AnyNestedCondition { public CombinedConditionsWithAnyMatch() { super(ConfigurationPhase.PARSE_CONFIGURATION); // super(ConfigurationPhase.REGISTER_BEAN); } @Conditional(CustomCondition.class) static class OnCustomCondition {} @Conditional(AnotherCustomCondition.class) static class OnAnotherCustomCondition {} } ConfigurationPhase Check the ConfigurationPhase parameter passed into constructor. If you want to apply your combined condition to @Configuration class, use the value PARSE_CONFIGURATION. If you want to apply the condition to @Bean bean, use REGISTER_BEAN as shown in the example above. Spring Boot needs to make this distinction so it can apply the conditions at the right time during application context startup.\nApply Condition with Any Match Let\u0026rsquo;s apply this combined condition to a configuration class:-\n@Configuration @Conditional(CombinedConditionsWithAnyMatch.class) class ConditionalConfiguration { @Bean Bean bean(){ // TODO }; } This configuration will be loaded in Spring application context when any of the condition is true, means when any of the property is set to true:-\napplication.yml custom.condition.enabled: true another-custom.condition.enabled: false Combine Conditions with All Match We can combine multiple conditions such that it is matched only when all the conditions are matched. In another words, combining the conditions with \u0026ldquo;AND\u0026rdquo; logical operator.\nAllNestedConditions This time we combine the two conditions CustomCondition and AnotherCustomCondition with all match by extending Spring\u0026rsquo;s AllNestedConditions class:-\npublic class CombinedConditionsWithAllMatch extends AllNestedConditions { public CombinedConditionsWithAllMatch() { // super(ConfigurationPhase.PARSE_CONFIGURATION); super(ConfigurationPhase.REGISTER_BEAN); } @Conditional(CustomCondition.class) static class OnCustomCondition {} @Conditional(AnotherCustomCondition.class) static class OnAnotherCustomCondition {} } Apply Condition with All Match This time we passed ConfigurationPhase.REGISTER_BEAN into constructor as we want to apply the combined condition on @Bean bean like this:-\n@Configuration class ConditionalBeanConfiguration { @Bean @Conditional(CombinedConditionsWithAllMatch.class) //\u0026lt;- as method level annotation ConditionalBean conditionalBean(){ return new ConditionalBean(); }; } This bean will be loaded in Spring application context only when all of the conditions are true, means when all of the properties are set to true:-\napplication.yml custom.condition.enabled: true another-custom.condition.enabled: true Combine Conditions with None Match We can combine multiple conditions such that it is matched only when all the conditions are NOT matched by extending Spring\u0026rsquo;s NoneNestedCondition class:-\npublic class CombinedConditionsWithNoneMatch extends NoneNestedConditions { public CombinedConditionsWithNoneMatch() { super(ConfigurationPhase.PARSE_CONFIGURATION); // or super(ConfigurationPhase.REGISTER_BEAN); } @Conditional(CustomCondition.class) static class OnCustomCondition {} @Conditional(AnotherCustomCondition.class) static class OnAnotherCustomCondition {} } The combined condition will be matched with following properties:-\napplication.yml custom.condition.enabled: false another-custom.condition.enabled: false Predefined Conditional Annotations Spring Boot provides a set of predefined @ConditionalOn... annotations which are very handy. Let\u0026rsquo;s have a look at them:-\n@ConditionalOnProperty The @ConditionalOnProperty annotation is the most commonly used conditional annotation in Spring Boot. It allows to load classes or beans conditionally depending on a certain property:\n@Configuration @ConditionalOnProperty( value=\u0026#34;api.doc.enabled\u0026#34;, havingValue = \u0026#34;true\u0026#34;, matchIfMissing = true) class ApiDocConfig { // TODO } The ApiDocConfig is only loaded if the api.doc.enabled property is set to true. If the property is not set at all, it will still be loaded, because we have defined matchIfMissing = true. This way, we have created a config that is loaded by default until we set the property to false.\nA common use case is when you want to enable certain config in development environment but disable in production environment or vice versa:-\napplication-dev.yml api.doc.enabled: true application-prod.yml api.doc.enabled: false Another common use case is when you want to define a set of configuration files in a common project and use those configurations across many project. You can enable/disable them in specific project by setting the properties:-\nProject A -\u0026gt; application.yml api.doc.enabled: true Project B -\u0026gt; application.yml api.doc.enabled: false @ConditionalOnExpression If we have a more complex condition based on multiple properties, we can use @ConditionalOnExpression:-\n@Configuration @ConditionalOnExpression( \u0026#34;${api.doc.enabled:true} and \u0026#39;${spring.profile.active}\u0026#39;.equalsIgnoreCase(\u0026#39;DEV\u0026#39;)\u0026#34; ) class ApiDocConfig { // TODO } The ApiDocConfig is only loaded if property api.doc.enabled is set to true and spring.profile.active is equal to dev. By appending :true to the api.doc.enabled property, we tell Spring Boot to use true as a default value in the case the properties have not been set.\nWe can make full use of the Spring Expression Language in the expression.\n@ConditionalOnBean We might want to load a bean only if dependent bean is available in the application context:\n@Service @ConditionalOnBean(ApiDocConfig.class) class ApiDocService { // TODO } The ApiDocService is only loaded if there is a bean of class ApiDocConfig in the application context. This way we can define the dependencies of a bean on other beans.\n@ConditionalOnMissingBean Similarly, we can use @ConditionalOnMissingBean if we want to load a bean only if another bean doesn\u0026rsquo;t exist in the application context:\n@Configuration class DatabaseConfig { @Bean @ConditionalOnMissingBean DataSource dataSource() { return new InMemoryDataSource(); } } This examples loads the InMemoryDataSource into the application context if there is no other DataSource exist in application context. This is very similar to what Spring Boot does internally to provide an in-memory database in a test context.\n@ConditionalOnResource The @ConditionalOnResource is used when we want to load a bean depending upon the certain resource availability in the class path\n@Configuration @ConditionalOnResource(resources = \u0026#34;/logback.xml\u0026#34;) class LoggingConfig { // TODO } The LoggingConfig is only loaded when the logback.xml configuration file is available in the classpath. This way, we can create similar config classes that are only loaded if their respective configuration file is available.\nThe conditional annotations described above are the most common ones which we generally use in our Spring Boot Project. Spring Boot provides many other conditional annotations. They are, however, not as common and more suited for framework development rather than application development (Spring Boot uses some of them heavily behind the scene). So, let’s only have a brief look at them here.\n@ConditionalOnClass Load a bean only when the specified class is in the classpath. We can specify the class either by fully qualified name or value.\n@Service @ConditionalOnClass(name = \u0026#34;com.example.config.LoggingConfig\u0026#34;) class LoggingService { // Code runs when class in available in classpath } @Service @ConditionalOnClass(LoggingConfig.class) class LoggingService { // Code runs when class in available in classpath } Similarly we can use @ConditionalOnMissingClass annotation to load a bean only if specified class in NOT in the classpath.\n@ConditionalOnWebApplication Load a bean only when application is a web application. By default, any web application will match but it can be narrowed using the type attribute.\n@Configuration @ConditionalOnWebApplication class RunsOnAnyWebApplication { // Code runs on web application } @Configuration @ConditionalOnWebApplication(type = ConditionalOnWebApplication.Type.SERVLET) class RunsOnServletBasedWebApplication { // Code runs on Servlet based web application } Similarly we can use @ConditionalOnNotWebApplication annotation to load a bean only when the application context is a NOT a web application context.\n@ConditionalOnJava Load a bean only when application is running on a specified JVM version. By default, version equal and above will match but it can be changed using the range attribute\n@Configuration @ConditionalOnJava(JavaVersion.EIGHT) class RunsOnJavaEightAndAbove { // Code runs on Java-8 and above versions } @Configuration @ConditionalOnJava(value = JavaVersion.ELEVEN, range = ConditionalOnJava.Range.OLDER_THAN) class RunsOnBelowJavaEleven { // Code runs below Java-11. // Code doesn\u0026#39;t run on Java-11 and above versions } @ConditionalOnCloudPlatform Load a bean only when application is running on a specified cloud platform:\n@Configuration @ConditionalOnCloudPlatform(CloudPlatform.KUBERNETES) class RunsOnKubernetesCloudPlatform { // Code runs on application running on Kubernetes } @ConditionalOnWarDeployment Load a bean only when the application is a traditional war packaging and deployment. This condition returns false for applications running with embedded servers.\n@Configuration @ConditionalOnWarDeployment class RunsWithWarPackages { // Code runs with WAR package deployment } @ConditionalOnJndi Load a bean only when a specified JNDI location exist. If no locations are specific the condition matches solely based on the presence of an javax.naming.InitialContext.\n@Configuration @ConditionalOnJndi(\u0026#34;java:comp/env/ejb/myEJB\u0026#34;) class RunsWithJndiLocationAvailability { // Code runs when JNDI location is available } @ConditionalOnSingleCandidate Similar to @ConditionalOnBean, but load a bean only when a single candidate for the specified bean class can be determined. The condition will also match if multiple matching bean instances are already contained in the BeanFactory but a primary @Primary candidate has been defined; essentially, the condition match if auto-wiring a bean with the defined type will succeed. It is strongly recommended to use this condition on auto-configuration classes only.\n@Configuration @ConditionalOnSingleCandidate(DataSource.class) class RunsWithSingleDataSourceBean { // Code runs when single data source bean is determined } @ConditionalOnManagementPort Load a bean based on management server port management.server.port conditions, when:-\nthe port is disabled means it is not defined the port is same or different from server port server.port @Configuration @ConditionalOnManagementPort(ManagementPortType.DISABLED) class ManagementPortIsDisabled { // Code runs when management port is disabled } @Configuration @ConditionalOnManagementPort(ManagementPortType.SAME) class ManagementPortIsSameAsServerPort { // Code runs when management port is same as server port } @Configuration @ConditionalOnManagementPort(ManagementPortType.DIFFERENT) class ManagementPortIsDifferentFromServerPort { // Code runs when management port is different from server port } @ConditionalOnAvailableEndpoint Load a bean when management endpoint is available. An endpoint is considered available if it is both enabled and exposed using management.endpoints.web.exposure.include\n@Configuration @ConditionalOnAvailableEndpoint(endpoint = InfoEndpoint.class) class InfoEndpointIsAvailable { // Code runs when info management endpoint in enabled and exposed } The endpoint should be a bean of either an @Endpoint or an @EndpointExtension. Info endpoint is provided by Spring Boot out of the box.\nCustom Management Endpoint You can create custom management endpoint like this:-\n@Component @Endpoint(id = \u0026#34;custom-endpoint\u0026#34;) public class CustomEndpoint { @ReadOperation public String print() { return \u0026#34;This is custom management endpoint\u0026#34;; } } Now let\u0026rsquo;s load a configuration only when our custom endpoint is available:-\n@Configuration @ConditionalOnAvailableEndpoint(endpoint = CustomEndpoint.class) class CustomEndpointConfiguration { // Configuration for custom endpoint } @ConditionalOnEnabledHealthIndicator Load a bean when health indicator is enabled from property management.health.\u0026lt;name\u0026gt;.enabled where \u0026lt;name\u0026gt; is the value specified.\n@Configuration @ConditionalOnEnabledHealthIndicator(value = \u0026#34;heartbeat\u0026#34;) class HeatbeatHealthIndicator { // Code runs when management.health.heartbeat.enabled property is set to true. } Conclusion The Conditional annotation gives more power to Spring Boot to provide opinionated configuration and also gives us flexibility to load beans based on custom conditions using @Conditional and very handy predefined @ConditionalOn... annotations.\nWe can also combine the custom conditions using AllNestedConditions, AnyNestedCondition, or NoneNestedCondition. These helps us to modularize code based on environment and other conditions.\nWe should use the Conditional annotations wisely as they are difficult to debug and manage when overused.\nThe source code for examples in this article is available on github/springboot-config\n","permalink":"https://codingnconcepts.com/spring-boot/conditional-annotations-in-spring-boot/","tags":["Spring Boot Basics","REST"],"title":"Conditional Annotations in Spring Boot"},{"categories":null,"contents":" We at \u0026ldquo;Coding N Concepts\u0026rdquo; do every effort to provide to the point and complete information about a topic in the blog posts. Please write to us and provide your valuable feedback to improve us further.\nAny question or query? do not hesitate get in touch with us!\nLoading…","permalink":"https://codingnconcepts.com/contact/","tags":null,"title":"Contact Us"},{"categories":null,"contents":" Social RSS Feed Follow.it Feedspot\nCoding N Concepts has been selected under Top 100 Programming Blogs over the web by Feedspot\nQubit Labs\nCoding N Concepts ranked 12th in article Best Software Engineer Blogs to Check Out Today published by Qubit Labs\nSubscribe ","permalink":"https://codingnconcepts.com/follow/","tags":null,"title":"Follow Us"},{"categories":["Markdown"],"contents":"In this post, we\u0026rsquo;ll understand Markdown, its working and applications with examples.\nWhat is Markdown? Markdown is a lightweight markup language that is used to format and style the content of a text document. Created by John Gruber in 2004, Markdown is now one of the world’s most popular markup languages.\nMarkup language is very easy to learn, read and write for most of the people even without any technical knowledge. The main advantage of Markdown is readability of the text even after adding markdown markup. Look at the below markdown file (.md file) which render this section for example:-\nwhat-is-markdown.md ## What is Markdown? Markdown is a lightweight **markup** language that is used to *format and style* the content of a text document. Created by [John Gruber](https://daringfireball.net/projects/markdown/) in 2004, Markdown is now one of the world’s most popular markup languages. Markup language is very easy to read and write for most humans. Text of the file is very much readable even after adding markdown markup. Look at the below markdown file *(.md file)* which render this section for example:- Read Markdown Syntax Guide for detailed explanation and Markdown Cheat Sheet for a quick overview of Markdown syntax.\nHow does Markdown Work? You add markdown markup on content in plain text files generally having .md or .markdown extensions, and then Markdown applications convert those markdown files into HTML.\nThese application can be anything like Text Editors, Messengers, Static Site Generator, e-Book Generator, PDF Converter, Presentation Generator or a Command line script.\nThese applications use Markdown Processor (or Markdown Parser) behind the scenes which convert the Markdown files to HTML or other formats.\nJohn Gruber written the original Markdown (also known as Gruber Markdown) and its parser in Perl. After that Markdown has evolved and today we have different flavours of Markdown and their parser implementation in different programming languages such as C, Java, JavaScript, GO, Perl etc.\nMarkdown Flavours Markdown flavours are the extension (or superset) of Gruber Markdown. Some of the popular Markdown flavours are:-\nCommonMark is the most popular and widely used flavour of Markdown which is an effort to make a “standard, unambiguous syntax specification for Markdown, along with a suite of comprehensive tests to validate Markdown implementations against this specification”. Markdown Extra as the name suggest, provide support for additional elements like tables, definition lists, footnotes, abbreviations, etc. Critic Markup provide additional syntax for review like addition, deletion, substitution, comment, highlight, etc. Multi Markdown (MMD in short) provide additional features like internal cross referencing, footnote, maths formula, glossary, etc. R Markdown supports dozens of static and dynamic output formats including HTML, PDF, MS Word, Beamer, HTML5 slides, Tufte-style handouts, books, dashboards, shiny applications, scientific articles, websites, and more. GitHub Flavoured Markdown (GFM in short) is a superset of CommonMark which provide additional Github-specific features to be used for user content on GitHub.com. Where Markdown is used? Markdown is fast and easy to take notes; formatting text in chats, comments, and messaging applications; create content for the website, and publish documents like PDF, ePub and Presentations, etc.\nHere are some of the applications which supports Markdown:-\nMarkdown Editors The best way to learn Markdown is to use it. There are many online Markdown editors available where you can try writing in Markdown.\nDillinger Dillinger is one of the famous online Markdown editor which allows you to type Markdown in left pane and show live HTML preview in the right pane. You can also import and export the file as Markdown, HTML or PDF.\nHere is the list of other Markdown online editors:-\nWeb Github Dillinger https://dillinger.io/ https://github.com/joemccann/dillinger Mark https://mark.reaper.im/ https://github.com/barelyhuman/mark Minimalist Online Markdown Editor http://markdown.pioul.fr/ https://github.com/pioul/Minimalist-Online-Markdown-Editor StackEdit https://stackedit.io/ https://github.com/benweet/stackedit MarkTwo https://marktwo.app/ https://github.com/anthonygarvan/marktwo Markdown Tutorial https://www.markdowntutorial.com/ https://github.com/gjtorikian/markdowntutorial.com Messaging Applications Many chat bots and messengers now understand the power of Markdown. They provide basic formatting support using Markdown syntax.\nWhatsapp You can format the text on Whatsapp which supports Markdown alike syntax for bold, italic, strikethrough, monospace, and code block.\nWhatsapp Chat Facebook If you\u0026rsquo;re chatting with someone on Facebook on a computer, you can change how your text looks with Markdown alike syntax. Keep in mind that text formatting is only visible on a computer, and will not appear in the Messenger app on mobile.\nFacebook Messenger Team messaging applications such as Slack, Discord, and Mattermost also supports Markdown.\nCollaboration Platforms Most of the collaboration platforms provide good support for formatting span and block elements using Markdown.\nStackoverflow A private collaboration \u0026amp; Knowledge sharing platform Stackoverflow supports basic syntax of Markdown for writing Questions and Answers.\nStackoverflow with Markdown GitHub GitHub is an open source community to manager code repositories. GitHub heavily use GitHub Flavoured Markdown to write Documents, raise Issues with Markdown.\nGitHub Document with Markdown Markdown to Website Many static website generator provides support for Markdown files. They convert Markdown (.md) files to website pages (.html).\nJekyll and Hugo are two recommended open source static site generator if you wish to create a static website for blog, book, resume, or profile. They provide free themes and templates which you can use to quickly build a website with little or no knowledge about HTML and CSS. You just need to add the markdown files with your content and they will be added to a website. You can also host these websites on GitHub Pages which is also free!\nblot.im and smallvictori.es to check out, If you’re looking for the simplest possible way to create a website with Markdown files. After you sign up for one of these services, they create a Dropbox folder on your computer. Just drag and drop your Markdown files into the folder and — poof! — they’re on your website. It couldn’t be easier.\nHere is the list of other static website generators:-\nWeb Github Jekyll https://jekyllrb.com/ https://github.com/jekyll/jekyll Hugo https://gohugo.io/ https://github.com/gohugoio/hugo Middleman https://middlemanapp.com/ https://github.com/middleman/middleman Hexo https://hexo.io/ https://github.com/hexojs/hexo Grav https://getgrav.org/ https://github.com/getgrav/grav Sculpin https://sculpin.io/ https://github.com/sculpin/sculpin Pelican https://blog.getpelican.com/ https://github.com/getpelican/pelican Svbtle https://svbtle.com/about - VeuPress https://vuepress.vuejs.org/ https://github.com/vuejs/vuepress Markdown to eBook Write and create an eBook with Markdown\nGitBook is a modern documentation platform where you can structure your markdown files to convert to a book or documentation. Pandoc is a universal document converter which can be used to convert markdown to anything for e.g. ePub books. BookDown to write HTML, PDF, ePub, and Kindle books with R Markdown Markdown to PDF Convert Markdown to PDF\nDillinger editor allows you to write Markdown and export it as PDF. MarkdownToPDF is a website to upload a Markdown file and convert to PDF instantly. markdown-pdf is a npm module to convert Markdown to PDF. md2pdf is online app to write Markdown and transform it to PDF. Markdown to Presentation Create and edit your slides presentations with Markdown.\nRemark - A simple, in-browser, Markdown-driven slideshow tool targeted at people who know their way around HTML and CSS. Cleaver - Comes with default stylesheet and many other themes to use for the slides. Marp - Markdown presentation ecosystem. CLI, Web, IDE plugin and more to convert Markdown into various formats Deckset - Write your presentation as a Markdown text document, open it in Deckset, and present. Change and add slides on the fly with just a few keystrokes, even while presenting or in the few precious minutes before. Choose from dozens of themes and customize them to your needs. Hyperdeck - A new app that combines the best of both worlds in one iPad / macOS app. With top notch support for source code, tables, and animations. gitpitch - Markdown Presentations For Everyone on GitHub, GitLab, Bitbucket, GitBucket, Gitea, and Gogs. hacker-slides - Minimal UI for building presentation slides from markdown. mdp - Command-line based Markdown presentation tool. Like PowerPoint and Vim but with Markdown! reveal.js - Open source HTML presentation framework with Markdown support. ","permalink":"https://codingnconcepts.com/markdown/markdown-getting-started/","tags":["Markdown","Popular Posts"],"title":"MarkDown Getting Started"},{"categories":["Markdown"],"contents":"Markdown cheat sheet gives you a quick overview of all the Markdown syntax elements.\nIt doesn\u0026rsquo;t cover all the cases, so if you need more information about these elements, Please refer the comprehensive guide for markdown syntax\nCheat sheet Element Markdown Output Headings # Heading 1 ## Heading 2 ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6 Heading 1 Heading 2 Heading 2 Heading 2 Heading 2 Heading 2 Paragraphs This is a paragraph This is a paragraph\nOrdered List 1. First Item 2. Second Item 3. Third Item List Item 1 List Item 2 List Item 3 Unordered List - First Item - Second Item - Third Item List Item 1 List Item 2 List Item 3 Definition List First Term : Definition of first term. Second Term : Definition of second term. : Another definition. First Term Definition of first term. Second Term Definition of second term. Another definition. Blockquotes \u003e This is a quote This is a quote Code Blocks › › › › function test() { › › › › › › console.log(\"markdown\"); › › › › } function test() { console.log(\"markdown\"); } Horizontal Rule --- Table Header 1 | Header 2 ----------|---------- Cell 11 | Cell 12 Cell 21 | Cell 22 Header 1 Header 2 Cell 11 Cell 12 Cell 21 Cell 22 Bold I am **bold** I am bold Italic I am *italic* I am italic Strikethrough I am ~~strike~~ I am strike Link [example link](http://example.com/) example link Image ![Blog](/img/logo.png) Code code `syntax` code syntax ","permalink":"https://codingnconcepts.com/markdown/markdown-cheat-sheet/","tags":["Markdown","Cheatsheet"],"title":"MarkDown Cheat Sheet"},{"categories":["Markdown"],"contents":"In this comprehensive guide, we\u0026rsquo;ll learn Markdown syntax with examples.\nOverview Markdown is a lightweight markup language that is used to format and style the content of text document. Created by John Gruber in 2004, Markdown is now one of the world’s most popular markup languages.\nYou write markdown in plain text format, which is easier to read and write for most humans, and then Markdown processor convert the plain text into HTML.\nThere are many markdown processors available. Markdown syntax and support for elements may slightly differ depending upon which markdown processor is being used.\nLet\u0026rsquo;s learn how to format text using various Markdown syntax with examples:-\nBlock Elements Headings Hash # is used to create headings. The number of hashes denotes the heading level for e.g. ### denotes the third level heading.\nMarkdown HTML Output # Heading 1 \u0026lt;h1\u0026gt;Heading 1\u0026lt;/h1\u0026gt; Heading 1 ## Heading 2 \u0026lt;h2\u0026gt;Heading 1\u0026lt;/h2\u0026gt; Heading 2 ### Heading 3 \u0026lt;h3\u0026gt;Heading 3\u0026lt;/h3\u0026gt; Heading 3 #### Heading 4 \u0026lt;h4\u0026gt;Heading 4\u0026lt;/h4\u0026gt; Heading 4 ##### Heading 5 \u0026lt;h5\u0026gt;Heading 5\u0026lt;/h5\u0026gt; Heading 5 ###### Heading 6 \u0026lt;h6\u0026gt;Heading 6\u0026lt;/h6\u0026gt; Heading 6 ✅ Always add space between # and the heading for e.g. # Heading 1 works but #Heading 1 doesn\u0026rsquo;t.\n✅ Headings are supported by all Markdown processors.\nAlternate syntax for h1 and h2 Alternate syntax is available for first and second level of headings. Add any number of equal signs = and hyphens - below the text for first level and second level heading respectively.\nMarkdown HTML Output Heading 1 = \u0026lt;h1\u0026gt;Heading 1\u0026lt;/h1\u0026gt; Heading 1 Heading 1 =========== \u0026lt;h1\u0026gt;Heading 1\u0026lt;/h1\u0026gt; Heading 1 Heading 2 - \u0026lt;h2\u0026gt;Heading 1\u0026lt;/h2\u0026gt; Heading 2 Heading 2 ----------- \u0026lt;h2\u0026gt;Heading 1\u0026lt;/h2\u0026gt; Heading 2 Paragraphs Single Paragraph A simple line of text in Markdown creates a paragraph.\nMarkdown HTML Output This is a paragraph \u0026lt;p\u0026gt;This is a paragraph\u0026lt;/p\u0026gt; This is a paragraph\nMultiple Paragraphs A blank line is used to separate paragraphs.\nMarkdown HTML Output I love Markdown. \u0026#13; It's easier to read and write as compare to HTML. \u0026lt;p\u0026gt;I love Markdown.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;It's easier to read and write as compare to HTML. \u0026lt;/p\u0026gt; I love Markdown.\nIt's easier to read and write as compare to HTML.\nIf a blank line is not used then it is considered as a single paragraph even if the text is in multiple lines.\nMarkdown HTML Output I love Markdown. It's easier to read and write as compare to HTML. \u0026lt;p\u0026gt;I love Markdown. It's easier to read and write as compare to HTML. \u0026lt;/p\u0026gt; I love Markdown. It's easier to read and write as compare to HTML.\nParagraph with line break To add a line break in a paragraph, end a line with two or more spaces.\nMarkdown HTML Output I love Markdown.››› It's easier to read and write as compare to HTML. \u0026lt;p\u0026gt;I love Markdown.\u0026lt;br\u0026gt; It's easier to read and write as compare to HTML. \u0026lt;/p\u0026gt; I love Markdown.\nIt's easier to read and write as compare to HTML.\nNote: \u0026quot; › \u0026quot; denotes the space in above example.\n❌ Do not indent the paragraph with spaces or tabs. It may render different output by different Markdown processors.\n✅ Always keep paragraph left-aligned.\n✅ Paragraphs are supported by all Markdown processors.\nList Ordered List Ordered or Numbered list can be rendered by adding a number followed by period . for e.g. 1. before text.\n💡 The numbers don’t have to be in numerical order, but the list should start with the number one i.e. 1.\nMarkdown HTML Output 1. First Item 2. Second Item 3. Third Item \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;First Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Second Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Third Item\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; List Item 1 List Item 2 List Item 3 1. First Item 1. Second Item 1. Third Item \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;First Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Second Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Third Item\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; List Item 1 List Item 2 List Item 3 1. First Item 5. Second Item 2. Third Item \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;First Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Second Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Third Item\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; List Item 1 List Item 2 List Item 3 Nested Ordered List Nested list items can be created by adding indentation to the line. Each indentation creates a deeper nested level.\nMarkdown HTML Output 1. First Item 1. First Major Item 2. Second Major Item 1. First Minor Item 2. Second Minor Item 2. Second Item 3. Third Item \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;First Item\u0026lt;/li\u0026gt; \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;First Major Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Second Major Item\u0026lt;/li\u0026gt; \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;First Minor Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Second Minor Item\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;li\u0026gt;Second Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Third Item\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; First Item First Major Item Second Major Item First Minor Item Second Minor Item Second Item Third Item ❌ You can also use parenthesis 1) instead of period 1. but not recommended, because few markdown processor support this.\n✅ Ordered lists are supported by all Markdown processors.\nUnordered List Unordered list can be rendered by adding hyphen (or dash) -, asterisk *, or plus + in front of a text.\nMarkdown HTML Output * First Item * Second Item * Third Item \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;First Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Second Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Third Item\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; First Item Second Item Third Item - First Item - Second Item - Third Item \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;First Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Second Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Third Item\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; First Item Second Item Third Item + First Item + Second Item + Third Item \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;First Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Second Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Third Item\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; First Item Second Item Third Item Nested Unordered List Nested list items can be created by adding indentation to the line. Each indentation creates deeper nested level.\nMarkdown HTML Output - First Item - First Major Item - Second Major Item - First Minor Item - Second Minor Item - Second Item - Third Item \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;First Item\u0026lt;/li\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;First Major Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Second Major Item\u0026lt;/li\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;First Minor Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Second Minor Item\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;li\u0026gt;Second Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Third Item\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; First Item First Major Item Second Major Item First Minor Item Second Minor Item Second Item Third Item ❌ Do not mix and match hyphen (or dash) -, asterisk *, or plus + in the same list. Different markdown processors handle it in differently so pick one and stick with it.\n✅ Unordered lists are supported by all Markdown processors.\nEscape period in ordered and unordered list If you start an ordered or unordered list item with a number followed by a period for e.g. 1. 1. or - 1. then it is treated as nested ordered list.\nMarkdown HTML Output 1. 1. First Item 2. 99. Second Item \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt; \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;First Item\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;ol start=\"2\"\u0026gt; \u0026lt;li\u0026gt;Second Item\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; First Item Second Item - 1. First Item - 99. Second Item \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt; \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;First Item\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;ol start=\"2\"\u0026gt; \u0026lt;li\u0026gt;Second Item\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; First Item Second Item You can get rid of this by adding a backslash \\ in front of period for e.g. - 1\\.\nMarkdown HTML Output 1. 1\\. First Item 2. 99\\. Second Item \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;1. First Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;99. Second Item\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; 1. First Item 99. Second Item Markdown HTML Output - 1\\. First Item - 99\\. Second Item \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;1. First Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;99. Second Item\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; 1. First Item 99. Second Item Escape dash, asterisk and plus in ordered and unordered list If you start an ordered or unordered list item with dash -, asterisk * or plus + sign then it is treated as nested unordered list. You can escape this by adding a backslash \\ in front of them for e.g. \\-, \\* or \\+\nMarkdown HTML Output 1. \\- First Item 2. \\* Second Item 3. \\+ Third Item \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;- First Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;* Second Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;+ Third Item\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; - First Item * Second Item + Third Item Markdown HTML Output - \\- First Item - \\* Second Item - \\+ Third Item \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;- First Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;* Second Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;+ Third Item\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; - First Item * Second Item + Third Item Definition Lists Some Markdown processors allow you to create definition lists of terms and their corresponding definitions. To create a definition list, type the term on the first line. On the next line, type a colon : followed by a space and the definition.\nMarkdown HTML Output First Term : Definition of the first term. Second Term : Definition of the second term. : Another definition of the second term. \u0026lt;dl\u0026gt; \u0026lt;dt\u0026gt;First Term\u0026lt;/dt\u0026gt; \u0026lt;dd\u0026gt;Definition of the first term.\u0026lt;/dd\u0026gt; \u0026lt;dt\u0026gt;Second Term\u0026lt;/dt\u0026gt; \u0026lt;dd\u0026gt;Definition of the second term.\u0026lt;/dd\u0026gt; \u0026lt;dd\u0026gt;Another definition of the second term.\u0026lt;/dd\u0026gt; \u0026lt;/dl\u0026gt; First Term Definition of the first term. Second Term Definition of the second term. Another definition of the second term. ❌ Definition lists are NOT supported by all Markdown processors.\nBlockquotes Blockquotes with single paragraph Blockquotes is created by adding greater sign \u0026gt; in front of a text.\nMarkdown HTML Output \u003e This is a quote \u0026lt;blockquote\u0026gt;This is a quote\u0026lt;/blockquote\u0026gt; This is a quote Blockquotes with multiple paragraphs Blockquotes can contain multiple paragraphs. Add a greater sign \u0026gt; on the blank lines as well between the paragraphs.\nMarkdown HTML Output \u003e This is a blockquote with two paragraphs. I am first paragraph. \u003e \u003e I am second paragraph. \u0026lt;blockquote\u0026gt; \u0026lt;p\u0026gt;This is a blockquote with two paragraphs. I am first paragraph.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;I am second paragraph.\u0026lt;/p\u0026gt; \u0026lt;/blockquote\u0026gt; This is a blockquote with two paragraphs. I am first paragraph.\nI am second paragraph.\nNested Blockquotes Blockquotes can be nested (i.e. a blockquote-in-a-blockquote) by adding additional \u0026gt; for each nested level.\nMarkdown HTML Output \u003e First level of quote. \u003e \u003e\u003e This is nested blockquote. \u003e \u003e Back to the first level. \u0026lt;blockquote\u0026gt; \u0026lt;p\u0026gt;First level of quote.\u0026lt;/p\u0026gt; \u0026lt;blockquote\u0026gt; \u0026lt;p\u0026gt;This is nested blockquote.\u0026lt;/p\u0026gt; \u0026lt;/blockquote\u0026gt; \u0026lt;p\u0026gt;Back to the first level.\u0026lt;/p\u0026gt; \u0026lt;/blockquote\u0026gt; First level of quote.\nThis is nested blockquote.\nBack to the first level.\n\u003e First level of quote \u003e \u003e\u003e Second level of quote \u003e\u003e\u003e Third level of quote \u003e \u003e Back to the first level \u0026lt;blockquote\u0026gt; \u0026lt;p\u0026gt;First level of quote\u0026lt;/p\u0026gt; \u0026lt;blockquote\u0026gt; \u0026lt;p\u0026gt;Second level of quote\u0026lt;/p\u0026gt; \u0026lt;blockquote\u0026gt; \u0026lt;p\u0026gt;Third level of quote\u0026lt;/p\u0026gt; \u0026lt;/blockquote\u0026gt; \u0026lt;/blockquote\u0026gt; \u0026lt;p\u0026gt;Back to the first level\u0026lt;/p\u0026gt; \u0026lt;/blockquote\u0026gt; First level of quote\nSecond level of quote\nThird level of quote\nBack to the first level\nFormatted Blockquotes Blockquotes can be formatted with other elements such as headers, lists, emphasis and code blocks. Not all the elements works though, try them and see yourself.\nMarkdown HTML Output \u003e ##### This is a header \u003e \u003e 1. first list item. \u003e 2. second list item. \u003e \u003e some **example** `code`: \u003e \u003e return \"markdown\"; \u0026lt;blockquote\u0026gt; \u0026lt;h5\u0026gt;This is a header\u0026lt;/h5\u0026gt; \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;first list item.\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;second list item.\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;p\u0026gt;some \u0026lt;b\u0026gt;example\u0026lt;/b\u0026gt; \u0026lt;code\u0026gt;code\u0026lt;/code\u0026gt;:\u0026lt;/p\u0026gt; \u0026lt;pre\u0026gt; return \"markdown\"; \u0026lt;/pre\u0026gt; \u0026lt;/blockquote\u0026gt; This is a header first list item. second list item. some example code:\nreturn \"markdown\"; ✅ Blockquotes are supported by all Markdown processors.\nCode Blocks Code blocks are used to write programming code snippet. You can create a code block by indenting every line of the block by at least 4 spaces or 1 tab\nMarkdown HTML Output › › › › function test() { › › › › › › console.log(\"markdown\"); › › › › } \u0026lt;pre\u0026gt;\u0026lt;code\u0026gt; \u0026nbsp;\u0026nbsp;function test() { \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;console.log(\"markdown\"); \u0026nbsp;\u0026nbsp;} \u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt; function test() { console.log(\"markdown\"); } Note: \u0026quot; › \u0026quot; denotes the space in above example.\nFenced Code Blocks If you find adding 4 spaces or one tab on each line of the block is more inconvenient then some markdown processors also support three backticks ``` or three tildes ~~~ before and after the code block.\nMarkdown HTML Output ``` function test() { \u0026nbsp;\u0026nbsp;console.log(\"markdown\"); } ``` \u0026lt;pre\u0026gt;\u0026lt;code\u0026gt; function test() { \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;console.log(\"markdown\"); } \u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt; function language() { console.log(\"markdown\"); } Syntax Highlighting Many Markdown processors support syntax highlighting for fenced code blocks. This feature allows you to add color highlighting for whatever language your code was written in. To add syntax highlighting, specify a language next to the three backticks for e.g. ```javascript\nMarkdown Output ```javascript function language() { console.log(\"markdown\"); } ``` function language() { console.log(\"markdown\"); } ```json { \"firstName\": \"John\", \"lastName\": \"Smith\", \"age\": 25 } ``` { \"firstName\": \"John\", \"lastName\": \"Smith\", \"age\": 25 } ```ruby def index puts \"hello world\" end ``` def index puts \"hello world\" end ```html \u0026lt;div class=\"row\"\u0026gt; \u0026lt;div class=\"col-md-6\"\u0026gt; \u0026lt;h1\u003eHello World\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u003e ``` \u0026lt;div class=\"row\"\u0026gt; \u0026lt;div class=\"col-md-6\"\u0026gt; \u0026lt;h1\u0026gt;Hello World\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; Note that syntax highlighting support for a language and how the syntax are displayed for that language varies across different Markdown processors. You can try from this full list of languages, if that works for you:-\nabap (\u0026#39;*.abap\u0026#39;) ada (\u0026#39;*.adb\u0026#39;, \u0026#39;*.ads\u0026#39;, \u0026#39;*.ada\u0026#39;) ahk (\u0026#39;*.ahk\u0026#39;, \u0026#39;*.ahkl\u0026#39;) apacheconf (\u0026#39;.htaccess\u0026#39;, \u0026#39;apache.conf\u0026#39;, \u0026#39;apache2.conf\u0026#39;) applescript (\u0026#39;*.applescript\u0026#39;) as (\u0026#39;*.as\u0026#39;) as3 (\u0026#39;*.as\u0026#39;) asy (\u0026#39;*.asy\u0026#39;) bash (\u0026#39;*.sh\u0026#39;, \u0026#39;*.ksh\u0026#39;, \u0026#39;*.bash\u0026#39;, \u0026#39;*.ebuild\u0026#39;, \u0026#39;*.eclass\u0026#39;) bat (\u0026#39;*.bat\u0026#39;, \u0026#39;*.cmd\u0026#39;) befunge (\u0026#39;*.befunge\u0026#39;) blitzmax (\u0026#39;*.bmx\u0026#39;) boo (\u0026#39;*.boo\u0026#39;) brainfuck (\u0026#39;*.bf\u0026#39;, \u0026#39;*.b\u0026#39;) c (\u0026#39;*.c\u0026#39;, \u0026#39;*.h\u0026#39;) cfm (\u0026#39;*.cfm\u0026#39;, \u0026#39;*.cfml\u0026#39;, \u0026#39;*.cfc\u0026#39;) cheetah (\u0026#39;*.tmpl\u0026#39;, \u0026#39;*.spt\u0026#39;) cl (\u0026#39;*.cl\u0026#39;, \u0026#39;*.lisp\u0026#39;, \u0026#39;*.el\u0026#39;) clojure (\u0026#39;*.clj\u0026#39;, \u0026#39;*.cljs\u0026#39;) cmake (\u0026#39;*.cmake\u0026#39;, \u0026#39;CMakeLists.txt\u0026#39;) coffeescript (\u0026#39;*.coffee\u0026#39;) console (\u0026#39;*.sh-session\u0026#39;) control (\u0026#39;control\u0026#39;) cpp (\u0026#39;*.cpp\u0026#39;, \u0026#39;*.hpp\u0026#39;, \u0026#39;*.c++\u0026#39;, \u0026#39;*.h++\u0026#39;, \u0026#39;*.cc\u0026#39;, \u0026#39;*.hh\u0026#39;, \u0026#39;*.cxx\u0026#39;, \u0026#39;*.hxx\u0026#39;, \u0026#39;*.pde\u0026#39;) csharp (\u0026#39;*.cs\u0026#39;) css (\u0026#39;*.css\u0026#39;) cython (\u0026#39;*.pyx\u0026#39;, \u0026#39;*.pxd\u0026#39;, \u0026#39;*.pxi\u0026#39;) d (\u0026#39;*.d\u0026#39;, \u0026#39;*.di\u0026#39;) delphi (\u0026#39;*.pas\u0026#39;) diff (\u0026#39;*.diff\u0026#39;, \u0026#39;*.patch\u0026#39;) dpatch (\u0026#39;*.dpatch\u0026#39;, \u0026#39;*.darcspatch\u0026#39;) duel (\u0026#39;*.duel\u0026#39;, \u0026#39;*.jbst\u0026#39;) dylan (\u0026#39;*.dylan\u0026#39;, \u0026#39;*.dyl\u0026#39;) erb (\u0026#39;*.erb\u0026#39;) erl (\u0026#39;*.erl-sh\u0026#39;) erlang (\u0026#39;*.erl\u0026#39;, \u0026#39;*.hrl\u0026#39;) evoque (\u0026#39;*.evoque\u0026#39;) factor (\u0026#39;*.factor\u0026#39;) felix (\u0026#39;*.flx\u0026#39;, \u0026#39;*.flxh\u0026#39;) fortran (\u0026#39;*.f\u0026#39;, \u0026#39;*.f90\u0026#39;) gas (\u0026#39;*.s\u0026#39;, \u0026#39;*.S\u0026#39;) genshi (\u0026#39;*.kid\u0026#39;) glsl (\u0026#39;*.vert\u0026#39;, \u0026#39;*.frag\u0026#39;, \u0026#39;*.geo\u0026#39;) gnuplot (\u0026#39;*.plot\u0026#39;, \u0026#39;*.plt\u0026#39;) go (\u0026#39;*.go\u0026#39;) groff (\u0026#39;*.(1234567)\u0026#39;, \u0026#39;*.man\u0026#39;) haml (\u0026#39;*.haml\u0026#39;) haskell (\u0026#39;*.hs\u0026#39;) html (\u0026#39;*.html\u0026#39;, \u0026#39;*.htm\u0026#39;, \u0026#39;*.xhtml\u0026#39;, \u0026#39;*.xslt\u0026#39;) hx (\u0026#39;*.hx\u0026#39;) hybris (\u0026#39;*.hy\u0026#39;, \u0026#39;*.hyb\u0026#39;) ini (\u0026#39;*.ini\u0026#39;, \u0026#39;*.cfg\u0026#39;) io (\u0026#39;*.io\u0026#39;) ioke (\u0026#39;*.ik\u0026#39;) irc (\u0026#39;*.weechatlog\u0026#39;) jade (\u0026#39;*.jade\u0026#39;) java (\u0026#39;*.java\u0026#39;) js (\u0026#39;*.js\u0026#39;) jsp (\u0026#39;*.jsp\u0026#39;) lhs (\u0026#39;*.lhs\u0026#39;) llvm (\u0026#39;*.ll\u0026#39;) logtalk (\u0026#39;*.lgt\u0026#39;) lua (\u0026#39;*.lua\u0026#39;, \u0026#39;*.wlua\u0026#39;) make (\u0026#39;*.mak\u0026#39;, \u0026#39;Makefile\u0026#39;, \u0026#39;makefile\u0026#39;, \u0026#39;Makefile.*\u0026#39;, \u0026#39;GNUmakefile\u0026#39;) mako (\u0026#39;*.mao\u0026#39;) maql (\u0026#39;*.maql\u0026#39;) mason (\u0026#39;*.mhtml\u0026#39;, \u0026#39;*.mc\u0026#39;, \u0026#39;*.mi\u0026#39;, \u0026#39;autohandler\u0026#39;, \u0026#39;dhandler\u0026#39;) markdown (\u0026#39;*.md\u0026#39;) modelica (\u0026#39;*.mo\u0026#39;) modula2 (\u0026#39;*.def\u0026#39;, \u0026#39;*.mod\u0026#39;) moocode (\u0026#39;*.moo\u0026#39;) mupad (\u0026#39;*.mu\u0026#39;) mxml (\u0026#39;*.mxml\u0026#39;) myghty (\u0026#39;*.myt\u0026#39;, \u0026#39;autodelegate\u0026#39;) nasm (\u0026#39;*.asm\u0026#39;, \u0026#39;*.ASM\u0026#39;) newspeak (\u0026#39;*.ns2\u0026#39;) objdump (\u0026#39;*.objdump\u0026#39;) objectivec (\u0026#39;*.m\u0026#39;) objectivej (\u0026#39;*.j\u0026#39;) ocaml (\u0026#39;*.ml\u0026#39;, \u0026#39;*.mli\u0026#39;, \u0026#39;*.mll\u0026#39;, \u0026#39;*.mly\u0026#39;) ooc (\u0026#39;*.ooc\u0026#39;) perl (\u0026#39;*.pl\u0026#39;, \u0026#39;*.pm\u0026#39;) php (\u0026#39;*.php\u0026#39;, \u0026#39;*.php(345)\u0026#39;) postscript (\u0026#39;*.ps\u0026#39;, \u0026#39;*.eps\u0026#39;) pot (\u0026#39;*.pot\u0026#39;, \u0026#39;*.po\u0026#39;) pov (\u0026#39;*.pov\u0026#39;, \u0026#39;*.inc\u0026#39;) prolog (\u0026#39;*.prolog\u0026#39;, \u0026#39;*.pro\u0026#39;, \u0026#39;*.pl\u0026#39;) properties (\u0026#39;*.properties\u0026#39;) protobuf (\u0026#39;*.proto\u0026#39;) py3tb (\u0026#39;*.py3tb\u0026#39;) pytb (\u0026#39;*.pytb\u0026#39;) python (\u0026#39;*.py\u0026#39;, \u0026#39;*.pyw\u0026#39;, \u0026#39;*.sc\u0026#39;, \u0026#39;SConstruct\u0026#39;, \u0026#39;SConscript\u0026#39;, \u0026#39;*.tac\u0026#39;) rb (\u0026#39;*.rb\u0026#39;, \u0026#39;*.rbw\u0026#39;, \u0026#39;Rakefile\u0026#39;, \u0026#39;*.rake\u0026#39;, \u0026#39;*.gemspec\u0026#39;, \u0026#39;*.rbx\u0026#39;, \u0026#39;*.duby\u0026#39;) rconsole (\u0026#39;*.Rout\u0026#39;) rebol (\u0026#39;*.r\u0026#39;, \u0026#39;*.r3\u0026#39;) redcode (\u0026#39;*.cw\u0026#39;) rhtml (\u0026#39;*.rhtml\u0026#39;) rst (\u0026#39;*.rst\u0026#39;, \u0026#39;*.rest\u0026#39;) sass (\u0026#39;*.sass\u0026#39;) scala (\u0026#39;*.scala\u0026#39;) scaml (\u0026#39;*.scaml\u0026#39;) scheme (\u0026#39;*.scm\u0026#39;) scss (\u0026#39;*.scss\u0026#39;) smalltalk (\u0026#39;*.st\u0026#39;) smarty (\u0026#39;*.tpl\u0026#39;) sourceslist (\u0026#39;sources.list\u0026#39;) splus (\u0026#39;*.S\u0026#39;, \u0026#39;*.R\u0026#39;) sql (\u0026#39;*.sql\u0026#39;) sqlite3 (\u0026#39;*.sqlite3-console\u0026#39;) squidconf (\u0026#39;squid.conf\u0026#39;) ssp (\u0026#39;*.ssp\u0026#39;) tcl (\u0026#39;*.tcl\u0026#39;) tcsh (\u0026#39;*.tcsh\u0026#39;, \u0026#39;*.csh\u0026#39;) tex (\u0026#39;*.tex\u0026#39;, \u0026#39;*.aux\u0026#39;, \u0026#39;*.toc\u0026#39;) text (\u0026#39;*.txt\u0026#39;) v (\u0026#39;*.v\u0026#39;, \u0026#39;*.sv\u0026#39;) vala (\u0026#39;*.vala\u0026#39;, \u0026#39;*.vapi\u0026#39;) vbnet (\u0026#39;*.vb\u0026#39;, \u0026#39;*.bas\u0026#39;) velocity (\u0026#39;*.vm\u0026#39;, \u0026#39;*.fhtml\u0026#39;) vim (\u0026#39;*.vim\u0026#39;, \u0026#39;.vimrc\u0026#39;) xml (\u0026#39;*.xml\u0026#39;, \u0026#39;*.xsl\u0026#39;, \u0026#39;*.rss\u0026#39;, \u0026#39;*.xslt\u0026#39;, \u0026#39;*.xsd\u0026#39;, \u0026#39;*.wsdl\u0026#39;) xquery (\u0026#39;*.xqy\u0026#39;, \u0026#39;*.xquery\u0026#39;) xslt (\u0026#39;*.xsl\u0026#39;, \u0026#39;*.xslt\u0026#39;) yaml (\u0026#39;*.yaml\u0026#39;, \u0026#39;*.yml\u0026#39;) ✅ Code block (without Syntax highlighting) is supported by all Markdown processors.\n❌ Code blocks with Syntax highlighting is NOT supported by all Markdown processors.\nHorizontal Rules You can use three or more asterisks ***, dashes ---, or underscores ___ to create a horizontal rule. If you wish, you may use spaces between the hyphens or asterisks. Each of the following lines:-\n* * * *** ***** - - - --------------------------------------- produce a horizontal rule like below:-\n✅ Horizontal rules are supported by all Markdown processors.\nTables To create a table, use three or more hyphens --- to create each column’s header, and use pipes | to separate each column.\nMarkdown HTML Output Header 1 | Header 2 ----------|---------- Cell 11 | Cell 12 Cell 21 | Cell 22 \u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;Header 1\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Header 2\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;Cell 11\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;Cell 12\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;Cell 21\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;Cell 22\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/tbody\u0026gt; \u0026lt;/table\u0026gt; Header 1 Header 2 Cell 11 Cell 12 Cell 21 Cell 22 You can optionally add pipes on either end of the table.\nMarkdown Output | Header 1 | Header 2 | |----------|----------| | Cell 11 | Cell 12 | | Cell 21 | Cell 22 | Header 1 Header 2 Cell 11 Cell 12 Cell 21 Cell 22 Number of hyphens --- doesn\u0026rsquo;t affect the width of the column. For e.g. both columns will have equal (or auto) width in rendered table even though numbers of - are less in first column.\nMarkdown Output | Header 1 | Header 2 | | --- |------- | | Cell 11 | Cell 12 | | Cell 21 | Cell 22 | Header 1 Header 2 Cell 11 Cell 12 Cell 21 Cell 22 Table Column Text Alignment You can align text in the columns to the left, right, or center by adding a colon : to the left, right, or on both side of the hyphens within the header row.\nText Align Syntax left :--- center :---: right ---: Markdown Output | Index | Stationary Item | Amount |:--- | :---: | ---: | | 1. | Pencil | 10.00 | | 2. | Notebook | 120.00 | Index Stationary Item Amount 1. Pencil 10.00 2. Notebook 120.00 Table Cell Text Formatting You can format the text within table cells. For example, you can add links, code (not code blocks), and emphasis. You can’t add headings, blockquotes, lists, horizontal rules, images, or HTML tags.\nMarkdown Output | Header 1 | Header 2 | | -------- |------- | | **bold** | https://example.com | | *italic* | `code` | Header 1 Header 2 bold https://example.com italic code Escape Pipe in Tables If you want to use | character as a text in the table then you can escape it with blackslash i.e. \\|\nMarkdown Output | Index | Stationary Item | |:--- | :---: | ---: | | \\|1\\| | Pencil \\| Pen | | \\|2\\| | Notebook | Index Stationary Item |1| Pencil | Pen |2| Notebook ❌ Tables are NOT supported by all Markdown processors.\nTask Lists Task lists allow you to create a list of items with checkboxes. In Markdown applications that support task lists, checkboxes will be displayed next to the content.\nTo create a task list,\nadd - [] to create unchecked task list item, and\nadd - [x] to create checked task list item\nMarkdown Output - [x] Task 1 Completed - [ ] Task 2 in Progress - [ ] Task 3 Pending Task 1 Completed\nTask 2 in Progress\nTask 3 Pending ❌ Task lists are NOT supported by all Markdown processors.\nSpan Elements Emphasis Bold Add two asterisks ** or two underscores __ before and after a word for e.g. **bold** or __bold__ to render as bold.\nMarkdown HTML Output I am **bold** I am \u0026lt;b\u0026gt;bold\u0026lt;/b\u0026gt; I am bold I am __bold__ I am \u0026lt;b\u0026gt;bold\u0026lt;/b\u0026gt; I am bold I am **bold and stong** I am \u0026lt;b\u0026gt;bold and strong\u0026lt;/b\u0026gt; I am bold and strong I am __bold and stong__ I am \u0026lt;b\u0026gt;bold and strong\u0026lt;/b\u0026gt; I am bold and strong You can only use asterisks ** and not underscores __ to make part of the word bold. Using __ in part of the word considered as underscore _ and not syntax.\nMarkdown HTML Output I am half**bold** I am half\u0026lt;b\u0026gt;bold\u0026lt;/b\u0026gt; I am halfbold I am half__bold__ I am half__bold__ I am half__bold__ ❌ Do not to mix both asterisk and underscore for e.g. *_bold_*, _*bold*_, or _*bold_*. It may render different output by different Markdown processors.\n✅ Bold is supported by all Markdown processors.\nItalic Add an asterisk * or underscore _ before and after a word for e.g. *italic* or _italic_ to render as italic.\nMarkdown HTML Output I am *italic* I am \u0026lt;i\u0026gt;italic\u0026lt;/i\u0026gt; I am italic I am _italic_ I am \u0026lt;i\u0026gt;italic\u0026lt;/i\u0026gt; I am italic I am *italic and stylish* I am \u0026lt;i\u0026gt;italic and stylish\u0026lt;/i\u0026gt; I am italic and stylish I am _italic and stylish_ I am \u0026lt;i\u0026gt;italic and stylish\u0026lt;/i\u0026gt; I am italic and stylish You can only use asterisk * and not underscores _ to make part of the word italic. Using _ in part of the word considered as underscore _ and not syntax.\nMarkdown HTML Output I am half*italic* I am half\u0026lt;i\u0026gt;italic\u0026lt;/i\u0026gt; I am halfitalic I am half_italic_ I am half_italic_ I am half_italic_ ✅ Italic is supported by all Markdown processors.\nStrikethrough Add two tilde ~~ before and after a word for e.g. ~~strike~~ to render as strike.\nMarkdown HTML Output I am ~~strike~~ I am \u0026lt;del\u0026gt;strike\u0026lt;/del\u0026gt; I am strike I am ~~strike and through~~ I am \u0026lt;del\u0026gt;strike and through\u0026lt;/del\u0026gt; I am strike and through I am strike~~through~~ I am strike\u0026lt;del\u0026gt;through\u0026lt;/del\u0026gt; I am strikethrough ❌ Strikethrough is NOT supported by all Markdown processors.\nBold, Italic and Strike You can join asterisk *, underscore _ and tilde ~ together for e.g. **_~~bold, italic, and strike~~_** to render bold, italic and strike at the same time.\nMarkdown HTML Output I am ***bold and italic*** I am \u0026lt;b\u0026gt;\u0026lt;i\u0026gt;bold and italic\u0026lt;/i\u0026gt;\u0026lt;/b\u0026gt; I am bold and italic I am ___bold and italic___ I am \u0026lt;b\u0026gt;\u0026lt;i\u0026gt;bold and italic\u0026lt;/i\u0026gt;\u0026lt;/b\u0026gt; I am bold and italic I am __*bold and italic*__ I am \u0026lt;b\u0026gt;\u0026lt;i\u0026gt;bold and italic\u0026lt;/i\u0026gt;\u0026lt;/b\u0026gt; I am bold and italic I am **_bold and italic_** I am \u0026lt;b\u0026gt;\u0026lt;i\u0026gt;bold and italic\u0026lt;/i\u0026gt;\u0026lt;/b\u0026gt; I am bold and italic I am ***~~bold, italic, and strike~~*** I am \u0026lt;b\u0026gt;\u0026lt;i\u0026gt;\u0026lt;del\u0026gt;bold, italic, and strike\u0026lt;/del\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;/b\u0026gt; I am bold, italic, and strike I am **bold**, *italic*, and ~~strike~~ I am \u0026lt;b\u0026gt;bold\u0026lt;/b\u0026gt;, \u0026lt;i\u0026gt;italic\u0026lt;/i\u0026gt;, and \u0026lt;del\u0026gt;strike\u0026lt;/del\u0026gt; I am bold, italic, and strike Escape asterisk, underscore and tilde in emphasis You can escape the asterisk *, underscore _ and tilde ~ characters to use them in literal way by adding a backslash \\ in front of that character.\nMarkdown HTML Output **Escape \\* in Bold** \u0026lt;b\u0026gt;Escape * in Bold\u0026lt;/b\u0026gt; Escape * in Bold _Escape \\_ in Italic_ \u0026lt;i\u0026gt;Escape _ in Italic\u0026lt;/i\u0026gt; Escape _ in Italic ~~Escape \\~ in Strike~~ \u0026lt;del\u0026gt;Escape ~ in Strike\u0026lt;/del\u0026gt; Escape ~ in Strike Link Inline Link Inline links is created by wrapping link text in brackets [], followed by URL in parentheses () for e.g. [link text](URL)\nMarkdown HTML Output [example link](http://example.com/) \u0026lt;a href=\"http://example.com/\"\u0026gt; example link \u0026lt;/a\u0026gt; example link Inline Link with Title Inline Link can also be created with optional title which is displayed when hover on the link.\nMarkdown HTML Output [example link](http://example.com/ \"Example Title\") \u0026lt;a href=\"http://example.com/\" title=\"Example Title\"\u0026gt; example link \u0026lt;/a\u0026gt; example link Inline Link with URL and Email Address URL or email address can quickly be turned into a clickable link by wrapping it in angle brackets \u0026lt;\u0026gt;\nMarkdown HTML Output \u0026lt;https://example.com\u0026gt; \u0026lt;a href=\"http://example.com\"\u0026gt; http://example.com \u0026lt;/a\u0026gt; http://example.com \u0026lt;fake@example.com\u0026gt; \u0026lt;a href=\"mailto:fake@example.com\"\u0026gt; fake@example.com \u0026lt;/a\u0026gt; http://example.com Automatic URL and Email Address Link 💡 Many markdown processors (but not all) automatically turns URL and Email Address into clickable link even without wrapping in \u0026lt;\u0026gt;\nMarkdown HTML Output https://example.com \u0026lt;a href=\"http://example.com\"\u0026gt; http://example.com \u0026lt;/a\u0026gt; http://example.com fake@example.com \u0026lt;a href=\"mailto:fake@example.com\"\u0026gt; fake@example.com \u0026lt;/a\u0026gt; http://example.com Reference Links Reference-style links are constructed in two parts: first part is link text which you keep inline and the second part is URL which you define somewhere else in the document.\nMarkdown HTML Output I get 10 times more traffic from [Google][1] than from [Yahoo][2] or [MSN][3]. \u0026nbsp; ... ... ... \u0026nbsp; [1]: http://google.com/ \"Google\" [2]: http://yahoo.com/ \"Yahoo\" [3]: http://msn.com/ \"MSN\" \u0026lt;p\u0026gt;I get 10 times more traffic from \u0026lt;a href=\"http://google.com/\" title=\"Google\"\u003eGoogle\u0026lt;/a\u0026gt; than from \u0026lt;a href=\"http://yahoo.com/\" title=\"Yahoo\"\u003eYahoo\u0026lt;/a\u0026gt; or \u0026lt;a href=\"http://msn.com/\" title=\"MSN\"\u003eMSN\u0026lt;/a\u0026gt;. \u0026lt;/p\u0026gt; I get 10 times more traffic from Google than from Yahoo or MSN. ❌ Do not use space in the middle of the URL for e.g. [link](https://www.example.com/my great page), use encoded URL [link](https://www.example.com/my%20great%20page) instead.\n✅ Links and email addresses are supported by all Markdown processors.\nImage Inline Image Image is created by starting with exclamation !, followed by alt text in brackets [], followed by image path or URL in parentheses () for e.g. ![alt-text](image-path)\nMarkdown HTML Output ![Blog](/img/logo.png) \u0026lt;img src=\"/img/logo.png\" alt=\"Blog\"\u0026gt; Inline Image with Title Image can also be created with optional title which is displayed when hover on the image for e.g. ![alt-text](image-path \u0026quot;Title\u0026quot;)\nMarkdown HTML Output ![Blog](/img/logo.png \"CodingNConcepts\") \u0026lt;img src=\"./img/logo.png\" alt=\"Blog\" title=\"CodingNConcepts\"\u0026gt; Inline Image with Link Clickable image can also be created with link URL for e.g. [![alt-text](image-path \u0026quot;Title\u0026quot;)](URL)\nMarkdown HTML Output [![Blog](/img/logo.png \"CodingNConcepts\")] (https://codingnconcepts.com) \u0026lt;a href=\"https://codingnconcepts.com\"\u003e \u0026lt;img src=\"/img/logo.png\" alt=\"Blog\" title=\"CodingNConcepts\"\u0026gt; \u0026lt;/a\u0026gt; Reference Image Reference-style images are constructed in two parts: first part is alt text which you keep inline and the second part is image patg which you define somewhere else in the document.\nMarkdown HTML Output ![Blog][ref] \u0026nbsp; ... ... ... \u0026nbsp; [ref]: /img/logo.png \"CodingNConcepts\" \u0026lt;img src=\"./img/logo.png\" alt=\"Blog\" title=\"CodingNConcepts\"\u0026gt; ✅ Images are supported by all Markdown processors.\nCode Enclose the text in backticks ` to display that as a code for e.g `code`.\nMarkdown HTML Output execute command `nano` execute command \u0026lt;code\u0026gt;nano\u0026lt;/code\u0026gt; execute command nano Escape backticks in code You can escape the backtick ` character to use it in literal way by enclosing the code text in double backticks ``.\nMarkdown HTML Output execute command `` `nano` `` execute command \u0026lt;code\u0026gt;`nano`\u0026lt;/code\u0026gt; execute command `nano` Note the space between double backticks `` and `nano`, otherwise it won\u0026rsquo;t work.\nEscaping Characters Sometime we want to display the characters in literal way which are used as a Markdown syntax. In such cases, add a backslash \\ in front of the character.\nYou can escape the following characters in Markdown:-\nCharacter Example * see escape asterisk in bold and italic see escape asterisk in ordered and unordered list _ see escape underscore in bold and italic ~ see escape tilde in strikethrough ` see escape backticks in code . see escape period (dot) in ordered and unordered list | see escape pipe in tables - see escape hyphen (dash) in ordered and unordered list + see escape plus in ordered and unordered list {} curly braces [] brackets \u0026lt;\u0026gt; angle brackets () parentheses # hash (pound) ! exclamation \\ backslash Summary Congratulations! We have learned all the syntax supported by most of the Markdown processors.\nYou can further read following useful resources:-\nJohn Gruber’s Markdown documentation written by the creator of Markdown. Markdown Guide for getting-started with Markdown Markdown Tutorial to try Markdown syntax in the web browser. Awesome Markdown is a list of Markdown tools and learning resources. ","permalink":"https://codingnconcepts.com/markdown/markdown-syntax/","tags":["Markdown","HTML"],"title":"The Markdown Syntax Guide"},{"categories":["Kafka"],"contents":"In this tutorial, We\u0026rsquo;ll learn how to install and run Apache kafka broker in local machine running on Windows or Mac operating system.\nRunning kafka broker in local machine helps developer to code, debug and test kafka application in initial phase of development when Kafka infrastructure is not ready.\nKafka Installation on Windows If you are using Windows, then you can follow these steps to install Kafka:-\nInstall JDK 1.8 or higher JDK 1.8 or higher version is prerequisite for Kafka Installation. You can download and install JDK based on your OS and CPU architecture from here:-\nhttps://www.oracle.com/sg/java/technologies/javase-downloads.html\nOnce your JDK installation is complete, you must configure PATH and JAVA_HOME environment variables. You can set up your environment variables using the following steps:-\nStart system settings in the control panel. You can directly start it by pressing Win + Pause/Break key. Click on Advanced System Settings. In the advanced tab, click the environment variable button. In the user variables section, add new JAVA_HOME environment variable as your Java installation directory. In a typical case, your Java installation directory should be something like C:\\Program Files\\Java\\jdk1.8.0_191. However, if you have installed JDK as per the steps defined above, you should have noted down your JDK location as advised earlier. Now you can add a new PATH environment variable and specify the path as %JAVA_HOME%\\bin The final step is to test your JDK installation. Start windows command prompt and test JDK using below command. You should see the installed version:-\nC:\\\u0026gt;java -version java version \u0026#34;1.8.0_191\u0026#34; Java(TM) SE Runtime Environment (build 1.8.0_191-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode) Thats it! Your JDK in installed successfully.\nInstall Kafka Download and install Kafka binaries from here:-\nhttps://kafka.apache.org/downloads\nNote: At the time of writing this article, latest kafka version was kafka_2.13-2.8.0.tgz. It is recommended to use latest version available.\nDownload and install 7-zip (required to un-compress the Kafka binaries) from here:-\nhttp://www.7-zip.org/download.html\nUn-compress the kafka_2.13-2.8.0.tgz file using 7-zip, and you may have to use it twice to extract the files correctly.\nFor example, after exraction, my kafka location is as follow:-\nC:\\apache\\kafka_2.13-2.8.0 Next, we need to change Zookeeper configuration zookeeper.properties as follows:-\nOpen the property file:-\nC:\\apache\\kafka_2.13-2.8.0\\config\\zookeeper.properties and change the Zookeeper dataDir location config to a valid windows directory location. An example value is given below.\nzookeeper.properties dataDir = C:\\apache\\kafka_2.13-2.8.0\\zookeeper_data Make sure that zookeeper_data directory exist in specified location. If not, create one.\nWe also need to make some changes in the Kafka configurations server.properties as follows:-\nOpen the property file:-\nC:\\apache\\kafka_2.13-2.8.0\\config\\server.properties and change/add following configuration properties.\nserver.properties log.dirs = C:\\apache\\kafka_2.13-2.8.0\\kafka_logs offsets.topic.num.partitions = 1 offsets.topic.replication.factor = 1 min.insync.replicas = 1 default.replication.factor = 1 Make sure that kafka_logs directory exist in specified location. If not, create one. Also note that we are setting topic defaults to 1, and that makes sense because we will be running a single node Kafka on our machine.\nFinally, add Kafka C:\\apache\\kafka_2.13-2.8.0\\bin\\windows directory to the PATH environment variable. This directory contains a bunch of Kafka tools for the windows platform. We will be using some of those in the next section.\nRun Kafka Start Zookeeper in new terminal using:\nC:\\apache\\kafka_2.13-2.8.0\u0026gt;bin\\windows\\zookeeper-server-start.bat config\\zookeeper.properties Start Kafka in new terminal using:\nC:\\apache\\kafka_2.13-2.8.0\u0026gt;bin\\windows\\kafka-server-start.bat server\\zookeeper.properties Create a test kafka topic in new terminal using:\nC:\\apache\\kafka_2.13-2.8.0\u0026gt;bin\\windows\\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test Publish messages to test kafka topic by initializing a producer in new terminal using:\nC:\\apache\\kafka_2.13-2.8.0\u0026gt;bin\\windows\\kafka-console-producer.bat --broker-list localhost:9092 --topic test \u0026gt;send first message \u0026gt;send second message \u0026gt;wow it is working Consume messages from test kafka topic by intializing a consumer in new terminal using:\nC:\\apache\\kafka_2.13-2.8.0\u0026gt;bin\\windows\\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic test --from-beginning send first message send second message wow it is working You can List the available Kafka topics using:-\nC:\\apache\\kafka_2.13-2.8.0\u0026gt;bin\\windows\\kafka-topics.bat --list --zookeeper localhost:2181 Kafka Installation on Mac If you are using MacOS, then you can install Homebrew package manager and run following commands from terminal:-\nTo install Java (Prerequisite for Kafka), run\nbrew install openjdk@11 To install Kafka, run\n$ brew install kafka Here are the brew logs of Kafka Installation, Note that brew also installs Zookeeper as Kafka dependency:-\n==\u0026gt; Installing kafka dependency: zookeeper ==\u0026gt; Pouring zookeeper--3.7.0.catalina.bottle.tar.gz ==\u0026gt; Caveats To have launchd start zookeeper now and restart at login: brew services start zookeeper Or, if you don\u0026#39;t want/need a background service you can just run: zkServer start ==\u0026gt; Summary 🍺 /usr/local/Cellar/zookeeper/3.7.0: 1,073 files, 42.4MB ==\u0026gt; Installing kafka ==\u0026gt; Pouring kafka--2.8.0.catalina.bottle.tar.gz ==\u0026gt; Caveats To have launchd start kafka now and restart at login: brew services start kafka Or, if you don\u0026#39;t want/need a background service you can just run: zookeeper-server-start -daemon /usr/local/etc/kafka/zookeeper.properties \u0026amp; kafka-server-start /usr/local/etc/kafka/server.properties ==\u0026gt; Summary 🍺 /usr/local/Cellar/kafka/2.8.0: 200 files, 68.2MB ==\u0026gt; zookeeper To have launchd start zookeeper now and restart at login: brew services start zookeeper Or, if you don\u0026#39;t want/need a background service you can just run: zkServer start ==\u0026gt; kafka To have launchd start kafka now and restart at login: brew services start kafka Or, if you don\u0026#39;t want/need a background service you can just run: zookeeper-server-start -daemon /usr/local/etc/kafka/zookeeper.properties \u0026amp; kafka-server-start /usr/local/etc/kafka/server.properties Edit the server.properties file\n$ vim /usr/local/etc/kafka/server.properties Uncomment the listeners property and update the value from\nlisteners=PLAINTEXT://:9092\nto\nlisteners=PLAINTEXT://localhost:9092\nStart Zookeeper in new terminal using:\n$ zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties Start Kafka in new terminal using:\n$ kafka-server-start /usr/local/etc/kafka/server.properties Create a test kafka topic in new terminal using:\n$ kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test Publish messages to test kafka topic by initializing a producer in new terminal using:\n$ kafka-console-producer --broker-list localhost:9092 --topic test \u0026gt;send first message \u0026gt;send second message \u0026gt;wow it is working Consume messages from test kafka topic by intializing a consumer in new terminal using:\n$ kafka-console-consumer --bootstrap-server localhost:9092 --topic test --from-beginning send first message send second message wow it is working Delete the kafka logs if there is any issue in starting Kafka server\n$ rm -rf /tmp/kafka-logs Summary You have successfully installed and run a single-node Kafka broker on your local machine running on Windows or Mac operating system. This installation will help you to execute your Kafka application code locally and help you debug your application from the IDE.\n","permalink":"https://codingnconcepts.com/post/apache-kafka-installation/","tags":["Kafka"],"title":"Apache Kafka Installation on Windows and MacOS"},{"categories":["Markdown"],"contents":"Markdown is gaining more popularity compare to HTML to style and format your content in text editors, blogs, and social media. Why so?\nMarkdown is easier to write than HTML, and it\u0026rsquo;s easier for most humans to read and write Markdown than HTML. Writing HTML by hand is more painstaking and laborious due to its semantic tags for each element.\nHere is a comparison to show how you can write Markdown vs HTML to style and format your content:-\nMarkdown vs HTML Element Markdown HTML Output Headings # Heading 1 ## Heading 2 ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6 \u0026lt;h1\u0026gt;Heading 1\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;Heading 2\u0026lt;/h2\u0026gt; \u0026lt;h3\u0026gt;Heading 3\u0026lt;/h3\u0026gt; \u0026lt;h4\u0026gt;Heading 4\u0026lt;/h4\u0026gt; \u0026lt;h5\u0026gt;Heading 5\u0026lt;/h5\u0026gt; \u0026lt;h6\u0026gt;Heading 6\u0026lt;/h6\u0026gt; Heading 1 Heading 2 Heading 2 Heading 2 Heading 2 Heading 2 Paragraphs This is a paragraph \u0026lt;p\u0026gt;This is a paragraph\u0026lt;/p\u0026gt; This is a paragraph\nOrdered List 1. First Item 2. Second Item 3. Third Item \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;First Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Second Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Third Item\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; List Item 1 List Item 2 List Item 3 Unordered List - First Item - Second Item - Third Item \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;First Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Second Item\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Third Item\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; List Item 1 List Item 2 List Item 3 Definition List First Term : Definition of first term. Second Term : Definition of second term. : Another definition. \u0026lt;dl\u0026gt; \u0026lt;dt\u0026gt;First Term\u0026lt;/dt\u0026gt; \u0026lt;dd\u0026gt;Definition of first term.\u0026lt;/dd\u0026gt; \u0026lt;dt\u0026gt;Second Term\u0026lt;/dt\u0026gt; \u0026lt;dd\u0026gt;Definition of second term.\u0026lt;/dd\u0026gt; \u0026lt;dd\u0026gt;Another definition.\u0026lt;/dd\u0026gt; \u0026lt;/dl\u0026gt; First Term Definition of first term. Second Term Definition of second term. Another definition. Blockquotes \u003e This is a quote \u0026lt;blockquote\u0026gt;This is a quote\u0026lt;/blockquote\u0026gt; This is a quote Code Blocks › › › › function test() { › › › › › › console.log(\"markdown\"); › › › › } \u0026lt;pre\u0026gt;\u0026lt;code\u0026gt; \u0026nbsp;\u0026nbsp;function test() { \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;console.log(\"markdown\"); \u0026nbsp;\u0026nbsp;} \u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt; function test() { console.log(\"markdown\"); } Horizontal Rule --- \u0026lt;hr/\u0026gt; Table Header 1 | Header 2 ----------|---------- Cell 11 | Cell 12 Cell 21 | Cell 22 \u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;Header 1\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Header 2\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;Cell 11\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;Cell 12\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;Cell 21\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;Cell 22\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/tbody\u0026gt; \u0026lt;/table\u0026gt; Header 1 Header 2 Cell 11 Cell 12 Cell 21 Cell 22 Bold I am **bold** I am \u0026lt;b\u0026gt;bold\u0026lt;/b\u0026gt; I am bold Italic I am *italic* I am \u0026lt;i\u0026gt;italic\u0026lt;/i\u0026gt; I am italic Strikethrough I am ~~strike~~ I am \u0026lt;del\u0026gt;strike\u0026lt;/del\u0026gt; I am strike Link [example link](http://example.com/) \u0026lt;a href=\"http://example.com/\"\u0026gt; example link \u0026lt;/a\u0026gt; example link Image ![Blog](/img/logo.png) \u0026lt;img src=\"/img/logo.png\" alt=\"Blog\"\u0026gt; Code execute command `nano` execute command \u0026lt;code\u0026gt;nano\u0026lt;/code\u0026gt; execute command nano ","permalink":"https://codingnconcepts.com/markdown/markdown-vs-html/","tags":["Markdown","HTML"],"title":"MarkDown and HTML Comparison"},{"categories":null,"contents":" Coding N Concepts is a technical blog for developers by developers to understand coding and concepts in a simplified way.\nWe do every effort to provide to the point and complete information about a topic in the blog posts. We keep updating each post with the latest information.\nWe mainly focus on these areas:-\nProgramming Languages tutorials (mainly Java, Spring Boot, JavaScript, and CSS) System design tutorials such as AWS, Kafka, Elastic Puzzle Solving Interview Preparation Many Tools and Converters About Me Hey Everyone! I’m Ashish Lahoti. Welcome to my technology blog!\nI created this blog to narrate my coding and concepts learning experiences. I like sharing my experiments and ideas with everyone by writing articles on the latest technological trends.\nPrimarily I write about Java, Spring, Kafka, Javascript, CSS and the list goes on\u0026hellip;\nI’d love to hear from you! Have questions or suggestions? Feel free to email me on lahoti.ashish20@gmail.com\nThanks for reading!\nContributors CondingNConcepts is growing and it\u0026rsquo;s not possible without the contribution of a fantastic team of Authors\nIf you like our work then keep motivating us by buying us a coffee ","permalink":"https://codingnconcepts.com/about/","tags":null,"title":"About Us"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn different ways to initialize a Map with values in Java.\nUsing Map.of() and Map.ofEntries() It is possible to initialize a Map with values in a single expression if you are using Java 9 or higher version using Map.of() and Map.ofEntries() method. This is shortest possible way so far.\nMap.of() Java 9 provides mutiple Map.of() overloaded methods to initialize a Map with upto 10 key-value pairs.\nMap\u0026lt;String, Integer\u0026gt; emptyMap = Map.of(); Map\u0026lt;String, Integer\u0026gt; singletonMap = Map.of(\u0026#34;A\u0026#34;, 1); Map\u0026lt;String, Integer\u0026gt; map = Map.of(\u0026#34;A\u0026#34;, 1, \u0026#34;B\u0026#34;, 2, \u0026#34;C\u0026#34;, 3); Map.ofEntries() If you have more than 10 key-value pairs to initialize, then you should use Map.ofEntries() method. This method has no limit and you can define any number of key-value pairs.\nMap\u0026lt;String, Integer\u0026gt; map = Map.ofEntries( Map.entry(\u0026#34;A\u0026#34;, 1), Map.entry(\u0026#34;B\u0026#34;, 2), Map.entry(\u0026#34;C\u0026#34;, 3), Map.entry(\u0026#34;D\u0026#34;, 4), Map.entry(\u0026#34;E\u0026#34;, 5), Map.entry(\u0026#34;F\u0026#34;, 6), Map.entry(\u0026#34;G\u0026#34;, 7), Map.entry(\u0026#34;H\u0026#34;, 8), Map.entry(\u0026#34;I\u0026#34;, 9), Map.entry(\u0026#34;J\u0026#34;, 10), Map.entry(\u0026#34;K\u0026#34;, 11), Map.entry(\u0026#34;L\u0026#34;, 12) ); map.put(\u0026#34;M\u0026#34;, 13); // Throw UnsupportedOperationException map.remove(\u0026#34;A\u0026#34;); // Throw UnsupportedOperationException Mutable Map Thing to note that both Map.of() and Map.ofEntries() return an immutable map which means that adding or removing an element in Map result into java.lang.UnsupportedOperationException exception.\nYou can avoid this by creating a mutable map (by copying the immutable map to new HashMap) in this way:-\nMap\u0026lt;String, Integer\u0026gt; mutableEmptyMap = new HashMap\u0026lt;\u0026gt;(Map.of()); Map\u0026lt;String, Integer\u0026gt; mutableSingletonMap = new HashMap\u0026lt;\u0026gt;(Map.of(\u0026#34;A\u0026#34;, 1)); Map\u0026lt;String, Integer\u0026gt; mutableMap = new HashMap\u0026lt;\u0026gt;(Map.ofEntries( Map.entry(\u0026#34;A\u0026#34;, 1), Map.entry(\u0026#34;B\u0026#34;, 2), Map.entry(\u0026#34;C\u0026#34;, 3), Map.entry(\u0026#34;D\u0026#34;, 4), Map.entry(\u0026#34;E\u0026#34;, 5), Map.entry(\u0026#34;F\u0026#34;, 6), Map.entry(\u0026#34;G\u0026#34;, 7), Map.entry(\u0026#34;H\u0026#34;, 8), Map.entry(\u0026#34;I\u0026#34;, 9), Map.entry(\u0026#34;J\u0026#34;, 10), Map.entry(\u0026#34;K\u0026#34;, 11), Map.entry(\u0026#34;L\u0026#34;, 12) )); mutableMap.put(\u0026#34;M\u0026#34;, 13); // It works! mutableMap.remove(\u0026#34;A\u0026#34;); // It works! Using Java Collections Java Collections class provide methods to initialize emptyMap(), singletonMap() and unmodifiableMap(). Note that all these methods return immutable map\nMap\u0026lt;String, Integer\u0026gt; emptyMap = Collections.emptyMap(); Map\u0026lt;String, Integer\u0026gt; singletonMap = Collections.singletonMap(\u0026#34;A\u0026#34;, 1); singletonMap.put(\u0026#34;B\u0026#34;, 2); // Throw UnsupportedOperationException singletonMap.remove(\u0026#34;A\u0026#34;); // Throw UnsupportedOperationException Map\u0026lt;String, Integer\u0026gt; mutableMap = new HashMap\u0026lt;\u0026gt;(singletonMap); mutableMap.put(\u0026#34;B\u0026#34;, 2); // It works! Map\u0026lt;String, Integer\u0026gt; immutableMap = Collections.unmodifiableMap(mutableMap); immutableMap.put(\u0026#34;B\u0026#34;, 2); // Throw UnsupportedOperationException Initialize Map as an instance variable If you initialize a Map as an instance variable, keep the initialization in a constructor or instance initializer:-\npublic class MyClass { Map\u0026lt;String, Integer\u0026gt; instanceMap = new HashMap\u0026lt;\u0026gt;(); { instanceMap.put(\u0026#34;A\u0026#34;, 1); instanceMap.put(\u0026#34;B\u0026#34;, 2); } } Initialize Map as a static variable If you initialize a Map as a static class variable, keep the initialization in a static initializer:-\npublic class MyClass { static Map\u0026lt;String, Integer\u0026gt; staticMap = new HashMap\u0026lt;\u0026gt;(); static{ staticMap.put(\u0026#34;A\u0026#34;, 1); staticMap.put(\u0026#34;B\u0026#34;, 2); } } Using Double Brace Initialization You can initialize map with values using Double Brace Initialization:-\nMap\u0026lt;String, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;() {{ put(\u0026#34;A\u0026#34;, 1); put(\u0026#34;B\u0026#34;, 2); }}; In Double brace initialization {{ }}, first brace creates a new Anonymous Inner Class, the second brace declares an instance initializer block that is run when the anonymous inner class is instantiated.\nThis approach is not recommended as it creates an extra class at each usage. It also holds hidden references to the enclosing instance and any captured objects. This may cause memory leaks or problems with serialization.\nThe alternative approach for this is to create a function to initialize a map:-\n// It works for all Java versions, mutable map. Map\u0026lt;String, Integer\u0026gt; map = createMap(); map.put(\u0026#34;C\u0026#34;, \u0026#34;3\u0026#34;); // It works! private static Map\u0026lt;String, String\u0026gt; createMap() { Map\u0026lt;String, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); map.put(\u0026#34;A\u0026#34;, 1); map.put(\u0026#34;B\u0026#34;, 2); return map; } Using Stream Collectors.toMap() We can also use Java 8 Stream API to initialize a Map with values.\nWhen both key and value are of same type (e.g. String):-\nMap\u0026lt;String, String\u0026gt; mutableMap1 = Stream.of(new String[][]{ {\u0026#34;A\u0026#34;, \u0026#34;a\u0026#34;}, {\u0026#34;B\u0026#34;, \u0026#34;b\u0026#34;}, {\u0026#34;C\u0026#34;, \u0026#34;c\u0026#34;} }).collect(Collectors.toMap(p -\u0026gt; p[0], p -\u0026gt; p[1])); When both key and value are of different type (e.g. String and Integer):-\nMap\u0026lt;String, Integer\u0026gt; mutableMap2 = Stream.of(new Object[][]{ {\u0026#34;A\u0026#34;, 1}, {\u0026#34;B\u0026#34;, 2}, {\u0026#34;C\u0026#34;, 3} }).collect(Collectors.toMap(p -\u0026gt; (String) p[0], p -\u0026gt; (Integer) p[1])); Another approach that can easily accommodate different types for key and value involves creating a stream of map entries.\nMap\u0026lt;String, Integer\u0026gt; mutableMap3 = Stream.of( new AbstractMap.SimpleEntry\u0026lt;\u0026gt;(\u0026#34;A\u0026#34;, 1), new AbstractMap.SimpleEntry\u0026lt;\u0026gt;(\u0026#34;B\u0026#34;, 2), new AbstractMap.SimpleEntry\u0026lt;\u0026gt;(\u0026#34;C\u0026#34;, 3)) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)); Map\u0026lt;String, Integer\u0026gt; mutableMap4 = Stream.of( new AbstractMap.SimpleImmutableEntry\u0026lt;\u0026gt;(\u0026#34;A\u0026#34;, 1), new AbstractMap.SimpleImmutableEntry\u0026lt;\u0026gt;(\u0026#34;B\u0026#34;, 2), new AbstractMap.SimpleImmutableEntry\u0026lt;\u0026gt;(\u0026#34;C\u0026#34;, 3)) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)); The only difference between SimpleEntry and SimpleImmutableEntry is that you can set the value of SimpleEntry instance once initialized whereas set value of SimpleImmutableEntry after initialization throw UnsupportedOperationException.\nNote that all the maps we have initialized using streams so far are mutable map means we can add or remove elements from them. You can initialize an immutable map using streams in this way:-\nMap\u0026lt;String, Integer\u0026gt; map5 = Stream.of( new AbstractMap.SimpleEntry\u0026lt;\u0026gt;(\u0026#34;A\u0026#34;, 1), new AbstractMap.SimpleEntry\u0026lt;\u0026gt;(\u0026#34;B\u0026#34;, 2), new AbstractMap.SimpleEntry\u0026lt;\u0026gt;(\u0026#34;C\u0026#34;, 3)) .collect(Collectors.collectingAndThen( Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue), Collections::unmodifiableMap )); Conclusion Let\u0026rsquo;s look at the summary of all the ways to initialize a Map with values:-\nUsing Map.of() and Map.ofEntries() – Recommended this single line expression if you use Java 9 and above Using Java Collections – Works with all Java versions. Useful to define singleton map upto Java 8 Using Double Brace Initialization - Avoid Double braces initialization. Create a method instead. Initialize Map as an instance variable - Recommended to initialize instance variable Initialize Map as a static variable - Recommended to initialize static variable Using Stream Collectors.toMap() – Too many lines of code. We can use other alternatives if possible to avoid boilerplate code. ","permalink":"https://codingnconcepts.com/java/initialize-map-with-values-in-java/","tags":["Java Collection"],"title":"Initialize Map with Values in Java"},{"categories":["Spring Boot"],"contents":"In this article, we\u0026rsquo;ll learn step by step, how to build GraphQL API with Spring Boot.\nGraphQL GraphQL is a query language to fetch data from APIs. GraphQL is developed by Facebook as an alternate of REST APIs. GraphQL solves some of the issues which arise when REST APIs evolves and scale.\nREST vs GraphQL Any REST endpoint generally returns a response in JSON in specific format. You either get a full JSON response or nothing at all. GraphQL provides ability to query API and get exactly what you need, nothing more and nothing less. Any REST endpoint generally returns a response of specific Domain. Sometime clients need to call multiple REST endpoint to collect data. GraphQL provides ability to collect data in a single query. You perform CRUD operations in REST using different HTTP verbs (GET, POST, PUT, DELETE etc.) whereas CRUD operations in GraphQL is performed using Query (or Mutation). Clients query the GraphQL API by making an HTTP POST request with Query (or Mutation) as Request Body. GraphQL Terminology Let\u0026rsquo;s have a look at GraphQL\u0026rsquo;s basic terminology.\nSchema: is a GraphQL schema which includes Query and Mutation Query: is a read operation requested to a GraphQL Server. Mutation: is a create, update and delete operations requested to GraphQL Server. Resolver: is responsible for mapping the Query (or Mutation) operation to backend service responsible to handle the request. Fields: are the properties of the GraphQL objects for e.g. User, Post and Comment etc. Scalar: is the type of the field for e.g. Int, Float, String, Boolean, ID. ID represents unique identifier, serialized as String. Enum: is a special kind of Scalar that is restricted to a particular set of allowed values. Interface: is an abstract type that includes a certain set of fields that a type must include to implement the interface. Input: is a GraphQL object passed, which is passed in Mutation operation such as create and update. GraphQL Schema It\u0026rsquo;s time to define some GraphQL schema where you can relate to some of the GraphQL Terminology:-\nschema.graphqls # Query for read operations type Query { users: [User], # [User] means List of User userById(id:ID): User\t} # Mutation for create, update and delete operations type Mutation { createUser(input:UserInput): User, updateUser(input:UserInput): User, deleteUser(id:ID): Boolean } # Interface interface Person { id: ID!\t# id of type ID is Non-Null name: String! # name of type String is Non-Null } # User Object type User implements Person { id: ID!, name: String!, age: Int, height: Float, gender: Gender } ## Enum Scalar enum Gender { MALE FEMALE } # UserInput to use in Mutation input UserInput { name: String, age: Int, height: Float, gender: Gender } # Post Object type Post { id: ID!, userId: String, title: String, body: String, comments: [Comment] # Nested Query Object } # Comment Object type Comment { id: ID!, postId: String, name: String, email: String, body: String, post: Post # Nested Query Object } GraphQL-Java GraphQL-Java is the Java (Server) implementation of GraphQL Specification. It provies Java library to define GraphQL Schema, Query and Mutation and resolve the using Resolver.\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.graphql-java\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;graphql-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;16.2/version\u0026gt; \u0026lt;/dependency\u0026gt; You have to write a lot of boilerplate code if you use graphql-java.\nFortunately, We have two libraries available which provides a wrapper on top of graphql-java and saves use from writing boilerplate code. They are:-\nGraphQL Java Kickstart Netflix DGS GraphQL Java Kickstart GraphQL-Java-Kiskstart provides a wrapper on top of graphql-java and has following features:-\nProvide comprehensive Spring boot configuration to customize GraphQL Java Server Auto-detect schema files in src/main/resources/*.*/*.graphqls directory. This is where you write GraphQL schema, queries and mutation. Concepts of Resolver. Implement GraphQLQueryResolver, GraphQLMutationResolver and GraphQLResolver\u0026lt;T\u0026gt; to specify how to fetch data for the queries, mutation and nested data respectively. Easy integration with build tools such as GraphiQL, PlayGround and Voyager by adding runtime dependency. Provide comprehensive Spring Boot Configurations to customize these tools. Easy to write integration test using GraphQLTestTemplate provided by test dependency. Excellent tutorial series by Philip Starritt which gives you quick start. Project Setup Requirement Java 1.8 and Above Spring Boot Framework \u0026gt; 2.x.x (web) Gradle repositories { mavenCentral() } dependencies { // to turn spring boot application into GraphQL server implementation \u0026#39;com.graphql-java-kickstart:graphql-spring-boot-starter:11.0.0\u0026#39; // to embed Altair tool for schema introspection and query debugging runtimeOnly \u0026#39;com.graphql-java-kickstart:altair-spring-boot-starter:11.0.0\u0026#39; // to embed GraphiQL tool for schema introspection and query debugging runtimeOnly \u0026#39;com.graphql-java-kickstart:graphiql-spring-boot-starter:11.0.0\u0026#39; // to embed GraphQL Playground tool for schema introspection and query debugging runtimeOnly \u0026#39;com.graphql-java-kickstart:playground-spring-boot-starter:11.0.0\u0026#39; // to embed Voyager tool for visually explore GraphQL APIs as an interactive graph runtimeOnly \u0026#39;com.graphql-java-kickstart:voyager-spring-boot-starter:11.0.0\u0026#39; // testing facilities testImplementation \u0026#39;com.graphql-java-kickstart:graphql-spring-boot-starter-test:11.0.0\u0026#39; } Maven \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.graphql-java-kickstart\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;graphql-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;11.0.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- to embed Altair tool --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.graphql-java-kickstart\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;altair-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;11.0.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- to embed GraphiQL tool --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.graphql-java-kickstart\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;graphiql-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;11.0.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- to embed GraphQL Playground tool --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.graphql-java-kickstart\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;playground-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;11.0.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- to embed Voyager tool --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.graphql-java-kickstart\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;voyager-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;11.0.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- testing facilities --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.graphql-java-kickstart\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;graphql-spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;11.0.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Writing the Schema GraphQL library auto detects the schema files with \u0026ldquo;.graphqls\u0026rdquo; extension in classpath resources and wires them to model and resolver spring beans. Things to note:-\nWe can have multiple schema files in the classpath, so we can split the schema into modules as required. We must have only one root Query and one root Mutation across all the schema files. This is a limitation of the GraphQL Schema itself, and not of the Java implementation. schema.graphqls # Query for read operations type Query { users: [User], # [User] means List of User userById(id:ID): User\t} # Mutation for create, update and delete operations type Mutation { createUser(input:UserInput): User, updateUser(input:UserInput): User, deleteUser(id:ID): Boolean } GraphQL Query Resolver Next, we implement GraphQLQueryResolver to resolve Queries (Read Operations). Note that the method names (users, userById) and return types (User) are same in GraphQL schema and QueryResolver class.\n@Component @RequiredArgsConstructor public class UserQueryResolver implements GraphQLQueryResolver { private final UserService userService; List\u0026lt;User\u0026gt; users() { return userService.getAllUsers();} User getUserById(Long userId) { return userService.getUserById(userId);} } GraphQL Mutation Resolver Next, we implement GraphQLMutationResolver to resolve Mutations (Create, Update and Delete Operations). Note the method names and return types.\n@Component @RequiredArgsConstructor public class UserMutationResolver implements GraphQLMutationResolver { private final UserService userService; User createUser(UserInput input) { return userService.createUser(input); } User updateUser(UserInput input) { return userService.updateUser(input); } Boolean deleteUser(Long id) { return userService.deleteUser(id); } } GraphQL Field Resolver Sometimes, GraphQL object can have nested GraphQL object, which result in nested queries. We implement GraphQLResolver\u0026lt;T\u0026gt; to resolve such nested Queries. For e.g. User can have multiple posts [Post]\n@Component @RequiredArgsConstructor public class UserPostResolver implements GraphQLResolver\u0026lt;User\u0026gt; { private final PostService postService; List\u0026lt;Post\u0026gt; posts(User user) { return postService.getAllPostsByUserId(user.getId());} } Next Steps This is a good starting point. Next steps are configuring tools such as Playground and Voyager, Write Test cases etc. You can download graphql-java-kickstart-example github project for more details.\nNetflix DGS Netflix-DGS is developed by Netflix on top of graphql-java and recently made it public to use. It has following features:-\nDo not provide Spring boot based configurations Auto-detect schema files in src/main/resources/schema/*.*/*.graphqls directory. This is where you write GraphQL schema, queries and mutation. Concepts of DataFetcher. Provide annotations @DgsComponent at class level and @DgsQuery, @DgsMutation, @DgsData at method level to specify how to fetch data for the queries, mutation and nested data respectively. Provide integration with GraphiQL Provide good support to write unit and integration test cases using DgsQueryExecutor Follow example to quick start Project Setup Requirement Java 1.8 and Above Spring Boot Framework \u0026gt; 2.x.x (web) Gradle repositories { mavenCentral() } dependencies { implementation \u0026#34;com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter:latest.release\u0026#34; } Maven \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.netflix.graphql.dgs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;graphql-dgs-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;!-- Make sure to set the latest framework version! --\u0026gt; \u0026lt;version\u0026gt;${dgs.framework.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; DataFetcher We can resolve Queries and Mutations, all in a single DataFetcher class file. We use @DgsComponent annotation at class level for auto spring bean registy and use following annotations at method level:-\n@DgsQuery for Root Queries @DgsMutation for Root Mutations, and @DgsData for Nested Queries @DgsComponent @RequiredArgsConstructor public class UserDataFetcher { private final UserService userService; private final PostService postService; @DgsQuery public List\u0026lt;User\u0026gt; users() { return userService.getAllUsers();} @DgsQuery public User userById(String id) { return userService.getUserById(Long.parseLong(id));} @DgsMutation public User createUser(@InputArgument(\u0026#34;input\u0026#34;) UserInput userInput) { return userService.createUser(userInput); } @DgsMutation public User updateUser(@InputArgument(\u0026#34;input\u0026#34;) UserInput userInput) { return userService.updateUser(userInput); } @DgsMutation public Boolean deleteUser(String id) { return userService.deleteUser(Long.parseLong(id)); } @DgsData(parentType = \u0026#34;User\u0026#34;) public List\u0026lt;Post\u0026gt; posts(DgsDataFetchingEnvironment dfe){ User user = dfe.getSource(); return postService.getAllPostsByUserId(user.getId()); } } Next Steps Next steps are to try GraphiQL tool for Query introspection and debugging exposed at /grahiql, Write test cases etc. You can download netflix-dgs-example github project for more details.\nConclusion In this tutorial, We learned about GraphQL basics, its terminology. We also compared the two GraphQL-Java wrapper libraries, GraphQL Java Kickstart and Netflix DGS and their usage.\n","permalink":"https://codingnconcepts.com/spring-boot/build-graphql-api-with-spring-boot/","tags":["Spring Boot API","GraphQL"],"title":"Build GraphQL API with Spring Boot"},{"categories":["Tools"],"contents":" Calculate Income Tax for Singapore Tax Residents based on latest tax rates provided by IRAS\n(A) Salary Monthly Yearly (B) Bonus Monthly Yearly (C) Tax Saving Monthly Yearly *Age is used to calculate Tax Relief Your Age (D) Tax Relief (A+B-C-D) Taxable Income Income Tax $ Income Tax Rates for Singapore Residents Chargeable Income Income Tax Rate (%) Gross Tax Payable ($) First $20,000 0 0 $20,000 - $30,000 2 0 - $200 $30,000 - $40,000 3.5 $200 - $550 $40,000 - $80,000 7 $550 - $3,350 $80,000 - $120,000 11.5 $3350 - $7,950 $120,000 - $160,000 15 $7,950 - $13,950 $160,000 - $200,000 18 $13,950 - $21,150 $200,000 - $240,000 19 $21,150 - $28,750 $240,000 - $280,000 19.5 $28,750 - $36,550 $280,000 - $320,000 20 $36,550 - $44,550 $320,000 - $500,000 22 $44,550 - $84,150 $500,000 - $1,000,000 23 $84,150 - $199,150 Above $1,000,000 24 Above $199,150 Reference: https://www.iras.gov.sg/taxes/individual-income-tax/basics-of-individual-income-tax/tax-residency-and-tax-rates/individual-income-tax-rates\nAmount of Earned Income Relief The amount of relief (shown in the table below) is based on your age and taxable earned income in the previous year.\nYour age as of 31 Dec of the previous year *Maximum amount claimable Below 55 $1,000 55 to 59 $6,000 60 and above $8,000 Reference: https://www.iras.gov.sg/taxes/individual-income-tax/basics-of-individual-income-tax/tax-residency-and-tax-rates/individual-income-tax-rates\n","permalink":"https://codingnconcepts.com/tools/singapore-income-tax/","tags":["Calculator"],"title":"Income Tax Calculator for Singapore Tax Residents"},{"categories":["Javascript"],"contents":"Optional chaining ?. operator is used to access the nested object properties with implicit nullish check.\nOverview How do you access the nested property of an object with nullish ( null and undefined) check? Say we have to access user details from a response of a web api?\nYou can use a nested ternary operator ? ... : like this:-\nconst userName = response ? (response.data ? (response.data.user ? response.data.user.name : null) : null) : null; or you can have nullish check in if condition like this:-\nlet userName = null; if(response \u0026amp;\u0026amp; response.data \u0026amp;\u0026amp; response.data.user){ userName = response.data.user.name; } or even better can make it a single liner chained \u0026amp;\u0026amp; condition like this:-\nconst userName = response \u0026amp;\u0026amp; response.data \u0026amp;\u0026amp; response.data.user \u0026amp;\u0026amp; response.data.user.name; What is common in above code is that chaining can go really lengthy at times and becomes more difficult to format and read. This is where optional chaining ?. operator comes to the rescue which provides implicit nullish check and make our code even smaller and better.\nconst userName = response?.data?.user?.name; Isn\u0026rsquo;t it?\nSyntax Optional chaining ?. operator is introduced in Javascript ES2020 which has following syntax:-\nobj.val?.prop returns obj.val.prop if val exists, otherwise undefined. obj.func?.(args) returns obj.func(args) if func exists, otherwise undefined. obj.arr?.[index] returns obj.array[index] if array exists, otherwise undefined. Using optional chaining ?. operator Let\u0026rsquo;s see usage of ?. operator with user object:-\nconst user = { name: \u0026#34;John\u0026#34;, age: 21, homeaddress: { country: \u0026#34;USA\u0026#34; }, hobbies: [{name: \u0026#34;Coding\u0026#34;}, {name: \u0026#34;Cooking\u0026#34;}], getFirstName: function(){ return this.name; } } with Properties Access a property which exist returns value:-\nconsole.log(user.homeaddress.country); // prints \u0026#34;USA\u0026#34;; Access a property which doesn\u0026rsquo;t exist throws error:-\nconsole.log(user.officeaddress.country); // throws error \u0026#34;Uncaught TypeError: Cannot read property \u0026#39;country\u0026#39; of undefined\u0026#34; Access a property with Optional chaining operator ?. which doesn\u0026rsquo;t exist returns undefined:-\nconsole.log(user.officeaddress?.country); // prints \u0026#34;undefined\u0026#34; with Functions Call a function which exist returns value:-\nconsole.log(user.getFirstName()); // prints \u0026#34;John\u0026#34;; Call a function which doesn\u0026rsquo;t exist throws error:-\nconsole.log(user.getLastName()); // throws error \u0026#34;Uncaught TypeError: user.getLastName is not a function\u0026#34;; Call a function with Optional chaining operator ?. which doesn\u0026rsquo;t exist returns undefined:-\nconsole.log(user.getLastName?.()); // prints \u0026#34;undefined\u0026#34; with Arrays Access existing index of an array returns value:-\nconsole.log(user.hobbies[0].name); // prints \u0026#34;Coding\u0026#34;; Access non-existing index of an array throws error:-\nconsole.log(user.hobbies[3].name); // throws error \u0026#34;Uncaught TypeError: Cannot read property \u0026#39;name\u0026#39; of undefined\u0026#34; Access non-existing index of an array with Optional chaining operator ?. returns undefined:-\nconsole.log(user.hobbies[3]?.name); // prints \u0026#34;undefined\u0026#34; Access an array which doesn\u0026rsquo;t exist throws error:-\nconsole.log(user.dislikes[0].name); // throws error \u0026#34;Uncaught TypeError: Cannot read property \u0026#39;0\u0026#39; of undefined\u0026#34; Access an array with Optional chaining operator ?. which doesn\u0026rsquo;t exist returns undefined:-\nconsole.log(user.dislikes?.[0]?.name); // prints \u0026#34;undefined\u0026#34; with Nullish Coalescing ?? Operator Now you know that optional chaining operator ?. returns undefined if object doesn\u0026rsquo;t exist. Sometime you want to return a value instead of undefined which is possible by using Nullish Coalescing ?? operator along with optional chaining ?. operator.\nWithout Nullish Coalescing ?? Operator, returns undefined:-\nconst country = user.officeaddress?.country; console.log(country); // prints \u0026#34;undefined\u0026#34; With Nullish Coalescing ?? Operator, returns default value:-\nconst country = user.officeaddress?.country ?? \u0026#34;USA\u0026#34;; console.log(country); // prints \u0026#34;USA\u0026#34; ","permalink":"https://codingnconcepts.com/javascript/optional-chaining-operator-javascript/","tags":["Javascript Operator"],"title":"Optional Chaining ?. operator in JavaScript"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to loop through elements of an Array using different ways in JavaScript.\nfor loop The for is used to loop through an the index of an Array\nconst array = [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;]; for(var i=0; i\u0026lt;array.length; i++){ console.log(i, array[i]); } Output 0 \u0026#34;one\u0026#34; 1 \u0026#34;two\u0026#34; 2 \u0026#34;three\u0026#34; for-in loop The for-in statement loops through the index of an Array.\nconst array = [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;]; for (const index in array){ console.log(index, array[index]); } Output 0 \u0026#34;one\u0026#34; 1 \u0026#34;two\u0026#34; 2 \u0026#34;three\u0026#34; for-of loop The for-of statement loops through the values of an Array.\nconst array = [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;]; for (const element of array){ console.log(element); } Output one two three Array.forEach() The Array.forEach() method takes a callback function to loop through an Array. We can use ES6 arrow functions in callback.\nconst array = [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;]; array.forEach((item, index) =\u0026gt; console.log(index, item)); Output 0 \u0026#34;one\u0026#34; 1 \u0026#34;two\u0026#34; 2 \u0026#34;three\u0026#34; That\u0026rsquo;s it! These are the four different ways to loop-through an Array in JavaScript. It is recommended to use Array.forEach() with arrow function which makes your code very short and easy to understand.\nThough there is a limitation that we can not use break; and continue; flow control statements with Array.forEach() method. If you want to do so, use for, for-in or for-of loop instead.\n","permalink":"https://codingnconcepts.com/javascript/how-to-loop-through-array-in-javascript/","tags":["Javascript Array"],"title":"How to Loop through an Array in JavaScript"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to search elements in an Array using different Array methods in JavaScript.\nArray.filter() Array.filter() method takes a condition as a function and return an array of elements satisfying that condition.\nExample 1 const array = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]; const allEvenNumbers = array.filter(n =\u0026gt; n%2==0); // [2, 4, 6, 8, 10] const allOddNumbers = array.filter(n =\u0026gt; n%2==1); // [1, 3, 5, 7, 9] const greaterThanNine = array.filter(n =\u0026gt; n\u0026gt;9); // [10] Example 2 const fruits = [ { name: \u0026#34;mango\u0026#34;, color: \u0026#34;yellow\u0026#34;, calories: 135}, { name: \u0026#34;banana\u0026#34;, color: \u0026#34;yellow\u0026#34;, calories: 60 }, { name: \u0026#34;apple\u0026#34;, color: \u0026#34;red\u0026#34;, calories: 65 }, { name: \u0026#34;orange\u0026#34;, color: \u0026#34;orange\u0026#34;, calories: 50 }, { name: \u0026#34;kiwi\u0026#34;, color: \u0026#34;green\u0026#34;, calories: 46 }, ]; const yellowFruits = fruits.filter(fruit =\u0026gt; fruit.color == \u0026#34;yellow\u0026#34;).map(fruit =\u0026gt; fruit.name); // [\u0026#34;mango\u0026#34;, \u0026#34;banana\u0026#34;] const lowCalories = fruits.filter(fruit =\u0026gt; fruit.calories \u0026lt;=50).map(fruit =\u0026gt; fruit.name); // [\u0026#34;orange\u0026#34;, \u0026#34;kiwi\u0026#34;] Array.find() Array.find() method takes a condition as a function and return the first element satisfying that condition.\nExample 1 const array = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]; const allEvenNumbers = array.find(n =\u0026gt; n%2==0); // 2 const allOddNumbers = array.find(n =\u0026gt; n%2==1); // 1 const greaterThanNine = array.find(n =\u0026gt; n\u0026gt;9); // 10 Example 2 const fruits = [ { name: \u0026#34;mango\u0026#34;, color: \u0026#34;yellow\u0026#34;, calories: 135}, { name: \u0026#34;banana\u0026#34;, color: \u0026#34;yellow\u0026#34;, calories: 60 }, { name: \u0026#34;apple\u0026#34;, color: \u0026#34;red\u0026#34;, calories: 65 }, { name: \u0026#34;orange\u0026#34;, color: \u0026#34;orange\u0026#34;, calories: 50 }, { name: \u0026#34;kiwi\u0026#34;, color: \u0026#34;green\u0026#34;, calories: 46 }, ]; const yellowFruits = fruits.find(fruit =\u0026gt; fruit.color == \u0026#34;yellow\u0026#34;); console.log(yellowFruits); // prints {name: \u0026#34;mango\u0026#34;, color: \u0026#34;yellow\u0026#34;, calories: 135} const lowCalories = fruits.find(fruit =\u0026gt; fruit.calories \u0026lt;=50); console.log(lowCalories); // prints {name: \u0026#34;orange\u0026#34;, color: \u0026#34;orange\u0026#34;, calories: 50} Array.includes() Array.includes() method look for the given value and return true if find in the array, otherwise return false.\nExample 1 const array = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]; const includesTen = array.includes(10); // true const includesTwenty = array.includes(20); // false Example 2 const fruits = [ { name: \u0026#34;mango\u0026#34;, color: \u0026#34;yellow\u0026#34;, calories: 135}, { name: \u0026#34;banana\u0026#34;, color: \u0026#34;yellow\u0026#34;, calories: 60 }, { name: \u0026#34;apple\u0026#34;, color: \u0026#34;red\u0026#34;, calories: 65 }, { name: \u0026#34;orange\u0026#34;, color: \u0026#34;orange\u0026#34;, calories: 50 }, { name: \u0026#34;kiwi\u0026#34;, color: \u0026#34;green\u0026#34;, calories: 46 }, ]; const includesMango = fruits.map(fruit =\u0026gt; fruit.name).includes(\u0026#34;mango\u0026#34;); // true const includesBlue = fruits.map(fruit =\u0026gt; fruit.color).includes(\u0026#34;blue\u0026#34;); // false Example 3 Count the number of vowels in a given string:-\nconst vowels = [\u0026#39;a\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;u\u0026#39;]; const string = \u0026#34;codingnconcepts\u0026#34;; const countVowels = word.split(\u0026#34;\u0026#34;).map(char =\u0026gt; vowels.includes(char) ? 1 : 0).reduce((a, b) =\u0026gt; a + b); console.log(countVowels); // prints \u0026#34;4\u0026#34; Array.indexOf() Array.indexOf() method look for the given value and return index if find in the array, otherwise return -1.\nExample 1 const array = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]; const indexOfTen = array.indexOf(10); // 9 const indexOfTwenty = array.indexOf(20); // -1 Example 2 const fruits = [ { name: \u0026#34;mango\u0026#34;, color: \u0026#34;yellow\u0026#34;, calories: 135}, { name: \u0026#34;banana\u0026#34;, color: \u0026#34;yellow\u0026#34;, calories: 60 }, { name: \u0026#34;apple\u0026#34;, color: \u0026#34;red\u0026#34;, calories: 65 }, { name: \u0026#34;orange\u0026#34;, color: \u0026#34;orange\u0026#34;, calories: 50 }, { name: \u0026#34;kiwi\u0026#34;, color: \u0026#34;green\u0026#34;, calories: 46 }, ]; const indexOfMango = fruits.map(fruit =\u0026gt; fruit.name).indexOf(\u0026#34;mango\u0026#34;); // 0 const indexOfBlue = fruits.map(fruit =\u0026gt; fruit.color).indexOf(\u0026#34;blue\u0026#34;); // -1 ","permalink":"https://codingnconcepts.com/javascript/how-to-search-array-in-javascript/","tags":["Javascript Array"],"title":"How to Search an Array in JavaScript"},{"categories":["Hugo"],"contents":"If you are using Hugo as a static site generator tool and using google analytics for your website then you can customize the analytics script and replace the old google analytics script analytics.js with recommended new script gtag.js with just few changes.\nAdd property to config.toml If you are already using google analytics for your website then this property should be there otherwise you can add.\nconfig.toml googleAnalytics = \u0026#34;UA-123456789-1\u0026#34; # Get this Tracking-ID from your google analytics account You can create an account to register your website with Google Analytics if you don\u0026rsquo;t have one and get this Tracking-ID.\nCreate google_analytics.html Partial Next, We are going to create a partial called google_analytics.html which is responsible for injecting google analytics script in your hugo blog pages. All you need to do is copy the below code snippet and save it under $hugo/layouts/partials/google_analytics.html.\ngoogle_analytics.html {{- with .Site.GoogleAnalytics -}} \u0026lt;!-- Global site tag (gtag.js) - Google Analytics --\u0026gt; \u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;js\u0026#39;, new Date()); gtag(\u0026#39;config\u0026#39;, \u0026#39;{{ . }}\u0026#39;); \u0026lt;/script\u0026gt; \u0026lt;script async src=\u0026#34;https://www.googletagmanager.com/gtag/js?id={{ . }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{- end -}} Add google_analytics.html partial to baseof.html Next, You need to add google_analytics.html partial to baseof.html file (or any other file which is responsible to generate \u0026lt;head\u0026gt;\u0026lt;/head\u0026gt; section of Hugo pages) as below:-\nbaseof.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; ... {{- if not .Site.IsServer }} {{ partial \u0026#34;google_analytics.html\u0026#34; . }} {{- end }} ... \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; ... \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; This above code snippet add the google analytics script in your hugo pages when it is not running as local server.\nRemove internal partial from baseof.html Please note that most of the hugo themes use Hugo\u0026rsquo;s internal google analytics template _internal/google_analytics_async.html. If you come across below code snippet in baseof.html file (or any other file) then you can remove that since this is no longer required.\n{{- if not .Site.IsServer }} {{ template \u0026#34;_internal/google_analytics_async.html\u0026#34; . }} {{- end }} That\u0026rsquo;s it. Congratulations. You have added custom google analytics script to your pages successfully.\n","permalink":"https://codingnconcepts.com/hugo/custom-google-analytics-hugo/","tags":null,"title":"Customize Google Analytics in Hugo Website"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn basics of Java Annotations, Built-in Java Annotations, How to create single-value, multi-value, repeated and type annotations and How to use them at runtime.\nOverview Annotations are very powerful feature of Java. You must have come across @Override, @Deprecated, @SuppressWarnings, which are built-in annotations in Java.\nMost of the Java-based framework heavily use annotations. For instance, Spring Framework provides @Autowired, @Controller, @Component, \u0026hellip; . Similarly Hibernate Framework provides @Entity, @Table, @Column, \u0026hellip; . Annotations make these frameworks easier to use and at the same time make the source code more readable.\nWhat are Annotations? Annotations are used to add some marker or data at class, method, or field level. This data is used at compile-time or run-time to perform some actions.\nIt is important to note that Annotations doesn\u0026rsquo;t have any direct effect on the behavior of class, method, or field, where it is applied. For example, adding @Override, @Deprecated or @SuppressWarnings at method level doesn\u0026rsquo;t change the output of that method.\nHowever frameworks such as Spring and Hibernate use these annotations to perform some actions at runtime. For example, adding @Autowired at field level instruct the Spring framework to inject the dependency of this field at runtime.\nLet\u0026rsquo;s understand two important concepts of annotations now - Target and Retention Policy\nTarget Target defines where the annotation can be applied for example, at class, method or field level. This information is required while creating an annotation. There are the types of possible targets:-\n@Target(ElementType.*) Annotation can be applied on \u0026hellip; ElementType.TYPE class, interface or enum ElementType.METHOD method ElementType.FIELD field ElementType.CONSTRUCTOR constructor ElementType.PARAMETER parameter of a method or constructor ElementType.PACKAGE package Note that some annotation like @Override can be applied at method level only while others like @Deprecated, @SuppressWarnings can be applied at any level - class, method, or field.\nRetention Policy Retention policy defines the life-cycle of the annotation. This information is also required while creating an annotation. There are three types of retention policies:-\n@Retention(RetentionPolicy.*) Description RetentionPolicy.SOURCE Annotations are retained only in source code but ignored in .class file by the compiler RetentionPolicy.CLASS Annotations are retained in .class file by the compiler but ignored by the JVM at run time. This is the default behavior RetentionPolicy.RUNTIME Annotations are retained in .class file by the compiler and also retained by the JVM at run time, so they can be used by java reflection Note that you normally use RetentionPolicy.RUNTIME while creating your own custom annotation because you may want to use the data from your annotation at runtime using java reflection. We will talk more about it later in the post while creating custom annotations.\nJava Built-in Annotations Now we understand that What is Target and Retention Policy, which are two required information to create any annotation, Let\u0026rsquo;s look at the java built-in annotations:-\n@Override @Target(ElementType.METHOD) @Retention(RetentionPolicy.SOURCE) public @interface Override { } We see that @Override annotation can be applied at methods only (target) and part of the source code only (retention policy).\n@Override public String toString() { } When we use @Override annotation at method level, it informs the complier that this method is meant to override a method declared in a superclass. It assures that method is overridden with correct signature. If not so, complier throws compilation error.\n@Deprecated @Target(value={CONSTRUCTOR, FIELD, LOCAL_VARIABLE, METHOD, PACKAGE, PARAMETER, TYPE}) @Retention(RetentionPolicy.RUNTIME) public @interface Deprecated { } We see that @Deprecated annotation can be applied anywhere such as class, field, method, constructor, package, or method arguments level. Also it is retained at runtime means information can be extracted from this annotation using java reflection at runtime.\n@Deprecated static void deprecatedMethod() { } } When we use @Deprecated annotation at method level, it informs the compiler that this method is deprecated and should no longer be used. The compiler generates a warning whenever a program uses this method.\n@SuppressWarnings @Target({TYPE, FIELD, METHOD, PARAMETER, CONSTRUCTOR, LOCAL_VARIABLE}) @Retention(RetentionPolicy.SOURCE) public @interface SuppressWarnings { String[] value(); } We see that @SuppressWarnings annotation can be applied anywhere and retained in the source code only. We also see that this annotations takes comma separated String values. Few possible values are \u0026ldquo;rawtypes\u0026rdquo;, \u0026ldquo;unused\u0026rdquo;, \u0026ldquo;unchecked\u0026rdquo; and \u0026ldquo;deprecation\u0026rdquo;.\n@SuppressWarnings({ \u0026#34;rawtypes\u0026#34;, \u0026#34;unused\u0026#34;, \u0026#34;unchecked\u0026#34;, \u0026#34;deprecation\u0026#34; }) @SuppressWarnings(\u0026#34;deprecation\u0026#34;) void useDeprecatedMethod() { // suppress the deprecation warning while calling a deprecated method objectOne.deprecatedMethod(); } @SuppressWarnings({ \u0026#34;rawtypes\u0026#34;, \u0026#34;unchecked\u0026#34; }) void useRawCollection() { // suppress the rawtypes warning when initialize non-parameterized (raw) ArrayList // suppress the unchecked warning when adding an integer to raw ArrayList ArrayList list=new ArrayList(); list.add(1); } When we use @SuppressWarnings annotation at method level, it informs the compiler to suppress the warnings which it would generate otherwise.\nHow to create Custom Annotation? Let\u0026rsquo;s create a custom Annotation @ClassInfo\nAnnotations are created by using @interface, followed by annotation name which is ClassInfo in our case. An annotation class can have one or more elements. They look like methods. For example in the below code, we have six elements namely author, date, currentRevision, lastModified, lastModifiedBy, and reviewers. Default values to element can be assigned using default keyword @Documented @Inherited @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @interface ClassInfo { String author(); String date(); int currentRevision() default 1; String lastModified() default \u0026#34;N/A\u0026#34;; String lastModifiedBy() default \u0026#34;N/A\u0026#34;; // Note use of array String[] reviewers(); } Meta-Annotations You see that we have applied few annotations to our custom annotation ClassInfo. These are called as meta-annotations, which provide additional information about our custom annotation. They are as follows:-\n@Documented indicates to include the @ClassInfo annotation in JavaDoc. By default annotations are not included in JavaDoc. @Inherited indicates to inherit the @ClassInfo annotation in subclasses. By default annotations are not inherited. @Target(ElementType.TYPE) indicates that @ClassInfo annotation can be apply at class level only @Retention(RetentionPolicy.RUNTIME) indicates that @ClassInfo annotation elements can be accessed during runtime using java reflection. How to use Custom Annotation? Now we have created our custom annotation @ClassInfo, Let\u0026rsquo;s use it on the class\n@ClassInfo ( author = \u0026#34;John Doe\u0026#34;, date = \u0026#34;3/17/2002\u0026#34;, // Note array notation reviewers = {\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Cindy\u0026#34;} ) class MyClass { } Note that we have skipped currentRevision, lastModified, lastModifiedBy elements while using @ClassInfo annotation. Default values are assigned to these elements.\nLet\u0026rsquo;s see how to get the information from our custom @ClassInfo annotation at runtime\npublic class AnnotationsTest { public static void main(String[] args) { printClassInfo(new MyClass()); } public static void printClassInfo(Object object) { Class\u0026lt;?\u0026gt; clazz = object.getClass(); if (clazz.isAnnotationPresent(ClassInfo.class)) { ClassInfo classInfo = clazz.getAnnotation(ClassInfo.class); System.out.println(\u0026#34;author: \u0026#34; + classInfo.author()); System.out.println(\u0026#34;date: \u0026#34; + classInfo.date()); System.out.println(\u0026#34;currentRevision: \u0026#34; + classInfo.currentRevision()); System.out.println(\u0026#34;lastModified: \u0026#34; + classInfo.lastModified()); System.out.println(\u0026#34;lastModifiedBy: \u0026#34; + classInfo.lastModifiedBy()); System.out.println(\u0026#34;reviewers: \u0026#34; + Arrays.toString(classInfo.reviewers())); } else { System.out.println(\u0026#34;Class Info is not available\u0026#34;); } } } Output author: John Doe date: 3/17/2002 currentRevision: 1 lastModified: N/A lastModifiedBy: N/A reviewers: [Alice, Bob, Cindy] @Inherited class MyChildClass extends MyClass {} Remember, We applied @Inherited meta-annotation on our custom annotation class ClassInfo while creation. That means, @ClassInfo info applied at MyClass will be inherited by its child class MyChildClass as well resulting both produce same output when passed in printClassInfo method.\npublic static void main(String[] args) { printClassInfo(new MyClass()); printClassInfo(new MyChildClass()); // same output as: new MyClass() } Repeated Annotation Java 8 has introduced the Repeated annotation where same annotation can be used multiple times at class, method or field level. Before Java 8, this result into compilation error.\nLet\u0026rsquo;s create @Author annotation which can be used repeatedly at class level. It requires two steps:-\nFirst create a repeatable annotation @Author. We need to apply @Repeatable meta-annotation on our custom annotation to use it multiple time. Note that we need to pass a container annotation class in @Repeatable. Second create a container annotation @Authors which holds all repeatable Author values. This container annotation must have an element named value() and of type array Author[] @Repeatable(Authors.class) @interface Author { String name(); } @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @interface Authors { Author[] value(); //must have this signature } Now we have created our repeatable @Author annotation. Let\u0026rsquo;s use it on MyClass multiple times and get the list of all the authors at runtime.\n@Author(name = \u0026#34;Author 1\u0026#34;) @Author(name = \u0026#34;Author 2\u0026#34;) @Author(name = \u0026#34;Author 3\u0026#34;) class MyClass { } Java 8 internally treats this Repeating Annotation as an instance of @Authors holding an array of @Author.\npublic class RepeatedAnnotationsTest { public static void main(String[] args) {\tSystem.out.println(getListOfAuthors(new MyClass())); } public static List\u0026lt;String\u0026gt; getListOfAuthors(Object object) { Class\u0026lt;?\u0026gt; clazz = object.getClass(); if (clazz.isAnnotationPresent(Authors.class)) { Author[] authors = clazz.getAnnotationsByType(Author.class); return Arrays.stream(authors).map(author -\u0026gt; author.name()).collect(Collectors.toList()); } else { System.out.println(\u0026#34;Authors not available\u0026#34;); } return null; } } Output [Author 1, Author 2, Author 3] Type Annotation Java 8 has introduced Type annotation by adding two new target types. Let\u0026rsquo;s use them to create @NotNull annotation.\n@Target({ElementType.TYPE_USE, ElementType.TYPE_PARAMETER}) @Retention(RetentionPolicy.RUNTIME) public @interface NotNull { } The ElementType.TYPE_PARAMETER means the annotation can be applied on the declaration of a type variable (e.g., class MyClass {\u0026hellip;}).\nThe ElementType.TYPE_USE means the annotation can be applied on any use of a type (e.g., types appearing in declarations, generics, and type casts)\nLet\u0026rsquo;s use the @NotNull annotation on the type declaration of a field and use it to validate the null value. Throw exception if any field declared with @NotNull annotation contains null value.\nclass MyClass { private @NotNull String key; } public class TypeAnnotationsTest { public static void main(String[] args) throws IllegalArgumentException, IllegalAccessException { validateProperties(new MyClass()); // value of field key is null } public static void validateProperties(Object object) throws IllegalArgumentException, IllegalAccessException { Class\u0026lt;?\u0026gt; clazz = object.getClass(); Field[] fields = clazz.getDeclaredFields(); for(Field field: fields) { field.setAccessible(true); if(field.getAnnotatedType().isAnnotationPresent(NotNull.class)) { if(field.get(object) == null) { throw new NullPointerException(\u0026#34;@NotNull field \\\u0026#34;\u0026#34; + field.getName() + \u0026#34;\\\u0026#34; can not be null\u0026#34;); } } } } } Output Exception in thread \u0026#34;main\u0026#34; java.lang.NullPointerException: @NotNull field \u0026#34;key\u0026#34; can not be null at com.example.core.TypeAnnotationsTest.validateProperties(TypeAnnotationsTest.java:14) at com.example.core.TypeAnnotationsTest.main(TypeAnnotationsTest.java:4) Some of the example usage of type annotations are as follows:-\n// declaration @Encrypted String data; // generics List\u0026lt;@NonNull String\u0026gt; strings; // type cast String myString = (@NonNull String) myObject; // constructor new @Interned MyObject(); new @NonEmpty @Readonly List\u0026lt;String\u0026gt;(myNonEmptyStringSet); // implements clause class UnmodifiableList\u0026lt;T\u0026gt; implements @Readonly List\u0026lt;T\u0026gt; { ... } // throw exception declaration void monitorTemperature() throws @Critical TemperatureException { ... } Conclusion We see that Java Annotations are widely used by Java and its popular framework such as Spring and Hibernate. Most of the time, we as a developer are consumers of these annotations, rather than their creator. Though it is good to have knowledge of how custom annotations are created and used behind the scene.\n","permalink":"https://codingnconcepts.com/java/annotations-in-java/","tags":["Core Java","Annotation"],"title":"All About Annotations in Java"},{"categories":null,"contents":"Elasticsearch (popularly known as ELK stack) is an open-source, distributed, and near real-time search engine, Which can be used to solve a variety of use-cases. Let\u0026rsquo;s understand the basic concepts of Elasticsearch in this post.\nWhat is Elasticsearch? Elasticsearch is an open-source, distributed, and near real-time search engine. Elasticsearch is complimented by Logstash and Kibana to make it more powerful. This trio combination is known as ELK stack. Logstash are used to collect the data in realtime from .log or .txt files and store it in Elasticsearch. Kibana is used to explore, visualize and query data in realtime stored in Elasticsearch Uses of Elasticsearch? Elasticsearch is most popularly used for following use cases:-\nE-commerce website like search box where you can search for products from a very large variety of product inventory. Google like search engine where you get search suggestions as you type ahead, fuzzy search i.e. \u0026ldquo;do you mean this?\u0026rdquo; when you type something wrong, and get the search result sorted by most relevant first. Relevancy logic can be customized based on the search term is appearing in title, description, keywords, or body content of the document. Centralized logging and monitoring system in Microservice-based applications. NoSQL, flexible-schema, JSON document based storage system. How does Elasticsearch works? Elasticsearch is built on top of Apache Lucene search engine Java Library. Elasticsearch is also written in Java so require Java environment to run it. Let\u0026rsquo;s understand the key concepts of Elasticsearch:-\nDocument Document is basic unit of data in JSON format which are indexed in Elasticsearch. Document can have just a text (unstructured data) or collection of key-value pairs (structured data). You can think of a document like a record or row in a relational database.\nField Each document is a collection of fields, which are key-value pairs. Field\u0026rsquo;s value can be of type text, numeric, date, or geo. You can think of a field like a column in a relational database.\nIndex An index is a collection of documents that have similar characteristics. An index is the highest level entity that you can query against in Elasticsearch. You can think of the index as being similar to a database in a relational database schema. Any documents in an index are typically logically related. In the context of an e-commerce website, for example, you can have an index for Customers, one for Products, one for Orders, and so on. An index is identified by a name that is used to refer to the index while performing indexing, search, update, and delete operations against the documents in it.\nInverted Index Elasticsearch uses an inverted index that supports very fast full-text searches. An inverted index is a data structure which stores a mapping of each unique word, to its appearance in document or a set of documents. It directs you from a word to document(s). An inverted index split the string in document to individual search term (i.e. word) and then map each search term to the document.\ndoc_id_1 = I love Vanilla Cake. Cake makes me happy. doc_id_2 = Both Chocolate and Vanilla Cake are available in our store. doc_id_3 = I bought Cookies for the party. Term Document Frequency ------------------------------------------------------------------------ Vanilla doc_id_1, doc_id_2 2 (1 in doc_id_1, 1 in doc_id_2) Cake doc_id_1, doc_id_2 3 (2 in doc_id_1, 1 in doc_id_2) Chocolate doc_id_2 1 (1 in doc_id_2) Cookies doc_id_3 1 (1 in doc_id_3) Cluster An Elasticsearch cluster is a group of one or more node instances that are connected together. The power of an Elasticsearch cluster lies in the distribution of data, searching, and indexing, across all the nodes in the cluster. You can think of a cluster as distributed relational database.\nNode A node is a single server that is a part of a cluster. A node stores data and participates in the cluster’s indexing and search capabilities.\nShard Elasticsearch provides the ability to subdivide the index into multiple pieces called shards. Shard allows an index to be distributed in a cluster. Each shard is in itself a fully-functional and independent Lucene “index” that can be hosted on any node within a cluster. By distributing the documents in an index across multiple shards, and distributing those shards across multiple nodes, Elasticsearch can ensure redundancy, which both protects against hardware failures and increases query capacity as nodes are added to a cluster.\nReplica Elasticsearch allows you to make one or more copies of your index’s shards which are called “replica shards” or just “replicas”. Basically, a replica shard is a copy of a primary shard. Each document in an index belongs to one primary shard. Replicas provide redundant copies of your data to protect against hardware failure and increase capacity to serve read requests like searching or retrieving a document.\nAnalogy with Relational Database Elasticsearch Relational Database Cluster Database Shard Shard Index Table Field Column Document Row How to search data from Elasticsearch? Elasticsearch provides REST APIs to perform search queries which can be executed from command-line using Curl or through the Developer Console in Kibana. Elasticsearch also provides clients in many languages such as Java, JavaScript, Go, .NET, PHP, Perl, Python, and Ruby. Kibana can be used to search, explore and visualize data stored in Elasticsearch without much technical knowledge. Elasticsearch REST APIs provide JSON style comprehensive search capabilities using Query DSL. ","permalink":"https://codingnconcepts.com/post/elastic-search-basics/","tags":["ElasticSearch","Popular Posts"],"title":"Elastic Search - Basic Concepts"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to generate a random number, generate a random array or list, get a random element from an array or list, and shuffle the list elements.\nGenerate Random Number Java Random class is having many useful built-in methods for generating random numbers as follows:-\nnextInt(): Returns a random int value within the range: -2,147,483,648\u0026lt;= value \u0026lt;= 2,147,483, 647 nextInt(int range): Returns a random int value within the range: 0 \u0026lt;= value \u0026lt; range nextDouble(): Returns a random double value within the range: 0.0 \u0026lt;= value \u0026lt; 1.0 nextFloat(): Returns a random float value within the range: 0.0 \u0026lt;= value \u0026lt; 1.0 nextLong(): Returns a random long value. import java.util.Random; class generateRandom { public static void main( String args[] ) { //Creating an object of Random class Random random = new Random(); //Calling the nextInt() method System.out.println(\u0026#34;A random int: \u0026#34; + random.nextInt()); //Calling the overloaded nextInt() method System.out.println(\u0026#34;A random int from 0 to 49: \u0026#34;+ random.nextInt(50)); //Calling the nextDouble() method System.out.println(\u0026#34;A random double: \u0026#34;+ random.nextDouble()); //Calling the nextFloat() method System.out.println(\u0026#34;A random float: \u0026#34;+ random.nextFloat()); //Calling the nextLong() method System.out.println(\u0026#34;A random long: \u0026#34;+ random.nextLong()); } } Output A random int: -1836807784 A random int from 0 to 49: 8 A random double: 0.8662629682535646 A random float: 0.47197896 A random long: -4184120388957206673 Generate Random Integer within a specific Range Let\u0026rsquo;s create a method that uses Random class to generate a random Integer within the specific range of min and max values inclusive.\npublic static int generateRandomInt(int min, int max){ return min + new Random().nextInt(max-min+1); } The method returns a random integer every time you run it.\nSystem.out.println(generateRandomInt(1, 5)); // Prints random integer \u0026#34;4\u0026#34; // Prints random integer \u0026#34;1\u0026#34; // Prints random integer \u0026#34;5\u0026#34; Generate Array of Random Numbers within a specific Range Let\u0026rsquo;s create a method that uses Random class and Java Streams to generate an Array of random numbers of a given size and all elements of that Array should be in the given range of min and max values inclusive.\npublic static int[] generateRandomArray(int size, int min, int max) { return IntStream .generate(() -\u0026gt; min + new Random().nextInt(max - min + 1)) .limit(size) .toArray(); } Let\u0026rsquo;s use the above method to generate an Array of 5 random numbers between 0 to 9 inclusive. The method will generate a random array every time you run it.\nSystem.out.println(Arrays.toString(generateRandomArray(5, 0, 9))); // Prints random array \u0026#34;[1, 9, 7, 0, 4]\u0026#34; // Prints random array \u0026#34;[7, 1, 2, 8, 5]\u0026#34; // Prints random array \u0026#34;[5, 7, 5, 2, 3]\u0026#34; Generate List of Random Numbers within a specific Range Let\u0026rsquo;s create a method using Random class and Java Streams to generate a List of random numbers of a given size and all elements in that List should be in the given range of min and max values inclusive.\npublic static List\u0026lt;Integer\u0026gt; generateRandomList(int size, int min, int max) { return IntStream .generate(() -\u0026gt; min + new Random().nextInt(max-min+1)) .limit(size) .boxed() .collect(Collectors.toList()); } Let\u0026rsquo;s use the above method to generate a List of 5 random numbers between 0 to 9 inclusive. The method will generate a random list every time you run it.\nSystem.out.println(generateRandomList(5, 0, 9)); // Prints random list \u0026#34;[3, 5, 7, 2, 4]\u0026#34; // Prints random list \u0026#34;[8, 7, 7, 5, 4]\u0026#34; // Prints random list \u0026#34;[5, 9, 8, 7, 5]\u0026#34; Get Random Element from an Array You can get a random element from an Array of elements by generating a random index within the length range of the Array.\nLet\u0026rsquo;s run the below code snippet, which prints the random user from the Array every time:-\nString[] users = new String[] { \u0026#34;Adam\u0026#34;, \u0026#34;Bill\u0026#34;, \u0026#34;Charlie\u0026#34;, \u0026#34;David\u0026#34;, \u0026#34;Eva\u0026#34;, \u0026#34;Fang\u0026#34;, \u0026#34;George\u0026#34;, \u0026#34;Harry\u0026#34;, \u0026#34;Ivy\u0026#34;, \u0026#34;Jack\u0026#34; }; Random random = new Random(); System.out.println(users[random.nextInt(users.length)]); // Prints random user \u0026#34;Eva\u0026#34; // Prints random user \u0026#34;Charlie\u0026#34; // Prints random user \u0026#34;Fang\u0026#34; Get Random Element from a List You can get a random element from a List of elements by generating a random index within the size range of the List.\nLet\u0026rsquo;s run the below code snippet, which prints the random user from the list every time:-\nList\u0026lt;String\u0026gt; users = List.of(\u0026#34;Adam\u0026#34;, \u0026#34;Bill\u0026#34;, \u0026#34;Charlie\u0026#34;, \u0026#34;David\u0026#34;, \u0026#34;Eva\u0026#34;, \u0026#34;Fang\u0026#34;, \u0026#34;George\u0026#34;, \u0026#34;Harry\u0026#34;, \u0026#34;Ivy\u0026#34;, \u0026#34;Jack\u0026#34;); Random random = new Random(); System.out.println(users.get(random.nextInt(users.size()))); // Prints random user \u0026#34;David\u0026#34; // Prints random user \u0026#34;George\u0026#34; // Prints random user \u0026#34;Charlie\u0026#34; Shuffle Elements in a List Java Collections provide a built-in shuffle() method to shuffle the elements of a List.\nLet\u0026rsquo;s shuffle the list of users and return users with the random sequence:-\nList\u0026lt;String\u0026gt; userList = List.of(\u0026#34;Adam\u0026#34;, \u0026#34;Bill\u0026#34;, \u0026#34;Charlie\u0026#34;, \u0026#34;David\u0026#34;, \u0026#34;Eva\u0026#34;, \u0026#34;Fang\u0026#34;, \u0026#34;George\u0026#34;, \u0026#34;Harry\u0026#34;, \u0026#34;Ivy\u0026#34;, \u0026#34;Jack\u0026#34;); Collections.shuffle(userList); System.out.println(userList); // Prints random list everytime \u0026#34;[Charlie, Bill, Harry, George, Jack, Ivy, Fang, Eva, David, Adam]\u0026#34; Collections.shuffle(userList); System.out.println(userList); // Prints random list everytime \u0026#34;[Eva, Ivy, Fang, Jack, Harry, Bill, Charlie, George, David, Adam]\u0026#34; Let\u0026rsquo;s return 5 random users from the given list of users:-\nCollections.shuffle(userList); List\u0026lt;String\u0026gt; fiveUsers = userList.subList(0, 5); System.out.println(fiveUsers); // Prints \u0026#34;[Eva, Bill, David, George, Fang]\u0026#34; ","permalink":"https://codingnconcepts.com/java/generate-random-numbers-in-java/","tags":["Core Java","Numbers"],"title":"How to generate Random Numbers in Java"},{"categories":["Interview Questions"],"contents":"List of frequently asked relational database interview questions. Keep following this post for regular updates.\nWhat are ACID properties in a database? ACID is an acronym for Atomicity, Consistency, Isolation, and Durability, which refers to the four properties of a transaction in Database. These properties ensures the accuracy and integrity of the data in database.\nA transaction is a sequence of instructions that are executed as a single unit of work.\nAtomicity - A transaction is an atomic unit; hence, all the instructions within a transaction will successfully execute, or none of them. If any of the instructions fail, the entire transaction should abort and rollback.\nFor example, transferring money between bank accounts is a transaction, where the value must be debited from one account and credit in another account. If any of them failed, then other must be rolled back.\nConsistency - A database is initially in a consistent state, and it should remain consistent after every transaction, whether transaction execute successfully or fails.\nFor example, a transaction must fail, while trying to insert duplicate in a table column having unique constraint.\nIsolation - If the multiple transactions are running concurrently, they should not be affected by each other; i.e., the result should be the same as the result obtained if the transactions were running sequentially.\nFor example, if two clients are trying to buy at the same time the last available product on the web site, when the first user finishes the shopping, it will make the transaction of the other user be interrupted.\nDurability - Changes that have been persisted to the database should remain even in the case of software and hardware failure.\nWhat is SQL? SQL (Structured Query Language) is a language used for storing, reading, updating, and deleting data using simple code snippets, called queries, in an RDBMS (relational database management system).\nGiven below is an example. You first create a table, insert some data in it and then view all the data in that table.\n-- Creating a table in database -- CREATE TABLE \u0026lt;Table name\u0026gt; CREATE TABLE Employee ( -- \u0026lt;Variable name\u0026gt; \u0026lt;Variable type\u0026gt;\tID int, Name varchar(255), Gender varchar(10), Age int ); -- Inserting INSERT INTO -- \u0026lt;Table name\u0026gt; Employee -- \u0026lt;Variable names\u0026gt; (ID, Name, Gender, Age) VALUES -- \u0026lt;Variable values\u0026gt; (1, \u0026#39;Adam\u0026#39;, \u0026#39;Male\u0026#39;, 22), (2, \u0026#39;Eva\u0026#39;, \u0026#39;Female\u0026#39;, 28), (3, \u0026#39;Joy\u0026#39;, \u0026#39;Male\u0026#39;, 10); -- Selecting everything we stored in the table to view SELECT * FROM Employee; What are different type of SQL statements? Data Definition Language (DDL) commands are used to define the structure that holds the data. These commands are auto-committed i.e. changes done by the DDL commands on the database are saved permanently and cannot be rolled back\ne.g. CREATE, ALTER, TRUNCATE, DROP, RENAME, COMMENT Data Manipulation Language (DML) commands are used to manipulate the data of the database. These commands are not auto-committed and can be rolled back\ne.g. SELECT, INSERT, UPDATE, DELETE, MERGE Data Control Language (DCL) commands are used to deal with the rights, permission, and other controls of the database\ne.g. GRANT, REVOKE Transaction Control Language (TCL) commands are used to deal with transactions within the database\ne.g. COMMIT, ROLLBACK, SAVEPOINT What are different types of clauses in SQL? SQL clauses are used to qualify a database query by restricting or altering the values that it returns. The types of SQL clauses are:\nFROM: Used to specify which tables the data will be pulled from. For example, get all records from Employee table:-\nSELECT * FROM Employee; WHERE: Used to filter the records by one or more conditions. For example, get all Female employee\u0026rsquo;s record over the age 18:-\nSELECT * FROM Employee WHERE Gender = \u0026#39;Female\u0026#39; and Age \u0026gt; 18; ORDER BY: Used to sort the records returned by query. For example, get all the employee\u0026rsquo;s records sorted by Name in ascending order:-\nSELECT * FROM Employee ORDER BY Name; GROUP BY: Used to group the records together that have similar values. For example, get average salary of employees by department:-\nSELECT DepartmentId, AVG(Salary) FROM Employee GROUP BY DepartmentID; HAVING: Used in combination with the GROUP BY clause. Used to filter the records from each group after groupings are made.\nSELECT DepartmentId, AVG(Salary) FROM Employee GROUP BY DepartmentID HAVING AVG(Salary) \u0026gt; 3000; DISTINCT: Used to get distinct records from a table, duplicate records are removed. For example, you may wish to list the different gender in Employee\u0026rsquo;s table:-\nSELECT DISTINCT(Gender) FROM Employee; What is the difference between the WHERE and HAVING clause? When GROUP BY is not used, the WHERE and HAVING clauses are essentially equivalent.\nHowever, when GROUP BY is used:\nThe WHERE clause is used to filter records from all rows before groupings are made. The HAVING clause is used to filter records from each group of rows after groupings are made. What are Aggregate Functions? An aggregate function is a function that\u0026rsquo;s applied on multiple values, and returns a single value after the calculation. In SQL, there are multiple aggregate functions. The most commonly used ones are:\nMIN() - to find the minium out of all values of a column MAX() - to find the maximum out of all values of a column SUM() - to find the sum of all values of a column AVG() - to find the average of all values of a column COUNT() - to find the count of all values of a column SELECT MIN(Age), MAX(Age), SUM(Age), AVG(Age), COUNT(Age) FROM Employee; What are TRUNCATE, DELETE and DROP statements? DELETE is a DML command used to delete specific rows from a table using WHERE clause. All the rows can be deleted if WHERE clause is not specified. It requires explicit commit to make its effect permanent. It can be rolled back. DELETE FROM Employees WHERE EmployeeId \u0026gt; 1000; TRUNCATE is a DDL command used to delete all the rows from the table. It does not require a commit to make the changes permanent and this is the reason why rows deleted by truncate cannot be rolled back TRUNCATE TABLE Employees; DROP is a DDL command used to delete the table structure including all the rows from the database. It cannot be rolled back DROP TABLE Employees; What are the different types of JOIN clauses in SQL? A JOIN clause combines records from multiple tables into a single table, based on the common values that they share between one or more columns.\nSuppose we have Employee and Department tables which share common department_id column.\nEmployee +----+---------+--------+--------+---------------+ | id | name | gender | salary | department_id | +----+---------+--------+--------+---------------+ | 1 | Adam | m | 2500 | 1 | | 2 | Becca | f | 1500 | 1 | | 3 | Charlie | m | 5500 | 2 | | 4 | Eva | f | 500 | NULL | +----+---------+--------+--------+---------------+ Department +---------------+---------+ | department_id | name | +---------------+---------+ | 1 | HR | | 2 | Dev | | 3 | IT | | 4 | QA | +---------------+---------+ The different types of JOIN clauses in SQL are:\nINNER JOIN (a.k.a. “Simple Join”) Returns all records that have at least one match in both tables. This is the default type of join if no specific JOIN type is specified. SELECT * FROM Employee e JOIN Department d ON e.department_id = d.department_id; -- OR SELECT * FROM Employee e, Department d WHERE e.department_id = d.department_id; LEFT JOIN Returns all rows from the left table, and the matched rows from the right table; i.e., the results will contain all records from the left table, even if the JOIN condition doesn’t find any matching records in the right table. This means that if the ON clause doesn’t match any records in the right table, the JOIN will still return a row in the result for that record in the left table, but with NULL in each column from the right table. SELECT * FROM Employee e LEFT JOIN Department d ON e.department_id = d.department_id; RIGHT JOIN (or RIGHT OUTER JOIN): Returns all rows from the right table, and the matched rows from the left table. This is the exact opposite of a LEFT JOIN; i.e., the results will contain all records from the right table, even if the JOIN condition doesn’t find any matching records in the left table. This means that if the ON clause doesn’t match any records in the left table, the JOIN will still return a row in the result for that record in the right table, but with NULL in each column from the left table. SELECT * FROM Employee e RIGHT JOIN Department d ON e.department_id = d.department_id; FULL JOIN (or FULL OUTER JOIN): Returns all rows for which there is a match in EITHER of the tables. Conceptually, a FULL JOIN combines the effect of applying both a LEFT JOIN and a RIGHT JOIN; i.e., its result set is equivalent to performing a UNION of the results of left and right outer queries. SELECT * FROM Employee e FULL JOIN Department d ON e.department_id = d.department_id; CROSS JOIN Returns all records where each row from the first table is combined with each row from the second table (i.e., returns the Cartesian product of the sets of rows from the joined tables).\nNote that a CROSS JOIN can either be specified using the CROSS JOIN syntax (“explicit join notation”) or listing the tables in the FROM clause separated by commas without using a WHERE clause to supply join criteria (“implicit join notation”). SELECT * FROM Employee e CROSS JOIN Department d ON e.department_id = d.department_id; -- OR SELECT * FROM Employee e, Department d; What are Constraints? Constraints are used to specify the rules concerning data in the table. It can be applied for single or multiple fields in an SQL table during creation of table or after creationg using the ALTER TABLE command. The constraints are:\nNOT NULL - Restricts NULL value from being inserted into a column. CHECK - Verifies that all values in a field satisfy a condition. DEFAULT - Automatically assigns a default value if no value has been specified for the field. UNIQUE - Ensures unique values to be inserted into the field. INDEX - Indexes a field providing faster retrieval of records. PRIMARY KEY - Uniquely identifies each record in a table. FOREIGN KEY - Ensures referential integrity for a record in another table. CREATE TABLE table_name ( column1 datatype constraint, column2 datatype constraint, column3 datatype constraint, .... ); CREATE TABLE Employee ( ID int NOT NULL UNIQUE, LastName varchar(255) NOT NULL, FirstName varchar(255), Age int CHECK (Age\u0026gt;=18), City varchar(255) DEFAULT \u0026#39;NA\u0026#39;, ); What is a NOT NULL constraint? The NOT NULL constraint ensures that all values in a column must always contains a value, and not contains NULL. This provides guarantee that record cannot be inserted or updated with NULL value for that column.\n/* Create table with NOT NULL constraint on ID, LastName and FirstName column */ CREATE TABLE Employee ( ID int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255) NOT NULL, Age int ); /* Add NOT NULL constraint on Age column */ ALTER TABLE Employee MODIFY Age int NOT NULL; What is a UNIQUE constraint? A UNIQUE constraint ensures that all values in a column are different. This provides a guarantee of uniqueness for the column(s) and helps identify each row uniquely. There can be multiple unique constraints defined per table.\n/* Create table with UNIQUE constraint on ID column */ CREATE TABLE Employee ( ID int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255), Age int, UNIQUE (ID) ); /* Create table with UNIQUE constraint on multiple columns */ CREATE TABLE Employee ( ID int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255), Age int, CONSTRAINT UC_Employee UNIQUE (ID, LastName) ); /* Add UNIQUE constraint on ID column */ ALTER TABLE Employee ADD UNIQUE (ID); /* Add UNIQUE constraint on multiple columns */ ALTER TABLE Employee ADD CONSTRAINT UC_Employee UNIQUE (ID, LastName); /* Drop UNIQUE constraint */ ALTER TABLE Employee DROP CONSTRAINT UC_Employee; What is a Primary Key constraint? The PRIMARY KEY constraint uniquely identifies each row in a table. It must contain UNIQUE values and has an implicit NOT NULL constraint. A table can have one and only one primary key, which is comprised of single or multiple columns.\n/* Create table with PRIMARY KEY constraint on ID column */ CREATE TABLE Employee ( ID int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255), Age int, PRIMARY KEY (ID) ); /* Create table with PRIMARY KEY constraint on multiple columns */ CREATE TABLE Employee ( ID int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255), Age int, CONSTRAINT PK_Employee PRIMARY KEY (ID, LastName) ); /* Add PRIMARY KEY constraint on ID column */ ALTER TABLE Employee ADD PRIMARY KEY (ID); /* Add PRIMARY KEY constraint on multiple columns */ ALTER TABLE Employee ADD CONSTRAINT PK_Employee PRIMARY KEY (ID, LastName); /* Drop UNIQUE constraint */ ALTER TABLE Employee DROP CONSTRAINT PK_Employee; What is a Foreign Key constraint? A FOREIGN KEY is a key used to link two tables together.\nA FOREIGN KEY comprises of single or collection of columns in a table that essentially refer to the PRIMARY KEY in another table. Foreign key constraint ensures referential integrity in the relation between two tables.\nThe table containing the foreign key is called the child table, and the table containing the candidate key is called the referenced or parent table.\n/* Department table with ID as PRIMARY KEY */ CREATE TABLE Department ( ID INT NOT NULL, Name VARCHAR(255), PRIMARY KEY (ID) ); /* Employee table with DepartmentID as FOREIGN KEY which is linked to ID column of Department table */ CREATE TABLE Employee ( ID INT NOT NULL, Name VARCHAR(255), DepartmentID INT, PRIMARY KEY (ID), FOREIGN KEY (DepartmentID) REFERENCES Department(ID) ); /* Add FOREIGN KEY constraint on DepartmentID column */ ALTER TABLE Employee ADD CONSTRAINT FK_DepartmentEmployee FOREIGN KEY (DepartmentID) REFERENCES Department(ID); What is Index? An index is a structure in a database that can help retrieve data faster. When you search table data with an SQL query, it will search the whole table and return the result. An unindexed table is called a heap. The data stored in such tables are usually not arranged in any particular way. It is stored in the order it was entered. Therefore, searching for data can be very slow and frustrating.\nWhen you query an indexed table, the database will go to the index first and retrieve the corresponding records directly. The two main index types are:\nClustered Non-clustered What is Clustered Index? Clustered index defines the order in which data is physically stored in a table. Therefore, only one clustered index can be created on a given database table.\nQ: What is the maximum number of clustered indexes a table can have?\nA: 1\nThis is similar to a physical telephone directory, where the records are sorted by first name, and last name.\nIn SQL Server, the primary key constraint automatically creates a clustered index on that particular column.\nCREATE TABLE Employee ( id INT PRIMARY KEY, name VARCHAR(50) NOT NULL, gender VARCHAR(50) NOT NULL ) CREATE CLUSTERED INDEX IX_tblEmployee_Id ON student(id ASC) Notice that \u0026ldquo;id\u0026rdquo; column is primary key in \u0026ldquo;Employee\u0026rdquo; table so clustered index is automatically created for \u0026ldquo;id\u0026rdquo; column. The clustered index physically stores and maintains the records in \u0026ldquo;Employee\u0026rdquo; table in the ascending order of \u0026ldquo;id\u0026rdquo; even if you insert by records by random \u0026ldquo;id\u0026rdquo;.\nClustered index on \u0026ldquo;id\u0026rdquo; make sure that you always get records sorted by \u0026ldquo;id\u0026rdquo; using following statement:-\nSELECT * from Employee What is Non-clustered Index? Non-clustered index doesn’t sort the physical data inside the table. In fact, a non-clustered index is stored at one place and table data is stored in another place.\nThis is similar to a textbook where the book content is located in one place and the index is located in the last few pages. This allows for more than one non-clustered index per table.\nQ: If we create many non-clustered indexes on a table, what operations are likely to be considerably slower than before?\nA: Write operations (INSERT, UPDATE, DELETE, MERGE)\nQ: When we talk about non-clustered index, how many KEY columns can it have and how many INCLUDED columns?\nA: 16 key columns and unlimited include columns (all table columns can be included).\nQ: Which datastructure is used generally to create non-clustered indexes on a table?\nA: B and B+ Trees are mostly used. Some of the database also use BitMap data structure to create indexes where cardinality is low.\nWhat is the difference between Clustered and Non-Clustered Index? Clustered vs Non-Clustered Index Clustered Index Non-Clustered Index Can be only one clustered index per table Can create multiple non-clustered indexes on a single table Clustered indexes only sort tables. Therefore, they do not consume extra storage. Non-clustered indexes are stored in a separate place from the actual table claiming more storage space. Clustered indexes are faster Non-clustered indexes are slower What is Composite Index? Whether clustered or non-clustered index, when multiple columns are used to create index then it is known as Composite Index\nCREATE NONCLUSTERED INDEX IX_tblEmployee_Name_Gender ON student(name ASC, gender DESC) Note that the sequence of column in Composite Index matters. Above composite index will be used to find records for below queries:-\nSELECT * from Employee where name=\u0026#34;John\u0026#34; and gender=\u0026#34;Male\u0026#34; SELECT * from Employee where name=\u0026#34;Eva\u0026#34; but not for below query:-\nSELECT * from Employee where gender=\u0026#34;Female\u0026#34; What is Cardinality for Database Query Optimization? When it comes to database query optimization, Cardinality refers to number of unique values in a column of a table. We can calculate cardinality using SQL statement:-\nSELECT DISTINCT COUNT(columnName) from tableName; The higher the cardinality of column, the lower the duplicate values in the column. There are three types of SQL cardinality :-\nHigh Cardinality refers to columns with mostly uncommon or unique values.\nsuch as employee_id, email_address, phone_number columns in Employee Table.\nUnique indexes are used to improve performance of such columns. Normal Cardinality refers to columns with somewhat uncommon values.\nsuch as first_name, last_name columns in Employee Table. Low Cardinality refers to columns with mostly duplicate values\nsuch as gender, joining_year columns in Employee Table.\nBit-map indexes are used to improve the performance of such columns. How to do Performance tuning of SQL queries? Queries on large databases can have significant execution times. Few ways to optimize SQL queries are as follows:-\nUse Indexes A straightforward, yet effective, method for query optimization is indexing. This way, the whole database won’t need to be searched for these queries. For example, if a system needed to query first and last names and ID numbers of employees based on their age, the following script could be used to create the index:\nCREATE INDEX age_queries ON Employees (last_name, first_name, id, age); Use Inner Join instead of Where\nTo link data from two or more tables, the Where clause is used. However, Where uses a kind of join that joins each record of each table and then filters it for the result. This means that if there are 100 records in the Employee table and 100 in the Salary table, it will generate a table of 10,000 records and then check for the correct records for this query.\nSelect Employees.first_name, Employees.last_name, Employees.id From Employees, Salary Where Employees.id = Salary.id; In contrast, if the Inner Join is used, the required records will be produced without anything in the middle.\nSelect Employees.first_name, Employees.last_name, Employees.id From Employees Inner Join Salary On Employees.id = Salary.id; Use Limit for testing\nBefore running a query on a large database with other potential users, it is advised to use the limit operator to ensure that it actually works. The limit keyword returns a limited number of results from a query, as shown below.\nSelect Employees.first_name, Employees.last_name, Employees.id From Employees Inner Join Salary On Employees.id = Salary.id Where Salary.amount \u0026gt; 4500 Limit 10; This will return only ten names and ID numbers of Employees with salaries greater than 4,500.\nUse Explain\nWhen querying a large database, it might be smart to use the Explain operator before running the query. Explain displays what is called a Query Plan. The query plan shows which indexing is used, order of execution and the approximate time each step might take. Using this information, more time-expensive steps may be better optimized. The following example shows how to use this operator:\nExplain Select Employees.first_name, Employees.last_name, Employees.id From Employees Inner Join Salary On Employees.id = Salary.id Where Salary.amount \u0026gt; 4500; Be Specific with Select queries\nWhen working with Select queries, it is smart to avoid the * operator, which returns all columns for the queried records unless otherwise required. Instead, only the required information should be extracted. For example:\nSelect * From Employees; This operation would generate all columns of all records from the Employees table. This is inefficient if only the names were required. In that case, the following query should be used:\nSelect first_name, last_name From Employees; Find Second Highest Salary from Employee Table? +----+--------+ | Id | Salary | +----+--------+ | 1 | 100 | | 2 | 200 | | 3 | 300 | +----+--------+ SELECT max(Salary) AS SecondHighestSalary FROM Employee WHERE Salary \u0026lt; (SELECT max(Salary) FROM Employee) +---------------------+ | SecondHighestSalary | +---------------------+ | 200 | +---------------------+ Find Duplicate Emails from Employee Table? +----+---------+ | Id | Email | +----+---------+ | 1 | a@b.com | | 2 | c@d.com | | 3 | a@b.com | +----+---------+ SELECT Email FROM Employee GROUP BY Email HAVING count(Email) \u0026gt; 1; +---------+ | Email | +---------+ | a@b.com | +---------+ Find Emails starting with vowels (i.e a,e,i,o,u) from Employee Table? +----+-------------------+ | Id | Email | +----+-------------------+ | 1 | adam@gmail.com | | 2 | charlie@yahoo.com | | 3 | david@hotmail.com | | 4 | eva@apple.com | | 5 | fang@bing.com | +----+-------------------+ SELECT Email FROM Employee WHERE Email REGEXP \u0026#34;^[aeiou]\u0026#34;; +------------------+ | Email | +------------------+ | adam@gmail.com | | eva@apple.com | +------------------+ Swap values of gender column in Employee Table? Column gender has m=male and f=female. Swap all f and m values (i.e., change all f values to m and vice versa) with a single update statement and no intermediate temp table.\n+----+------+--------+--------+ | id | name | gender | salary | +----+------+--------+--------+ | 1 | A | m | 2500 | | 2 | B | f | 1500 | | 3 | C | m | 5500 | | 4 | D | f | 500 | +----+------+--------+--------+ There are multiple solutions possible for this:-\nUsing CASE\nUPDATE salary SET gender = CASE gender WHEN \u0026#39;m\u0026#39; THEN \u0026#39;f\u0026#39; ELSE \u0026#39;m\u0026#39; END; Using IF\nUPDATE salary SET gender = IF (gender = \u0026#34;m\u0026#34;, \u0026#34;f\u0026#34;, \u0026#34;m\u0026#34;); Using XOR ^ Operation\nUPDATE salary SET gender = CHAR(ASCII(\u0026#39;f\u0026#39;) ^ ASCII(\u0026#39;m\u0026#39;) ^ ASCII(gender)); Next Steps\u0026hellip; Practice SQL queries online using this free tool:- https://www.sql-practice.com/\n","permalink":"https://codingnconcepts.com/relational-database-interview-questions/","tags":["Interview Q\u0026A","Database Q\u0026A"],"title":"Relational Database Interview Questions"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to design our own HashMap data structure in Java.\nOverview A hash map is a data structure that stores key-value pairs. It insert and search the pair in constant time O(1) in best case. It is backed by an array, also known as hash table. Each index of an array is know as hash bucket. The key is sent to a hash function that performs arithmetic operations on it and returns the index of the key-value pair in the hash table. If hash function of two keys return the same index, it cause hash collision. In this case new pair is linked to the already store pair in that index and form a linked list If a bucket is having more than one key-value pairs linked by a linked list then equals function is used to find the correct key-value pair. Implementation Entry\u0026lt;K, V\u0026gt; class to hold key-value pair. It is a generic class which is having key of type K, and value of type V. It also holds the address of next Entry in case of hash collision MyHashMap\u0026lt;K, V\u0026gt; has hash method to find the index of the bucket for the key-value pair. MyHashMap\u0026lt;K, V\u0026gt; has put and get methods to insert and find the key-value pairs respectively. package com.example.datastructure; public class MyHashMap\u0026lt;K, V\u0026gt; { private static int DEFAULT_CAPACITY = 16; private Entry\u0026lt;K, V\u0026gt;[] table; private int capacity; MyHashMap() { this(DEFAULT_CAPACITY); } MyHashMap(int capacity) { this.capacity = capacity; /** * Initialize \u0026#34;Hash Table\u0026#34; with initial capacity which is nothing but an Array * Each index of an array is called \u0026#34;Hash Bucket\u0026#34; */ this.table = new Entry[capacity]; } /** * Each Entry stores a key-value pair Each Entry also stores the address of next * Entry in case of \u0026#34;Hash Collision\u0026#34; */ static class Entry\u0026lt;K, V\u0026gt; { K key; V value; Entry\u0026lt;K, V\u0026gt; next; Entry(K key, V value) { this.key = key; this.value = value; } Entry(K key, V value, Entry\u0026lt;K, V\u0026gt; next) { this.key = key; this.value = value; this.next = next; } } public void put(K key, V value) { if (key == null) { return; } // Create a key-value pair Entry\u0026lt;K, V\u0026gt; newEntry = new Entry\u0026lt;\u0026gt;(key, value); // Find the right Bucket by hashing the key int hash = hash(key); // if - Empty Bucket if (table[hash] == null) { table[hash] = newEntry; // else - \u0026#34;Hash Bucket\u0026#34; is not Empty, Known as \u0026#34;Hash Collision\u0026#34; // New Entry is created and linked to Previous Node in Same Bucket } else { Entry\u0026lt;K, V\u0026gt; current = table[hash]; Entry\u0026lt;K, V\u0026gt; previous = null; while (current != null) { if (current.key.equals(key)) { current.value = newEntry.value; return; } previous = current; current = current.next; } previous.next = newEntry; } } public V get(K key) { if (key == null) { return null; } // Find the right Bucket by hashing the key int hash = hash(key); // if - \u0026#34;Hash Bucket\u0026#34; is Empty, Return null if (table[hash] == null) { return null; // else - \u0026#34;Hash Bucket\u0026#34; is not Empty // Traverse through all the linked Nodes in the Bucket // Use `equals` method to find the correct key-value pair } else { Entry\u0026lt;K, V\u0026gt; current = table[hash]; while (current != null) { if (current.key.equals(key)) { return current.value; } current = current.next; } } return null; } private int hash(K key) { // Using modulo \u0026#34;% capacity\u0026#34; to make sure that returned hash in the range of // underlying Array size return Math.abs(key.hashCode()) % capacity; } public static void main(String[] args) { MyHashMap\u0026lt;String, Integer\u0026gt; likesPerPost = new MyHashMap\u0026lt;String, Integer\u0026gt;(); likesPerPost.put(\u0026#34;Learning Hash Map\u0026#34;, 5); System.out.println(likesPerPost.get(\u0026#34;Learning Hash Map\u0026#34;)); } } ","permalink":"https://codingnconcepts.com/java/design-hash-map-in-java/","tags":["Java Collection"],"title":"Design your own Hash Map in Java"},{"categories":null,"contents":"In this tutorial, we\u0026rsquo;ll learn how to calculate time complexity of a function execution with examples.\nTime Complexity Time complexity is generally represented by big-oh notation 𝘖.\nIf time complexity of a function is 𝘖(n), that means function will take n unit of time to execute.\nThese are the general types of time complexity which you come across after the calculation:-\nType Name 𝘖 (1) Constant 𝘖 (log₂n) Logarithmic 𝘖 (√n Root 𝘖 (n) Linear 𝘖 (n²) Quadratic 𝘖 (n³) Cubic 𝘖 (2ⁿ) Exponential 𝘖 (nⁿ) Exponential Time Complexity in the increasing order of their value:-\n1 \u0026lt; log₂n \u0026lt; √n \u0026lt; n \u0026lt; nlog₂n \u0026lt; n² \u0026lt; n³ ... \u0026lt; 2ⁿ \u0026lt; 3ⁿ ... \u0026lt; nⁿ Time Complexity Calculation We are going to understand time complexity with loads of examples:-\nfor loop Let\u0026rsquo;s look at the time complexity of for loop with many examples, which are easier to calculate:-\nExample 1 for(int i=0; i\u0026lt;n; i++){} loop run `n` times hence Time Complexity = 𝘖(n) Example 2 for(int i=0; i\u0026lt;n; i=i+2){} loop run `n/2` times which is still linear hence Time Complexity = 𝘖(n) Example 3 for(int i=n; i\u0026gt;1; i--){} loop run `n` times hence Time Complexity = 𝘖(n) Example 4 for(int i=1; i\u0026lt;n; i++){} for(int j=1; j\u0026lt;n; j++){} two individual loops run `n+n = 2n` times which is still linear hence Time Complexity = 𝘖(n) Example 5 for(int i=0; i\u0026lt;n; i++){ for(int j=0; j\u0026lt;n; j++){} } Nested loop run `nxn = n²` times which is non-linear hence Time Complexity = 𝘖(n²) Example 6 for(int i=1; i\u0026lt;n; i=i*2){} Calculation: ―――――――――――――――――――――――――――― values of `i` in `for` loop ―――――――――――――――――――――――――――― 1 2 2² 2³ . . 2ᵏ ―――――――――――――――――――――――――――― The loop will terminate when:- i ≥ n 2ᵏ ≥ n k ≥ log₂(n) so log₂(n) is total unit of time taken by the loop hence Time complexity = 𝘖(log₂(n)) Example 7 for(int i=1; i\u0026lt;n; i=i*3){} Calculation: Similar to the last example, value of `i` in `for` loop is i = 3ᵏ The loop will terminate when:- i ≥ n 3ᵏ ≥ n k ≥ log₃n so log₃n is total unit of time taken by the loop hence Time complexity = 𝘖(log₃n) Example 8 for(int i=n; i\u0026gt;1; i=i/2){} Calculation: ―――――――――――――――――――――――――――― values of `i` in `for` loop ―――――――――――――――――――――――――――― n n/2 n/2² n/2³ . . n/2ᵏ ―――――――――――――――――――――――――――― The loop will terminate when:- i ⋜ 1 n/2ᵏ ⋜ 1 2ᵏ ≥ n k ≥ log₂(n) so log₂(n) is total unit of time taken by the loop hence Time complexity = 𝘖(log₂(n)) Example 9 for(int i=0; i\u0026lt;n; i++){ for(int j=1; j\u0026lt;n; j=j*2){} } Calculation: outer loop complexity = 𝘖(n) inner loop complexity = 𝘖(log₂(n)) hence Time complexity = 𝘖(nlog₂(n)) Example 10 int p = 0; for(int i=1; p\u0026lt;=n; i++){ p = p + i; } Calculation: value of `i` and `p` in `for` loop:- ――――――――――――――――――― i p ――――――――――――――――――― 1 0+1=1 2 1+2=3 3 1+2+3=4 4 1+2+3+4 . . . . k 1+2+3+4+...+k k k(k+1)/2 ――――――――――――――――――― The loop will terminate when:- p \u0026gt; n k(k+1)/2 \u0026gt; n k² \u0026gt; n k \u0026gt; √n so √n is total unit of time taken by the loop hence Time complexity = 𝘖(√n) Example 11 int p = 0; for(int i=1; i\u0026lt;n; i=i*2){ p++; } for(int j=1; j\u0026lt;p; j=j*2){ // statement } Calculation: 1. first `for` loop will take log₂(n) unit of time to execute. At the end of first loop value of p = log₂(n) 2. second `for` loop will take log₂(p) unit of time to execute. hence Time Complexity = 𝘖(log₂(p)) = 𝘖(log₂log₂(n)) while loop If you understand how to calculate the time complexity of for loop then while loop is piece of cake.\nExample 1 int i=1; while(i\u0026lt;n){ //statement i=i*2; } Time Complexity = 𝘖(log₂(n)) Example 2 int i=n; while(i\u0026gt;1){ //statement i=i/2; } Time Complexity = 𝘖(log₂(n)) Example 3 int i=1; int k=1; while(k\u0026lt;n){ //statement k=k+i; i++; } Time Complexity = 𝘖(√n) Variable Time Complexity It is not necessary that function always take fixed unit of time to execute, sometime it depends on the input parameters. Here are some examples where time complexity is not fixed:-\nExample 1 method(n, m){ while(n!=m){ if(n\u0026gt;m){ n = n-m; }else{ m = m-n; } } } best case time = 𝘖(1) (when n = m) worst case time = 𝘖(n) (when n is very larger then m (e.g. n=16, m=1)) Example 2 method(n){ if(n\u0026lt;5){ //statement }else{ for(i=0;i\u0026lt;n;i++){ //statement } } } best case time = 𝘖(1) (when n \u0026lt; 5) worst case time = 𝘖(n) (when n ≥ 5) Recursive Functions Let\u0026rsquo;s see the time complexity of recurring (or recursive) functions:-\nExample 1 test(int n){ if(n\u0026gt;0){ //statement test(n-1); } } // T(n) = T(n-1) + 1 = 𝘖(n) Calculation: Base case T(0) = 1 Time taken by nth task is time taken by (n-1)th task plus 1 T(n) = T(n-1) + 1 --(1) Similarly time taken by (n-1)th task is (n-2)th task plus 1 T(n-1) = T(n-2) + 1 --(2) T(n-2) = T(n-3) + 1 --(3) T(n) = T(n-3) + 3 --after substituting (2),(3) in (1) T(n) = T(n-k) + n ... Assume (n-k)th is 0th task means n=k T(n) = T(0) + n T(n) = 1 + n ⋍ n hence Time Complexity = 𝘖(n) Example 2 test(int n){ if(n\u0026gt;0){ for(int i=0; i\u0026lt;n; i++){ //statement } test(n-1); } } // T(n) = T(n-1) + n = 𝘖(n²) Calculation: Base case T(0) = 1 Time taken by nth task:- T(n) = T(n-1) + n T(n-1) = T(n-2) + n-1 T(n-2) = T(n-3) + n-2 .. .. T(n) = T(n-3) + (n-2) + (n-1) + n T(n) = T(n-k) + (n-(k-1)) + (n-(k-2)) + ... + (n-1) + n Assume (n-k)th is 0th task means n=k T(n) = T(n-n) + (n-n+1) + (n-n+2) + ... + (n-1) + n T(n) = T(0) + 1 + 2 + 3 + ... + n T(n) = 1 + n(n+1)/2 ⋍ n² hence Time Complexity = 𝘖(n²) Example 3 test(int n){ if(n\u0026gt;0){ for(int i=0; i\u0026lt;n; i=i*2){ //statement } test(n-1); } } // T(n) = T(n-1) + log₂(n) = 𝘖(nlog₂(n)) Calculation: Base case T(0) = 1 Time taken by nth task:- T(n) = T(n-1) + log₂(n) T(n-1) = T(n-2) + log₂(n-1) T(n-2) = T(n-3) + log₂(n-2) .. .. T(n) = T(n-3) + log₂(n-2) + log₂(n-1) + log₂(n) T(n) = T(n-k) + log₂1 + log₂2 + ... + log₂(n-1) + log₂(n) Assume (n-k)th is 0th task means n=k T(n) = T(0) + log₂(n)! T(n) = 1 + nlog₂(n) ⋍ nlog₂(n) hence Time Complexity = 𝘖(nlog₂(n)) Example 4 test(int n){ if(n\u0026gt;0){ //statement test(n-1); test(n-1); } } // T(n) = 2T(n-1) + 1 = 𝘖(2ⁿ) Calculation: Base case T(0) = 1 Time taken by nth task:- T(n) = 2T(n-1) + 1 T(n-1) = 2T(n-2) + 1 T(n-2) = 2T(n-3) + 1 .. .. T(n) = 2[2[2T(n-3) + 1] + 1] + 1 T(n) = 2³T(n-3) + 2² + 2 + 1 T(n) = 2ᵏT(n-k) + 2ᵏ⁻¹ + 2ᵏ⁻² + .. + 2² + 2 + 1 Assume (n-k)th is 0th task means n=k T(n) = 2ⁿT(0) + (2ⁿ-1) T(n) = 2ⁿ + (2ⁿ-1) ⋍ 2ⁿ hence Time Complexity = 𝘖(2ⁿ) Example 5 test(int n){ if(n\u0026gt;0){ //statement test(n-1); test(n-1); test(n-1); } } T(n) = 3T(n-1) + 1 Time Complexity = 𝘖(3ⁿ) Example 6 test(int n){ if(n\u0026gt;0){ for(int i=0; i\u0026lt;n; i++){ //statement } test(n-1); test(n-1); } } // T(n) = 2T(n-1) + n Time Complexity = 𝘖(n2ⁿ) Example 7 test(int n){ if(n\u0026gt;0){ //statement test(n/2); } } Base case T(1) = 1 T(n) = T(n/2) + 1 T(n/2) = T(n/2²) + 1 T(n) = [T(n/2²) + 1] + 1 T(n) = T(n/2ᵏ) + k Assume (n/2ᵏ)th is last task means n/2ᵏ = 1 2ᵏ = n k = log₂(n) T(n) = T(1) + log₂(n) = 1 + log₂(n) ⋍ log₂(n) hence Time Complexity = 𝘖(log₂(n)) Example 8 test(int n){ if(n\u0026gt;0){ for(int i=0; i\u0026lt;n; i++){ //statement } test(n/2); } } Base case T(1) = 1 T(n) = T(n/2) + n T(n/2) = T(n/2²) + n/2 T(n) = [T(n/2²) + n/2] + n T(n) = T(n/2ᵏ) + n/2ᵏ⁻¹ + n/2ᵏ⁻² + .. + n/2² + n/2 + n Assume (n/2ᵏ)th is last task means n/2ᵏ = 1 2ᵏ = n k = log₂(n) T(n) = T(1) + n[1/2ᵏ⁻¹ + 1/2ᵏ⁻² + .. + 1/2 + 1] T(n) = 1 + n[1 + 1] = 1 + 2n ⋍ n hence Time Complexity = 𝘖(n) Example 9 test(int n){ if(n\u0026gt;0){ //statement test(n/2); test(n/2); } } T(n) = 2T(n/2) + 1 T(n) = 2ᵏT(n/2ᵏ) + k + k-1 + k-2 + ... + 1 Assume n/2ᵏ = 1 means k = log₂(n) T(n) = nT(1) + k(k+1)/2 ⋍ n + (log₂(n))² ⋍ n hence Time Complexity = 𝘖(n) Example 10 Quick Sort when pivot is middle element:-\nquickSort(int[] arr, int low, int high) { if (low \u0026lt; high){ int pi = partition(arr, low, high); // n quickSort(arr, low, pi - 1); // T(n/2) quickSort(arr, pi + 1, high); // T(n/2) } } Base case T(1) = 1 T(n) = 2T(n/2) + n T(n/2) = 2T(n/2²) + n/2 T(n) = 2[2T(n/2²) + n/2] + n T(n) = 2ᵏT(n/2ᵏ) + n + n + ... + n T(n) = 2ᵏT(n/2ᵏ) + nk Assume (n/2ᵏ)th is last task means n/2ᵏ = 1 2ᵏ = n k = log₂(n) T(n) = nT(1) + nlog₂(n) T(n) = n + nlog₂(n) = nlog₂(n) Time Complexity = 𝘖(nlog₂(n)) Asymptotic Notations We can represent the function complexity in following ways:-\nSymbol Name Bound 𝘖 big-oh upper bound Ω big-omega lower bound ⍬ big-theta average bound Example 1 For e.g. f(n) = 2n + 3 can be represented as 𝘖(n) or any notation with higher weightage such as 𝘖(nlog₂n) or 𝘖(n²) or 𝘖(n³) ... Ω(n) or any notation with lower weightage such as Ω(√n), Ω(log₂n), Ω(1) ... ⍬(n) and only ⍬(n) since this is average bound Ideally you represent the function complexity to nearest type of complexity so in above case 𝘖(n), Ω(n), ⍬(n) are best representations. Example 2 f(n) = 2n² + 3n + 4 1n² ≤ 2n² + 3n + 4 ≤ 9n² can be represented as 𝘖(n²), Ω(n²), or ⍬(n²) Example 3 f(n) = n²log₂n + n n²log₂n ≤ 2n²log₂n + n ≤ 3n²log₂n can be represented as 𝘖(n²log₂n), Ω(n²log₂n), or ⍬(n²log₂n) Example 4 f(n) = !n = nx(n-1)x(n-2)x ...x2x1 = nⁿ n ≤ !n ≤ nⁿ can be represented as 𝘖(nⁿ) upper-bound, Ω(n) lower-bound can not be represented as ⍬ since there is no common average-bound. Example 5 f(n) = log!n = log(nx(n-1)x(n-2)x ...x2x1) = log(nⁿ) = nlog(n) 1 ≤ log!n ≤ nlog(n) can be represented as 𝘖(nlog(n)) upper-bound, Ω(1) lower-bound can not be represented as ⍬ since there is no common average-bound. It is always preferable to represent complexity in big-theta ⍬, if possible, which is more accurate and tight bound. Big-oh 𝘖(n) is the most popular notation to represent function complexity which you come across. Note: Do not mix these notations with best case, worst case, or average case time complexity. All type of cases can be represented by 𝘖, Ω, and ⍬ notations.\n","permalink":"https://codingnconcepts.com/coding/time-complexity/","tags":["Algorithm"],"title":"Basics of Time Complexity"},{"categories":["Interview Questions"],"contents":"This System Design Cheat Sheet gives you a quick idea to tackle with the building blocks of any system design interview questions. Keep following this post for regular updates.\nPrimary Database Primary database is your single source of truth when you save data. You always rely on this data.\nWhich database to choose? We always get confused with the database to use as our primary database. Here are some points to keep in mind to make a better decision.\nSQL Database Relational, Table-based When you require ACID (Atomicity, Consistency, Isolation, and Durability) properties. You choose SQL Database for saving financial transactions which gives you ACID guarantee that multiple records belongs to the same transaction are saved either all or nothing. You choose SQL Database when rigid schema is required. For e.g. you want to maintain strict relationships between the tables. you want to maintain unique, not_null constraints. You choose SQL Database when records are updated very frequently. You choose SQL Database when Joins and complex queries are required. SQL Databases are: Oracle, MySQL, MariaDB, Microsoft SQL Server, and PostgreSQL NoSQL Database Document-based, key-value pairs, Graph database You choose NoSQL Database when flexible schema is required.\nFor e.g. e-commerce like Amazon can have Products in varied schema. Electronic products can have capacity (lt), power-rating (3-star) whereas Clothing products can have size (S,M,L,XL), material (cotton). You choose NoSQL when horizontal scaling is required. They scale very well. NoSQL Databases are: MongoDB, BigTable, Redis, RavenDb, Neo4j, and CouchDb If you need any of the ACID property then use SQL Database else use NoSQL Database.\nScalability As your company grow, data grow and scalability comes into picture:-\nVertical Scaling Database can be scaled vertically by adding more power (CPU, RAM or SSD) to same machine, so that database can handle more load. Vertical scaling has limitations as can not add power to same machine beyond a point. SQL Databases are often scaled vertically Horizontal Scaling Databases can be scaled horizontally by adding more machines into the pool of databases. Data is saved in pool of databases using Database Sharding or also known as Horizontal Partitioning\nDatabase sharding is easier in NoSQL databases as compare to SQL databases.\nDatabase sharding means partitioning of data in multiple databases. Incoming data can be saved in databases based on some key. for e.g. userId in case of saving user profile.\ntotal database in horizontal partitioning = 5 [DB_1, DB_2, DB_3, DB_4, DB_5] consistent hashing = 5 databases makes a consistent hash ring DB_1 ➚ ↘ DB_5 DB_2 ↖ ↙ DB_4 ⟵ DB_3 write user_1 hash(userId_1) = 1002 database to write this record = 1002%5 = 2 (DB_2) write user_2 hash(userId_2) = 5119 database to write this record = 5119%5 = 4 (DB_4) read user_1 hash(userId_1) = 1002 database to read this record = 1002%5 = 2 (DB_2) DB_4 goes down.... write user_3 hash(userId_3) = 1234 database to write this record = 1234%5 = 4 (DB_4) DB_4 is not available, write to the next available db in consistent hash ring i.e. DB_5 Availability Availability is measured based on replication factor. Replication factor = 3 means a database is having 2 additional copies which keep themselves in-sync asynchronously.\nActive-Active Data read and write happen in any database replica.\n(Eventual read and write Consistency, High Availability, High Performance)\n┌―――――――――――┐ write ➔ | DB_1 | ← read └―――――――――――┘ ⇡⇣ ┌―――――――――――┐ write ➔ | DB_2 | ← read └―――――――――――┘ Eventual Read Consistency:- 1. record_A write request to DB_1 2. record_A read request from DB_2 immediately record_A is not be available in DB_2 if sync is yet to happen Eventual Write Consistency:- 1. record_A write request to DB_1 2. record_A update request to DB_2 immediately record_A is not be available in DB_2 if sync is yet to happen Active-Active is not generally used because eventual write consistency is not acceptable in most of the non-functional requirement.\nActive-Passive (Master-Slave) Data read and write happen only in master database. Data is synced with slave database asynchronously. When Master goes down, Slave started serving as master.\n(High read and write Consistency, High Availability, Low Performance)\n┌―――――――――――――――――――┐ write ➔ | DB_1 (master) | ← read └―――――――――――――――――――┘ ⇣ ┌―――――――――――――――――――┐ | DB_1\u0026#34; (slave) | └―――――――――――――――――――┘ Active-Passive is generally used in financial sector where read and write consistency of financial transactions is very important.\nWrite-Read (Master Slave) Data write happen in master database. Data read happen from any database.\n(High write and eventual read Consistency, High Availability, Low write and High read Performance)\n┌―――――――――――――――――――┐ write ➔ | DB_1 (master) | ← read └―――――――――――――――――――┘ ⇣ ┌―――――――――――――――――――┐ | DB_1\u0026#34; (slave) | ← read └―――――――――――――――――――┘ Write-Read is generally used in social media such as Facebook, LinkedIn, Twitter, Instagram where read/write ratio is very high and eventual read consistency is acceptable.\nCluster When data is partitioned across multiple database\ntotal database in cluster = 5 [DB_1, DB_2, DB_3, DB_4, DB_5] replication factor = 3 consistent hashing = 5 databases makes a consistent hash ring DB_1 ➚ ↘ DB_5 DB_2 ↖ ↙ DB_4 ⟵ DB_3 Data write for DB_1 is copied to DB_2 and DB_3 (next 2 databases in the ring) Data write for DB_2 is copied to DB_3 and DB_4 (next 2 databases in the ring) ... ... Data write for DB_5 is copied to DB_1 and DB_2 (next 2 databases in the ring) In consistent hash ring, next two databases either can contain 100% data copy of previous database or share the 50%-50% data copy depending upon the requirement.\nGeo Sharding Geo sharding is nothing but database sharding based on geo location.\nUse-case It is used by ride hailing apps such as Uber, Grab to store live locations of drivers and passengers. It is also used by tinder and other dating sites to provide recommendations based on location. How to do? Google S2 library divides the geo locations into cells (for e.g. 1 cell = 100x100 miles square). You get the live location (coordinates lat, long) of user and pass to S2 library, which returns the cell_id in which that location falls into. Use this cell_id as a key to partition the data. Consistent Hashing Consistent hashing is the algorithm used by:\nMost of the load balancers to distribute the load across multiple services. Most of the database horizontal partitioning to distribute the data across multiple databases. The idea of consistent hashing is to place all the services (or databases) across the consistent hash ring and distribute the load based on some hashing algorithm. This gives us the flexibility to remove or add the service (or database) to the ring without disturbing the whole cluster with minimum or no data loss.\ntotal database in cluster = 5 [DB_1, DB_2, DB_3, DB_4, DB_5] consistent hashing = 5 databases makes a consistent hash ring DB_1 ➚ ↘ DB_5 DB_2 ↖ ↙ DB_4 ⟵ DB_3 If DB_2 goes down, Next database in the ring i.e. DB_3 is going to take additional load (future writes). If replication is not implemented then existing data of DB_2 will be lost but other database will still work. If DB_6 is added newly between DB_5 and DB_1 then it share the 50% load of the next database in the ring i.e. DB_1. File Storage Options (Image/Video) Scalability E-Commerce like Amazon, Social Media like Instagram, Facebook, Twitter, Streaming Provider like Netflix requires to store a huge amount of Images and Videos.\nImages and Videos are stored a BLOB (Binary Large Object) and Database Storage is not a good option for this kind of storage. Why?\nYou are not going to query on BLOB Storage You do not need any ACID guarantees for BLOB storage as such provided by RDBMS Costly to store in Database Cheaper and scalable options to store such files are Distributed File Systems (DFS) such as Sharepoint, Amazon S3.\nDetails about the image/video such as its DFS URL, metadata can be save in Database, referred by Image ID.\nID URL METADATA IMAGE_THUMBNAIL/VIDEO_COVER_PHOTO It is also a smart idea to save thumbnail of image, or cover photo of the video in database, size of which should not exceed few KBs. Initially the thumbnail or cover photo can be sent to the client along with the URL of image/video. It is a good experience for the user to see something while image/video is downloading/streaming from DFS.\nAvailability You may want to use Content Delivery Network (CDN) which distribute the same images/videos to different locations geographically near to the client locations.\nCaching You need to use Caching at some point of time in your system design for faster read.\nFew famous Caching solutions are:\nRedis (recommended) Memcached Hazelcast Text Search Capabilities Almost all the websites requires searching capabilities in one way or another. For e.g. Amazon for product search, Netflix for movies search, Youtube for video search, Social media websites for user profile search, and so and so forth.\nSome of the popular text search engine:\nElastic Search (recommended) Apache Solr Apache Lucene Apache Lucene is the core framework written based on Map-Reduce and Inverted-Index algorithm. Elastic Search and Apache Solr has been written on top of Apache Lucene provide additional search capabilities such as fuzzy search, type ahead, search suggestions. You don\u0026rsquo;t rely on these for storage. You generally save data in your primary database and write to Elastic Search or Apache Solr asynchronously. Reporting and Analytics Data warehouse where you generally dump a lot of data and they provide very complex querying capabilities.\nApache Hadoop Amazon Redshift Elastic Search Top Hashing Algorithms Consistent Hashing Consistent Hashing is used to route the request to distributed resources. For e.g.\nLoad Balancer use Consistent Hashing algorithm to route the request to specific Application Server. Used in database sharding (also known as horizontal partitioning) to save the date in specific shard. Bloom Filter Bloom filter takes the constant time and space to answer Yes/No to this kind of questions:-\nDoes this username exist or already taken? Is this item exist in the set? Bloom filter gives:-\n100% accurate result when it says \u0026ldquo;No\u0026rdquo; Probabilistic result when it says \u0026ldquo;Yes\u0026rdquo;. Sometimes it might be false positive. Bloom filter data-structure is represented as an array of bits like this:-\nIndex 0 1 2 3 4 5 6 7 8 Bit Array | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Case 1 Check username \u0026ldquo;John\u0026rdquo; exist?\nhash1(John) = 100 =\u0026gt; 100%8 = 4 hash2(John) = 54 =\u0026gt; 54%8 = 6 Since 4th and 6th bit of our array is not set, \u0026ldquo;John\u0026rdquo; doesn\u0026rsquo;t exist for sure. Set the bits 4th and 6th bit now:-\nIndex 0 1 2 3 4 5 6 7 8 Bit Array | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | Case 2 Check username \u0026ldquo;Bill\u0026rdquo; exist?\nhash1(Bill) = 81 =\u0026gt; 81%8 = 1 hash2(Bill) = 31 =\u0026gt; 31%8 = 7 Since 1st and 7th bit of our array is not set, \u0026ldquo;Bill\u0026rdquo; doesn\u0026rsquo;t exist for sure. Set the 1st and 7th bit now:-\nIndex 0 1 2 3 4 5 6 7 8 Bit Array | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 1 | Case 3 Check username \u0026ldquo;Garry\u0026rdquo; exist?\nhash1(Garry) = 89 =\u0026gt; 89%8 = 1 hash2(Garry) = 90 =\u0026gt; 90%8 = 2 Since 1st bit is set but 2nd bit is not set, \u0026ldquo;Garry\u0026rdquo; doesn\u0026rsquo;t exist for sure. Set the 2nd bit now:-\nIndex 0 1 2 3 4 5 6 7 8 Bit Array | 0 | 1 | 1 | 0 | 1 | 0 | 1 | 1 | Case 4 Check username \u0026ldquo;Jack\u0026rdquo; exist?\nhash1(Jack) = 89 =\u0026gt; 89%8 = 1 hash2(Jack) = 90 =\u0026gt; 90%8 = 2 Since both 1st and 2nd bit are set, \u0026ldquo;Jack\u0026rdquo; may or may not exist. Check the database.\nConclusion Number of bits takes very less space and in practice numbers of bits in array can be huge providing more precise results. We have used only 8 bits in the example. Bloom filter provide 100% accurate result if user doesn\u0026rsquo;t exist. Bloom filter provide estimated result if user exist. Estimated result is more precise if hash function is evenly distributed and more hash functions are used. We have used only 2 hash functions in the example. Count-Min Sketch Count-Min Sketch is probabilistic data structure which takes a constant time and space to answer this kind of questions:-\nCount views of a particular YouTube video? Find the top trending posts on Instagram? ","permalink":"https://codingnconcepts.com/system-design-cheat-sheet/","tags":["System Design"],"title":"System Design Cheat Sheet"},{"categories":["Interview Questions","Java"],"contents":"Comprehensive List of Java Multithreading (Concurrency) Interview Questions based on my personal interview experience over the last few years. Keep following this post link for regular updates.\nAlso Read Core Java Interview Questions\nAlso Read Java 8 Interview Questions\nPrint \u0026ldquo;Ping Pong\u0026rdquo; using two threads alternatively. We can signal two threads to run alternatively using different ways in Java:\nUsing wait notify 1st thread print \u0026ldquo;Ping\u0026rdquo; and go to wait state. 2nd thread wakes up from wait state, print \u0026ldquo;Ping\u0026rdquo;, notify 1st thread, goes back to wait state. 1st thread wakes up from wait state, print \u0026ldquo;Pong\u0026rdquo;, notify 2nd thread, goes back to wait state. Step 2 and 3 repeats and print \u0026ldquo;Ping Pong\u0026rdquo; alternatively. Using ReentrantLock Condition ReentrantLock Condition provides two methods await() and signal() which works very similar to wait notify methods.\nRead Ping Pong using Threads in Java for Java programs using Object\u0026rsquo;s wait-notify and ReentrantLock Condition\u0026rsquo;s await-signal.\nHow to run 5 threads sequentially? Smart Answer: The whole point of having threads is to run them concurrently. If you want to run threads sequentially, better not use them.\nUsing Thread.join() Let\u0026rsquo;s create a task which prints 1 to 100\nclass Task implements Runnable{ int index; Task(int index){ this.index = index; } @Override public void run() {\tfor(int i = 1; i \u0026lt;= 100; i++) { System.out.print(\u0026#34;Thread\u0026#34; + index + \u0026#34;: \u0026#34; + i + \u0026#34; \u0026#34;); } System.out.println(); }\t} Let\u0026rsquo;s create 5 threads to run the tasks sequentially by joining the them one after the another.\nThread t1 = new Thread(new Task(1)); Thread t2 = new Thread(new Task(2)); Thread t3 = new Thread(new Task(3)); Thread t4 = new Thread(new Task(4)); Thread t5 = new Thread(new Task(5)); t1.start(); t1.join(); t2.start(); t2.join(); t3.start(); t3.join(); t4.start(); t4.join(); t5.start(); Using SingleThreadExecutor You can create single thread executor service and submit the task 5 times to execute them sequentially.\nExecutorService service = Executors.newSingleThreadExecutor(); for(int i=1; i\u0026lt;= 5; i++){ Task task = new Task(i); service.submit(task); } A Task is running in a separate thread. Stop the task if it exceeds 10 minutes. We can use Thread.sleep() to sleep the thread for 10 minutes and Thread.interrupt() to interrupt a thread after 10 minutes.\nWe can interrupt a thread in three ways:-\nUsing thread interrupt() Using Executor thread pool shutdownNow() Using Callable returned future cancel() public class StopThreadAfterTimeout { public static void main(String[] args) throws InterruptedException, ExecutionException { usingThreadInterrupt(); //usingThreadPoolShutdownNow(); //usingFutureCancel(); } private static void usingThreadInterrupt() throws InterruptedException { // 1. create a thread Thread t1 = new Thread(() -\u0026gt; { while (!Thread.currentThread().isInterrupted()) { // next step } }); // 2. timeout after 10 minutes Thread.sleep(10 * 60 * 1000); // 3. stop the thread t1.interrupt(); } private static void usingThreadPoolShutdownNow() throws InterruptedException { ExecutorService threadPool = Executors.newFixedThreadPool(2); // 1. create a thread threadPool.submit(() -\u0026gt; { while (!Thread.currentThread().isInterrupted()) { // next step } }); // 2. timeout after 10 minutes Thread.sleep(10 * 60 * 1000); // 3. stop the thread threadPool.shutdownNow(); // internally call thread interrupt } private static void usingFutureCancel() throws InterruptedException, ExecutionException { ExecutorService service = Executors.newFixedThreadPool(2); // 1. submit new callable task which return future object Future\u0026lt;?\u0026gt; future = service.submit(() -\u0026gt; { while (!Thread.currentThread().isInterrupted()) { // next step } }); // 2. wait for 10 minutes to get response try { future.get(10, TimeUnit.MINUTES); } catch (TimeoutException e) { // 3. stop the thread future.cancel(true); // internally call thread interrupt } } } How to decide Thread Pool Size? It depends on the type of tasks you want to execute:-\n1. CPU Intensive Tasks If you are executing CPU intensive tasks such as cryptographic hash function then max thread pool size should be equal to number of cores in processor.\nFor example a 4 core processor can run only 4 threads at a time so threadPoolSize = 4 is ideal for threads taking lot of CPU.\npublic class IdealThreadPoolSize { public static void main(String[] args) { // get count of available cores int coreCount = Runtime.getRuntime().availableProcessors(); ExecutorService service = Executors.newFixedThreadPool(coreCount); // submit the tasks for execution for (int i = 0; i \u0026lt; 100; i++) { service.execute(new CpuIntestiveTask()); } } static class CpuIntestiveTask implements Runnable { public void run() { // some CPU intensive operations } } } 2. IO Intensive Tasks If you are executing IO intensive tasks such as Database or HTTP calls, where threads have a tendency to wait after requesting for the resources. In such case max thread pool size should be larger than the number of cores in processor.\nFor example for a threadPoolSize = 20, if 16 threads are waiting for resources or blocked due to IO operations, other 4 threads can be processed concurrently by 4 core processor.\nIdeal number of threadPoolSize depends on rate of task submission and average task wait time. ThreadPool typically place all the submitted task in BlockingQueue which is thread-safe. Idle threads fetch the task from the queue and process them.\npublic class IdealThreadPoolSize { public static void main(String[] args) { // much higher count for IO tasks ExecutorService service = Executors.newFixedThreadPool(20); // submit the tasks for execution for (int i = 0; i \u0026lt; 100; i++) { service.execute(new IOTask()); } } static class IOTask implements Runnable { public void run() { // some IO operations which will cause thread to block/wait } } } What are the types of Executor Thread Pool? FixedThreadPool - Create a fixed number of threads in the beginning and place them in the pool. SingleThreadExecutor - Create a single thread and place it in the pool. CachedThreadPool - Create a new thread only if all the other threads are busy and place it in the pool. Ideal for short lived tasks. ScheduledThreadPool - Create a fixed number of threads in the beginning and place them in the pool. Create a new thread if all the other threads are busy. Ideal for running the tasks at regular time interval or delay. // for fixed number of threads ExecutorService fixedThreadPool = Executors.newFixedThreadPool(10); // for a pool of single thread ExecutorService singleThreadPool = Executors.newSingleThreadExecutor(); // for lot of short lived tasks ExecutorService cachedThreadPool = Executors.newCachedThreadPool(); // for scheduling of tasks ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(10); // task to run after 10 second delay scheduledThreadPool.schedule(new Task(), 10, TimeUnit.SECONDS); // task to run after initial delay of 15 seconds at every 10 second interval scheduledThreadPool.scheduleAtFixedRate(new Task(), 15, 10, TimeUnit.SECONDS); // task to run after initial delay of 15 second at every 10 second after previous task completes scheduledThreadPool.scheduleWithFixedDelay(new Task(), 15, 10, TimeUnit.SECONDS); Thread Pool Parameters Thread Pool corePoolSize maxPoolSize keepAliveTime QueueType FixedTheadPool constr-arg same as core NA LinkedBlockingQueue SingleThreadExecutor 1 1 NA LinkedBlockingQueue CachedThreadPool 0 Integer.MAX_VALUE 60s SynchronousQueue ScheduledThreadPool constr-arg Integer.MAX_VALUE 60s DelayedWorkQueue Custom Thread Pool If you do not want to use any out of the box available thread pools. You can always create your customize thread pool as below:-\n// custom thread pool ExecutorService customThreadPool = // (corePoolSize: 10, maxPoolSize: 100, keepAliveTime: 120s, QueueType: ArrayBlockingQueue) new ThreadPoolExecutor(10, 100, 120, TimeUnit.SECONDS, new ArrayBlockingQueue\u0026lt;\u0026gt;(30)); How to customize a thread in Executor Thread Pool? You can implement the newThread() method of class ThreadFactory to spawn custom threads in Executor Thread Pool.\nLet\u0026rsquo;s Generate custom threads and change their names to Test_Thread_\u0026lt;Number\u0026gt;:-\npublic class ExecuterTest { public static void main(String[] args) { ExecutorService service = Executors.newFixedThreadPool(5, new ExecutorThreadFactory(\u0026#34;Test_Thread_\u0026#34;)); for(int i=1; i\u0026lt;=5; i++){ service.execute(new Task()); }\t} } class Task implements Runnable { @Override public void run() { System.out.println(Thread.currentThread().getName());\t} } class ExecutorThreadFactory implements ThreadFactory { AtomicInteger counter = new AtomicInteger(); String threadNamePrefix; ExecutorThreadFactory(String prefix){ this.threadNamePrefix = prefix; } @Override public Thread newThread(Runnable r) { Thread thread = new Thread(r, threadNamePrefix + counter.incrementAndGet()); thread.setDaemon(false); return thread; }\t} Output Test_Thread_1 Test_Thread_4 Test_Thread_3 Test_Thread_2 Test_Thread_5 Runnable vs Callable Interface Runnable and Callable interfaces are used to create a task and submit to thread pool for processing.\nRunnable Interface Callable Interface has run() method has call() method run() do not return value call() return value run() cannot throw checked exception call() can throw checked exception execute Runnable task using ExecutorService execute() method submit Callable task using ExecutorService submit() method execute() do not return anything submit() return Future object public class RunnableVsCallable { public static void main(String[] args) throws InterruptedException, ExecutionException { ExecutorService service = Executors.newSingleThreadExecutor(); // execute runnable task which return nothing service.execute(new RunnableTask()); // submit callable task which return Future object Future\u0026lt;Integer\u0026gt; future = service.submit(new CallableTask()); // get result from future object which is blocking operation Integer result = future.get(); // shutdown the thread gracefully service.shutdown(); } static class RunnableTask implements Runnable { @Override public void run() { try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } } static class CallableTask implements Callable\u0026lt;Integer\u0026gt; { @Override public Integer call() throws InterruptedException { Thread.sleep(1000); return new Random().nextInt(); } } } Synchronized Block vs Reentrant Lock Reentrant locks are explicit, whereas Synchronized block is implicit Reentrant locks provides flexibility to lock/unlock in any scopes, whereas Synchronized block scope is limited to curly braces {}\nIn below example lock() is outside and unlock() is inside the try-finally block. private static ReentrantLock lock = new ReentrantLock(); private static void accessResource() { lock.lock(); try{ // access the resource }finally{ lock.unlock(); } } public static void main(String[] args){ Thread t1 = new Thread(() -\u0026gt; accessResource)); t1.start(); Thread t2 = new Thread(() -\u0026gt; accessResource)); t2.start(); Thread t3 = new Thread(() -\u0026gt; accessResource)); t3.start(); } The same accessResource() method using Synchronized block: private static void accessResource() { synchronized(this) { // equivalent to lock.lock() // access the resource } // equivalent to lock.unlock() } Reentrant locks lock() can be called multiple times without calling unlock() method hence the name reentrant.\nCalling lock() two times means lock inside lock. You need to call unlock() two times as well to come out of the lock completely. Reentrant locks are unfair by default. You can enable fairness by passing boolean true new ReentrantLock(true) in constructor. Fairness provides more chance to acquire lock to the threads waited for longest time in the queue. Performance is slow when fairness is turned on. Reentrant locks provide ability to tryLock() and tryLock(timeout) private static void accessResource() { boolean lockAcquired = lock.tryLock(); // boolean lockAcquired = lock.tryLock(5, TimeUnit.SECONDS); if(lockAcquired) { lock.lock(); try{ // access the resource }finally{ lock.unlock(); } } else { // do something else } } Reentrant Lock vs ReadWrite Lock ReentrantLock consist of only one lock, only one thread can acquire that lock at a time ReentrantReadWriteLock consist of ReadLock and WriteLock. Though they are two separate instances, either read or write operation is allowed at a time.\nEither one thread can acquire a write lock or multiple threads can acquire a read lock at a time. Both ReadLock and WriteLock use the same Queue behind the scene waiting for their turn.\npublic class ReadWriteLock { private ReentrantReadWriteLock lock = new ReentrantReadWriteLock(); private ReentrantReadWriteLock.ReadLock readLock = lock.readLock(); private ReentrantReadWriteLock.WriteLock writeLock = lock.writeLock(); private void readResource() { readLock.lock(); // read the resource readLock.unlock(); } private void writeResource() { writeLock.lock(); // write the resource writeLock.unlock(); } public static void main(String[] args) { ReadWriteLock obj = new ReadWriteLock(); Thread t1 = new Thread(() -\u0026gt; obj.readResource()); t1.start(); Thread t2 = new Thread(() -\u0026gt; obj.readResource()); t2.start(); Thread t3 = new Thread(() -\u0026gt; obj.writeResource()); t3.start(); Thread t4 = new Thread(() -\u0026gt; obj.writeResource()); t4.start(); Thread t5 = new Thread(() -\u0026gt; obj.readResource()); t5.start(); //t1,t2 can read at the same time //t3,t4 can not write when t1,t2 are reading //t4 can not write when t3 is writing //t5 can not read when t3 is writing }\t} What is volatile keyword? Each thread runs in one core of multi-core processor. Each Core has their own local cache. All the cores share one shared cache.\nthread-1 thread-2 ┌―――――――――――――┐―――――――――――――┐ | Core 1 | Core 2 | |―――――――――――――|―――――――――――――| | Local Cache | Local Cache | |―――――――――――――┘―――――――――――――| | Shared Cache | └―――――――――――――――――――――――――――┘ When you apply volatile keyword to a property. Any updates in that property done by thread-1 in local-cache is pushed down to shared-cache to make sure that the update is visible to thread-2\n// atomic operation volatile boolean flag = true; volatile keyword doesn\u0026rsquo;t work when you do compound operations such as count++ which is read, increment and write back. In such case we can use AtomicInteger or AtomicLong\nType Use Case volatile Flags AtomicInteger, AtomicLong Counters What is ThreadLocal Object? ThreadLocal object is used to create object per thread instances for memory efficiency and thread-safety.\nFor example, SimpleDateFormat is not a thread-safe object and it is an expensive object to create every time we use it. It is a good candidate to create as a ThreadLocal object.\nWe created a fixed thread pool of 10 threads and submitted 1000 tasks. In this case, 10 ThreadLocal\u0026lt;SimpleDateFormat\u0026gt; objects will be created, one for each thread.\nformatter.get() method make sure that it returns the object of the currently running thread out of those 10 formatter objects.\n// create SimpleDateFormat ThreadLocal object and initialize with value class ThreadSafeFormatter { public static ThreadLocal\u0026lt;SimpleDateFormat\u0026gt; formatter = ThreadLocal.withInitial(() -\u0026gt; new SimpleDateFormat(\u0026#34;yyyy-MM-dd\u0026#34;)); } public class UserService { private static ExecutorService threadPool = Executors.newFixedThreadPool(10); public static void main(String[] args){ // submit 1000 tasks for(int i=0; i\u0026lt;1000; i++){ threadPool.submit(() -\u0026gt; new UserService().birthDate(i)); } } public String birthDate(int userId){ Date birthDate = birthDateFromDb(userId); // get formatter object of currently running thread final SimpleDateFormat formatter = ThreadSafeFormatter.formatter.get(); return formatter.format(birthDate); } } What is Semaphore? Semaphore is used when you want to limit maximum number of concurrent calls to a particular resource in multi-threaded environment.\npublic class Semaphore { public void acquire(); public void release(); } Semaphore(3) means only 3 threads can acquire() a lock and use resources at a time. As soon as one thread out of 3 release() the lock, 4th thread can acquire() the lock and use the resources.\npublic class SemaphoreTest { public static void main(String[] args) throws InterruptedException {\t// ThreadPool of 50 threads ExecutorService service = Executors.newFixedThreadPool(50); // Semaphore to rum max 3 concurrent calls Semaphore semaphore = new Semaphore(3); // Execute 1000 tasks (initiate 1000 threads) to test IntStream.of(1000).forEach(i -\u0026gt; service.execute(new Task(semaphore))); // Shutdown the ExecutorService gracefully service.shutdown(); service.awaitTermination(1, TimeUnit.MINUTES); } static class Task implements Runnable { private Semaphore semaphore; Task(Semaphore semaphore) { this.semaphore = semaphore; } @Override public void run() {\t// Start code -\u0026gt; can be executed by 50 threads concurrently // since thread pool size is 50 try { // Only 3 threads can acquire lock at a time since semaphore count is 3 // Other threads wait here. semaphore.acquire(); } catch (InterruptedException e) { e.printStackTrace(); } // Slow Code -\u0026gt; Heavy operations like IO // can be executed by only 3 threads // Release the lock for other threads to come in Slow code semaphore.release(); // End code -\u0026gt; can be executed by 50 threads concurrently // since thread pool size is 50 } } } What is CountDownLatch? CountDownLatch is used when we want our main thread to wait until all the dependent services are initialized and up.\npublic class CountDownLatch { public void countDown(); public void await(); } Create a latch = CountDownLatch(3) means three services to be initialized before main thread. Each service call latch.countDown() after initializing which decrement the countdown by 1. Main thread waits for all services to be initialized (until countdown reaches zero) using latch.await(). Once all 3 services are initialized (countdown reaches zero), main thread start doing its job. public class CountDownLatchTest { public static void main(String[] args) throws InterruptedException{ CountDownLatch latch = new CountDownLatch(3); Thread cacheService = new Thread(new Service(\u0026#34;Cache Service\u0026#34;, latch)); Thread alertService = new Thread(new Service(\u0026#34;Alert Service\u0026#34;, latch)); Thread validationService = new Thread(new Service(\u0026#34;Validation Service\u0026#34;, latch)); cacheService.start(); alertService.start(); validationService.start(); latch.await(); //wait here till latch count reaches 0 then only process next line of code. System.out.println(\u0026#34;All Services are up and running. Main Thread started processing...\u0026#34;);\t} static class Service implements Runnable{ private String name; private CountDownLatch latch; Service(String name, CountDownLatch latch){ this.name = name; this.latch = latch; } @Override public void run() { // startup task System.out.println(name + \u0026#34; is up\u0026#34;); latch.countDown(); // continue w/ other tasks }\t} } Output Cache Service is up Validation Service is up Alert Service is up All Services are up and running. Main Thread started processing... How to start 5 threads at the same time? Well, you can have max number of parallel threads run at the same time is equal to number of cores in CPU. If you have a 4 cores CPU, only 4 threads can start at the exact same time.\nThough the idea of the question is if you have any Java concurrency feature which allows you to trigger the threads at the same time. Answer is CountDownLatch.\nCreate a CountDownLatch of 5 Create and start all the 5 threads All the threads wait until CountDownLatch reaches zero All the threads start at the same time from that point public class StartThreadsAtSameTime { public static void main(String[] args) throws InterruptedException { // Initialize Countdown = 5 CountDownLatch latch = new CountDownLatch(5); // Initialize and start 5 threads passing the same latch object for (int i = 0; i \u0026lt; 5; i++) { Thread t = new Thread(new Task(latch)); t.start(); } System.out.println(\u0026#34;All threads wait until Countdown = 0\u0026#34;); // Decrement the Countdown 5 times for (int i = 0; i \u0026lt; 5; i++) { latch.countDown(); } // All threads start printing at this point } static class Task implements Runnable { private CountDownLatch latch; Task(CountDownLatch latch) { this.latch = latch;} @Override public void run() { try { // thread wait for Countdown = 0 latch.await(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + \u0026#34; is running\u0026#34;); } } } Output All threads wait until Countdown = 0 Thread-2 is running Thread-1 is running Thread-3 is running Thread-0 is running Thread-4 is running What is CyclicBarrier? CyclicBarrier is used when two or more threads are required to reach at certain barrier point.\nThey wait for each other at barrier point using await(). Once all reach at barrier point, they start from there again. Since it is a CyclicBarrier, at this point, cycle is reset. All three threads wait again at next barrier point using await() Once all reach at next barrier point, they start from there again. This cycle goes on\u0026hellip; public class CyclicBarrierTest { public static void main(String[] args) { CyclicBarrier barrier = new CyclicBarrier(3); Thread t1 = new Thread(new Task(barrier)); Thread t2 = new Thread(new Task(barrier)); Thread t3 = new Thread(new Task(barrier)); t1.start(); t2.start(); t3.start(); } static class Task implements Runnable { private CyclicBarrier barrier; Task(CyclicBarrier barrier) { this.barrier = barrier;} @Override public void run() { while(true) { String threadName = Thread.currentThread().getName(); System.out.println(threadName + \u0026#34; running towards barrier\u0026#34;); try { barrier.await(); // wait for other threads to reach at barrier point } catch (InterruptedException | BrokenBarrierException e) { e.printStackTrace(); } // barrier cycle is reset at this point System.out.println(threadName + \u0026#34; crossed barrier\u0026#34;);\ttry { Thread.sleep(1000); // just sleep for a while to verify in console } catch (InterruptedException e) { e.printStackTrace(); } }\t} } } Output Thread-0 running towards barrier Thread-2 running towards barrier Thread-1 running towards barrier Thread-1 crossed barrier Thread-0 crossed barrier Thread-2 crossed barrier Thread-1 running towards barrier Thread-2 running towards barrier Thread-0 running towards barrier Thread-0 crossed barrier Thread-1 crossed barrier Thread-2 crossed barrier ... ... ... Retrieve Price from N Sources Asynchronously We can implement a Scatter Gather Pattern using CompletableFuture where we run three tasks asynchronously to fetch price of a product from 3 websites.\nNote that when you call CompletableFuture.runAsync(Runnable runnable) then Java runs this in a separate thread of ForkJoinPool.commonPool() thread pool by default. You do not need to create any separate thread pool.\nAlso note that CompletableFuture.get() method is blocking operations. We do not want our pricing result to wait forever if any website is down thatswhy we have given a timeout of 5 seconds.\nWe wait for them for max 5 seconds and return the prices. Two things can happen:-\nIf all three tasks completed (run method executed) within 4 seconds, prices from all websites are returned at 4th second. If only two tasks are completed within 5 seconds, prices from only two websites are returned at 5th second.\npublic class ScatterGatherPattern { public static void main(String[] args) throws InterruptedException, ExecutionException, TimeoutException { ScatterGatherPattern scatterGather = new ScatterGatherPattern(); scatterGather.getPrices(1); } private Set\u0026lt;Double\u0026gt; getPrices(int productId) throws InterruptedException, ExecutionException, TimeoutException { Set\u0026lt;Double\u0026gt; prices = Collections.synchronizedSet(new HashSet\u0026lt;\u0026gt;()); CompletableFuture\u0026lt;Void\u0026gt; task1 = CompletableFuture.runAsync(new Task(\u0026#34;amazon.com\u0026#34;, productId, prices)); CompletableFuture\u0026lt;Void\u0026gt; task2 = CompletableFuture.runAsync(new Task(\u0026#34;ebay.com\u0026#34;, productId, prices)); CompletableFuture\u0026lt;Void\u0026gt; task3 = CompletableFuture.runAsync(new Task(\u0026#34;wallmart.com\u0026#34;, productId, prices)); CompletableFuture\u0026lt;Void\u0026gt; allTasks = CompletableFuture.allOf(task1, task2, task3); // wait for all the taks to complete, but max for 5 seconds allTasks.get(5, TimeUnit.SECONDS); //blocking operation return prices; } private static class Task implements Runnable { private String url; private int productId; private Set\u0026lt;Double\u0026gt; set; Task(String url, int productId, Set\u0026lt;Double\u0026gt; set) { this.url = url; this.productId = productId; this.set = set; } @Override public void run() { double price = 0; // make http call (url, productId) to get price set.add(price); } } } Also Read Async Fetch Data from N Sources and Combine In Java\nFuture vs CompletableFuture Future CompletableFuture Introduced in Java 5 (2004) Introduced in Java 8 (2014) ExecuterService.submit() returns a Future object CompletableFuture.supplyAsnc() returns CompletableFuture object Futures cannot be chained together CompletableFutures can be chained to perform async operations What is DeadLock. How to Detect and Avoid them? Deadlock is a situation where:-\nthread_1 acquires lock_A and waiting to acquire lock_B thread_2 acquires lock_B and waiting to acquire lock_A Now both thread_1 and thread_2 will wait for each other indefinitely and result into deadlock situation.\npublic class DeadlockTest { private Lock lockA = new ReentrantLock(); private Lock lockB = new ReentrantLock(); public static void main(String[] args){ DeadlockTest test = new DeadlockTest(); test.execute(); } private void execute() { new Thread(this::processThis).start(); new Thread(this::processThat).start(); } private void processThis() { lockA.lock(); System.out.println(Thread.currentThread().getName() + \u0026#34; processing resource A\u0026#34;); lockB.lock(); System.out.println(Thread.currentThread().getName() + \u0026#34; processing resource B\u0026#34;); lockA.unlock(); lockB.unlock(); } private void processThat() { lockB.lock(); System.out.println(Thread.currentThread().getName() + \u0026#34; processing resource B\u0026#34;); lockA.lock(); System.out.println(Thread.currentThread().getName() + \u0026#34; processing resource A\u0026#34;); lockA.unlock(); lockB.unlock(); } } How to detect a DeadLock? Deadlock can be detected at runtime by creating Thread dump of the application which represent the state of the application at that point of time.\nWe can use tools such as VisualVM, JProfiler, YourKit to get the thread dump. We can also use any of the following commands to get the thread dump:-\nkill -3 \u0026lt;process_id\u0026gt; jstack \u0026lt;process_id\u0026gt; \u0026gt; ./out.txt Alternatively we can run this program constantly in the background to check for deadlocks:-\nprivate static void detectedDeadLock() { ThreadMXBean threadBean = ManagementFactory.getThreadMXBean(); long[] threadIds = threadBean.findDeadlockedThreads(); boolean deadLock = threadIds != null \u0026amp;\u0026amp; threadIds.length \u0026gt; 0; System.out.println(\u0026#34;DeadLock found: \u0026#34; + deadLock); } How to avoid a DeadLock? Use timeouts while acquiring locks if possible, so that thread do not wait for indefinitely Lock lock = new ReentrantLock(); lock.tryLock(2, TimeUnit.SECONDS); If same locks are used in different methods, try to acquire the locks in same order. private void processThis() { lockA.lock(); lockB.lock(); // ... } private void processThat() { lockA.lock(); lockB.lock(); // ... } Parallelism vs Concurrency Parallelism Concurrency Doing a lot of things at once Dealing with lot of things at once If your machine is having 4 core CPU then you can run at most 4 tasks in parallel If your Java ThreadPool size is 20 then you can run at most 20 tasks concurrently in different threads If you have 1 core CPU, you can not achieve Parallelism If you have 1 core CPU, you can still achieve Concurrency If you have ThreadPool of size 1, you can still achieve Parallelism If you have ThreadPool of size 1, you can not achieve Concurrency Let\u0026rsquo;s look at various use cases to understand the difference:-\nConcurrent, but not Parallel If machine is having 1 core CPU and you are using ThreadPool of size 20. Say you submit 20 similar tasks to ThreadPool.\nAt any moment, all the 20 similar tasks are in progress (concurrency=20), and only one tasks is actually running on CPU (parallelism=1) while others are waiting.\nParallel, but not Concurrent If machine is having 4 core CPU and you are using 20 ThreadPools of size 1. Say you submit 20 different tasks to each ThreadPool.\nAt any moment, all the 20 different types of tasks are in progress, and only 1 similar type of task in progress (concurrency=1), and 4 tasks are actually running on CPU (parallelism=4) while others are waiting.\nPlease note that we say tasks are running concurrently only when they are similar tasks, or they are accessing/updating same resources, or they need to coordinate.\nNot Parallel, not Concurrent If machine is having 1 core CPU and you are using ThreadPool of size 1. Say you submit 20 similar tasks to ThreadPool.\nAt any moment, Only 1 task is in progress (concurrency=1), and only one tasks is actually running on CPU (parallelism=1) while no one is waiting.\nHow to handle exceptions thrown from ExecutorService tasks? There are multiple ways to handle the exceptions thrown by tasks executed by ExecutorService:-\n1. Use Callable instead of Runnable Callable swallow the exception and throw ExecutionException when future.get() gets called. We can catch the exception at this point and do something about it!\npublic void submitThenThrowUncheckedThenGet() { final ExecutorService executorService = Executors.newFixedThreadPool(10); final Future\u0026lt;Object\u0026gt; future = executorService.submit(() -\u0026gt; { throw new RuntimeException(\u0026#34;Uncaught exception in callable task\u0026#34;); }); try { future.get(); } catch (ExecutionException | InterruptedException e) { System.out.println(\u0026#34;Caught the exception ( \u0026#34; + e.getMessage() + \u0026#34; )\u0026#34;); } executorService.shutdown(); } Output Caught the exception ( java.lang.RuntimeException: Uncaught exception in callable task ) 2. Handle with UncaughtExceptionHandler Register the threads spawned from ThreadFactory with UncaughtExceptionHandler. This handler will get called everytime a registered thread come across an uncaught exceptions. Now use this ThreadFactory to initialize ExecutorService\npublic static class AppExceptionHandler implements Thread.UncaughtExceptionHandler { @Override public void uncaughtException(Thread t, Throwable e) { System.out.println(\u0026#34;Caught the exception ( \u0026#34; + e.getMessage() + \u0026#34; ) thrown by thread [\u0026#34; + t.getName() + \u0026#34;]\u0026#34;); } } public static class AppThreadFactory implements ThreadFactory { @Override public Thread newThread(Runnable r) { final Thread thread = new Thread(r); thread.setUncaughtExceptionHandler(new AppExceptionHandler()); return thread; } } public void executeThenThrowUnchecked() { final ExecutorService executorService = Executors.newFixedThreadPool(1, new AppThreadFactory()); executorService.execute(() -\u0026gt; { throw new RuntimeException(\u0026#34;Uncaught exception in runnable task\u0026#34;); }); executorService.shutdown(); } Output Caught the exception ( Uncaught exception in runnable task ) thrown by thread [Thread-0] 3. Handle with overriding afterExecute in ThreadPoolExecutor Create custom ThreadPoolExecutor and override afterExecute method to check for exceptions. Now use this ThreadPoolExecutor to initialize ExecutorService\npublic static class MonitoringThreadPoolExecutor extends ThreadPoolExecutor { public MonitoringThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue) { super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue); } @Override protected void afterExecute(Runnable r, Throwable t) { super.afterExecute(r, t); if(t != null){ System.out.println(\u0026#34;Caught the exception ( \u0026#34; + t.getMessage() + \u0026#34; )\u0026#34;); } } } public void executeWithCustomThreadPoolExecutorThenThrowUnchecked() { final ExecutorService executorService = new MonitoringThreadPoolExecutor(1, 1, 0, TimeUnit.SECONDS, new LinkedBlockingQueue\u0026lt;\u0026gt;()); executorService.execute(() -\u0026gt; { throw new RuntimeException(\u0026#34;Uncaught exception in runnable task\u0026#34;); }); executorService.shutdown(); } Output Caught the exception ( Uncaught exception in runnable task ) Exception in thread \u0026#34;pool-1-thread-1\u0026#34; java.lang.RuntimeException: Uncaught exception in runnable task at com.example.thread.ExceptionHandlingInThread.lambda$executeWithCustomThreadPoolExecutorThenThrowUnchecked$2(ExceptionHandlingInThread.java:76) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at java.base/java.lang.Thread.run(Thread.java:834) ","permalink":"https://codingnconcepts.com/java-multithreading-interview-questions/","tags":["Interview Q\u0026A","Java Q\u0026A"],"title":"Java Multithreading (Concurrency) Interview Questions"},{"categories":["Java"],"contents":"This problem is frequently asked in the interview to check your understanding on threads and your programming skills. Problem statement might change for e.g.\nPrint ping pong using two threads. Print alternate number using two threads. Print even numbers by one thread and odd numbers by another thread. Using Wait Notify We can print the \u0026ldquo;Ping Pong\u0026rdquo; alternatively using Wait Notify. This is what happen behind the scene:-\n1st thread print \u0026ldquo;Ping\u0026rdquo; and go to wait state. 2nd thread wakes up from wait state, print \u0026ldquo;Ping\u0026rdquo;, notify 1st thread, goes back to wait state. 1st thread wakes up from wait state, print \u0026ldquo;Pong\u0026rdquo;, notify 2nd thread, goes back to wait state. Step 2 and 3 repeats and print \u0026ldquo;Ping Pong\u0026rdquo; alternatively. package com.abc; public class PingPong { public static void main(String[] args) { Object LOCK_OBJECT = new Object(); Thread ping = new Thread(new PingPongThread(LOCK_OBJECT, \u0026#34;Ping\u0026#34;)); Thread pong = new Thread(new PingPongThread(LOCK_OBJECT, \u0026#34;Pong\u0026#34;)); ping.start(); pong.start(); } } class PingPongThread implements Runnable{ private Object LOCK_OBJECT; private String name; public PingPongThread(Object LOCK_OBJECT, String name) { this.LOCK_OBJECT = LOCK_OBJECT; this.name = name; } @Override public void run() { synchronized (LOCK_OBJECT) { while(true) { System.out.println(name); try { Thread.sleep(1000); } catch (InterruptedException e1) { e1.printStackTrace(); } LOCK_OBJECT.notify(); try { LOCK_OBJECT.wait(1000); } catch (InterruptedException e) { e.printStackTrace(); } } } } } Output Ping Pong Ping Pong ... ... Things to note in above program:\nwait() and notify() are Object\u0026rsquo;s method so you need to create a LOCK_OBJECT to call these methods. You pass this LOCK_OBJECT from constructor. Both threads try to acquire a lock on LOCK_OBJECT, when one thread is having lock on this object, other thread can not access the resources. wait() and notify() methods can only be called within the Synchronized Block. wait() and notify() methods are called inside while(true) loop to keep running the thread. wait() and sleep() method throw InterruptedException exception. The sleep() method is optional. It is used to add a delay of 1 sec in printing \u0026ldquo;Ping\u0026rdquo; and \u0026ldquo;Pong\u0026rdquo; in order to verify the sequence in console. Using ReentrantLock Condition ReentrantLock locks are new in Java, which provide Condition having two methods await() and signal(). These two methods works very similar to Object\u0026rsquo;s wait() and notify() methods with more flexibility.\nIn this program we\u0026rsquo;ll print \u0026ldquo;Ping\u0026rdquo; and \u0026ldquo;Pong\u0026rdquo; alternatively exactly 5 number of times each and stop the threads.\npackage com.example.thread; import java.util.concurrent.TimeUnit; import java.util.concurrent.locks.Condition; import java.util.concurrent.locks.ReentrantLock; public class PingPongUsingReentrantCondition { private static ReentrantLock lock = new ReentrantLock(true); private Condition conditionMet = lock.newCondition(); public static void main(String[] args) { PingPongUsingReentrantCondition pingPong = new PingPongUsingReentrantCondition(); Thread t1 = new Thread(() -\u0026gt; pingPong.pingpong(\u0026#34;Ping\u0026#34;, 5)); Thread t2 = new Thread(() -\u0026gt; pingPong.pingpong(\u0026#34;Pong\u0026#34;, 5)); t1.start(); t2.start(); } public void pingpong(String s, int times) { int counter = 1; while(counter\u0026lt;=times) {\trun(s, counter); counter = counter+1; } } public void run(String s, int counter) { lock.lock(); try { conditionMet.await(2, TimeUnit.SECONDS); System.out.println(s + \u0026#34; (\u0026#34; + counter + \u0026#34;)\u0026#34;); conditionMet.signal(); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } } Output Ping (1) Pong (1) Ping (2) Pong (2) Ping (3) Pong (3) Ping (4) Pong (4) Ping (5) Pong (5) ","permalink":"https://codingnconcepts.com/java/ping-pong-using-threads-in-java/","tags":["Core Java","Java Threads"],"title":"Ping Pong using Threads in Java"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn different ways of adding new elements at the beginning of an Array in JavaScript.\nUsing Array.unshift() The easiest way to add elements at the beginning of an array is to use unshift() method.\nvar fruits = [\u0026#34;Apple\u0026#34;, \u0026#34;Banana\u0026#34;, \u0026#34;Mango\u0026#34;]; fruits.unshift(\u0026#34;Orange\u0026#34;); console.log(fruits); // Prints [\u0026#34;Orange\u0026#34;, \u0026#34;Apple\u0026#34;, \u0026#34;Banana\u0026#34;, \u0026#34;Mango\u0026#34;] fruits.unshift(\u0026#34;Guava\u0026#34;, \u0026#34;Papaya\u0026#34;); console.log(fruits); // Prints [\u0026#34;Guava\u0026#34;, \u0026#34;Papaya\u0026#34;, \u0026#34;Orange\u0026#34;, \u0026#34;Apple\u0026#34;, \u0026#34;Banana\u0026#34;, \u0026#34;Mango\u0026#34;] It adds the elements of your original array. Sometime you wish not to alter the original array, and assign it to a new variable instead.\nIn Javascript, you can do this in multiple ways in a single statement.\nUsing Spread Operator (\u0026hellip;) We can use spread operator ... to make a copy of an array. It’s short syntax is very handy.\nvar fruits = [\u0026#34;Apple\u0026#34;, \u0026#34;Banana\u0026#34;, \u0026#34;Mango\u0026#34;]; var moreFruits = [\u0026#34;Orange\u0026#34;, ...fruits]; console.log(moreFruits); // Prints [\u0026#34;Orange\u0026#34;, \u0026#34;Apple\u0026#34;, \u0026#34;Banana\u0026#34;, \u0026#34;Mango\u0026#34;] var someoMoreFruits = [\u0026#34;Guava\u0026#34;, \u0026#34;Papaya\u0026#34;, ...moreFruits]; console.log(someoMoreFruits); // Prints [\u0026#34;Guava\u0026#34;, \u0026#34;Papaya\u0026#34;, \u0026#34;Orange\u0026#34;, \u0026#34;Apple\u0026#34;, \u0026#34;Banana\u0026#34;, \u0026#34;Mango\u0026#34;] console.log(fruits); // Prints [\u0026#34;Apple\u0026#34;, \u0026#34;Banana\u0026#34;, \u0026#34;Mango\u0026#34;] New elements are added at the beginning followed by copy of the original Array (using ...) and assigned to a new variable.\nWe see that our original array remains the same.\nUsing Array.concat() We can also user concat() method to join two (or more) arrays at the beginning.\nvar fruits = [\u0026#34;Apple\u0026#34;, \u0026#34;Banana\u0026#34;, \u0026#34;Mango\u0026#34;]; var moreFruits = [\u0026#34;Orange\u0026#34;]; var someoMoreFruits = [\u0026#34;Guava\u0026#34;, \u0026#34;Papaya\u0026#34;]; var allFruits = someoMoreFruits.concat(moreFruits, fruits); console.log(allFruits); // Prints [\u0026#34;Guava\u0026#34;, \u0026#34;Papaya\u0026#34;, \u0026#34;Orange\u0026#34;, \u0026#34;Apple\u0026#34;, \u0026#34;Banana\u0026#34;, \u0026#34;Mango\u0026#34;] ","permalink":"https://codingnconcepts.com/javascript/how-to-add-element-at-beggining-of-javascript-array/","tags":["Javascript Array"],"title":"How to Add Element at Beginning of an Array in JavaScript"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn different ways to initialize List, ArrayList and LinkedList with values in single line in Java.\nJava 8 or earlier Initialize Immutable List This is the simplest way to initialize a List:-\n/** * Immutable list with inline elements */ List\u0026lt;String\u0026gt; list = Arrays.asList(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;baz\u0026#34;); /** * Immutable list with array */ String[] names = { \u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34; }; List\u0026lt;String\u0026gt; anotherList = Arrays.asList(names); anotherList.add(\u0026#34;baz\u0026#34;) // Throw UnsupportedOperationException exception The only drawback is that the initalized list is immutable. That means adding or removing elements in the list throw java.lang.UnsupportedOperationException exception.\nIt is useful when you just need it for iteration and read-only purpose.\nInitialize Mutable List If you want to intialize a mutable list where you can add or remove elements. You wrap immutable list with ArrayList or LinkedList:-\nList\u0026lt;String\u0026gt; arrayList = new ArrayList\u0026lt;\u0026gt;(Arrays.asList(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;)); arrayList.add(\u0026#34;baz\u0026#34;); // It works! List\u0026lt;String\u0026gt; linkedList = new LinkedList\u0026lt;\u0026gt;(Arrays.asList(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;)); linkedList.remove(\u0026#34;foo\u0026#34;); // It works! Collections.addAll() You can also use Collections.addAll() static method to add values to ArrayList or LinkedList\nList\u0026lt;String\u0026gt; arrayList = new ArrayList\u0026lt;String\u0026gt;(); Collections.addAll(arrayList, \u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;baz\u0026#34;); Double Brace Initialization Another way is to making an anonymous inner class with an instance initializer. This is also known as an double brace initialization. However that looks like an overkill to create a inner class just to create an ArrayList or LinkedList\nList\u0026lt;String\u0026gt; strings = new ArrayList\u0026lt;String\u0026gt;() { { add(\u0026#34;A\u0026#34;); add(\u0026#34;B\u0026#34;); add(\u0026#34;C\u0026#34;); } }; Java 9 or later Initialize Immutable List List.of() is added in Java 9 which can be used to initialize an immutable list with values.\nList\u0026lt;String\u0026gt; list = List.of(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;); list.add(\u0026#34;baz\u0026#34;) // Throw UnsupportedOperationException exception With Java 10 or later, this can be even more shortened with the var keyword.\nvar list = List.of(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;baz\u0026#34;); Initialize Mutable List You can also define mutable list with ArrayList or LinkedList wrapper:-\nList\u0026lt;String\u0026gt; arrayList = new ArrayList\u0026lt;\u0026gt;(List.of(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;)); arrayList.add(\u0026#34;baz\u0026#34;); // It works! List\u0026lt;String\u0026gt; linkedList = new LinkedList\u0026lt;\u0026gt;(List.of(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;)); linkedList.remove(\u0026#34;foo\u0026#34;); // It works! Using Streams You can also use Stream API which is more flexible:-\nStream\u0026lt;String\u0026gt; strings = Stream.of(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;baz\u0026#34;); You can combine two or more Streams like this:-\nStream\u0026lt;String\u0026gt; strings = Stream.concat(Stream.of(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;), Stream.of(\u0026#34;baz\u0026#34;, \u0026#34;qux\u0026#34;)); You can transform a Stream into mutable List or ArrayList like this:-\nList\u0026lt;String\u0026gt; list = Stream.of(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;baz\u0026#34;).collect(Collectors.toList()); list.remove(\u0026#34;baz\u0026#34;); // It works! List\u0026lt;String\u0026gt; arrayList = Stream.of(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;).collect(Collectors.toCollection(ArrayList::new)); arrayList.add(\u0026#34;baz\u0026#34;); // It works! ","permalink":"https://codingnconcepts.com/java/initialize-list-with-values-in-java/","tags":["Java Collection"],"title":"Initialize List with Values in Java"},{"categories":["Java"],"contents":"Problem: Given a Roman numeral, convert it to an Integer.\nSource Code: RomanToInteger.java\nAlso read Convert Integer to Roman in Java\nAbout Roman Numerals A Quick look at roman numerals:-\nI = 1 II = 2 III = 3 IV = 4 V = 5 VI = 6 VII = 7 VIII = 8 IX = 9 X = 10 ... ... L = 50 C = 100 D = 500 M = 1000 Things to note about roman numerals:\nWhen a symbol appears after a larger (or equal) symbol it is added\nExample: VI = V + I = 5 + 1 = 6\nExample: LXX = L + X + X = 50 + 10 + 10 = 70 But if the symbol appears before a larger symbol it is subtracted\nExample: IV = V − I = 5 − 1 = 4\nExample: IX = X − I = 10 − 1 = 9 If I appears just before V (5) and X (10), makes 4 and 9. If X appears just before L (50) and C (100), makes 40 and 90. If C appears just before D (500) and M (1000), makes 400 and 900. Approach Naive Approach In this approach we\u0026rsquo;ll start checking each roman symbol from left to right keeping in mind the things about roman numerals as stated above.\n// Approach 1 public static final int romanToInteger(String roman) { int number = 0; for (int i = 0; i \u0026lt; roman.length(); i++) { char c = roman.charAt(i); switch (c) { case \u0026#39;I\u0026#39;: number = (i != roman.length() - 1 \u0026amp;\u0026amp; (roman.charAt(i + 1) == \u0026#39;V\u0026#39; || roman.charAt(i + 1) == \u0026#39;X\u0026#39;)) ? number - 1 : number + 1; break; case \u0026#39;V\u0026#39;: number += 5; break; case \u0026#39;X\u0026#39;: number = (i != roman.length() - 1 \u0026amp;\u0026amp; (roman.charAt(i + 1) == \u0026#39;L\u0026#39; || roman.charAt(i + 1) == \u0026#39;C\u0026#39;)) ? number - 10 : number + 10; break; case \u0026#39;L\u0026#39;: number += 50; break; case \u0026#39;C\u0026#39;: number = (i != roman.length() - 1 \u0026amp;\u0026amp; (roman.charAt(i + 1) == \u0026#39;D\u0026#39; || roman.charAt(i + 1) == \u0026#39;M\u0026#39;)) ? number - 100 : number + 100; break; case \u0026#39;D\u0026#39;: number += 500; break; case \u0026#39;M\u0026#39;: number += 1000; break; } } return number; } Improvement using Map We can further enhance our code to remove the switch statement in our code by saving roman numerals and their number values in Map.\n// Approach 2 public static final int romanToInteger(String s) { Map\u0026lt;Character, Integer\u0026gt; values = new LinkedHashMap\u0026lt;\u0026gt;(); values.put(\u0026#39;I\u0026#39;, 1); values.put(\u0026#39;V\u0026#39;, 5); values.put(\u0026#39;X\u0026#39;, 10); values.put(\u0026#39;L\u0026#39;, 50); values.put(\u0026#39;C\u0026#39;, 100); values.put(\u0026#39;D\u0026#39;, 500); values.put(\u0026#39;M\u0026#39;, 1000); int number = 0; for (int i = 0; i \u0026lt; s.length(); i++) { if (i+1 == s.length() || values.get(s.charAt(i)) \u0026gt;= values.get(s.charAt(i + 1))) { number += values.get(s.charAt(i)); } else { number -= values.get(s.charAt(i)); } } return number; } Dynamic (Recursive) Approach using Map Though this is not an appropriate problem to apply dynamic programming. Still Let\u0026rsquo;s try our hands on how we can solve the same problem using recursive approach.\nThe changes we\u0026rsquo;ve done to make it recursive:-\nBrought the Map outside of method Created a private method getNumber() which gives us the number with respect to roman symbol. Added base condition index == s.length() in method to stop recursion // Approach 3 private static Map\u0026lt;Character, Integer\u0026gt; map = new LinkedHashMap\u0026lt;\u0026gt;(); static { map.put(\u0026#39;I\u0026#39;, 1); map.put(\u0026#39;V\u0026#39;, 5); map.put(\u0026#39;X\u0026#39;, 10); map.put(\u0026#39;L\u0026#39;, 50); map.put(\u0026#39;C\u0026#39;, 100); map.put(\u0026#39;D\u0026#39;, 500); map.put(\u0026#39;M\u0026#39;, 1000); } public static final int romanToInteger(String s) { return romanToInteger(s, 0); } private static final int romanToInteger(String s, int index) { if(index == s.length()) { return 0; } return getNumber(s, index) + romanToInteger(s.substring(index+1, s.length())); } private static final int getNumber(String s, int index) { if(index+1 == s.length()) { return map.get(s.charAt(index)); } if (map.get(s.charAt(index)) \u0026gt;= map.get(s.charAt(index+1))) { return map.get(s.charAt(index)); } else { return -map.get(s.charAt(index)); }\t} ","permalink":"https://codingnconcepts.com/java/roman-to-integer/","tags":["Numbers","Java Algorithm"],"title":"Convert Roman to Integer in Java"},{"categories":["Java"],"contents":"Problem: Given an Integer input, convert it to a Roman numeral. Input is within the range from 1 to 3999.\nSource Code: IntegerToRoman.java\nAlso read Convert Roman to Integer in Java\nAbout Roman Numerals Roman numerals are represented by seven different symbols: I, V, X, L, C, D and M.\nSymbol Value I 1 V 5 X 10 L 50 C 100 D 500 M 1000 These symbols can be combined like this to create numbers:\n1 2 3 4 5 6 7 8 9 I II III IV V VI VII VIII IX 10 20 30 40 50 60 70 80 90 X XX XXX XL L LX LXX LXXX XC 100 200 300 400 500 600 700 800 900 C CC CCC CD D DC DCC DCCC CM Things to note in roman numbers:\nWhen a symbol appears after a larger (or equal) symbol it is added\nExample: VI = V + I = 5 + 1 = 6\nExample: LXX = L + X + X = 50 + 10 + 10 = 70 But if the symbol appears before a larger symbol it is subtracted\nExample: IV = V − I = 5 − 1 = 4\nExample: IX = X − I = 10 − 1 = 9 I can be placed before V (5) and X (10) to make 4 and 9. X can be placed before L (50) and C (100) to make 40 and 90. C can be placed before D (500) and M (1000) to make 400 and 900. Approach Naive Approach In this approach we\u0026rsquo;ll start checking with highest possible number which can be converted into Roman numerals i.e. 1000 = M. If possible to convert then:\nWe will append that roman numeral to the new initialized string We will subtract the highest number from the given number We\u0026rsquo;ll follow this checking approach till the smallest possible number which can be converted into Roman numerals i.e 1 = I\npublic static String integerToRoman1(int number) { StringBuilder s = new StringBuilder(); while (number \u0026gt;= 1000) { s.append(\u0026#34;M\u0026#34;); number -= 1000; } while (number \u0026gt;= 900) { s.append(\u0026#34;CM\u0026#34;); number -= 900; } while (number \u0026gt;= 500) { s.append(\u0026#34;D\u0026#34;); number -= 500; } while (number \u0026gt;= 400) { s.append(\u0026#34;CD\u0026#34;); number -= 400; } while (number \u0026gt;= 100) { s.append(\u0026#34;C\u0026#34;); number -= 100; } while (number \u0026gt;= 90) { s.append(\u0026#34;XC\u0026#34;); number -= 90; } while (number \u0026gt;= 50) { s.append(\u0026#34;L\u0026#34;); number -= 50; } while (number \u0026gt;= 40) { s.append(\u0026#34;XL\u0026#34;); number -= 40; } while (number \u0026gt;= 10) { s.append(\u0026#34;X\u0026#34;); number -= 10; } while (number \u0026gt;= 9) { s.append(\u0026#34;IX\u0026#34;); number -= 9; } while (number \u0026gt;= 5) { s.append(\u0026#34;V\u0026#34;); number -= 5; } while (number \u0026gt;= 4) { s.append(\u0026#34;IV\u0026#34;); number -= 4; } while (number \u0026gt;= 1) { s.append(\u0026#34;I\u0026#34;); number -= 1; } return s.toString(); } Improvement using Arrays We can further enhance our code to remove the repetition of while code. We\u0026rsquo;ll save largest to smallest numbers and their roman numerals in Arrays [].\n// Approach 2 private static final int[] values = { 1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1 }; private static final String[] romanLiterals = { \u0026#34;M\u0026#34;, \u0026#34;CM\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;CD\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;XC\u0026#34;, \u0026#34;L\u0026#34;, \u0026#34;XL\u0026#34;, \u0026#34;X\u0026#34;, \u0026#34;IX\u0026#34;, \u0026#34;V\u0026#34;, \u0026#34;IV\u0026#34;, \u0026#34;I\u0026#34; }; public static final String integerToRoman2(int number) { StringBuilder s = new StringBuilder(); for (int i = 0; i \u0026lt; values.length; i++) { while (number \u0026gt;= values[i]) { number -= values[i]; s.append(romanLiterals[i]); } } return s.toString(); } Dynamic (Recursive) Approach using Arrays Let\u0026rsquo;s try our hands on dynamic programming and solve the same problem using recursive approach.\nNote that we have created a private method getFloorIndex which gives us the index of greatest value less than or equal to the given number.\n// Approach 3 private static final int[] values = { 1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1 }; private static final String[] romanLiterals = { \u0026#34;M\u0026#34;, \u0026#34;CM\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;CD\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;XC\u0026#34;, \u0026#34;L\u0026#34;, \u0026#34;XL\u0026#34;, \u0026#34;X\u0026#34;, \u0026#34;IX\u0026#34;, \u0026#34;V\u0026#34;, \u0026#34;IV\u0026#34;, \u0026#34;I\u0026#34; }; public static final String integerToRoman3(int number) { int i = getFloorIndex(number); if (number == values[i]) { return romanLiterals[i]; } return romanLiterals[i] + integerToRoman3(number - values[i]); } private static int getFloorIndex(int number) { for (int i = 0; i \u0026lt; values.length; i++) { while (number \u0026gt;= values[i]) { return i; } } return -1; } Dynamic (Recursive) Approach using TreeMap Let\u0026rsquo;s make our code more concise by saving largest to smallest number and their roman numerals in TreeMap instead of Arrays [].\nNote that TreeMap has a built-in floorKey method which gives the greatest key less than or equal to the given number.\n// Approach 4 private static final TreeMap\u0026lt;Integer, String\u0026gt; treemap = new TreeMap\u0026lt;Integer, String\u0026gt;(); static { treemap.put(1000, \u0026#34;M\u0026#34;); treemap.put(900, \u0026#34;CM\u0026#34;); treemap.put(500, \u0026#34;D\u0026#34;); treemap.put(400, \u0026#34;CD\u0026#34;); treemap.put(100, \u0026#34;C\u0026#34;); treemap.put(90, \u0026#34;XC\u0026#34;); treemap.put(50, \u0026#34;L\u0026#34;); treemap.put(40, \u0026#34;XL\u0026#34;); treemap.put(10, \u0026#34;X\u0026#34;); treemap.put(9, \u0026#34;IX\u0026#34;); treemap.put(5, \u0026#34;V\u0026#34;); treemap.put(4, \u0026#34;IV\u0026#34;); treemap.put(1, \u0026#34;I\u0026#34;); } public static final String integerToRoman5(int number) { int l = treemap.floorKey(number); if (number == l) { return treemap.get(number); } return treemap.get(l) + integerToRoman5(number - l); } ","permalink":"https://codingnconcepts.com/java/integer-to-roman/","tags":["Java Algorithm","Numbers"],"title":"Convert Integer to Roman in Java"},{"categories":["Tools"],"contents":" Decimal Number e.g. 11 Roman Numeral e.g. XI Note: You can also input Roman Numeral to convert to Number\nAbout Roman Numerals Roman numerals are represented by seven different symbols: I, V, X, L, C, D and M.\nSymbol Value I 1 V 5 X 10 L 50 C 100 D 500 M 1000 These symbols can be combined like this to create numbers:\n1 2 3 4 5 6 7 8 9 I II III IV V VI VII VIII IX 10 20 30 40 50 60 70 80 90 X XX XXX XL L LX LXX LXXX XC 100 200 300 400 500 600 700 800 900 C CC CCC CD D DC DCC DCCC CM Things to note about roman numerals:\nWhen a symbol appears after a larger (or equal) symbol it is added\nExample: VI = V + I = 5 + 1 = 6\nExample: LXX = L + X + X = 50 + 10 + 10 = 70 But if the symbol appears before a larger symbol it is subtracted\nExample: IV = V − I = 5 − 1 = 4\nExample: IX = X − I = 10 − 1 = 9 I can be placed before V (5) and X (10) to make 4 and 9. X can be placed before L (50) and C (100) to make 40 and 90. C can be placed before D (500) and M (1000) to make 400 and 900. Romanize Number Javascript method used in this converter to convert from Decimal Number to Roman:-\nconst romanize = (number) =\u0026gt; { const lookup = {M:1000,CM:900,D:500,CD:400,C:100,XC:90,L:50,XL:40,X:10,IX:9,V:5,IV:4,I:1}; let roman = \u0026#39;\u0026#39;; for (let i in lookup ) { while ( number \u0026gt;= lookup[i] ) { roman += i; number -= lookup[i]; } } return roman; } Roman to Decimal Number Javascript method used in this converter to convert from Roman to Decimal Number:-\nconst romanToInteger = (roman) =\u0026gt; { const lookup = {M:1000,CM:900,D:500,CD:400,C:100,XC:90,L:50,XL:40,X:10,IX:9,V:5,IV:4,I:1}; const array = roman.split(\u0026#39;\u0026#39;); let total = 0, current, currentValue, next, nextValue; for(let i=0; i\u0026lt;array.length; i++){ current = array[i]; currentValue = lookup[current]; next = array[i+1]; nextValue = lookup[next]; if(currentValue \u0026lt; nextValue) { total -= currentValue; }else { total += currentValue; } } return total; } ","permalink":"https://codingnconcepts.com/tools/number-to-roman/","tags":["Roman","Converter"],"title":"Number to Roman Converter"},{"categories":null,"contents":"माँ, क्या लिखुँ तुम्हारे बारे में?\nमेरे शब्दकोष का पहला शब्द हो तुम,\nमेरे रेंगने से पैरों पे खड़े होने का सफर हो तुम।\nरात भर जाग जाग कर सुलाने वाली तुम,\nथोड़ी भी चोट लग जाये तो सहलाने वाली तुम।\nमाँ, क्या लिखुँ तुम्हारे बारे में?\nकितना भी खा पी के सेहत बना लूँ,\nबेटा कमजोर हो गया है, बोल के थोड़ा और खिलाती है।\nनौकरी की मज़बूरी में जो दूर हो गया बहोत,\nतेरे हाथ के खाने की याद बहोत आती है।\nमाँ, क्या लिखुँ तुम्हारे बारे में?\nतू सब जानती है, मुझे खुद से ज्यादा पहचानती है,\nलाख जतन कर लूँ कुछ छिपाने की, तू सब भांप जाती है।\nउलझनें कितनी भी हो जिंदगी में,\nतेरी गोद में सर रखते ही सब छू मंतर हो जाती है।\n~~ आशीष लाहोटी ( १५ सितम्बर २०२० माँ का जन्मदिन ) ~~\n","permalink":"https://codingnconcepts.com/poetry/maa/","tags":null,"title":"माँ, क्या लिखुँ तुम्हारे बारे में?"},{"categories":["Javascript"],"contents":"In this quick tutorial, we\u0026rsquo;ll learn how to combine two or more JSON object into one object in JavaScript.\nObject.assign() The Object.assign is the most straightforward approach to add two or more objects.\nvar obj1 = { eat: \u0026#39;pizza\u0026#39;, drink: \u0026#39;coke\u0026#39; }; var obj2 = { drive: \u0026#39;car\u0026#39;, ride: \u0026#39;bus\u0026#39; } var obj3 = { pet: \u0026#39;dog\u0026#39; } var obj4 = Object.assign({}, obj1, obj2, obj3); console.log(obj4); // {eat: \u0026#34;pizza\u0026#34;, drink: \u0026#34;coke\u0026#34;, drive: \u0026#34;car\u0026#34;, ride: \u0026#34;bus\u0026#34;, pet: \u0026#34;dog\u0026#34;} Spread Operator (\u0026hellip;) If you have started using ES6 then it is recommended to use spread operator (\u0026hellip;) which is more handy to combine two or more objects.\nvar obj1 = { eat: \u0026#39;pizza\u0026#39;, drink: \u0026#39;coke\u0026#39; }; var obj2 = { drive: \u0026#39;car\u0026#39;, ride: \u0026#39;bus\u0026#39; } var obj3 = { pet: \u0026#39;dog\u0026#39; } var obj4 = { ...obj1, ...obj2, ...obj3 }; console.log(obj4); // {eat: \u0026#34;pizza\u0026#34;, drink: \u0026#34;coke\u0026#34;, drive: \u0026#34;car\u0026#34;, ride: \u0026#34;bus\u0026#34;, pet: \u0026#34;dog\u0026#34;} ","permalink":"https://codingnconcepts.com/javascript/combine-json-objects-in-javascript/","tags":["Javascript Object"],"title":"Combine Two or more JSON Objects in JavaScript"},{"categories":["Hugo"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to disable, rename, or customize RSS XML Feed for Hugo website.\nOverview When you generate a website using Hugo static site generator, Hugo out of the box generate RSS XML based on the RSS 2.0 Template\nThe RSS XML index.xml is auto generated for -\nhome for whole website e.g. https://example.com/index.xml section for each section e.g. https://example.com/\u0026lt;SECTION\u0026gt;/index.html taxonomyTerm for each taxonomy term e.g. https://example.com/categories/index.html taxonomy for each taxonomy e.g. https://example.com/categories/\u0026lt;CATEGORY\u0026gt;/index.html Following is a sample RSS XML index.xml for whole website:-\nindex.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; standalone=\u0026#34;yes\u0026#34;?\u0026gt; \u0026lt;rss version=\u0026#34;2.0\u0026#34; xmlns:atom=\u0026#34;http://www.w3.org/2005/Atom\u0026#34;\u0026gt; \u0026lt;channel\u0026gt; \u0026lt;title\u0026gt;Coding N Concepts\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;https://codingnconcepts.com/\u0026lt;/link\u0026gt; \u0026lt;description\u0026gt;Recent content on Coding N Concepts\u0026lt;/description\u0026gt; \u0026lt;generator\u0026gt;Hugo -- gohugo.io\u0026lt;/generator\u0026gt; \u0026lt;language\u0026gt;en-us\u0026lt;/language\u0026gt; \u0026lt;lastBuildDate\u0026gt;Fri, 04 Sep 2020 00:00:00 +0000\u0026lt;/lastBuildDate\u0026gt;\u0026lt;atom:link href=\u0026#34;https://codingnconcepts.com/index.xml\u0026#34; rel=\u0026#34;self\u0026#34; type=\u0026#34;application/rss+xml\u0026#34; /\u0026gt; \u0026lt;item\u0026gt; \u0026lt;title\u0026gt;Customize RSS feed in Hugo Website\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;https://codingnconcepts.com/hugo/custom-rss-feed-hugo/\u0026lt;/link\u0026gt; \u0026lt;pubDate\u0026gt;Fri, 04 Sep 2020 00:00:00 +0000\u0026lt;/pubDate\u0026gt; \u0026lt;guid\u0026gt;https://codingnconcepts.com/hugo/custom-rss-feed-hugo/\u0026lt;/guid\u0026gt; \u0026lt;description\u0026gt;\u0026amp;lt;p\u0026amp;gt;In this tutorial, we\u0026amp;amp;rsquo;ll learn how to disable or customize RSS Feed for Hugo website.\u0026amp;lt;/p\u0026amp;gt;\u0026lt;/description\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;item\u0026gt; ... \u0026lt;/item\u0026gt; ... \u0026lt;/channel\u0026gt; \u0026lt;/rss\u0026gt; Disable RSS Feed There are two ways to disable generation of RSS Xml :-\nDisable using config.toml This approach is recommended, when you want to disable RSS permanently everytime you build your website.\nconfig.toml (default) First of all we\u0026rsquo;ll see what is the default RSS configuration in config.toml\n[outputs] page = [\u0026#34;HTML\u0026#34;] home = [\u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;] section = [\u0026#34;HTML\u0026#34;,\u0026#34;RSS\u0026#34;] taxonomyTerm = [\u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;] taxonomy = [\u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;] We see from above configuration that RSS pages are generated for home, section, taxonomyTerm and taxonomy out of the box.\nconfig.toml (RSS disabled) You can disable the generation of RSS Feed by configuration change in config.toml\n[outputs] page = [\u0026#34;HTML\u0026#34;] home = [\u0026#34;HTML\u0026#34;] section = [\u0026#34;HTML\u0026#34;] taxonomyTerm = [\u0026#34;HTML\u0026#34;] taxonomy = [\u0026#34;HTML\u0026#34;] Disable using command-line This approach is recommended, when you want to disable RSS for a specific build of your website.\nTo generate a hugo website without RSS, execute following command from terminal:\nhugo --disableKinds=RSS To serve website in the localhost environment without a sitemap, execute following command from terminal:\nhugo server --disableKinds=RSS Rename RSS Feed Hugo generates the RSS feed in index.xml. If you want to change the name from index.xml to say rss.xml then add following configuration in config.toml\nconfig.toml [outputFormats] [outputFormats.RSS] mediatype = \u0026#34;application/rss\u0026#34; baseName = \u0026#34;rss\u0026#34; Customize RSS Feed Hugo has a built-in RSS template, but if you want to customize RSS rss.xml then first of all copy and paste below hugo\u0026rsquo;s default RSS template in layouts/_default/rss.xml location:-\n/layouts/_default/rss.xml {{- $pctx := . -}} {{- if .IsHome -}}{{ $pctx = .Site }}{{- end -}} {{- $pages := slice -}} {{- if or $.IsHome $.IsSection -}} {{- $pages = $pctx.RegularPages -}} {{- else -}} {{- $pages = $pctx.Pages -}} {{- end -}} {{- $limit := .Site.Config.Services.RSS.Limit -}} {{- if ge $limit 1 -}} {{- $pages = $pages | first $limit -}} {{- end -}} {{- printf \u0026#34;\u0026lt;?xml version=\\\u0026#34;1.0\\\u0026#34; encoding=\\\u0026#34;utf-8\\\u0026#34; standalone=\\\u0026#34;yes\\\u0026#34;?\u0026gt;\u0026#34; | safeHTML }} \u0026lt;rss version=\u0026#34;2.0\u0026#34; xmlns:atom=\u0026#34;http://www.w3.org/2005/Atom\u0026#34;\u0026gt; \u0026lt;channel\u0026gt; \u0026lt;title\u0026gt;{{ if eq .Title .Site.Title }}{{ .Site.Title }}{{ else }}{{ with .Title }}{{.}} on {{ end }}{{ .Site.Title }}{{ end }}\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;{{ .Permalink }}\u0026lt;/link\u0026gt; \u0026lt;description\u0026gt;Recent content {{ if ne .Title .Site.Title }}{{ with .Title }}in {{.}} {{ end }}{{ end }}on {{ .Site.Title }}\u0026lt;/description\u0026gt; \u0026lt;generator\u0026gt;Hugo -- gohugo.io\u0026lt;/generator\u0026gt;{{ with .Site.LanguageCode }} \u0026lt;language\u0026gt;{{.}}\u0026lt;/language\u0026gt;{{end}}{{ with .Site.Author.email }} \u0026lt;managingEditor\u0026gt;{{.}}{{ with $.Site.Author.name }} ({{.}}){{end}}\u0026lt;/managingEditor\u0026gt;{{end}}{{ with .Site.Author.email }} \u0026lt;webMaster\u0026gt;{{.}}{{ with $.Site.Author.name }} ({{.}}){{end}}\u0026lt;/webMaster\u0026gt;{{end}}{{ with .Site.Copyright }} \u0026lt;copyright\u0026gt;{{.}}\u0026lt;/copyright\u0026gt;{{end}}{{ if not .Date.IsZero }} \u0026lt;lastBuildDate\u0026gt;{{ .Date.Format \u0026#34;Mon, 02 Jan 2006 15:04:05 -0700\u0026#34; | safeHTML }}\u0026lt;/lastBuildDate\u0026gt;{{ end }} {{- with .OutputFormats.Get \u0026#34;RSS\u0026#34; -}} {{ printf \u0026#34;\u0026lt;atom:link href=%q rel=\\\u0026#34;self\\\u0026#34; type=%q /\u0026gt;\u0026#34; .Permalink .MediaType | safeHTML }} {{- end -}} {{ range $pages }} \u0026lt;item\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;{{ .Permalink }}\u0026lt;/link\u0026gt; \u0026lt;pubDate\u0026gt;{{ .Date.Format \u0026#34;Mon, 02 Jan 2006 15:04:05 -0700\u0026#34; | safeHTML }}\u0026lt;/pubDate\u0026gt; {{ with .Site.Author.email }}\u0026lt;author\u0026gt;{{.}}{{ with $.Site.Author.name }} ({{.}}){{end}}\u0026lt;/author\u0026gt;{{end}} \u0026lt;guid\u0026gt;{{ .Permalink }}\u0026lt;/guid\u0026gt; \u0026lt;description\u0026gt;{{ .Summary | html }}\u0026lt;/description\u0026gt; \u0026lt;/item\u0026gt; {{ end }} \u0026lt;/channel\u0026gt; \u0026lt;/rss\u0026gt; Now let\u0026rsquo;s do changes in above template to meet our requirement:-\nAdd Full Content in rss.xml This is most common use case where you want to show full post content instead of just summary in RSS feed.\nFor this just replace this line in default RSS template\n\u0026lt;description\u0026gt;{{ .Summary | html }}\u0026lt;/description\u0026gt; with this one\n\u0026lt;description\u0026gt;{{ .Content | html }}\u0026lt;/description\u0026gt; Exclude specific pages from rss.xml This is also very common use case where you want to skip some pages from rss.xml.\nNotice that we have added a condition {{ if ne .Params.rss_ignore true }} in default RSS template to exclude pages based on rss_ignore flag\n/layouts/_default/sitemap.xml ... {{ range $pages }}{{ if ne .Params.rss_ignore true }} \u0026lt;item\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;{{ .Permalink }}\u0026lt;/link\u0026gt; \u0026lt;pubDate\u0026gt;{{ .Date.Format \u0026#34;Mon, 02 Jan 2006 15:04:05 -0700\u0026#34; | safeHTML }}\u0026lt;/pubDate\u0026gt; {{ with .Site.Author.email }}\u0026lt;author\u0026gt;{{.}}{{ with $.Site.Author.name }} ({{.}}){{end}}\u0026lt;/author\u0026gt;{{end}} \u0026lt;guid\u0026gt;{{ .Permalink }}\u0026lt;/guid\u0026gt; \u0026lt;description\u0026gt;{{ .Summary | html }}\u0026lt;/description\u0026gt; \u0026lt;/item\u0026gt; {{ end }}{{ end }} ... Now any page, you want to exclude from RSS Feed, add the rss_ignore: true property in front-matter of that page\nblog-page.md --- title: Customize RSS feed in Hugo Website ... rss_ignore: true --- Summary In this tutorial, we\u0026rsquo;ve learned how to -\nDisable auto RSS feed generation Rename generated index.xml to some other name rss.xml Customize default RSS feed template for your hugo website. If you have any other requirements of rss.xml customization, or you are facing issue following the tutorial. Please comment, I\u0026rsquo;ll try to solve your problem at earliest possible.\n","permalink":"https://codingnconcepts.com/hugo/custom-rss-feed-hugo/","tags":null,"title":"Customize RSS in Hugo Website"},{"categories":["Spring Boot"],"contents":"In this tutorial, we\u0026rsquo;ll compare Spring Boot Configuration using YAML file and Property file and their advantages one over the other.\nOverview A common practice in Spring Boot project is to externalize the configuration through a properties file, YAML file, environment variable, or command-line arguments.\nProperties file application.properties is widely used for external configuration, however YAML file application.yml is gaining popularity due to its simplicity and JSON like format.\nHierarchical Configuration application.properties Each line is a single configuration in .properties file. Therefore, we need to use some prefixes to define hierarchical configuration such as below example, where spring.datasource is a prefix for url, username, and password configuration.\nspring.datasource.url=jdbc:mysql://localhost/test spring.datasource.username=SA spring.datasource.password=password application.yml On the other hand .yml files are very convenient to define hierarchical data which is very similar to JSON format\nspring: datasource: url: jdbc:mysql://localhost/test username: SA password: password Clearly YAML file is more readable as compare to Properties file.\nNote that you use consistent spaces in next line to represent nested data in YAML file.\nList Configuration application.properties If you have a list of configurations then they can be defined in .properties file -\nIndividually in different line using indices such as servers[0], server[1] \u0026hellip; ,or Together in single line using comma separated values such as dev, prod application.servers[0]=localhost application.servers[1]=abc.test.com application.servers[2]=xyz.dev.com application.environment=dev, prod application.yml Similar list of configuration can be defined in .yml file -\nIndividually in different line prefixed with - , or Together in single line using comma separated values such as [dev, prod] application: servers: - localhost - abc.test.com - xyz.dev.com environment: [dev, prod] Nested List Configuration YAML file becomes more concise and readable as compare to Properties file when the List configuration contains further nested data.\napplication.properties application.servers[0].ip=127.0.0.1 application.servers[0].path=/path1 application.servers[1].ip=127.0.0.2 application.servers[1].path=/path2 application.servers[2].ip=127.0.0.3 application.servers[2].path=/path3 application.yml application: servers: - ip: \u0026#39;127.0.0.1\u0026#39; path: \u0026#39;/path1\u0026#39; - ip: \u0026#39;127.0.0.2\u0026#39; path: \u0026#39;/path2\u0026#39; - ip: \u0026#39;127.0.0.3\u0026#39; path: \u0026#39;/path3\u0026#39; Map Configuration application.properties A Map configurations can be defined in .properties file -\nIndividually in different line using key=value pair … ,or Together in single line using comma separated values such as { key1: \u0026quot;value1\u0026quot;, key2: \u0026quot;value2\u0026quot;} map1.key1=value1 map1.key2=value2 map2={ key1: \u0026#34;value1\u0026#34;, key2: \u0026#34;value2\u0026#34; } application.yml Similar Map configuration can be defined in .yml file -\nIndividually in different line using key: value pair, or Together in single line using comma separated values such as { key1: \u0026quot;value1\u0026quot;, key2: \u0026quot;value2\u0026quot;} map1: key1: value1 key2: value2 map2: { key1: \u0026#34;value1\u0026#34;, key2: \u0026#34;value2\u0026#34; } Profile Based Configuration A common practice in Spring Boot configuration is to have YAML or Properties file for each profile (environment) - test, dev, staging, prod.\nSpring Boot picks up .properties or .yml files in the following sequence:-\napplication-{profile}.properties | .yml application.properties | .yml We can pass the profile (for e.g. prod) from command-line argument for e.g. --spring.profiles.active=prod or from application.properties | .yml\napplication.properties In case of multiple profiles, each profile is configured in different application-{profile}.properties file.\napplication.properties\n#Logging logging.level.root=ERROR logging.level.org.springframework.web=INFO logging.level.com.codingnconcepts=DEBUG #spring spring.main.banner-mode=off spring.profiles.active=dev server.email=default@codingnconcepts.com application-dev.properties\n#dev environment server.email: dev@codingnconcepts.com server.cluster[0].ip=127.0.0.1 server.cluster[0].path=/dev1 server.cluster[1].ip=127.0.0.2 server.cluster[1].path=/dev2 server.cluster[2].ip=127.0.0.3 server.cluster[2].path=/dev3 application-prod.properties\n#production environment server.email: prod@codingnconcepts.com server.cluster[0].ip=192.168.0.1 server.cluster[0].path=/app1 server.cluster[1].ip=192.168.0.2 server.cluster[1].path=/app2 server.cluster[2].ip=192.168.0.3 server.cluster[2].path=/app3 Properties which are not configured in application-{profile}.properties such as logging.level, are picked up from application.properties file.\nLet\u0026rsquo;s pass different profiles using command-line arguments:-\n# The \u0026#39;dev\u0026#39; profile is configured in application.properties # Profile: dev, picks application-dev.properties or YAML $ java -jar target/spring-boot-profile-1.0.jar ServerProperties{email=\u0026#39;dev@codingnconcepts.com\u0026#39;, cluster=[ Cluster{ip=\u0026#39;127.0.0.1\u0026#39;, path=\u0026#39;/dev1\u0026#39;}, Cluster{ip=\u0026#39;127.0.0.2\u0026#39;, path=\u0026#39;/dev2\u0026#39;}, Cluster{ip=\u0026#39;127.0.0.3\u0026#39;, path=\u0026#39;/dev3\u0026#39;} ]} # Profile: prod, picks application-prod.properties or YAML $ java -jar -Dspring.profiles.active=prod target/spring-boot-profile-1.0.jar ServerProperties{email=\u0026#39;prod@codingnconcepts.com\u0026#39;, cluster=[ Cluster{ip=\u0026#39;192.168.0.1\u0026#39;, path=\u0026#39;/app1\u0026#39;}, Cluster{ip=\u0026#39;192.168.0.2\u0026#39;, path=\u0026#39;/app2\u0026#39;}, Cluster{ip=\u0026#39;192.168.0.3\u0026#39;, path=\u0026#39;/app3\u0026#39;} ]} # Profile: abc, a non-existent profile $ java -jar -Dspring.profiles.active=abc target/spring-boot-profile-1.0.jar ServerProperties{email=\u0026#39;null\u0026#39;, cluster=[]} application.yml We can create multiple profiles in single .yml file by using three dashes ---.\nlogging: level: root: ERROR org.springframework.web: INFO com.codingnconcepts: DEBUG spring: profiles: active: \u0026#34;dev\u0026#34; main: banner-mode: \u0026#34;off\u0026#34; server: email: default@codingnconcepts.com --- spring: profiles: dev server: email: dev@codingnconcepts.com cluster: - ip: 127.0.0.1 path: /dev1 - ip: 127.0.0.2 path: /dev2 - ip: 127.0.0.3 path: /dev3 --- spring: profiles: prod server: email: prod@codingnconcepts.com cluster: - ip: 192.168.0.1 path: /app1 - ip: 192.168.0.2 path: /app2 - ip: 192.168.0.3 path: /app3 Rules for picking up profile specific configuration from YAML file is same as Property file.\nConclusion In this article, we\u0026rsquo;ve seen some differences between Properties and YAML Spring Boot configuration files. We saw that YAML is more human friendly and concise compare to Properties file.\nAlso learn how to read configuration properties from YAML file in Spring Boot\n","permalink":"https://codingnconcepts.com/spring-boot/spring-boot-application-properties-vs-yml/","tags":["Spring Boot Basics","YAML"],"title":"application.properties vs application.yml in Spring Boot"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to inspect overlay elements using Chrome DevTool.\nOverlay Elements Overlay Elements are those elements which are rendered over another element. For example:\nWhen you click on a Combo Box, drop down values are displayed as an overlay When you click or hover on a Menu, list of submenu(s) are displayed as an overlay When you click or hover on an Icon, tooltip is displayed as an overlay When you start typing in a Search Box, search suggestions are displayed as an overlay One thing common in all those overlay elements is that as soon as their origin element (such as Combo Box, Menu, Icon, or Search Box) loose focus, they disappear.\nI have had a hard time inspecting and debugging the CSS of these overlay elements since they disappear as soon as I click on Chrome DevTool Inspect ⬉ Icon. Until I found this cool trick.\nInspect Overlay Elements Using Break on subtree modifications We\u0026rsquo;ll inspect the Google search suggestions overlay which appears while typing in Search Box. Follow the steps:\nFirst of all we need to open a Chrome DevTool. We can do it in multiple ways: From Chrome’s main menu, select More tools ➞ Developer tools Right Click near the element, select inspect Click on DevTool\u0026rsquo;s Elements Tab. Right Click on \u0026lt;body\u0026gt; HTML element. Select Break on ➞ subtree modifications That\u0026rsquo;s it. Now when you start typing in Google search box. Search suggestion appears as an overlay elements which is the DOM modification, breakpoint is applied on the \u0026lt;body\u0026gt; element and freeze the browser.\nNow you can inspect the search suggestions overlay element, it will not disappear.\nNote: You are applying the break on \u0026lt;body\u0026gt; elements so you will see undesired breakpoints as you start typing in the search box. Keep pressing F8 to skip those breakpoints until your overlay element appears.\nChrome DevTool Break on Subtree Modifications\nUsing Break on attribute modifications We\u0026rsquo;ll inspect Amazon Sign in overlay menu which appears on hover on Account. Follow the steps:\nOpen Chrome DevTool Click on DevTool\u0026rsquo;s Elements Tab Inspect the Account menu element using Inspect Icon ⬉. It will focus that particular HTML element in Element tab Right Click on \u0026lt;a\u0026gt; HTML element Select Break on ➞ attribute modifications Chrome DevTool Break on Attribute Modifications\nThat’s it. Now when you hover on Account menu. It modifies the class attribute of \u0026lt;a\u0026gt; element, which is an attribute modification, breakpoint is applied on the \u0026lt;a\u0026gt; element and freeze the browser.\nNow you can inspect the Overlay menu, it will not disappear.\n","permalink":"https://codingnconcepts.com/javascript/inspect-overlay-elements-chrome-devtools/","tags":["Javascript Console"],"title":"Inspect Overlay Elements using Chrome DevTools"},{"categories":["Interview Questions"],"contents":"These manual testing interview questions are based on my personal interview experience and feedback from other interviewees. Keep following this post for regular updates.\nQ1. What is the difference between Regression and Retesting ? Regression Testing is performed to check unexpected side-effects even any code change has not unfavorably disturbed current features \u0026amp; functions of an Application. Retesting is a process to check specific test cases that are found with bug/s in the final execution. Q2. What will be your approach if you are not clear about the requirement? I will look for specification documents if available and try to understand the requirement on my own. In case of any issue, I will approach BA for clarification. If BAs are also not clear, request them to confirm the requirement from business users. I will also take help from my peers if they have a clear understanding of the requirement. Q3. Are you familiar with Agile and Scrum Methodologies? How they are different from others? Agile is a continuous iteration of development and testing in the software development process which allows changed in project development requirement with an increment approach.\nQ4. What are the Scrum Ceremonies? Scrum is executed inside what are called Sprints and every Sprint is a short iterations of work which is usually of two weeks or lesser. A sprint has four different scrum ceremonies to ensure proper execution:\nSprint Planning Meeting -\nWhat are we going to work on, and how are we going to do it? Daily Stand-up Meeting -\nWhat’s done on the prior day, what needs to be done today, identify any impediments, and creates visibility around the work that everyone is doing in the Sprint. Sprint Review Meeting -\nBased on stack holders feedback, items will be looped back into the Product Backlog, where it can be added in the future Sprint. Scrum Retrospective Meeting -\nWhat went well, what did not and what can be improved. Q4. What are SDLC and STLC? SDLC stands for Software Development Life Cycle, which is a sequence of different activities performed during the software development process. STLC stands for Software Testing Life Cycle, which is a sequence of different activities performed during software testing process. Q5. What is Defect Life Cycle? A defect life cycle represents the flow of bug from newly open to until it closes. Simpler implementations of the bug life cycle may not include all the states, The defect life cycle can vary from organization to organization and also from project to project based on several factors like organization policy, software development model.\nIt has different states as mentioned below -\nNew Assigned Open Fixed Pending Retest Retest Verified Reopen Closed Duplicate Rejected Deferred Not a Bug Q6. How do you raise a defect in HP QC and what parameters you key in? To log a new defect in HP QC we navigate to Defects Tab -\u0026gt; New Defect. while adding a new test case we key in Parameters as follows - Summary Assigned to Detected in Release Project Status Priority Reproducible Description Attachment (Screenshots of error) Linked entities (If required)\nQ7. How do you write a test case, what parameters you key in? For adding a test case we key in Parameter as follows :- Test case Id Release Project Version Pre-requisite Test Data Test Objective Test Steps / description Actual result Expected result Status\nQ8. What is the prerequisite to login on the home page? User should have required access and credential for login on website. User should have compatible browser version. Q9. Suppose 10 tasks are assigned to 4 developers in a sprint. What will be your approach to test those development tasks? Initially, my approach will be to test module wise and later will perform end to end testing once all developer code is ready to test.\nQ10. What is priority and severity? Priority is defined degree of fix and severity says how early it has to be fixed. The tester can set defect priority in case he/she is blocked to run further scenarios which are interdependent.\nThere is four combinations for these terms as follows -\nLow severity and low priority\nExample - A website’s privacy policy, or disclaimer came across a typo error, or you noticed that the colour-font is not as per your website’s theme. It is not as it should be so it is a bug, however, since it is not affecting the functionality of your web-app, and also because it wouldn’t be noticed by the majority of users. It would be okay to keep it as low severity and low priority. High severity and high priority\nExample - On an eCommerce website, every customers get error message on the booking form and cannot place orders, or the product page throws a Error 500 response. Low severity and High priority\nExample - Latest browser version, shows that the buttons are slightly overlapping with each other. Although, they are still clickable separately, the visual representation is getting disturbed. High severity and Low priority\nExample - Customers who uses very old browsers cannot continue with their purchase of a product. Because the number of customers with very old browsers is very low, it is a low priority to fix the issue. Q11. You have submitted the form and details will be saved in DB and result can be verified on third-party platform, What will be your test steps? Test case steps are as follows :-\nLogin to Page with valid credentials. Submit the form with required details. Verify expected response on web. Verify the submitted details Saved correctly on a third-party platform. Q12. Your test outcome will reflect on a third-party platform, What will be your approach if third-party platform is down? I will follow-up with the concerned team regarding this and will execute my test cases once it’s accessible.\nQ13. What is static and dynamic testing? Under Static Testing, code is not executed. It manually checks the code, requirement documents, and design documents to find errors. Hence, the name \u0026ldquo;static\u0026rdquo;. The main objective of this testing is to improve the quality of software products by finding errors in the early stages of the development cycle so it is a kind of verification process.\nUnder Dynamic Testing, a code is executed. It checks for functional behavior of software system, memory/CPU usage, and overall performance of the system. The main objective of this testing is to confirm that the software product works in conformance with the business requirements. This testing is also called an Execution technique or validation testing.\nQ14. What is Sanity testing? Sanity testing is a subset of regression testing. After receiving the software build, sanity testing is performed to ensure that the code changes introduced are working as expected. This testing is a checkpoint to determine if testing for the build can proceed or not.\nQ15. What is the SQL query to fetch the no of records from the Database Table? Select count * from table Employee.\nQ16. What is different Test Techniques? Test Techniques are as follows -\nBlack Box Testing - also known as Specification based testing, analyses the functionality of a software/application without knowing much about the internal structure/design of the item. White Box Testing - also known as Structure based testing, requires a profound knowledge of the code as it includes testing of some structural part of the application. Experience Based Testing - Its a constant approach to studies, analyzes the product and accordingly applies his skills, traits, and experience to develop test strategy and test cases to perform necessary testing. ","permalink":"https://codingnconcepts.com/top-manual-testing-interview-questions/","tags":["Interview Q\u0026A","Testing Q\u0026A"],"title":"Top Manual Testing Interview Questions"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to transform a Java Object to JSON-LD and vice versa. Also learn how to verify the schema of JSON-LD.\nJSON-LD JSON-LD is a JSON-based format which is used to represent structured data and linked data. Schema of JSON-LD can be found in documentation of schema.org.\nJSON-LD is widely used in websites to markup the schema of website pages. This helps website indexers (such as Google) to understand the website better and boost SEO. Though the JSON-LD usage are not limited to websites, also used in REST Web services, and unstructured databases such as Apache CouchDB and MongoDB.\nTransform Java Object to JSON-LD Let\u0026rsquo;s transform the Java object to JSON-LD using jackson-jsonld which is a Jackson module that provides the annotation to generate JSON-LD documents.\nAdd Maven Dependency If you are using maven, add jackson-jsonld dependency in your pom.xml\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.io-informatics.oss\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jackson-jsonld\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.1.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Transform Java Object to JSON-LD Next, We create a Person class and annotate it with json-ld annotations.\npackage com.cnc; import ioinformarics.oss.jackson.module.jsonld.annotation.JsonldId; import ioinformarics.oss.jackson.module.jsonld.annotation.JsonldProperty; import ioinformarics.oss.jackson.module.jsonld.annotation.JsonldType; @JsonldType(\u0026#34;http://schema.org/Person\u0026#34;) public class Person { @JsonldId public String id; @JsonldProperty(\u0026#34;http://schema.org/name\u0026#34;) public String name; @JsonldProperty(\u0026#34;http://schema.org/jobTitle\u0026#34;) public String jobtitle; @JsonldProperty(\u0026#34;http://schema.org/url\u0026#34;) public String url; } Finally, we create a Person object instance and transform it in JSON-LD\n// Register Jsonld Module with Jackson ObjectMapper objectMapper = new ObjectMapper(); objectMapper.configure(SerializationFeature.INDENT_OUTPUT, true); objectMapper.registerModule(new JsonldModule(() -\u0026gt; objectMapper.createObjectNode())); // Create Person Instance Person person = new Person(); person.id = \u0026#34;mailto:lahoti.ashish20@gmail.com\u0026#34;; person.name = \u0026#34;Ashish Lahoti\u0026#34;; person.jobtitle = \u0026#34;Software Developer\u0026#34;; person.url = \u0026#34;https://codingnconcepts.com\u0026#34;; // Transform to JSON-LD String personJsonLd = objectMapper.writer().writeValueAsString(JsonldResource.Builder.create().build(person)); System.out.println(personJsonLd); Output { \u0026#34;@context\u0026#34; : { \u0026#34;jobtitle\u0026#34; : \u0026#34;http://schema.org/jobTitle\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;http://schema.org/name\u0026#34;, \u0026#34;url\u0026#34; : \u0026#34;http://schema.org/url\u0026#34; }, \u0026#34;@type\u0026#34; : \u0026#34;http://schema.org/Person\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;Ashish Lahoti\u0026#34;, \u0026#34;jobtitle\u0026#34; : \u0026#34;Software Developer\u0026#34;, \u0026#34;url\u0026#34; : \u0026#34;https://codingnconcepts.com\u0026#34;, \u0026#34;@id\u0026#34; : \u0026#34;mailto:lahoti.ashish20@gmail.com\u0026#34; } Transform Nested Java Object to JSON-LD We can also transform nested java objects to complex JSON-LD. Let\u0026rsquo;s annotate Organization and Location classes with json-ld annotations.\n@JsonldType(\u0026#34;http://schema.org/Organization\u0026#34;) public class Organization { @JsonldId public Integer id; @JsonldProperty(\u0026#34;http://schema.org/url\u0026#34;) public String url; @JsonldProperty(\u0026#34;http://schema.org/name\u0026#34;) public String name; @JsonldProperty(\u0026#34;http://schema.org/location\u0026#34;) public Location location; } @JsonldType(\u0026#34;http://schema.org/PostalAddress\u0026#34;) public class Location { @JsonldProperty(\u0026#34;http://schema.org/streetAddress\u0026#34;) public String address; @JsonldProperty(\u0026#34;http://schema.org/addressLocality\u0026#34;) public String city; @JsonldProperty(\u0026#34;http://schema.org/addressRegion\u0026#34;) public String state; @JsonldProperty(\u0026#34;http://schema.org/addressCountry\u0026#34;) public String country; @JsonldProperty(\u0026#34;http://schema.org/postalCode\u0026#34;) public String zipcode; } Now, create an instance of Organization which also have a Location property and transform the instance to JSON-LD\n// Register Jsonld Module with Jackson ObjectMapper objectMapper = new ObjectMapper(); objectMapper.configure(SerializationFeature.INDENT_OUTPUT, true); objectMapper.registerModule(new JsonldModule(() -\u0026gt; objectMapper.createObjectNode())); // Create Organization and Location Instances Organization org = new Organization(); org.id = 12345; org.url = \u0026#34;https://codingnconcepts.com\u0026#34;; org.name = \u0026#34;CodingNConcepts\u0026#34;; Location location = new Location(); location.address = \u0026#34;7 S. Broadway\u0026#34;; location.city = \u0026#34;Denver\u0026#34;; location.state = \u0026#34;CO\u0026#34;; location.country = \u0026#34;USA\u0026#34;; location.zipcode = \u0026#34;80209\u0026#34;; org.location = location; // Transform to JSON-LD String orgJsonLd = objectMapper.writer().writeValueAsString(JsonldResource.Builder.create().build(org)); System.out.println(orgJsonLd); Output { \u0026#34;@context\u0026#34; : { \u0026#34;name\u0026#34; : \u0026#34;http://schema.org/name\u0026#34;, \u0026#34;location\u0026#34; : \u0026#34;http://schema.org/location\u0026#34;, \u0026#34;url\u0026#34; : \u0026#34;http://schema.org/url\u0026#34; }, \u0026#34;@type\u0026#34; : \u0026#34;http://schema.org/Organization\u0026#34;, \u0026#34;url\u0026#34; : \u0026#34;https://codingnconcepts.com\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;CodingNConcepts\u0026#34;, \u0026#34;location\u0026#34; : { \u0026#34;@type\u0026#34; : \u0026#34;http://schema.org/PostalAddress\u0026#34;, \u0026#34;@context\u0026#34; : { \u0026#34;zipcode\u0026#34; : \u0026#34;http://schema.org/postalCode\u0026#34;, \u0026#34;country\u0026#34; : \u0026#34;http://schema.org/addressCountry\u0026#34;, \u0026#34;address\u0026#34; : \u0026#34;http://schema.org/streetAddress\u0026#34;, \u0026#34;city\u0026#34; : \u0026#34;http://schema.org/addressLocality\u0026#34;, \u0026#34;state\u0026#34; : \u0026#34;http://schema.org/addressRegion\u0026#34; }, \u0026#34;address\u0026#34; : \u0026#34;7 S. Broadway\u0026#34;, \u0026#34;city\u0026#34; : \u0026#34;Denver\u0026#34;, \u0026#34;state\u0026#34; : \u0026#34;CO\u0026#34;, \u0026#34;country\u0026#34; : \u0026#34;USA\u0026#34;, \u0026#34;zipcode\u0026#34; : \u0026#34;80209\u0026#34; }, \u0026#34;@id\u0026#34; : 12345 } Transform JSON-LD to Java Object We can also transform the JSON-LD document to a plain JSON object and further transform the JSON to Java Object using Jackson Object Mapper. Let\u0026rsquo;s use jsonld-java which is a Java implementation of the JSON-LD 1.0 specification.\nAdd Maven Dependency If you are using maven, add jsonld-java dependency in your pom.xml\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.github.jsonld-java\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jsonld-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.13.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Transform JSON-LD to JSON Let\u0026rsquo;s user the Person JSON-LD and save it in a file input.json\ninput.json\n{ \u0026#34;@context\u0026#34; : { \u0026#34;jobtitle\u0026#34; : \u0026#34;http://schema.org/jobTitle\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;http://schema.org/name\u0026#34;, \u0026#34;url\u0026#34; : \u0026#34;http://schema.org/url\u0026#34; }, \u0026#34;@type\u0026#34; : \u0026#34;http://schema.org/Person\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;Ashish Lahoti\u0026#34;, \u0026#34;jobtitle\u0026#34; : \u0026#34;Software Developer\u0026#34;, \u0026#34;url\u0026#34; : \u0026#34;https://codingnconcepts.com\u0026#34;, \u0026#34;@id\u0026#34; : \u0026#34;mailto:lahoti.ashish20@gmail.com\u0026#34; } Next, read from this file and transform to a simple JSON string\n// Read from JSON-LD file InputStream inputStream = new FileInputStream(\u0026#34;input.json\u0026#34;); Object jsonObject = JsonUtils.fromInputStream(inputStream); // Transform to Simple JSON compact object Object compact = JsonLdProcessor.compact(jsonObject, new HashMap\u0026lt;\u0026gt;(), new JsonLdOptions()); // Convert JSON to String String compactContent = JsonUtils.toPrettyString(compact); System.out.println(compactContent); Output { \u0026#34;@id\u0026#34; : \u0026#34;mailto:lahoti.ashish20@gmail.com\u0026#34;, \u0026#34;@type\u0026#34; : \u0026#34;http://schema.org/Person\u0026#34;, \u0026#34;http://schema.org/jobTitle\u0026#34; : \u0026#34;Software Developer\u0026#34;, \u0026#34;http://schema.org/name\u0026#34; : \u0026#34;Ashish Lahoti\u0026#34;, \u0026#34;http://schema.org/url\u0026#34; : \u0026#34;https://codingnconcepts.com\u0026#34; } Transform JSON to Java Object We can further map this JSON string to a PersonJson POJO using Jackson ObjectMapper\n@JsonIgnoreProperties(ignoreUnknown = true) public class PersonJson { @JsonProperty(\u0026#34;@id\u0026#34;) private String id; @JsonProperty(\u0026#34;@type\u0026#34;) private String type; @JsonProperty(\u0026#34;http://schema.org/name\u0026#34;) private String name; @JsonProperty(\u0026#34;http://schema.org/url\u0026#34;) private String url; @JsonProperty(\u0026#34;http://schema.org/jobTitle\u0026#34;) private String jobtitle; } // Map JSON to POJO ObjectMapper objectMapper = new ObjectMapper(); PersonJson person = objectMapper.readValue(compactContent, PersonJson.class); System.out.println(person); Verify Transformed JSON-LD You can verify the schema of transformed JSON-LD using Structured Data Testing Tool.\nClick on the link and select \u0026ldquo;CODE SNIPPET\u0026rdquo; tab. Then copy paste the generated JSON-LD. Click \u0026ldquo;Run Test\u0026rdquo;. It tells if the schema of generated JSON-LD is correct and show errors if any.\nThe full source code of this tutorial is available on GitHub.\n","permalink":"https://codingnconcepts.com/java/transform-json-ld-in-java/","tags":["Advance Java","JSON-LD"],"title":"Transform JSON-LD in Java"},{"categories":["Postman"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to generate Curl command using Postman.\nOverview Many times we need to execute GET, POST, PUT, PATCH, DELETE HTTP requests to verify endpoints. Normally we use:\nBrowser to execute GET request Postman to execute GET, POST, PUT, PATCH, DELETE requests There are many times when we are stuck in a situation where:\nBrowser is not available on the machine Postman is not able to install on a certain machine Using Linux environment In such cases, Curl command is very useful and can be used to execute HTTP endpoints from a command prompt or terminal.\nInstalling Curl The curl package is pre-installed on most Linux distributions today.\nIf curl is not installed you can easily install it using the package manager of your distribution.\nInstall Curl on Ubuntu and Debian sudo apt update sudo apt install curl Install Curl on CentOS and Fedora sudo yum install curl Generate Curl from Postman Considering that you are already familiar with Postman. Follow these steps:\nCreate an HTTP request using Postman\nClick on the code \u0026lt;/\u0026gt; icon from the menu on the right. You may find the code icon or text button somewhere else depending on the Postman version.\nA dialog \u0026ldquo;Code Snippet\u0026rdquo; will appear. Select \u0026ldquo;cURL\u0026rdquo; from the dropdown and copy the generated command code snippet.\ncurl \u0026#39;https://reqres.in/api/users\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --data \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;morpheus\u0026#34;, \u0026#34;job\u0026#34;: \u0026#34;leader\u0026#34; }\u0026#39; Postman generates multi-line cURL code snippets by default. Mac and Linux support multi-line cURL code snippets but not Windows.\nYou can change the default behavior by changing the settings using the icon next to the dropdown.\n","permalink":"https://codingnconcepts.com/postman/how-to-generate-curl-command-from-postman/","tags":null,"title":"How to generate Curl Command from Postman"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to make AJAX calls in JavaScript.\nAJAX The term AJAX stands for Asynchronous JavaScript And XML\nThe term AJAX is used in JavaScript for making asynchronous network request to fetch resources. Resources are not limited to XML, as the term suggest which is confusing. The term AJAX is also used to fetch resources as JSON, HTML, or Plain Text. You may have heard that term already.\nThere are multiple ways to make network request and fetch data from the server. We\u0026rsquo;ll look at them one by one.\nXMLHttpRequest The XMLHttpRequest object (also known as XHR in short) is used in older days to retrieve data from the server asynchronously.\nThe XML comes in the name because it is first used to retrieve XML data. Now this can also be used to retrieve JSON, HTML or Plain text.\nExample: GET function success() { var data = JSON.parse(this.responseText); console.log(data); } function error(err) { console.log(\u0026#39;Error Occurred :\u0026#39;, err); } var xhr = new XMLHttpRequest(); xhr.onload = success; xhr.onerror = error; xhr.open(\u0026#34;GET\u0026#34;, \u0026#34;https://jsonplaceholder.typicode.com/posts/1\u0026#34;); xhr.send(); We see that to make a simple GET request, we need two listeners to handle success and failure of the request. We also need to make calls to open() and send() methods. The response from the server is stored in the responseText variable, which is converted to JavaScript object using JSON.parse().\nExample: POST function success() { var data = JSON.parse(this.responseText); console.log(data); } function error(err) { console.log(\u0026#39;Error Occurred :\u0026#39;, err); } var xhr = new XMLHttpRequest(); xhr.onload = success; xhr.onerror = error; xhr.open(\u0026#34;POST\u0026#34;, \u0026#34;https://jsonplaceholder.typicode.com/posts\u0026#34;); xhr.setRequestHeader(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json; charset=UTF-8\u0026#34;); xhr.send(JSON.stringify({ title: \u0026#39;foo\u0026#39;, body: \u0026#39;bar\u0026#39;, userId: 1 }) ); We see that POST request is similar to GET request. We need to additionally set the request header \u0026ldquo;Content-Type\u0026rdquo; using setRequestHeader and send the JSON body as string using JSON.stringify in the send method.\nXMLHttpRequest vs Fetch We have used XMLHttpRequest for several years to request data. The modern fetch() API allows you to make network requests similar to XMLHttpRequest (XHR). The main difference is that the fetch() API uses Promises, which enables a simpler and cleaner API, avoiding callback hell and having to remember the complex API of XMLHttpRequest.\nFetch API Fetch is a native JavaScript API to make AJAX calls which is supported by most of the browsers and widely used now a days.\nAPI Usage fetch(url, options) .then(response =\u0026gt; { // handle response data }) .catch(err =\u0026gt; { // handle errors }); API Arguments The fetch() API takes two arguments:\nurl is a mandatory argument, which is a path to the resource you want to fetch. options is an optional argument. You don\u0026rsquo;t need to provide this argument to make simple GET request. You need to pass this argument to provide additional information about the request such as method: GET | POST | PUT | DELETE | PATCH headers: Request Headers Object for e.g. { \u0026ldquo;Content-type\u0026rdquo;: \u0026ldquo;application/json; charset=UTF-8\u0026rdquo; } mode: cors | no-cors | same-origin | navigate cache: default | reload | no-cache body: Request body generally used in POST request\nAPI returns Promise Object The fetch() API returns a promise object.\nThe promise is rejected if there is a network error, this is handled in the .catch() block. The promise is resolved if there is a response from the server with any status code such as 200, 404, 500. Response Object can be handled in the .then() block.\nError Handling Please note that we expect status code as 200 (OK status) for successful response but fetch() API resolved the promise even if response is coming with error status code such as 404 (Resource Not found) and 500 (Internal Server Error). We need to handle those explicitly in .then() block.\nWe can see HTTP-status in response object:\nstatus – HTTP status code, e.g. 200. ok – boolean, true if the HTTP status code is 200-299.\nExample: GET const getTodoItem = fetch(\u0026#39;https://jsonplaceholder.typicode.com/todos/1\u0026#39;) .then(response =\u0026gt; response.json()) .catch(err =\u0026gt; console.error(err)); getTodoItem.then(response =\u0026gt; console.log(response)); Response ➤ { userId: 1, id: 1, title: \u0026#34;delectus aut autem\u0026#34;, completed: false } Two things to note in above code:-\nfetch API returns a promise object which we can assign to a variable and execute later. You have to additionally call response.json() to convert response object to JSON\nError Handling Let\u0026rsquo;s see what happens when HTTP GET request throw 500 error:-\nfetch(\u0026#39;http://httpstat.us/500\u0026#39;) // this API throw 500 error .then(response =\u0026gt; () =\u0026gt; { console.log(\u0026#34;Inside first then block\u0026#34;); return response.json(); }) .then(json =\u0026gt; console.log(\u0026#34;Inside second then block\u0026#34;, json)) .catch(err =\u0026gt; console.error(\u0026#34;Inside catch block:\u0026#34;, err)); Inside first then block ➤ ⓧ Inside catch block: SyntaxError: Unexpected token I in JSON at position 4 We see that even though API throw 500 error, It still goes inside first then() block where it\u0026rsquo;s not able to parse error JSON and throw error which is caught by catch() block.\nThat means we need to handle such errors explicitly like this if we are using fetch() API:-\nfetch(\u0026#39;http://httpstat.us/500\u0026#39;) .then(handleErrors) .then(response =\u0026gt; response.json()) .then(response =\u0026gt; console.log(response)) .catch(err =\u0026gt; console.error(\u0026#34;Inside catch block:\u0026#34;, err)); function handleErrors(response) { if (!response.ok) { // throw error based on custom conditions on response throw Error(response.statusText); } return response; } ➤ Inside catch block: Error: Internal Server Error at handleErrors (Script snippet %239:9) Example: POST fetch(\u0026#39;https://jsonplaceholder.typicode.com/todos\u0026#39;, { method: \u0026#39;POST\u0026#39;, body: JSON.stringify({ completed: true, title: \u0026#39;new todo item\u0026#39;, userId: 1 }), headers: { \u0026#34;Content-type\u0026#34;: \u0026#34;application/json; charset=UTF-8\u0026#34; } }) .then(response =\u0026gt; response.json()) .then(json =\u0026gt; console.log(json)) .catch(err =\u0026gt; console.log(err)) Response ➤ {completed: true, title: \u0026#34;new todo item\u0026#34;, userId: 1, id: 201} Two things to note in above code:-\nPOST request is similar to GET request. We need to additionally send the method, body and headers properties in second argument of fetch() API. We have to explicitly JSON.stringify() the request body params\nAxios API Axios API is very similar to fetch API with few enhancements. I personally prefer to use Axios API instead of fetch() API due to following reasons:-\nProvides different methods for GET axios.get(), POST axios.post(), \u0026hellip; which makes your code concise Consider 299++ response codes such as 404, 500 as errors which can be handled in catch() block so you do not need to handle those errors explicitly It provides backward compatibility with old browsers such as IE11 It returns the response as JSON object so you do not need to do any parsing It takes the POST request body as JSON object so you do not need to do any stringify Example: GET // way to include script in chrome console var script = document.createElement(\u0026#39;script\u0026#39;); script.type = \u0026#39;text/javascript\u0026#39;; script.src = \u0026#39;https://unpkg.com/axios/dist/axios.min.js\u0026#39;; document.head.appendChild(script); axios.get(\u0026#39;https://jsonplaceholder.typicode.com/todos/1\u0026#39;) .then(response =\u0026gt; console.log(response.data)) .catch(err =\u0026gt; console.error(err)); Response { userId: 1, id: 1, title: \u0026#34;delectus aut autem\u0026#34;, completed: false } We see that we get the response data directly using response.data without any parsing object unlike fetch() API.\nError Handling axios.get(\u0026#39;http://httpstat.us/500\u0026#39;) .then(response =\u0026gt; console.log(response.data)) .catch(err =\u0026gt; console.error(\u0026#34;Inside catch block:\u0026#34;, err)); ➤ Inside catch block: Error: Network Error We see that 500 error is also get caught by catch() block unlike fetch() API where we have to handle them explicitly.\nExample: POST axios.post(\u0026#39;https://jsonplaceholder.typicode.com/todos\u0026#39;, { completed: true, title: \u0026#39;new todo item\u0026#39;, userId: 1 }) .then(response =\u0026gt; console.log(response.data)) .catch(err =\u0026gt; console.log(err)) ➤ {completed: true, title: \u0026#34;new todo item\u0026#34;, userId: 1, id: 201} We see that POST method is very short and concise. You can directly pass the request body parameters unlike fetch() API where we stringify them.\n","permalink":"https://codingnconcepts.com/javascript/how-to-make-ajax-calls-in-javascript/","tags":["Javascript Core"],"title":"How to Make AJAX Calls in JavaScript"},{"categories":["Java"],"contents":"Builder Design Pattern is one of the commonly used design patterns. It falls under the category of Creational Design Patterns.\nOverview Analogy As the name suggests, Builder Design Pattern is used to build objects.\nMainly it is used to create complex object by using step by step approach and final step will return the object in complete state. Same construction process is used to create different representations of a complex object. This helps to make the object construction process generic.\nClass doesn\u0026rsquo;t create an object itself directly but delegates object creation to its Builder.\nNeed for Builder Design Pattern Let\u0026rsquo;s consider a case where we have entity that contains a lot of attributes.\nIn case of too many parameters of a single object where some may be optional and some required. In that case we would require to create multiple constructors (constructor overloading). Or we would be forced to send NULL for optional, not required parameters. Also, in case of same types of parameters, we would need to maintain order of parameters.\nWe can solve this problem by having a constructor for required attributes and setters for optional attributes. In that case, the problem would be that we would have an intermediate state of object in between during the process of building the final object i.e. till the last required setter call. This implies we would have an inconsistent state of theobject until the expected object is fully created. Ideally the object shouldn\u0026rsquo;t be available till it is in complete state. So we would need a way to create objects in complete state i.e. without the setters. This would ensure the immutability of the final object that is built. Immutability is the key of builder design pattern. How to implement Builder Design Pattern Create an entity class and add all the attributes of the class. Add a static nested Builder class. Copy all the attributes from outer class to builder class. Add methods in Builder class for each attribute to set on builder object and return the same builder object i.e. each of these methods are enriching the builder object with newer attribute. These method names need not follow setter methods rules, names could be verbose. We could add little optimization here by adding the constructor in builder class for required attributes and individual methods for optional attributes. (do this optimization if you feel required attributes list would never change, otherwise not a necessary step). Final step in object creation would be to add a build method in builder class that would actually call the private constructor of outer class to create an object. For this, we would need to add a private constructor in the outer entity class that accepts Builder object to construct the final entity object. Private constructor would ensure that the object is created through this builder only. Make sure not to add any setter methods in outer class. It will defeat the purpose of creating an immutable object. Finally, we would create a client class that uses the static inner Builder class to first create its object and calls different methods to enrich the builder and in the end calls build() which actually returns the required entity object. We notice that the object creation is a chained method call where each call is returning the builder object itself though we get the final object on call of build(). Code Example Now we know what are the classes required to build a Builder Design Pattern. Let\u0026rsquo;s look at the class diagram:\nLet\u0026rsquo;s create an entity class called Employee which would help us in creating desired objects with all required attributes and combination of optional attributes ensuring immutability.\npublic class Employee { private int employeeId; //required private String name; //required private String phone; //optional private String alternativePhone; //optional private String address; //optional //Note - only 1 private constructor to ensure all object creation goes through this private Employee(EmployeeBuilder employeeBuilder) { this.employeeId = employeeBuilder.employeeId; this.name = employeeBuilder.name; this.phone = employeeBuilder.phone; this.alternativePhone = employeeBuilder.alternativePhone; this.address = employeeBuilder.address; } //Note - No setters (provides immutability) // You can add getters if needed @Override public String toString() { return \u0026#34;Employee{\u0026#34; + \u0026#34;employeeId=\u0026#34; + employeeId + \u0026#34;, name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, phone=\u0026#39;\u0026#34; + phone + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, alternativePhone=\u0026#39;\u0026#34; + alternativePhone + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, address=\u0026#39;\u0026#34; + address + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#39;}\u0026#39;; } public static class EmployeeBuilder { private int employeeId; private String name; private String phone; private String alternativePhone; private String address; public EmployeeBuilder(int employeeId, String name) { this.employeeId = employeeId; this.name = name; } public EmployeeBuilder withPhone(String phone) { this.phone = phone; return this; } public EmployeeBuilder withAlternativePhone(String alternativePhone) { this.alternativePhone = alternativePhone; return this; } public EmployeeBuilder withAddress(String address) { this.address = address; return this; } public Employee build() { Employee employee = new Employee(this); return employee; } } } Let\u0026rsquo;s create a client (Main Class) to test object creation through Builder Design Pattern\npublic class Client { public static void main(String[] args) { //Object with all the attributes Employee employee1 = new Employee.EmployeeBuilder(112, \u0026#34;Ria Gupta\u0026#34;) .withPhone(\u0026#34;9876543210\u0026#34;) .withAlternativePhone(\u0026#34;9988776655\u0026#34;) .withAddress(\u0026#34;413-3rd Avenue, Shantinagar, Bangalore\u0026#34;) .build(); System.out.println(employee1); //Object with a combination of required and some optional attributes Employee employee2 = new Employee.EmployeeBuilder(113, \u0026#34;Rishabh Goyal\u0026#34;) .withPhone(\u0026#34;1234567890\u0026#34;) // no alternative phone // no address .build(); System.out.println(employee2); //Object with only required attributes Employee employee3 = new Employee.EmployeeBuilder(114, \u0026#34;Abhishek Bansal\u0026#34;) //no phone //no alternative phone no // address .build(); System.out.println(employee3); } } Output\rEmployee{employeeId=112, name=\u0026#39;Ria Gupta\u0026#39;, phone=\u0026#39;9876543210\u0026#39;, alternativePhone=\u0026#39;9988776655\u0026#39;, address=\u0026#39;413-3rd Avenue, Shantinagar, Banglore\u0026#39;}\rEmployee{employeeId=113, name=\u0026#39;Rishabh Goyal\u0026#39;, phone=\u0026#39;1234567890\u0026#39;, alternativePhone=\u0026#39;null\u0026#39;, address=\u0026#39;null\u0026#39;}\rEmployee{employeeId=114, name=\u0026#39;Abhishek Bansal\u0026#39;, phone=\u0026#39;null\u0026#39;, alternativePhone=\u0026#39;null\u0026#39;, address=\u0026#39;null\u0026#39;} Advantages of Builder Design Pattern Reduction in multiple constructors. Only one constructor that accepts Builder object is required. There is no need to pass null for optional parameters to the constructor. Object is instantiated fully i.e. in a complete state. We always get an immutable object since we didn\u0026rsquo;t provide any setter methods and hence state of object once created cannot be changed. Lesser error prone as user would know what they are passing in each method call. This also gives design flexibility and more readable of client code. Disadvantages of Builder Design Pattern The number of lines of code gets at least double using builder pattern as we copy all the fields of entity class to builder class. Code becomes more verbose. It requires the creation of a separate builder for each type of object. Conclusion In this article, we saw that Builder Design Pattern is a rescue where our use-case is such that we have a lot of parameters for a particular object type or we have requirement of adding new parameters may be in future. This should be used when we want to build immutable objects using theme construction process.\nIn addition to the benefits said above, opt for this pattern makes adding parameters easier and code becomes less error-prone and easy to read.\n","permalink":"https://codingnconcepts.com/java/builder-design-pattern-java/","tags":["Java Design Pattern"],"title":"Builder Design Pattern In Java"},{"categories":null,"contents":"Hi Travellers,\nThough this is a technical blog, I thought of helping people, who are willing to travel from Singapore to India under VBM (Vande Bharat Mission).\nHere is the list of frequently asked questions\u0026hellip;\n*(Please comment if you find something to be corrected, or added in this post. I\u0026rsquo;ll keep updating this post with more information.)\nAlso read India to Singapore (Vande Bharat Mission) FAQs\nChecklist to carry FAQs Document Remarks 1. Passport Should be valid for at least 6 months at the time of travelling 2. Valid EP/DP/SPASS for long term pass holders Required at India/Singapore Airport 3. Printout of HCI Registration Form email Required at India/Singapore Airport 4. Printout of Flight Ticket Booking Required at Singapore Airport 5. Printout of Negative RT-PCR Certificate Required at India Airport 6. Printout of Vaccination Certificate Required at India Airport 7. Printout of Air Suvidha Registration email Required at India Airport Process to follow Register with High Commission of India, Singapore (HCI) using below link and get a registration number via email:- https://www.hcisingapore.gov.in/strandedregind Book flight ticket online from Singapore to India.\nMost of the time Air India flights show unavailable online, In that case you can go to Air India Office to buy tickets physically at following address:- JetSpeed Travels Pte Ltd\nG.S.A Air India | R.A. Air India Express\nAddress: 3 Coleman Street, #03-07/08, Peninsula Shopping Centre, Singapore 179804\n(Nearest MRT: City Hall MRT Station)\nTel: +65 62214909\nDo RT-PCR SWAB test within 72 hrs of departure from MOH Approved Labs:-\nhttps://www.moh.gov.sg/licensing-and-regulation/regulations-guidelines-and-circulars/details/list-of-covid-19-swab-providers) Register with Air Suvidha Self Declaration form using below link and upload your passport, RT-PCR Negative Certificate, Vaccination Certificate:-\nhttps://www.newdelhiairport.in/airsuvidha/apho-registration Download the Vaccination Certificate and RT-PCR Negative Certificate from SG HealthHub or SG Notarise website:-\nhttps://www.healthhub.sg/\nhttps://www.notarise.gov.sg/ 2 copies of each Vaccination Certificate, RT-PCR Negative Certificate and Air Suvidha Self Declaration forms to be carried at Airport check-in counter. Disembarkation form to be filled and signed at Airport check-in counter. That\u0026rsquo;s It! Wish you a very happy and safe Journey! HCI Registration FAQs Register with High Commission of India, Singapore (HCI) using this link:-\nhttps://www.hcisingapore.gov.in/strandedregind Compelling Case Reason:-\nSelect whatever is best match. If not, select \u0026ldquo;Tourists/Visitors stranded Abroad\u0026rdquo; You should receive an email with PDF Attachment from HCI after completing the form. Take a printout of that PDF. Flight Ticket Booking FAQs It is recommended to book the flights with Air India and Air India Express. You can regularly check Air India Website Page for Evacuation Flights:\nhttp://www.airindia.in/evacuation-flight.htm\nhttp://www.airindia.in/EVACUATION-FLIGHT-SCHEDULE.htm You can also check Ministry of External Affairs, India Website for Full list of flights. These links are updated on regular basis. https://mea.gov.in/vande-Bharat-mission-list-of-flights.htm RT-PCR Test FAQs You need to do RT-PCR swab test within 72 hrs of departure List of MOH approved Covid-19 test provider using this link:-\nhttps://www.moh.gov.sg/licensing-and-regulation/regulations-guidelines-and-circulars/details/list-of-covid-19-swab-providers PDF of list of MOH approved labs:-\nhttps://www.moh.gov.sg/docs/librariesprovider5/covid19_test_providers/approved-covid-19-offsite-pcr-swab-providers02b1c233b76f4cddaad08dbe69d35574.pdf Links of some of the labs in singapore recommended by other travellers:- Cheapest in town:- https://www.sata.com.sg/pre-departure-swab-test/ Test at home:- https://bch-art.sg/ Vaccination Certificate FAQs You can only travel if you are fully vaccinated You can download the Vaccination Certificate from SG HealthHub or SG Notarise website.\nhttps://www.healthhub.sg/\nhttps://www.notarise.gov.sg/ You can login using username/password or using Singpass. Once login, you can download PDF file. You can navigate to SG HealthHub website from TraceTogether app or Singpass app as well. Air Suvidha Registration FAQs Once you book a flight ticket. Fill the Air Suvidha Self Reporting Form online:-\nhttps://www.newdelhiairport.in/airsuvidha/apho-registration. Do not confuse with the newdelhiairport.in url, This online form is mandatory for all indians international passengers arriving to India. You need to fill in your name, contact, passport number, and flight details. You also need to upload your passport, RT-PCR Negative Certificate, Vaccination Certificate. You will receive an Air Suvidha Registration Email with SR Number after filling the form. Download it, or print it. You will need this to show at the indian airport you are arriving. Baggage Allowance FAQs Air India Baggage allowance 30KG Check-in and 8KG Hand Luggage. Excess Baggage is allowed and chargeable at $28 Per KG. Air India Express Baggage allowance is 25KG Check-in and 7KG Hand Luggage. You can check-in another 10KG luggage at the cost of $40. Singapore Airport FAQs You are required to sign an Undertaking-cum-Indemnity Form and Embarkation Form at the Airport check-in counter. You will be required to report at indicated terminal of Changi International Airport minimum 3 hours before scheduled departure. Pregnant passengers between 32 and 35 weeks of pregnancy period are required to be in possession of Fit to Fly Certificate from a competent medical professional. Airline policy does not permit passengers of pregnancy period of 36 weeks and above to fly with Air India. You are requested to use private vehicle / taxi service for reaching Changi Airport (not to use MRT / Bus) Only bonafide passengers holding valid ticket (no accompanying individuals) will be permitted within Airport premises. Passengers requiring escort at Airport, owing to age, medical condition, etc may be permitted to be accompanied by one adult family member at the discretion of Changi Airport Management. In-flight FAQs Food packets including meal, snacks, water bottle, tissue papers will be kept at your seat. Face shield, mask hand sanitizer will also be kept at your seat. No or very less interaction with the crew India Airport FAQs After Landing, You walk towards the Immigration Counter. You need to show Air Suvidha Registration Email at India Airport. Soft copy would be enough. Your temperature will be checked. If you have temperature, you will be sidelined for further checks. After The immigration, proceed to baggage counter, and then customs. After clearing the custom. You will be asked to download Arogya Setu app. Singapore to India via Maldives FAQs Direct Singapore to India flights are not readily available and some people are choosing Singapore to India via Maldives route for emergency visit to India. Below is the experience of one of the traveller:-\nDocuments required Confirmed ticket for transit flight; layover in Male must be at least 4 hours Confirmed hotel booking for 1 night at Male Singapore Vaccination certificate notarised RT-PCR test results (maximum 72 hours before flight) Maldives Health Declaration - both arrival and departure - for each person travelling India Air Suvidha Form Passport IC / FIN India High Commission registration (optional) Travel Insurance (optional) (Please take print outs of all the above)\nSingapore airport experience The people at Singapore Airlines counter check all the above documents (except the two optional ones). It was straight forward, but some people who did not have confirmed hotel booking or 4 hour transit time were not allowed to board. They had to make changes to their schedule.\nMale airport experience Everyone arriving at Male was asked to clear out from immigration (passport was stamped) and collect their checked-in bags. Then they had to come back in through immigration, pass security procedures and check-in to the connecting flight in India. While this is a hassle, people who had a transit time of at least 4 hours were able to complete the above procedures easily.\nIndia airport experience The main things they checked were vaccination certificate, Singapore RT-PCR test result and Air Suvidha form. Immigration was quick and collection of bags took the normal time. They ask to do home quarantine.\nImportant Links Telegram groups to join High Commission of India, Singapore https://t.me/IndiatoSingapore Singapore to India https://t.me/sg2in\nMr. Gurmeet Singh and Mr. Robin Kapoor (GSA Air India) are active members of Singapore to India telegram group. Their contribution to answer your queries, provide latest updates is commendable. Some of the points in this post are shared by them. Facebook groups to join High Commission of India, Singapore https://www.facebook.com/IndiaInSingapore/ Twitter handles to join Air India https://twitter.com/airindiain Air India Express https://twitter.com/FlyWithIX High Commission of India, Singapore https://twitter.com/IndiainSingapor ","permalink":"https://codingnconcepts.com/others/singapore-to-india-vbm-faq/","tags":null,"title":"Singapore to India (Vande Bharat Mission) FAQs"},{"categories":["Hugo"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to manage multiple authors in Hugo Website\nOverview When you generate a blog website using Hugo static site generator then there might be a possibility that more then one author writing the content for your blog website.\nHugo doesn\u0026rsquo;t provide any out of the box support for multiple authors. In this post, We\u0026rsquo;ll see that how we can associate an author with each article. We\u0026rsquo;ll also see how we to show author name, a short description about author on each article.\nSupport for Multiple Authors Follow these steps to enable the multiple author support for your Hugo Website:\nCreate a Data file for Each Author In Hugo project structure, there should a data directory at root level, create one if it doesn\u0026rsquo;t exist.\nHugo provides the ability to access the files and their content in data directory, which we are going to use to fetch author\u0026rsquo;s details. From an organizational perspective, it is recommended to structure our directory as the following:\n▾ data ▾ authors ashishlahoti.json bobmarley.json charliechaplin.json davidbeckham.json ... Notice that we have created a JSON file for each individual author in data/authors directory. The naming convention doesn\u0026rsquo;t matter but it is recommended to use first name followed by last name for consistency.\nEach individual author JSON file for e.g. data/authors/ashishlahoti.json looks something like this:\n{ \u0026#34;name\u0026#34;: \u0026#34;Ashish Lahoti\u0026#34;, \u0026#34;bio\u0026#34;: \u0026#34;Ashish Lahoti has experience in full stack technologies such as Java, Spring Boot, JavaScript, CSS, HTML.\u0026#34;, \u0026#34;avatar\u0026#34;: \u0026#34;/media/authors/ashishlahoti.png\u0026#34;, \u0026#34;social\u0026#34;: { \u0026#34;linkedin\u0026#34;: \u0026#34;https://www.linkedin.com/in/lahotiashish\u0026#34; } } It is not necessary that you should follow the same JSON structure and use the same fields but it is recommended that whatever fields you add should be consistent across all your authors JSON files.\nNow that we have details of all the authors in data files, let\u0026rsquo;s use them.\nUse \u0026ldquo;author\u0026rdquo; in front-matter In Hugo, We generally write an article in markdown (.md) file and add metadata about the article in front-matter. A typical front-matter of an article looks like this:\n--- title: Multiple Authors Support in Hugo Website description: Understand how to manage multiple authors in Hugo Website date: 2020-07-26 author: ashishlahoti categories: - \u0026#34;Hugo\u0026#34; tags: - \u0026#34;authors\u0026#34; - \u0026#34;blogging\u0026#34; - \u0026#34;template\u0026#34; - \u0026#34;data\u0026#34; images: - \u0026#34;/img/logo/gohugo.png\u0026#34; --- In the above example, notice the author field and its value we\u0026rsquo;ve set. The value of the author field should match the author\u0026rsquo;s file name in data/authors directory. This is very important as we are going to get the author details from this file matching with author field value.\nDefault Author We\u0026rsquo;re also going to create a default author for the articles in case \u0026ldquo;author\u0026rdquo; is not provided in the front-matter for simplicity. Default author will be used as a fallback. We can create a data/authors/default.json file in data directory.\n▾ data ▾ authors default.json ... Example of the default.json file:\n{ \u0026#34;name\u0026#34;: \u0026#34;CodingNConcepts\u0026#34;, \u0026#34;bio\u0026#34;: \u0026#34;CodingNConcepts is a technical blog for developers from developers\u0026#34;, \u0026#34;avatar\u0026#34;: \u0026#34;/media/authors/codingnconcepts.png\u0026#34;, \u0026#34;social\u0026#34;: { \u0026#34;linkedin\u0026#34;: \u0026#34;https://www.linkedin.com/feed/hashtag/codingnconcepts/\u0026#34; } } Display Author Details in Generated Hugo Pages This is the interesting part. We can include the conditions like this to add author details:\n{{ $author := index .Site.Data.authors (.Params.author | default \u0026#34;default\u0026#34;) }} {{- if $author -}} {{ $author.name }} {{- end -}} Note that -\n.Site.Data.authors is used to fetch data from the data/authors directory .Params.author is used to get the value of the author from front-matter of the article | default \u0026quot;default\u0026quot; is used as a fallback in case author is not provided in the front-matter of the article $author.name will get the name field value from that particular author JSON file Practical Usage You can create a partial author to show author name in Hugo pages like this:\nlayouts/partials/author.html {{ $author := index .Site.Data.authors (.Params.author | default \u0026#34;ashishlahoti\u0026#34;) }} {{- if $author -}} \u0026lt;div class=\u0026#34;meta_item\u0026#34;\u0026gt; {{ partial \u0026#34;svg/author.svg\u0026#34; (dict \u0026#34;class\u0026#34; \u0026#34;meta_icon\u0026#34;) -}} \u0026lt;span class=\u0026#34;meta_text\u0026#34;\u0026gt;{{ $author.name }}\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; {{- end -}} The Author svg icon used as below:\nlayouts/partials/svg/author.svg \u0026lt;svg class=\u0026#34;{{ with .class }}{{ . }} {{ end }}icon icon-author\u0026#34; width=\u0026#34;16\u0026#34; height=\u0026#34;16\u0026#34; viewBox=\u0026#34;0 0 12 16\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M6 1c2.2 0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; You can also create a partial authorbox to show author details at the end of the page like this:\nlayouts/partials/authorbox.html {{- if .Param \u0026#34;authorbox\u0026#34; }} {{- $author := index .Site.Data.authors (.Params.author | default .Site.Author.code) -}} \u0026lt;div class=\u0026#34;authorbox\u0026#34;\u0026gt; {{- with $author.avatar }} \u0026lt;figure class=\u0026#34;authorbox__avatar\u0026#34;\u0026gt; \u0026lt;img alt=\u0026#34;{{ $author.name }} avatar\u0026#34; src=\u0026#34;{{ $author.avatar | relURL }}\u0026#34; class=\u0026#34;avatar\u0026#34; height=\u0026#34;90\u0026#34; width=\u0026#34;90\u0026#34;\u0026gt; \u0026lt;/figure\u0026gt; {{- end }} {{- with $author.name }} \u0026lt;div class=\u0026#34;authorbox__header\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;authorbox__name\u0026#34;\u0026gt;About {{ . }}\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; {{- end }} {{- with $author.bio }} \u0026lt;div class=\u0026#34;authorbox__description\u0026#34;\u0026gt; {{ . }} \u0026lt;/div\u0026gt; {{- end }} \u0026lt;/div\u0026gt; {{- end }} Show List of Articles of Individual Author Once you enable multiple author support for your Hugo Website. Next requirement comes where you want to display list of articles of individual author. Follow these steps:\nDefine an \u0026ldquo;author\u0026rdquo; taxonomy Define an \u0026ldquo;author\u0026rdquo; taxonomy similar to tag and category in your Hugo configuration file.\nconfig.toml [taxonomies] tag = \u0026#34;tags\u0026#34; category = \u0026#34;categories\u0026#34; author = \u0026#34;authors\u0026#34; Add \u0026ldquo;authors\u0026rdquo; field in front-matter Add the \u0026ldquo;authors\u0026rdquo; field in the front-matter of the article similar to tags and categories. Note that you can associate multiple authors in \u0026ldquo;authors\u0026rdquo; field just like you can have multiple tags and categories.\n--- title: Multiple Authors Support in Hugo Website description: Understand how to manage multiple authors in Hugo Website date: 2020-07-26 author: ashishlahoti categories: - \u0026#34;Hugo\u0026#34; tags: - \u0026#34;authors\u0026#34; - \u0026#34;blogging\u0026#34; - \u0026#34;template\u0026#34; - \u0026#34;data\u0026#34; images: - \u0026#34;/img/logo/gohugo.png\u0026#34; authors: - ashishlahoti --- If multiple authors have contributed to an article then \u0026ldquo;author\u0026rdquo; field can have primary author and \u0026ldquo;authors\u0026rdquo; field can have all the authors including primary author.\nThat\u0026rsquo;s it. You should be able to access the list of articles of a particular author using similar url:\nhttp://localhost:1313/authors/ashishlahoti/ Also list of all the authors would be accessible using this url:\nhttp://localhost:1313/authors/ Conclusion We saw that how we can manage multiple author details in their individual JSON files and how we can fetch the details from those files to display author name, bio (short description), avatar and social media contact details etc. We\u0026rsquo;ve also seen how we can show the list of articles of individual author.\n","permalink":"https://codingnconcepts.com/hugo/multiple-authors-hugo/","tags":null,"title":"Multiple Authors Support in Hugo Website"},{"categories":["Hugo"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to generate auto-numbering for headings, subheadings, and TOC in Hugo Pages.\nOverview When you generate a website using Hugo static site generator, It doesn\u0026rsquo;t provide any out of the box auto-numbering support for headings, subheadings, and TOC in Hugo Pages.\nThis blog website is also generated using Hugo and with few changes, we\u0026rsquo;re able to add auto-numbering support to our Hugo Pages.\nIf you\u0026rsquo;re still not clear what is auto-numbering all about then look at the auto incrementing numbers before the headings, subheadings and TOC in this post i.e. 1, 2, 2.1, 2.2, 2.3 etc.\n1. Overview 2. Auto-numbering Pages 2.1. Add “autonumbering” property in front-matter 2.2. Add CSS Class to single.html 2.3. Add CSS Styles to style.css Let\u0026rsquo;s discuss how to enable this feature.\nAuto-numbering Pages Follow these three steps to enable auto-numbering feature in your Hugo website:\nAdd \u0026ldquo;autonumbering\u0026rdquo; property in front-matter First of all, we\u0026rsquo;re going to add a custom property autonumbering in the front-matter of our Hugo pages.\nWe can enable or disable auto-numbering for a specific page using this property. If autonumbering: true, means it is enabled otherwise disabled.\ncontent/blog-page.md --- ... autonumbering: true --- Add CSS Class to single.html Next, we are going to edit layouts/_default/single.html file and add a condition on article element to add autonumbering CSS class based on autonumbering property is enabled or disabled.\nPlease note that if you are using any Hugo theme then you can find single.html in themes folder.\nlayouts/_default/single.html {{ define \u0026#34;main\u0026#34; }} \u0026lt;main\u0026gt; \u0026lt;article class=\u0026#34;post\u0026#34; {{- if .Param \u0026#34;autonumbering\u0026#34; }} autonumbering {{- end }}\u0026gt; \u0026lt;header\u0026gt; ... \u0026lt;/header\u0026gt;\t... \u0026lt;footer\u0026gt; ... \u0026lt;/footer\u0026gt; \u0026lt;/article\u0026gt; \u0026lt;/main\u0026gt; {{ end }} If autonumbering is enabled on any post. It will look like this:\n\u0026lt;main\u0026gt; \u0026lt;article class = \u0026#34;post autonumbering\u0026#34; \u0026gt;\u0026lt;/article\u0026gt; \u0026lt;/main\u0026gt; If autonumbering is disabled on any post. It will look like this:\n\u0026lt;main\u0026gt; \u0026lt;article class = \u0026#34;post\u0026#34; \u0026gt;\u0026lt;/article\u0026gt; \u0026lt;/main\u0026gt; Add CSS Styles to style.css At last, add the following CSS Styles snippet in the CSS files you would be using in your project. Location of css file may differ in your project.\nstatic/css/style.css /* Auto Numbering */ body {counter-reset: h2} h2 {counter-reset: h3} h3 {counter-reset: h4} h4 {counter-reset: h5} article[autonumbering] h2:before {counter-increment: h2; content: counter(h2) \u0026#34;. \u0026#34;} article[autonumbering] h3:before {counter-increment: h3; content: counter(h2) \u0026#34;.\u0026#34; counter(h3) \u0026#34;. \u0026#34;} article[autonumbering] h4:before {counter-increment: h4; content: counter(h2) \u0026#34;.\u0026#34; counter(h3) \u0026#34;.\u0026#34; counter(h4) \u0026#34;. \u0026#34;} article[autonumbering] .toc__menu ul { counter-reset: item } article[autonumbering] .toc__menu li a:before { content: counters(item, \u0026#34;.\u0026#34;) \u0026#34;. \u0026#34;; counter-increment: item } That\u0026rsquo;s it. You should be able to use autonumbering feature in your hugo pages now.\n","permalink":"https://codingnconcepts.com/hugo/auto-number-headings-hugo/","tags":null,"title":"Auto number Headings \u0026 TOC in Hugo Pages"},{"categories":null,"contents":"Hi Travellers,\nThough this is a technical blog, I thought of helping people, who are travelling from India to Singapore under VBM (Vande Bharat Mission). My wife was one of the traveller.\nHere is the list of frequently asked questions.\nAlso read Singapore to India (Vande Bharat Mission) FAQs\nChecklist to carry FAQs Congratulations on getting approval from MOM after multiple rejections and a flight ticket of course. Here is the checklist to carry before you start your journey.\nDocuments Checklist Document Remarks 1. Passport Should be valid for at least 6 months at the time of travelling 2. Valid EP/DP/IPA Required at India/Singapore Airport 3. Printout of Flight Ticket Booking Required at India Airport 4. Printout of MOM Entry Approval email Required at Singapore Airport 5. Printout of SHN Facility Sharing Approval email Required at Singapore Airport 6. Printout of SG Arrival Card - Health Declaration Required at Singapore Airport 7. Printout of Covid-19 PCR Test report Required at Singapore Airport 8. Printout of Health Declaration Form Required at India Airport, Download using this link, Printout is optional - same form will be provided at the airport to fill 9. Printout of Indemnity Quarantine Cost Form Required at India Airport, Download using this link, Printout is optional - same form will be provided at the airport to fill 10. Web Check-in and Boarding Pass Required at India Airport 11. Valid Singapore Phone number Which you have given at the time of Entry approval application Note: Taking printout of approvals and forms is not mandatory and soft copy also works. I recommend taking printout for a safer side.\nApps to Download \u0026amp; Install Apps Remarks 1. Arogya Setu Required at India Airport 2. TraceTogether Required at Singapore Airport 3. Homer Required in Singapore after you move to the SHN Facility i.e. Hotel Other Items Checklist Items Remarks 1. Thermometer You\u0026rsquo;ll need this in the hotel to measure your temperature everyday 2. Mask \u0026amp; Hand Sanitizer For your safety 3. Pen \u0026amp; few A4 Size Papers You\u0026rsquo;ll need this at both the airports MOM Entry Approval Application FAQs You cannot apply for yourself and your dependents. You need to request your employer (HR) to do so. MOM generally takes one day to reply. They inform the result to your employer via email. There are chances that your application would be rejected multiple times. Don\u0026rsquo;t lose hope. Keep trying. Keep requesting your employer (HR) to apply. Travel date needs to be mentioned in the application. Approval is valid for +-1 days i.e. If you have applied for the 22-Apr-2021 date then you can enter Singapore in between 21-Apr-2021 to 23-Apr-2021. You need to bear the total cost of $2200 for a 14-day stay at a dedicated SHN Hotel facility ($2000) and Covid-19 test ($200) If you are staying in the hotel room with your kid then you need to pay additional $600 per kid. For e.g. total cost of one adult with one kid in the hotel room is $2800. If you don\u0026rsquo;t have a valid singapore number then you can input your employer or relative mobile number in the application. After coming to singapore, you can buy new sim and update the phone number with MOM using this link: https://mom.gov.sg/feedbackshn\nAlternatively you can input your indian mobile number for which whatsapp will be working in singapore. MOM will call you on whatsapp. If you have cancelled your plan to come to Singapore after getting MOM Entry Approval then you need to ask your Employer to cancel the approval at least 7 days before your approved entry date. If you don\u0026rsquo;t do so, you still need to pay the full cost of Hotel Stay and Covid-19 test.\nPEP and Work Holiday pass holders can cancel the approval using this link: https://form.gov.sg/#!/5f39e47b344d060011507418 Flight Ticket Booking FAQs You can regularly check Air India Website Page for Evacuation Flights:- http://www.airindia.in/evacuation-flight.htm\nhttp://www.airindia.in/EVACUATION-FLIGHT-SCHEDULE.htm Check these links from Ministry of external affairs, India for full list of flights:- https://mea.gov.in/vande-Bharat-mission-list-of-flights.htm\nhttps://mea.gov.in/Phase-10.htm\nThese links are keep updating with latest flight details so it is advised to keep checking these links. Try to get the ticket in Air India or Air India Express. We have seen that some time other carriers cancel their flights at the last moment. Check the upcoming flights available and apply for entry approval for that particular date It is recommended that you book the flight ticket after getting entry approval. Though it is completely your choice. Send a copy of the flight ticket to your employer (HR) via mail once booked. If you cancel the flight ticket then you should get the refund back after deducting cancellation charges (Around INR 6000) within 2-4 months. You can call the customer care for cancellation and additionally can write an email to eCommerce@airindia.in and contactus@airindia.in stating the reason for cancellation. Special SHN Request FAQs You can apply for sharing SHN facility request or any other special request using this link: https://form.gov.sg/#!/5e79ee694c5bc7001181ce00 If you require a Separate SHN Facility such as Homestay because of medical or another critical condition then your employer (HR) needs to apply for this request. Your HR will write a mail to mfa@mfa.sg for this. Though there are very rare chances of getting approval until unless any extraordinary case. SG Arrival Card - Health Declaration FAQs It is mandatory to fill in Health Declaration form and carry printout:-\nhttps://eservices.ica.gov.sg/sgarrivalcard/ You can fill the form within 3 days before arrival to Singapore. This is the address you should fill in the form: Others ➔ SHN Dedicated Facility Covid-19 PCR Test FAQs You must take a COVID-19 polymerase chain reaction (PCR) test in India within 72 hours before departure. The test should be taken from laboratories which are internationally accredited or approved by the Indian Government (ICMR). Please follow the link to check ICMR approved labs:-\nhttps://covid.icmr.org.in/index.php/testing-facilities\nhttps://www.icmr.gov.in/pdf/covid/labs/archive/COVID_Testing_Labs_11062020.pdf The report must contain the following details: Full Name, Passport Number, Date of Birth, Age, Sex, Nationality, Time and Date of the swab collection, Method of testing (only PCR is accepted). Your report will be checked several times at the airport for these information. If some of the information is/are missing, you will be denied boarding. The airport authorities would ask you to call the testing center and ask for a complete report. Singapore citizens or permanent residents do not require this test. Inter-State Travel FAQs You can apply for state-wise e-pass for inter-state movement permission in case you are reaching at the airport from another state using this link: http://ficci.in/sector/130/add_Docs/State-wise-Links-for-e-passes.PDF Baggage Allowance FAQs International Flight Baggage for an international flight is 30KG Check-in and 7KG Hand Luggage. 10KG Check-in baggage is allowed on the Infant ticket. Maximum 2 nos of baggage can be Checked-in and 1 in Hand Luggage. Excess Baggage is allowed and cost INR 1500 Per KG. Domestic Flight Baggage for domestic flight is 20KG Check-in and 7KG Hand Luggage. Baby stroller is allowed as long as it is foldable (No Charge) India Airport FAQs Process You should reach at least 4 hours before the departure. At the terminal gate, you have to fill Two Heath Declaration Form and One Indemnity Quarantine Cost Form. You passport, flight ticket will be checked and your temperature will be recorded on your Health Declaration Form. Enter the terminal and proceed to the counter where you have to submit the Two Heath Declaration Form and One Indemnity Quarantine Cost Form. They will do stamping and give you one copy of the health form. Proceed to another counter where your all the documents including Singapore approval will be verified. Standard procedure hereafter - Boarding Pass, Security Check, Immigration. Before the boarding starts, Face shield will be given for window or aisle seat passengers and PPE suit will be given to middle seat passenger. If flight is not full then middle seat will remain empty. You should wear it before the boarding starts. Standard boarding procedure hereafter. Web Check-in Web Check-in is Mandatory for Air India Flight. Apply within 48 hours before the travel date using this link: https://ota.airindia.in/vandebharat/PassengerHealthUndertaking.aspx Sometimes Web Check-in fails or shows error. Call customer care for assistance. Sometimes Web Check-in is successful but boarding pass is not generated. Don\u0026rsquo;t worry, you can collect the boarding passes at the airport. In-flight FAQs Food packets including meal, snacks, water bottle, tissue papers will be kept at your seat. Extra water bottles were kept at the tail section near to the washroom (toilet). No or very less interaction with the crew Carry bag will also be kept at your seat. You should dispose the waste in the carry bag and take it with you at the time of alighting. Singapore Airport FAQs Process: At the immigration hall first they will do the introduction where SHN rules will be explained. Go to the officer and show your document where they will check your boarding pass, mom pre-entry approval letter, and health declaration form. Different counters are setup and you will be asked to stand in a queue. You have to provide your mobile number and they will send you a message to check your number is working or not. At the counter, they will verify your document and record it in there register and provide you a Hotel sticker which you have to stick on your cloth. Proceed for immigration. After that go to the baggage belt, your bags were already kept there. Collect your bags and proceed to the hotel counter. Register your name and wait until they form a group. An ambassador officer will be assigned to a group and he/she will take you outside the airport towards the bus. They will load your luggage in the bus. You will not get to know which hotel you are going to until you reach the hotel lobby. You can apply for the porter service at the Singapore Airport in advance if you need any baggage related assistance. Charges are around $3-$10. Make a reservation at least 8 hours before arrival using this link: https://smartecarte.com.sg/portal/ Singapore Hotel FAQs General Singapore Gov has booked several hotels across singapore to quarantine people coming from India and other counties. You cannot choose the hotel. Hotel will be allocated by Singapore Gov after your arrival at the airport. If you require any assistance during your hotel stay then apply using this link: https://form.gov.sg/#!/5f9a5d6b9ad89a0011c84802 Process Check-in is smooth and rooms will be already allocated once you arrived at the hotel. WiFi, Mini Fridge, Hot water kettle will be provided. Tea, Coffee, Water bottles, Shower gels, extra bedsheets will be stocked in your room to use for next 14 days. From 2nd day onwards, You will receive couple of calls from MOM on daily basis. On the 2nd or 3rd day, you will receive a message from MOM to download Homer app. You have to daily submit selfie and temperature 3 times in a day through Homer app. On the 5th day, your serology test will be done. Depending upon the result, you need/need not take the swab test on the 11th day. On the 11th day, your swab test will be done. You will be informed on 10th day via SMS about the location, date and time. The test is done via both nostrils and it is painless. You will get the results via sms within 48hrs. Food You will be provided Veg and Non-veg options from pre set menu. Breakfast, Dinner and Lunch meal box will be kept at your door for you to pick up. Please note that Indian food option is not available in most of the hotels. Adults cannot opt out from food options available at the hotel. Though you can opt out food for kids less then 3 years old. You need to write a mail to MOM (wpd_advisory@mom.gov.sg, mom_qops@mom.gov.sg) requesting to opt out for your kid. You will be refunded $600 on opting out for kids. You can order food from outside if you don\u0026rsquo;t like the food provided at the hotel. Cleaning No housekeeping. You need to do room and washroom cleaning by yourself in your 14 days stay, if you want to. You can carry the cleaning equipments, or order them online. Meet \u0026amp; Greet You cannot meet anyone including your family members during your stay at the hotel. Though someone can deliver items at the hotel reception for you. Hotel staff will keep those items outside your room for you to pick up. You cannot handover items to anyone including your family members (depends on the hotel if they allow to handover some important items such as documents and medicines). In such case you need to leave items outside your room for hotel staff to pick up. In short, in any case, you cannot leave your room and no one can come inside your room. Singapore Swab Test FAQs If you don\u0026rsquo;t get notification by 10th day about Swab test then fill this form https://mom.gov.sg/feedbackshn and also call SHN Hotline number (+65 68125555) to avoid any extension in your SHN in the hotel. After your swab test is done. You should get swab test result via SMS within 48 hrs. Alternatively you can login to https://www.healthhub.sg/ using your SingPass to check your test results. If you don\u0026rsquo;t receive Swab test results within 48 hrs (2 days) then fill this form https://mom.gov.sg/feedbackshn and also call SHN Hotline number (+65 68125555) to avoid any extension in your SHN in the hotel. Children under 12 years old will not be Swab tested as this is a bit painful process. However they will be Swab tested only if anyone accompanying them tested Covid-19 Positive. SHN Extension Your SHN might get extended even if you are tested Covid-19 negative If someone sitting near to you in the flight tested Covid-19 positive. Stay calm and follow the rules in such cases. Extension generally happens for 3 to 5 days. Covid-19 Positive Case If you find positive in swab test result then you will be transferred to the hospital and have to serve 14 to 21 days quarantine depending upon your health improvements. Singapore Gov bear the full cost of your treatment, if the last time you left singapore before 27 March 2020. You have to bear the full cost of the treatment, if the last time you left singapore after 27 March 2020. Important Links Department Email Hotline Number MOM wpd_advisory@mom.gov.sg +65 64385122 SHN ica_shnq@ica.gov.sg +65 68125555 Social Media to follow: You can join these telegram groups to get latest updates and post your queries High Commission of India, Singapore https://t.me/IndiatoSingapore IN to SG https://t.me/IndINSG You can join this facebook group to post your queries High Commission of India, Singapore https://www.facebook.com/IndiaInSingapore/ Follow these twitter handles to get latest updates on flights and other announcements: Air India https://twitter.com/airindiain Air India Express https://twitter.com/FlyWithIX High Commission of India, Singapore https://twitter.com/IndiainSingapor Please comment if you find something to be corrected, or added in this post. I\u0026rsquo;ll keep updating this post with more information.\n","permalink":"https://codingnconcepts.com/others/india-to-singapore-vbm-faq/","tags":null,"title":"India to Singapore (Vande Bharat Mission) FAQs"},{"categories":["Hugo"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to disable different kind of pages in Hugo website such as home, section, taxonomy, term, sitemap.xml, RSS feeds, robots.txt and 404 error pages.\nOverview When you generate a website using Hugo static site generator. Hugo auto generates different kind of pages for you which are generally required for a website.\nRefer this table for kind of pages Hugo support:\nKind Description Example home Home page of your website / section Section pages, which list all the pages rendered from a subdirectory in our content folder. /posts page Individual blog post pages /posts/my-post/ taxonomy Taxonomy e.g. Tags pages, which list all the tags of the website. /tags term Term pages e.g. \u0026ldquo;trending\u0026rdquo; tag, which list all the pages using this particular tag /tags/trending RSS RSS Feeds index.xml sitemap Sitemap of your website used for indexing pages of you website by search engines sitemap.xml 404 404 error page used when any URL not found in your website /unknownurl All these kind of pages are good to have for your website but sometime they are not required for a basic website.\nDisable Pages Good news is that you can disable any kind of page from the configuration using disableKinds which takes comma separated values. Possible values are: “page”, “home”, “section”, “taxonomy”, “term”, “RSS”, “sitemap”, “robotsTXT”, “404”.\nDisable Pages using config.toml This approach is recommended, when you want to disable pages permanently everytime you build your website.\nLet\u0026rsquo;s disable all the auto generated pages permanently by giving comma separated values in disableKinds configuration.\nconfig.toml title = \u0026#34;Hugo example site\u0026#34; baseurl = \u0026#34;https://www.example.com\u0026#34; disableKinds = [\u0026#34;page\u0026#34;, \u0026#34;home\u0026#34;, \u0026#34;section\u0026#34;, \u0026#34;taxonomy\u0026#34;, \u0026#34;term\u0026#34;, \u0026#34;RSS\u0026#34;, \u0026#34;sitemap\u0026#34;, \u0026#34;robotsTXT\u0026#34;, \u0026#34;404\u0026#34;] [taxonomies] category = \u0026#34;categories\u0026#34; tag = \u0026#34;tags\u0026#34; You can also prevent a specific page from generation by Hugo as follows:\nRSS Feed Hugo auto generate RSS Feed using built-in RSS 2.0 template for whole website, for each section, for each taxonomy term and for each taxonomy value pages. We can turn off the RSS Feed for all these pages as follows:\nconfig.toml disableKinds = [\u0026#34;RSS\u0026#34;] sitemap.xml Hugo auto generate sitemap.xml using built-in Sitemap Protocol v0.9 template for whole website. We can prevent sitemap.xml from generation as follows:\nconfig.toml disableKinds = [\u0026#34;sitemap\u0026#34;] robots.txt Hugo auto generate robots.txt for your website. We can turn off the robots.txt generation as follows:\nconfig.toml disableKinds = [\u0026#34;robotsTXT\u0026#34;] Alternatively we can turn off the enableRobotsTXT flag in config file\nconfig.toml enableRobotsTXT = false 404.html Hugo auto generate 404.html for your website. We can turn off the 404.html generation as follows:\nconfig.toml disableKinds = [\u0026#34;404\u0026#34;] Alternatively we can delete or rename 404.html file from /layouts/404.html location to turn it off.\nDisable Pages using command-line This approach is recommended, when you want to disable pages at runtime for a specific build of your website.\nLet\u0026rsquo;s see how we can disable all the auto generated pages in current build by executing the command from terminal:\nhugo --disableKinds=page,home,section,taxonomy,term,RSS,sitemap,robotsTXT,404 ","permalink":"https://codingnconcepts.com/hugo/disable-pages-hugo/","tags":null,"title":"Disable Pages in Hugo Website"},{"categories":["Hugo"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to disable taxonomy and term pages (such as Categories and Tags) in Hugo website.\nOverview When you generate a website using Hugo static site generator. Hugo automatically create taxonomies for tags and categories and generates the following pages by default:\nTaxonomy Pages for Categories /categories and Tags /tags which list out all the categories and tags of the website. Term Pages for Category e.g. /categories/technology and Tag e.g. /tags/trending which list out all the posts belongs to that category or tag. You can disable the default behavior of generating taxonomy and term pages.\nDisable Taxonomy and Term Pages Now you know about taxonomy and term pages. There are two ways to disable them:\nDisable using config.toml This approach is recommended, when you want to disable taxonomy and term pages permanently everytime you build your website.\nAdd the \u0026ldquo;taxonomy\u0026rdquo; and \u0026ldquo;term\u0026rdquo; values to the disableKinds configuration variable in your configuration file.\nconfig.toml title = \u0026#34;Hugo example site\u0026#34; baseurl = \u0026#34;https://www.example.com\u0026#34; disableKinds = [\u0026#34;taxonomy\u0026#34;, \u0026#34;term\u0026#34;] [taxonomies] category = \u0026#34;categories\u0026#34; tag = \u0026#34;tags\u0026#34; Disable using command-line This approach is recommended, when you want to disable taxonomy and term for a specific build of your website.\nExecute following command from terminal to disable them:\nhugo --disableKinds=taxonomy,term To serve website in the localhost environment, execute following command:\nhugo server --disableKinds=taxonomy,term ","permalink":"https://codingnconcepts.com/hugo/disable-taxonomy-pages-hugo/","tags":null,"title":"Disable Categories and Tags Pages in Hugo Website"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn error handling using try, catch, finally and throw statements. We\u0026rsquo;ll also learn about built-in JavaScript error objects (Error, SyntaxError, ReferenceError, etc.) and how to define custom errors.\nUsing try..catch..finally..throw We use try, catch, finally and throw keywords in error handling in JavaScript.\nThe try block wrap your code to check for errors. The throw keyword is used to throw custom errors. The catch block handle the caught errors. You chain catch block with try block. The finally block of code is always executed regardless of the result. You chain finally block with try and catch block.\ntry Every try block must be chained with at least one of the catch or finally block otherwise SyntaxError will be thrown.\nLet\u0026rsquo;s use try block alone to verify:\ntry { throw new Error(\u0026#39;Error while executing the code\u0026#39;); } ⓧ Uncaught SyntaxError: Missing catch or finally after try try..catch It is recommended to use try with catch block which handles the error gracefully thrown by try block.\ntry { throw new Error(\u0026#39;Error while executing the code\u0026#39;); } catch (err) { console.error(err.message); } ➤ ⓧ Error while executing the code try..catch with invalid code The try..catch cannot catch the exception of invalid JavaScript code, for example the below code in try block is syntactically wrong and cannot be caught by catch block.\ntry { ~!$%^\u0026amp;* } catch(err) { console.log(\u0026#34;code execution will never reach here\u0026#34;); } ➤ ⓧ Uncaught SyntaxError: Invalid or unexpected token try..catch with asynchronous code Similarly try..catch cannot catch the exception thrown inside asynchronous code which will be executed later such as setTimeout\ntry { setTimeout(function() { noSuchVariable; // undefined variable }, 1000); } catch (err) { console.log(\u0026#34;code execution will never reach here\u0026#34;); } Uncaught ReferenceError will be thrown after 1s\n➤ ⓧ Uncaught ReferenceError: noSuchVariable is not definedn We should use try..catch inside asynchronous code to handle the error gracefully in this way\nsetTimeout(function() { try { noSuchVariable; } catch(err) { console.log(\u0026#34;error is caught here!\u0026#34;); } }, 1000); Nested try..catch We can also use nested try and catch blocks and throw an error upwards like this:\ntry { try { throw new Error(\u0026#39;Error while executing the inner code\u0026#39;); } catch (err) { throw err; } } catch (err) { console.log(\u0026#34;Error caught by outer block:\u0026#34;); console.error(err.message); } Error caught by outer block: ➤ ⓧ Error while executing the code try..finally It is not recommended to use try with finally (without using catch block in between). Let\u0026rsquo;s see what happens:\ntry { throw new Error(\u0026#39;Error while executing the code\u0026#39;); } finally { console.log(\u0026#39;finally\u0026#39;); } finally ➤ ⓧ Uncaught Error: Error while executing the code We should note two things here:\nThe finally block is executed even after error is thrown from try block Error is not handled gracefully without catch block resulting in Uncaught Error try..catch..finally It is recommended to use try with catch block and optional finally block.\ntry { console.log(\u0026#34;Start of try block\u0026#34;); throw new Error(\u0026#39;Error while executing the code\u0026#39;); console.log(\u0026#34;End of try block -- never reached\u0026#34;); } catch (err) { console.error(err.message); } finally { console.log(\u0026#39;Finally block always run\u0026#39;); } console.log(\u0026#34;Code execution outside try-catch-finally block continue..\u0026#34;); Start of try block ➤ ⓧ Error while executing the code Finally block always run Code execution outside try-catch-finally block continue.. We should note two things here as well:\nThe code after throwing error in try block never reached. Error is handled gracefully this time by catch block. The finally block is executed even after error is thrown from try block. The finally block is generally used for cleaning up resources or closing the streams such as below:\ntry { openFile(file); readFile(file); } catch (err) { console.error(err.message); } finally { closeFile(file); } throw The throw statement is used to throw an exception.\nthrow \u0026lt;expression\u0026gt; // throw primitives and functions throw \u0026#34;Error404\u0026#34;; throw 42; throw true; throw {toString: function() { return \u0026#34;I\u0026#39;m an object!\u0026#34;; } }; // throw error object throw new Error(\u0026#39;Error while executing the code\u0026#39;); throw new SyntaxError(\u0026#39;Something is wrong with the syntax\u0026#39;); throw new ReferenceError(\u0026#39;Oops..Wrong reference\u0026#39;); // throw custom error object function ValidationError(message) { this.message = message; this.name = \u0026#39;ValidationError\u0026#39;; } throw new ValidationError(\u0026#39;Value too high\u0026#39;); Error handling in asynchronous Code It is recommended to use Promises and async await for asynchronous code (API calls) as they provide support for error handling.\nthen..catch with Promises You can chain multiple Promises using then() along with catch() to handle errors of individual promise in the chain like this:\nPromise.resolve(1) .then(res =\u0026gt; { console.log(res); // prints \u0026#39;1\u0026#39; throw new Error(\u0026#39;something went wrong\u0026#39;); // throw error return Promise.resolve(2); // code will not reach here }) .then(res =\u0026gt; { // code will not reach since promise not resolved in prev block here due to error console.log(res); }) .catch(err =\u0026gt; { console.error(err.message); // prints \u0026#39;something went wrong\u0026#39; return Promise.resolve(3); }) .then(res =\u0026gt; { console.log(res); // prints \u0026#39;3\u0026#39; }) .catch(err =\u0026gt; { // code will not reach since promise resolved in prev block console.error(err); }) Let\u0026rsquo;s look at the more practical example where we call an API using fetch which returns a promise object. We handle the API failure gracefully using catch block.\nfunction handleErrors(response) { if (!response.ok) { throw Error(response.statusText); } return response; } fetch(\u0026#34;http://httpstat.us/500\u0026#34;) .then(handleErrors) .then(response =\u0026gt; console.log(\u0026#34;ok\u0026#34;)) .catch(error =\u0026gt; console.log(\u0026#34;Caught\u0026#34;, error)); Caught Error: Internal Server Error at handleErrors (\u0026lt;anonymous\u0026gt;:3:15) try..catch with async await It is very easy to handle errors using try..catch when handling asynchronous with async await like this:\n(async function() { try { await fetch(\u0026#34;http://httpstat.us/500\u0026#34;); } catch (err) { console.error(err.message); } })(); Let\u0026rsquo;s look at the same example where we call an API using fetch which returns a promise object. We handle the API failure gracefully using try..catch block.\nfunction handleErrors(response) { if (!response.ok) { throw Error(response.statusText); } } (async function() { try { let response = await fetch(\u0026#34;http://httpstat.us/500\u0026#34;); handleErrors(response); let data = await response.json(); return data; } catch (error) { console.log(\u0026#34;Caught\u0026#34;, error) } })(); Caught Error: Internal Server Error at handleErrors (\u0026lt;anonymous\u0026gt;:3:15) at \u0026lt;anonymous\u0026gt;:11:7 Built-In JavaScript Errors Error JavaScript has built-in Error Object which is generally thrown by try block and caught in catch block.\nError Object consist of following properties:\nname: is the name of the error for e.g. \u0026ldquo;Error\u0026rdquo;, \u0026ldquo;SyntaxError\u0026rdquo;, \u0026ldquo;ReferenceError\u0026rdquo; etc. message: is the message about error details stack: is the stack trace of the error used for debugging purpose. Let\u0026rsquo;s create an Error Object and look at its name and message property:\nconst err = new Error(\u0026#39;Error while executing the code\u0026#39;); console.log(\u0026#34;name:\u0026#34;, err.name); console.log(\u0026#34;message:\u0026#34;, err.message); console.log(\u0026#34;stack:\u0026#34;, err.stack); name: Error message: Error while executing the code stack: Error: Error while executing the code at \u0026lt;anonymous\u0026gt;:1:13 JavaScript has following built-in errors which are inherited from the Error Object:\nEvalError The EvalError indicates an error regarding the global eval() function. This exception is not thrown by JavaScript anymore and it exist for backward compatibility.\nRangeError The RangeError is thrown when a value out of range.\n➤ [].length = -1 ⓧ Uncaught RangeError: Invalid array length ReferenceError The ReferenceError is thrown when a variable is referenced which does not exist.\n➤ x = x + 1; ⓧ Uncaught ReferenceError: x is not defined SyntaxError The SyntaxError is thrown when you have used any wrong syntax in JavaScript code.\n➤ function() { return \u0026#39;Hi!\u0026#39; } ⓧ Uncaught SyntaxError: Function statements require a function name ➤ 1 = 1 ⓧ Uncaught SyntaxError: Invalid left-hand side in assignment ➤ JSON.parse(\u0026#34;{ x }\u0026#34;); ⓧ Uncaught SyntaxError: Unexpected token x in JSON at position 2 TypeError The TypeError is thrown when the value is not of the expected type.\n➤ 1(); ⓧ Uncaught TypeError: 1 is not a function ➤ null.name; ⓧ Uncaught TypeError: Cannot read property \u0026#39;name\u0026#39; of null URIError The URIError is thrown when global URI handling function was used in a wrong way.\n➤ decodeURI(\u0026#34;%%%\u0026#34;); ⓧ Uncaught URIError: URI malformed Define and throw Custom Error We can also define our custom error in this way:\nclass CustomError extends Error { constructor(message) { super(message); this.name = \u0026#34;CustomError\u0026#34;; } }; const err = new CustomError(\u0026#39;Custom error while executing the code\u0026#39;); console.log(\u0026#34;name:\u0026#34;, err.name); console.log(\u0026#34;message:\u0026#34;, err.message); name: CustomError message: Custom error while executing the code We can further enhance our CustomError object to include error code as well:\nclass CustomError extends Error { constructor(message, code) { super(message); this.name = \u0026#34;CustomError\u0026#34;; this.code = code; } }; const err = new CustomError(\u0026#39;Custom error while executing the code\u0026#39;, \u0026#34;ERROR_CODE\u0026#34;); console.log(\u0026#34;name:\u0026#34;, err.name); console.log(\u0026#34;message:\u0026#34;, err.message); console.log(\u0026#34;code:\u0026#34;, err.code); name: CustomError message: Custom error while executing the code code: ERROR_CODE Let\u0026rsquo;s use this in try..catch block:\ntry{ try { null.name; }catch(err){ throw new CustomError(err.message, err.name); //message, code } }catch(err){ console.log(err.name, err.code, err.message); } CustomError TypeError Cannot read property \u0026#39;name\u0026#39; of null ","permalink":"https://codingnconcepts.com/javascript/error-handling-in-javascript/","tags":["Javascript Core"],"title":"Error Handling in JavaScript"},{"categories":["Tools"],"contents":" Enter root element font-size (default 16px)* px Enter font-size in pixel* px Converted rem rem ","permalink":"https://codingnconcepts.com/tools/pixel-to-rem/","tags":["CSS","Converter","REM"],"title":"Pixel to Rem Converter"},{"categories":["Tools"],"contents":" Enter color code in RGB R G B Converted Hex value Hex Color Note: You can also enter Hex value to convert to RGB\nFrequently used color codes Color Name #RRGGBB (Hex Code) R,G,B (Decimal code) \u0026nbsp; maroon #800000 (128,0,0) \u0026nbsp; dark red #8B0000 (139,0,0) \u0026nbsp; brown #A52A2A (165,42,42) \u0026nbsp; firebrick #B22222 (178,34,34) \u0026nbsp; crimson #DC143C (220,20,60) \u0026nbsp; red #FF0000 (255,0,0) \u0026nbsp; tomato #FF6347 (255,99,71) \u0026nbsp; coral #FF7F50 (255,127,80) \u0026nbsp; indian red #CD5C5C (205,92,92) \u0026nbsp; light coral #F08080 (240,128,128) \u0026nbsp; dark salmon #E9967A (233,150,122) \u0026nbsp; salmon #FA8072 (250,128,114) \u0026nbsp; light salmon #FFA07A (255,160,122) \u0026nbsp; orange red #FF4500 (255,69,0) \u0026nbsp; dark orange #FF8C00 (255,140,0) \u0026nbsp; orange #FFA500 (255,165,0) \u0026nbsp; gold #FFD700 (255,215,0) \u0026nbsp; dark golden rod #B8860B (184,134,11) \u0026nbsp; golden rod #DAA520 (218,165,32) \u0026nbsp; pale golden rod #EEE8AA (238,232,170) \u0026nbsp; dark khaki #BDB76B (189,183,107) \u0026nbsp; khaki #F0E68C (240,230,140) \u0026nbsp; olive #808000 (128,128,0) \u0026nbsp; yellow #FFFF00 (255,255,0) \u0026nbsp; yellow green #9ACD32 (154,205,50) \u0026nbsp; dark olive green #556B2F (85,107,47) \u0026nbsp; olive drab #6B8E23 (107,142,35) \u0026nbsp; lawn green #7CFC00 (124,252,0) \u0026nbsp; chart reuse #7FFF00 (127,255,0) \u0026nbsp; green yellow #ADFF2F (173,255,47) \u0026nbsp; dark green #006400 (0,100,0) \u0026nbsp; green #008000 (0,128,0) \u0026nbsp; forest green #228B22 (34,139,34) \u0026nbsp; lime #00FF00 (0,255,0) \u0026nbsp; lime green #32CD32 (50,205,50) \u0026nbsp; light green #90EE90 (144,238,144) \u0026nbsp; pale green #98FB98 (152,251,152) \u0026nbsp; dark sea green #8FBC8F (143,188,143) \u0026nbsp; medium spring green #00FA9A (0,250,154) \u0026nbsp; spring green #00FF7F (0,255,127) \u0026nbsp; sea green #2E8B57 (46,139,87) \u0026nbsp; medium aqua marine #66CDAA (102,205,170) \u0026nbsp; medium sea green #3CB371 (60,179,113) \u0026nbsp; light sea green #20B2AA (32,178,170) \u0026nbsp; dark slate gray #2F4F4F (47,79,79) \u0026nbsp; teal #008080 (0,128,128) \u0026nbsp; dark cyan #008B8B (0,139,139) \u0026nbsp; aqua #00FFFF (0,255,255) \u0026nbsp; cyan #00FFFF (0,255,255) \u0026nbsp; light cyan #E0FFFF (224,255,255) \u0026nbsp; dark turquoise #00CED1 (0,206,209) \u0026nbsp; turquoise #40E0D0 (64,224,208) \u0026nbsp; medium turquoise #48D1CC (72,209,204) \u0026nbsp; pale turquoise #AFEEEE (175,238,238) \u0026nbsp; aqua marine #7FFFD4 (127,255,212) \u0026nbsp; powder blue #B0E0E6 (176,224,230) \u0026nbsp; cadet blue #5F9EA0 (95,158,160) \u0026nbsp; steel blue #4682B4 (70,130,180) \u0026nbsp; corn flower blue #6495ED (100,149,237) \u0026nbsp; deep sky blue #00BFFF (0,191,255) \u0026nbsp; dodger blue #1E90FF (30,144,255) \u0026nbsp; light blue #ADD8E6 (173,216,230) \u0026nbsp; sky blue #87CEEB (135,206,235) \u0026nbsp; light sky blue #87CEFA (135,206,250) \u0026nbsp; midnight blue #191970 (25,25,112) \u0026nbsp; navy #000080 (0,0,128) \u0026nbsp; dark blue #00008B (0,0,139) \u0026nbsp; medium blue #0000CD (0,0,205) \u0026nbsp; blue #0000FF (0,0,255) \u0026nbsp; royal blue #4169E1 (65,105,225) \u0026nbsp; blue violet #8A2BE2 (138,43,226) \u0026nbsp; indigo #4B0082 (75,0,130) \u0026nbsp; dark slate blue #483D8B (72,61,139) \u0026nbsp; slate blue #6A5ACD (106,90,205) \u0026nbsp; medium slate blue #7B68EE (123,104,238) \u0026nbsp; medium purple #9370DB (147,112,219) \u0026nbsp; dark magenta #8B008B (139,0,139) \u0026nbsp; dark violet #9400D3 (148,0,211) \u0026nbsp; dark orchid #9932CC (153,50,204) \u0026nbsp; medium orchid #BA55D3 (186,85,211) \u0026nbsp; purple #800080 (128,0,128) \u0026nbsp; thistle #D8BFD8 (216,191,216) \u0026nbsp; plum #DDA0DD (221,160,221) \u0026nbsp; violet #EE82EE (238,130,238) \u0026nbsp; magenta / fuchsia #FF00FF (255,0,255) \u0026nbsp; orchid #DA70D6 (218,112,214) \u0026nbsp; medium violet red #C71585 (199,21,133) \u0026nbsp; pale violet red #DB7093 (219,112,147) \u0026nbsp; deep pink #FF1493 (255,20,147) \u0026nbsp; hot pink #FF69B4 (255,105,180) \u0026nbsp; light pink #FFB6C1 (255,182,193) \u0026nbsp; pink #FFC0CB (255,192,203) \u0026nbsp; antique white #FAEBD7 (250,235,215) \u0026nbsp; beige #F5F5DC (245,245,220) \u0026nbsp; bisque #FFE4C4 (255,228,196) \u0026nbsp; blanched almond #FFEBCD (255,235,205) \u0026nbsp; wheat #F5DEB3 (245,222,179) \u0026nbsp; corn silk #FFF8DC (255,248,220) \u0026nbsp; lemon chiffon #FFFACD (255,250,205) \u0026nbsp; light golden rod yellow #FAFAD2 (250,250,210) \u0026nbsp; light yellow #FFFFE0 (255,255,224) \u0026nbsp; saddle brown #8B4513 (139,69,19) \u0026nbsp; sienna #A0522D (160,82,45) \u0026nbsp; chocolate #D2691E (210,105,30) \u0026nbsp; peru #CD853F (205,133,63) \u0026nbsp; sandy brown #F4A460 (244,164,96) \u0026nbsp; burly wood #DEB887 (222,184,135) \u0026nbsp; tan #D2B48C (210,180,140) \u0026nbsp; rosy brown #BC8F8F (188,143,143) \u0026nbsp; moccasin #FFE4B5 (255,228,181) \u0026nbsp; navajo white #FFDEAD (255,222,173) \u0026nbsp; peach puff #FFDAB9 (255,218,185) \u0026nbsp; misty rose #FFE4E1 (255,228,225) \u0026nbsp; lavender blush #FFF0F5 (255,240,245) \u0026nbsp; linen #FAF0E6 (250,240,230) \u0026nbsp; old lace #FDF5E6 (253,245,230) \u0026nbsp; papaya whip #FFEFD5 (255,239,213) \u0026nbsp; sea shell #FFF5EE (255,245,238) \u0026nbsp; mint cream #F5FFFA (245,255,250) \u0026nbsp; slate gray #708090 (112,128,144) \u0026nbsp; light slate gray #778899 (119,136,153) \u0026nbsp; light steel blue #B0C4DE (176,196,222) \u0026nbsp; lavender #E6E6FA (230,230,250) \u0026nbsp; floral white #FFFAF0 (255,250,240) \u0026nbsp; alice blue #F0F8FF (240,248,255) \u0026nbsp; ghost white #F8F8FF (248,248,255) \u0026nbsp; honeydew #F0FFF0 (240,255,240) \u0026nbsp; ivory #FFFFF0 (255,255,240) \u0026nbsp; azure #F0FFFF (240,255,255) \u0026nbsp; snow #FFFAFA (255,250,250) \u0026nbsp; black #000000 (0,0,0) \u0026nbsp; dim gray / dim grey #696969 (105,105,105) \u0026nbsp; gray / grey #808080 (128,128,128) \u0026nbsp; dark gray / dark grey #A9A9A9 (169,169,169) \u0026nbsp; silver #C0C0C0 (192,192,192) \u0026nbsp; light gray / light grey #D3D3D3 (211,211,211) \u0026nbsp; gainsboro #DCDCDC (220,220,220) \u0026nbsp; white smoke #F5F5F5 (245,245,245) \u0026nbsp; white #FFFFFF (255,255,255) ","permalink":"https://codingnconcepts.com/tools/rgb-to-hex/","tags":["CSS","Converter"],"title":"RGB to HEX Converter"},{"categories":["CSS"],"contents":"In this tutorial, we\u0026rsquo;ll learn different units to measure font-size in CSS i.e. px, em, and rem, their differences, and which one to use when.\nDefault font-size Let\u0026rsquo;s understand the browser\u0026rsquo;s font-size first which is a key concept in difference between px, em, and rem.\nMost browsers provide an ability to change the font-size from the settings. Default font-size is 16px, which user can always change according to their preference as below:\nFont Size Setting in Chrome Browser\nYou can override the default font-size setting of the browser using CSS in this way:\nhtml { font-size: 32px; } or\nhtml { font-size: 200%; } Though it is not recommended to override default behavior of the browser otherwise it won\u0026rsquo;t honor user preference which is not a good idea.\nPX The pixel px is an absolute and fixed-size unit in CSS.\nPixels are easily translatable. For example using below CSS, font-size of p (paragraph) element will always remain 12px on all devices and screens regardless of changing the browser font-size setting, or any of its parent element\u0026rsquo;s font-size.\np { font-size: 12px; } Although the size of a pixel isn’t always the same across devices and screens, means the actual width of the block having width: 120px on laptop is not same as on iPad.\nWhat is the problem with px? The problem arises, when user changes the default font-size of the browser say they want to see bigger font-size. In that case p (paragraph) element with above CSS will still be displayed as font-size: 12px since its an absolute value. User preference is not reflected which is not considered as a good user experience.\nWhen to use px? Pixel px is still a good choice for fixed layout measurement and fixed spacing (padding, margin, etc.) but not a good choice for flexible layouts and font-size measurement.\nAlternate of px? The em and rem are the relative (or flexible) units as oppose to px, which is an absolute (or fixed) unit. Both em and rem are translated by the browser into pixel (px) values, depending on the default font-size setting of the browser. Say, if browser\u0026rsquo;s default font-size is 16px, then\n1em = 16px; 1rem = 16px; EM The em unit is relative to its direct or nearest parent element.\nIf font-size is not defined explicitly, that element will inherit it from the parent element. The inheritance continues to take place this way amongst ancestors up until the root element. Default font-size of the root element is provided by browser.\nWhen to use em? 1. Nested Structure\nWe can use the em where you want to apply font-size relative to the parent element such as menu structure.\n\u0026lt;style\u0026gt; .menu-container { font-size: 100px; border: 1px solid black; } .menu-item { font-size: 0.5em; padding-left: 20px; } .menu-item::before { content: \u0026#39;▾\u0026#39; } \u0026lt;/style\u0026gt; \u0026lt;div id=\u0026#34;container\u0026#34; class=\u0026#34;menu-container\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;menu\u0026#34; class=\u0026#34;menu-item\u0026#34;\u0026gt; Menu \u0026lt;div id=\u0026#34;submenu\u0026#34; class=\u0026#34;menu-item\u0026#34;\u0026gt; Submenu \u0026lt;div id=\u0026#34;subsubmenu\u0026#34; class=\u0026#34;menu-item\u0026#34;\u0026gt; Subsubmenu \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;another_subsubmenu\u0026#34;\u0026gt; Another Subsubmenu \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; Menu Submenu Subsubmenu Another Subsubmenu We see that font-size is reducing as we go to the nested levels of menu structure even though we have applied the same CSS class .menu-item to all nested menu items. Let\u0026rsquo;s break down how browser is calculating the the pixel (px) from rem relative values.\nThe #menu item font-size: 0.5em is relative to the #container so font-size in pixel would be 100x0.5 = 50px The #submenu item font-size: 0.5em is relative to the #menu so font-size in pixel would be 50x0.5 = 25px The #subsubmenu item font-size: 0.5em is relative to the #submenu so font-size in pixel would be 25x0.5 = 12.5px The #another_subsubmenu item class and font-size is not defined so font-size in pixel would be same as its parent #submenu i.e. 25px 2. Media Queries\nThe em should be used to define screen width in media queries. Here is an interesting post which explains why em should be used for media queries.\nWhat is the problem with em? The main problem with em is that you need to do all mathematical calculation of font-size of child elements as we did. Moreover, if you want to apply a specific font-size to child element, you cannot do that using em.\nAlternate of em? The rem is the solution of our problem. It is a relative unit and not dependent on parent elements. Let\u0026rsquo;s look at it.\nREM The rem unit is relative to the html (root) element.\nIf the font-size of root html element is 16px i.e.\n:root { font-size: 16px; } then\n1rem = 16px for all the elements.\nIf font-size is not explicitly defined in root element then 1rem will be equal to the default font-size provided by the browser (usually 16px).\nWhen to use rem? It is recommended to use rem for spacing (margin, padding, etc.) and font size in CSS as it honors user\u0026rsquo;s preferences and provide better user experience.\npx to rem when root is 16px you can use the following table to convert from px to rem when root font-size is 16px:\npx rem 10px 0.625rem 11px 0.6875rem 12px 0.75rem 13px 0.8125rem 14px 0.875rem 15px 0.9375rem 16px 1rem 17px 1.0625rem 18px 1.125rem 19px 1.1875rem 20px 1.25rem 21px 1.3125rem 22px 1.375rem 23px 1.4375rem 24px 1.5rem 25px 1.5625rem 26px 1.625rem 27px 1.6875rem 28px 1.75rem 29px 1.8125rem 30px 1.875rem 31px 1.9375rem 32px 2rem 33px 2.0625rem 34px 2.125rem 35px 2.1875rem 36px 2.25rem 37px 2.3125rem 38px 2.375rem 39px 2.4375rem 40px 2.5rem 41px 2.5625rem 42px 2.625rem 43px 2.6875rem 44px 2.75rem 45px 2.8125rem 46px 2.875rem 47px 2.9375rem 48px 3rem 49px 3.0625rem 50px 3.125rem 51px 3.1875rem 52px 3.25rem 53px 3.3125rem 54px 3.375rem 55px 3.4375rem 56px 3.5rem 57px 3.5625rem 58px 3.625rem 59px 3.6875rem 60px 3.75rem 61px 3.8125rem 62px 3.875rem 63px 3.9375rem 64px 4rem You can also calculate rem using Pixel to Rem Converter\nFinal Thoughts The best practice for the web developers is to:\nUse px for fixed size layout (width, height, etc.) or fixed spacing (margin, padding, etc.) Use em for nested elements (tree, menu, etc.) and media queries Use rem for flexible layout, spacing, and font-size ","permalink":"https://codingnconcepts.com/css/font-size-units-px-em-and-rem/","tags":["CSS","REM"],"title":"CSS units for font-size: px, em and rem"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll see the difference between Array.forEach() and Array.map() methods in JavaScript Array.\nThe forEach() and map() method are mostly used to iterate through an Array elements but there are few differences, We\u0026rsquo;ll look at them one by one.\nReturned Value The forEach() method returns undefined whereas map() returns a new array with transformed elements.\nLet\u0026rsquo;s find out the square of each element in an Array using these two methods:\nconst numbers = [1, 2, 3, 4, 5]; // using forEach() const squareUsingForEach = []; numbers.forEach(x =\u0026gt; squareUsingForEach.push(x*x)); // using map() const squareUsingMap = numbers.map(x =\u0026gt; x*x); console.log(squareUsingForEach); // [1, 4, 9, 16, 25] console.log(squareUsingMap); // [1, 4, 9, 16, 25] Since forEach() returns undefined, we need to pass an empty array to create a new transformed array. There is no such issue with map() method which returns the new transformed array directly. It is recommended to use map() method in such cases.\nChaining other methods The map() method output can be chained with other methods such as reduce(), sort(), filter() to perform multiple operations in a single statement.\nOn the other hand, forEach() is a terminal method means it cannot be chained with other methods since it returns undefined.\nLet\u0026rsquo;s find out the sum of square of each element in an Array using these two methods:\nconst numbers = [1, 2, 3, 4, 5]; // using forEach() const squareUsingForEach = [] let sumOfSquareUsingForEach = 0; numbers.forEach(x =\u0026gt; squareUsingForEach.push(x*x)); squareUsingForEach.forEach(square =\u0026gt; sumOfSquareUsingForEach += square); // using map() const sumOfSquareUsingMap = numbers.map(x =\u0026gt; x*x).reduce((total, value) =\u0026gt; total + value); console.log(sumOfSquareUsingForEach); // 55 console.log(sumOfSquareUsingMap); // 55 It is such a tedious job to use forEach() method when multiple operations are required. We can use map() method in such cases.\nPerformance We have created an array with 1 million random numbers (ranging from 1 to 1000). Let\u0026rsquo;s check the performance of each method.\n// Array: var numbers = []; for ( var i = 0; i \u0026lt; 1000000; i++ ) { numbers.push(Math.floor((Math.random() * 1000) + 1)); } // 1. forEach() console.time(\u0026#34;forEach\u0026#34;); const squareUsingForEach = []; numbers.forEach(x =\u0026gt; squareUsingForEach.push(x*x)); console.timeEnd(\u0026#34;forEach\u0026#34;); // 2. map() console.time(\u0026#34;map\u0026#34;); const squareUsingMap = numbers.map(x =\u0026gt; x*x); console.timeEnd(\u0026#34;map\u0026#34;); Here is the result after running the above code on MacBook Pro\u0026rsquo;s Google Chrome v83.0.4103.106 (64-bit). I suggest to copy the above code and try yourself in console.\nforEach: 26.596923828125ms map: 21.97998046875ms Clearly map() method performs better then forEach() for transforming elements.\nBreak the iteration This is not the difference between these two methods but important to know that there is no way to stop or break; the iteration if you are using forEach() or map() methods. The only way is to throw an exception from the callback function which may not be desired in most of the cases.\nIf we use break; statement within the callback function of forEach() or map() method,\nconst numbers = [1, 2, 3, 4, 5]; // break; inside forEach() const squareUsingForEach = []; numbers.forEach(x =\u0026gt; { if(x == 3) break; // \u0026lt;- SyntaxError squareUsingForEach.push(x*x); }); // break; inside map() const squareUsingMap = numbers.map(x =\u0026gt; { if(x == 3) break; // \u0026lt;- SyntaxError return x*x; }); JavaScript throws SyntaxError as follows:\nⓧ Uncaught SyntaxError: Illegal break statement If you need such behavior then you should use simple for loop or for-of / for-in loop.\nconst numbers = [1, 2, 3, 4, 5]; // break; inside for-of loop const squareUsingForEach = []; for(x of numbers){ if(x == 3) break; squareUsingForEach.push(x*x); }; console.log(squareUsingForEach); // [1, 4] Final Thoughts It is recommended to use map() to transform elements of an array since it is having short syntax, it\u0026rsquo;s chainable and has better performance.\nYou shouldn\u0026rsquo;t use map() if you\u0026rsquo;re not using returned array or not transforming elements of an array. It\u0026rsquo;s an anti-pattern; instead you should use forEach() method.\nAt last, if you want to stop or break the iteration of an array based on some condition then you should use simple for loop or for-of / for-in loop.\n","permalink":"https://codingnconcepts.com/javascript/difference-between-foreach-and-map-in-javascript-array/","tags":["Javascript Array"],"title":"Difference Between forEach() and map() in JavaScript Array"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to divide an Array in equal parts using Array.splice() method in JavaScript. We will also learn how it is different from Array.slice() method.\nDivide array in two equal parts We can divide an array in half in two steps:\nFind the middle index of the array using length/2 and Math.ceil() method, Get two equal parts of the array using this middle index and Array.splice() method const list = [1, 2, 3, 4, 5, 6]; const middleIndex = Math.ceil(list.length / 2); const firstHalf = list.splice(0, middleIndex); const secondHalf = list.splice(-middleIndex); console.log(firstHalf); // [1, 2, 3] console.log(secondHalf); // [4, 5, 6] console.log(list); // [] Array.splice() method changes the content of an array by removing, replacing or adding elements. Do not confuse this method with Array.slice() method which is used to make a copy of an array.\nlist.splice(0, middleIndex) removes first 3 elements starting from 0 index from an array and returns it. list.splice(-middleIndex) removes last 3 elements from an array and returns it. At the end of these two operations, since we have removed all the elements from the array, the original array is empty.\nAlso note that number of elements are even in the above case, in case of odd number of elements, first half will have one extra element.\nconst list = [1, 2, 3, 4, 5]; const middleIndex = Math.ceil(list.length / 2); list.splice(0, middleIndex); // returns [1, 2, 3] list.splice(-middleIndex); // returns [4, 5] Array.slice and Array.splice Sometime you wish not to alter the original array, this can also be done by chaining Array.slice() method with Array.splice()\nconst list = [1, 2, 3, 4, 5, 6]; const middleIndex = Math.ceil(list.length / 2); const firstHalf = list.slice().splice(0, middleIndex); const secondHalf = list.slice().splice(-middleIndex); console.log(firstHalf); // [1, 2, 3] console.log(secondHalf); // [4, 5, 6] console.log(list); // [1, 2, 3, 4, 5, 6]; We see that our original array remains the same since we make a copy of original array using Array.slice() before removing the elements using Array.splice().\nDivide array in three equal parts Let\u0026rsquo;s define an array and divide them in three equal parts using the Array.splice method.\nconst list = [1, 2, 3, 4, 5, 6, 7, 8, 9]; const threePartIndex = Math.ceil(list.length / 3); const thirdPart = list.splice(-threePartIndex); const secondPart = list.splice(-threePartIndex); const firstPart = list; console.log(firstPart); // [1, 2, 3] console.log(secondPart); // [4, 5, 6] console.log(thirdPart); // [7, 8, 9] Let\u0026rsquo;s break down how it happened:\nWe have first extracted the thirdPart using list.splice(-threePartIndex), which removes last 3 elements [7, 8, 9], at this point list contain only first 6 elements [1, 2, 3, 4, 5, 6]. Now we have extracted the secondPart using list.splice(-threePartIndex), which removes last 3 elements from remaining list = [1, 2, 3, 4, 5, 6] which is [4, 5, 6], at this point list contain only first 3 elements [1, 2, 3] which is firstPart. More about Array.splice Let\u0026rsquo;s look at more examples using Array.splice() method. Take note that Array.slice() has been used before Array.splice() because we do not want to alter the original array. You can omit the use of Array.slice() if you want to alter the original array in below examples.\n// Define an array const list = [1, 2, 3, 4, 5, 6, 7, 8, 9]; Get first element of an array list.slice().splice(0, 1); // [1] Get first five elements of an array list.slice().splice(0, 5); // [1, 2, 3, 4, 5] Get all elements after first five element of an array list.slice().splice(5); // [6, 7, 8, 9] Get last element of an array list.slice().splice(-1); // [9] Get last three elements of an array list.slice().splice(-3); // [7, 8, 9] ","permalink":"https://codingnconcepts.com/javascript/how-to-divide-array-in-equal-parts-in-javascript/","tags":["Javascript Array"],"title":"How to Divide an Array in Equal Parts in JavaScript"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to replace all occurrences of a string in JavaScript using String.replace() method. We will also look at String.split() and Array.join() approach.\nUsing String.replace Let\u0026rsquo;s take a quick look on the syntax:\nconst newStr = String.replace(\u0026#34;pattern\u0026#34;, \u0026#34;replacement\u0026#34;); The String.replace() method returns a new string after replacing the occurrences of matched pattern by replacement string.\nPattern can be String or RegEx Important point to note that the pattern can be a String or a RegEx.\nIf pattern is String, then only the first occurrence is replaced. If pattern is RegEx, then all the occurrence are replaced if global flag /g is used. You can also enable the case insensitive replacement by using /i flag. Let\u0026rsquo;s look at the example,\nconst lyrics = \u0026#34;Smelly cat, smelly cat what are they feeding you?\u0026#34;; console.log(lyrics.replace(\u0026#34;cat\u0026#34;, \u0026#34;kitty\u0026#34;)); //pattern is String console.log(lyrics.replace(/cat/, \u0026#34;kitty\u0026#34;)); //pattern is RegEx Output \u0026#34;Smelly kitty, smelly cat what are they feeding you?\u0026#34; \u0026#34;Smelly kitty, smelly cat what are they feeding you?\u0026#34; We see that pattern as String or RegEx both returned the same result and replaced only first occurrence.\nRegEx with global flag /g Let\u0026rsquo;s use the RegEx with global flag /g to replace all the occurrences.\nconst lyrics = \u0026#34;Smelly cat, smelly cat what are they feeding you?\u0026#34;; console.log(lyrics.replace(/cat/g, \u0026#34;kitty\u0026#34;)); Output \u0026#34;Smelly kitty, smelly kitty what are they feeding you?\u0026#34; RegEx with case insensitive flag /i Please note that above replacements are case sensitive, If we try this:\nconst lyrics = \u0026#34;Smelly cat, smelly cat what are they feeding you?\u0026#34;; console.log(lyrics.replace(/Smelly cat/g, \u0026#34;Sweet kitty\u0026#34;)); Output \u0026#34;Sweet kitty, smelly cat what are they feeding you?\u0026#34; We see that only first occurrence is replaced. We can enable case insensitive replacement by using /i flag in RegEx.\nconst lyrics = \u0026#34;Smelly cat, smelly cat what are they feeding you?\u0026#34;; console.log(lyrics.replace(/Smelly cat/gi, \u0026#34;Sweet kitty\u0026#34;)); Output \u0026#34;Sweet kitty, Sweet kitty what are they feeding you?\u0026#34; Yay! We are able to replace all occurrences of a string using Regex with global and case insensitive flag /\u0026lt;pattern\u0026gt;/gi\nUsing String.split and Array.join An alternative approach which is slower as compare to String.replace() is two step approach:-\nSplit the string using String.split() to remove the occurrence of matched (case sensitive) pattern and returns array, Join the array again using Array.join() with replacement string const newStr = String.split(\u0026#34;pattern\u0026#34;).join(\u0026#34;replacement\u0026#34;); Let\u0026rsquo;s look at the example.\nconst lyrics = \u0026#34;Smelly cat, smelly cat what are they feeding you?\u0026#34;; console.log(lyrics.split(\u0026#34;cat\u0026#34;).join(\u0026#34;kitty\u0026#34;)); Output \u0026#34;Smelly kitty, smelly kitty what are they feeding you?\u0026#34; ","permalink":"https://codingnconcepts.com/javascript/how-to-replace-all-occurrences-string-javascript/","tags":["Javascript String"],"title":"How to Replace all Occurrences of a String in JavaScript"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to reverse an Array in JavaScript using Array.reverse() method.\nArray.reverse() The easiest way to reverse an array is to use reverse() method.\nconst list = [1, 2, 3, 4, 5]; list.reverse(); console.log(list); // prints [5, 4, 3, 2, 1] Here is the catch, it reversed the elements of your original array. Sometime you wish not to alter the original array, and assign it to a new variable instead.\nYou can do in two steps:\nFirst you have to make a copy of the original array Reverse the copy and assign it to a new variable In Javascript, you can do this in multiple ways in a single statement.\nSpread Operator (\u0026hellip;) and Array.reverse() It is recommended to use spread operator ... to make a copy of an array and chain it with reverse() method. It’s short syntax is very handy.\nconst list = [1, 2, 3, 4, 5]; const reversedList = [...list].reverse(); console.log(list); // prints [1, 2, 3, 4, 5] console.log(reversedList); // prints [5, 4, 3, 2, 1] We see that our original array remains the same and reversed array is assigned to a new variable.\nArray.slice() and Array.reverse() You can also chain slice() method with reverse() method to make a new copy of reversed array.\nconst list = [1, 2, 3, 4, 5]; const reversedList = list.slice().reverse(); console.log(list); // prints [1, 2, 3, 4, 5] console.log(reversedList); // prints [5, 4, 3, 2, 1] Array.from() and Array.reverse() Another way is to chain Array.from() method with reverse() method to make a new copy of reversed array.\nconst list = [1, 2, 3, 4, 5]; const reversedList = Array.from(list).reverse(); console.log(list); // prints [1, 2, 3, 4, 5] console.log(reversedList); // prints [5, 4, 3, 2, 1] ","permalink":"https://codingnconcepts.com/javascript/how-to-reverse-array-in-javascript/","tags":["Javascript Array"],"title":"How to Reverse an Array in JavaScript"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn about Math Object in JavaScript, which allows us to use mathematical constants such as π, e, √2 and perform mathematical operations on numbers such as pow(), sqrt(), max(), min(), random(), abs(), ceil(), floor(), round(), and truc().\nMath Constants Math Object in JavaScript provides 8 mathematical constants that can be used:\nMath.E // returns 2.718281828459045 (Euler\u0026#39;s number) Math.PI // returns 3.141592653589793 (PI) Math.SQRT2 // returns 1.4142135623730951 (square root of 2) Math.SQRT1_2 // returns 0.7071067811865476 (square root of 1/2) Math.LN2 // returns 0.6931471805599453 (natural logarithm of 2) Math.LN10 // returns 2.302585092994046 (natural logarithm of 10 Math.LOG2E // returns 1.4426950408889634 (base 2 logarithm of E) Math.LOG10E // returns 0.4342944819032518 (base 10 logarithm of E) Calculate circumference of a circle Let\u0026rsquo;s use Math.PI constant to calculate the circumference 2πr of a circle given the radius r:\nfunction calculateCircumference(radius) { return 2 * Math.PI * radius; } calculateCircumference(10); // returns 62.83185307179586 Math Pow The Math.pow(a, b) function returns the a (base) to the power of b (exponent), i.e. ab.\nMath.pow(4, 3); // returns 64 i.e. 4×4×4 Math.pow(-4, 3); // returns -64 i.e. (-4)×(-4)×(-4) Math.pow(4, -3); // returns 0.015625 i.e. 1 ÷ (4 × 4 x 4) Math.pow(64, 0.5); // returns 8 i.e. 2√64 Math.pow(-64, 0.5); // returns NaN i.e. 2√-64 square root number cannot be negative Math.pow(64, -0.5); // returns 0.125 i.e. 1 ÷ 2√64 = 1/8 Let\u0026rsquo;s breakdown the result of above examples for clear understanding:\nMath.pow(a, b) ab breakdown result Math.pow(4, 3) 43 4 × 4 x 4 64 Math.pow(-4, 3) (-4)3 (-4) × (-4) x (-4) -64 Math.pow(4, -3) 4(-3) 1 ÷ (4 × 4 x 4) 0.015625 Math.pow(64, 0.5) 640.5 2√64 8 Math.pow(-64, 0.5) (-64)0.5 2√-64 NaN Math.pow(64, -0.5) 64(-0.5) 1 ÷ 2√64 0.125 Exponentiation Operator (**) It is very interesting to know that JavaScript ES6 provide shorthand syntax ** for Math power also known as exponentiation operator. So the above Math.pow examples are same as:\nconsole.log(4 ** 3); // returns 64 console.log((-4) ** 3); // returns -64 console.log(4 ** -3); // returns 0.015625 console.log(64 ** 0.5); // returns 8 console.log((-64) ** 0.5); // returns NaN console.log(64 ** -0.5); // returns 0.125 Please note that if you are using shorthand syntax with -ve base then you must wrap the base in parenthesis () to avoid SyntaxError.\n➤ -4 ** 3 Uncaught SyntaxError: Unary operator used immediately before exponentiation expression. Parenthesis must be used to disambiguate operator precedence ➤ (-4) ** 3 -64 Create square and cube methods using exponentiation operator Let\u0026rsquo;s create some math power methods using exponentiation operator **:\nconst square = x =\u0026gt; x ** 2; const cube = x =\u0026gt; x ** 3; square(4); // returns 16 cube(3); // returns 27 Math Sqrt The Math.sqrt(x) function returns the square root of the given number.\nMath.sqrt(0); // returns 0 Math.sqrt(null); // returns 0 Math.sqrt(25); // returns 5 Math.sqrt(1); // returns 1 Math.sqrt(0.5); // returns 0.7071067811865476 Math.sqrt(-5); // returns NaN as number cannot be negative Math.sqrt(NaN); // returns NaN as number cannot be NaN Create our own squareroot method Let\u0026rsquo;s create our own squareroot methods using ES6 exponentiation operator **:\nconst squareroot = x =\u0026gt; x ** 0.5; squareroot(0); // returns 0 squareroot(null); // returns 0 squareroot(25); // returns 5 squareroot(1); // returns 1 squareroot(0.5); // returns 0.707106781186547 squareroot(-5); // returns NaN as number cannot be negative squareroot(NaN); // returns NaN as number cannot be NaN Keep in mind that the square root of negative numbers doesn\u0026rsquo;t exist among the set of real numbers so JavaScript returns NaN. Square root of negative numbers is an imaginary number which is represented by i i.e. √-1 = i\nMath Max The Math.max(value1, value2, value3, ...) function returns the largest value out of given values.\nMath.max(1, 3, 2, 0, -1); // returns 3 If any argument cannot be converted to a number, NaN is returned.\nMath.max(1, 3, 2, \u0026#39;0\u0026#39;, -1); // returns 3 as \u0026#39;0\u0026#39; is converted to numeric 0 Math.max(1, 3, 2, \u0026#39;zero\u0026#39;, -1); // returns NaN as \u0026#39;zero\u0026#39; cannot be converted to numeric value Find the largest number from an array Let\u0026rsquo;s use Math.max function to find a largest number from an array in three ways:-\n① Using spread operator ...\nconst numbers = [1, 2, 3, 4, 5]; Math.max(...numbers); // returns 5 ② Using function prototype apply() method\nconst numbers = [1, 2, 3, 4, 5]; Math.max.apply(null, numbers); // returns 5 ③ Using Array.reduce() method\nconst numbers = [1, 2, 3, 4, 5]; numbers.reduce((a,b) =\u0026gt; Math.max(a,b)); // returns 5 However, both spread (...) and apply() will either fail or return the wrong result if the array has too many elements, because they try to pass the array elements as function parameters. The Array.reduce() method does not have this problem.\nMath Min The Math.min(value1, value2, value3, ...) function returns the smallest value out of given values.\nUsage example of Math.min is same as Math.max function.\nMath.min(1, 3, 2, 0, -1); // returns -1 Math.min(1, 3, 2, \u0026#39;0\u0026#39;, -1); // returns -1 Math.min(1, 3, 2, \u0026#39;zero\u0026#39;, -1); // returns NaN const numbers = [1, 2, 3, 4, 5]; Math.min(...numbers); // returns 1 Math.min.apply(null, numbers); // returns 1 numbers.reduce((a,b) =\u0026gt; Math.min(a,b)); // returns 1 Math Random The Math.random() functions returns a floating point random number between 0 (inclusive) to 1 (exclusive)\n0 ≤ Math.random() \u0026lt; 1 Let\u0026rsquo;s create some useful methods using Math.random() function:\nGenerate a random integer given the max range Let\u0026rsquo;s first create a method getRandomInt, which returns a random integer given the max range\nconst getRandomInt = (max) =\u0026gt; Math.floor(Math.random() * max) + 1; Let\u0026rsquo;s use this method to find a random user name from the given array\nconst userList = [\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;, \u0026#34;David\u0026#34;, \u0026#34;Eric\u0026#34;, \u0026#34;Franklin\u0026#34;, \u0026#34;Gavin\u0026#34;, \u0026#34;Harry\u0026#34;, \u0026#34;Iris\u0026#34;, \u0026#34;Joey\u0026#34;, \u0026#34;Kate\u0026#34;, \u0026#34;Leo\u0026#34;, \u0026#34;Monica\u0026#34;, \u0026#34;Nancy\u0026#34;, \u0026#34;Oscar\u0026#34;, \u0026#34;Phoebe\u0026#34;, \u0026#34;Quinn\u0026#34;, \u0026#34;Ross\u0026#34;, \u0026#34;Sofia\u0026#34;, \u0026#34;Tyler\u0026#34;, \u0026#34;Umar\u0026#34;, \u0026#34;Victor\u0026#34;, \u0026#34;Wilson\u0026#34;, \u0026#34;Xena\u0026#34;, \u0026#34;Yasmine\u0026#34;, \u0026#34;Zara\u0026#34;]; const getRandomUser = () =\u0026gt; userList[getRandomInt(userList.length)]; console.log(getRandomUser()); // Wilson console.log(getRandomUser()); // Charlie console.log(getRandomUser()); // Leo console.log(getRandomUser()); // Joey console.log(getRandomUser()); // Gavin Generate a random integer between two values (inclusive) Let\u0026rsquo;s first create a method getRandomIntInclusive, which returns a random integer between two values (inclusive)\nconst getRandomIntInclusive = (min, max) =\u0026gt; { min = Math.ceil(min); max = Math.floor(max); return Math.floor(Math.random() * (max - min + 1)) + min; }; Let\u0026rsquo;s use this method to generate an array of 10 elements with two-digits random numbers i.e. between 10 to 99\nvar numbers = []; var min = 10; var max = 99; for ( var i = 0; i \u0026lt; 10; i++ ) { numbers.push(getRandomIntInclusive(min, max)); } console.log(numbers); // [25, 55, 95, 51, 47, 10, 74, 86, 74, 77] Shuffle the elements of an array This is very interesting use of Math.random() function with Array.sort() functions to shuffle elements of an array. Let\u0026rsquo;s create our shuffleElements method:\nconst shuffleElements = (list) =\u0026gt; list.sort(() =\u0026gt; Math.random() - 0.5); The idea here is generate random number using Math.random() between -0.5 to 0.5 and feed it to Array.sort() function which sort an array based on returned value is +ve, or 0, or -ve\nLet\u0026rsquo;s use this method:\nconst list = [1, 2, 3, 4, 5, 6, 7, 8, 9]; console.log(shuffleElements(list)); // [6, 1, 2, 4, 3, 9, 5, 8, 7] console.log(shuffleElements(list)); // [9, 6, 8, 1, 2, 7, 4, 5, 3] console.log(shuffleElements(list)); // [6, 7, 3, 1, 2, 5, 8, 9, 4] Math Abs The Math.abs(x) function returns the absolute of the a number i.e. |x|\nMath.abs(\u0026#39;-1\u0026#39;); // 1 Math.abs(-2); // 2 Math.abs(null); // 0 Math.abs(\u0026#39;\u0026#39;); // 0 Math.abs([]); // 0 Math.abs([2]); // 2 Math.abs([1,2]); // NaN Math.abs({}); // NaN Math.abs(\u0026#39;string\u0026#39;); // NaN Math.abs(); // NaN Find absolute difference between two numbers Let\u0026rsquo;s use Math.abs() to find the absolute difference between two given numbers:\nconst difference = (a, b) =\u0026gt; Math.abs(a-b); console.log(difference(3, 5)); // 2 console.log(difference(5, 3)); // 2 Math Ceil The Math.ceil(x) function is used to round off the given number towards ceiling means upward direction.\nx ≤ Math.ceil(x) ≤ x+1 Math.ceil(0.95); // 1 Math.ceil(45.95); // 46 Math.ceil(45.05); // 46 Math.ceil(-0.95); // -0 Math.ceil(-45.05); // -45 Math.ceil(-45.95); // -45 Math.ceil(null); // 0 Math Floor The Math.floor(x) function is used to round off the given number towards floor means downward direction.\nx-1 ≤ Math.floor(x) ≤ x Math.floor(0.95); // 0 Math.floor(45.95); // 45 Math.floor(45.05); // 45 Math.floor(-0.95); // -1 Math.floor(-45.05); // -46 Math.floor(-45.95); // -46 Math.floor(null); // 0 Get a fraction part of a number Let\u0026rsquo;s create a method fraction to get a fraction part of the number using Math.floor() and Math.abs() functions:\nconst fraction = (x) =\u0026gt; Math.abs(x) - Math.floor(Math.abs(x)); fraction(45.95); // 0.95 fraction(45.05); // 0.05 fraction(-45.95); // 0.95 fraction(-45.05); // 0.05 Get quotient and remainder of a division of two whole numbers The Math.floor() and % can be used find quotient and remainder of a division of two whole numbers respectively:\nvar dividend = 14; var divisor = 3; console.log(\u0026#34;quotient\u0026#34;, Math.floor(dividend/divisor)); console.log(\u0026#34;remainder\u0026#34;, dividend % divisor); // quotient 4 // remainder 2 Math Round The Math.round(x) function is used to round off the given number to the nearest integer in any direction, upward or downward.\nconsole.log(Math.round(0.9)); // 1 console.log(Math.round(5.95), Math.round(5.5), Math.round(5.05)); // 6 6 5 console.log(Math.round(-5.05), Math.round(-5.5), Math.round(-5.95)); // -5 -5 -6 Math Trunc The Math.trunc(x) function returns the integer part of a number by removing decimal part.\nMath.trunc(0.123); // 0 Math.trunc(12.34); // 12 Math.trunc(-0.123); // -0 Math.trunc(-12.34); // -12 Math.trunc(\u0026#39;foo\u0026#39;); // NaN Note that Math.trunc(x) function do not apply any rounding logic and simply truncate the dot and decimal part of the number.\nCreate our own truncate method Let\u0026rsquo;s create our own truncate methods using Math.ceil() and Math.floor() functions:\nconst truncate = (x) =\u0026gt; x \u0026lt; 0 ? Math.ceil(x) : Math.floor(x); truncate(0.123); // 0 truncate(12.34); // 12 truncate(-0.123); // -0 truncate(-12.34); // -12 truncate(\u0026#39;foo\u0026#39;); // NaN Remaining Math Functions We have covered most of the frequently used Math Object functions in JavaScript in details. These are the list of remaining Math functions which might be required in complex mathematical calculation.\nMethod Description acos(x) Returns the arccosine of x, in radians acosh(x) Returns the hyperbolic arccosine of x asin(x) Returns the arcsine of x, in radians asinh(x) Returns the hyperbolic arcsine of x atan(x) Returns the arctangent of x as a numeric value between -PI/2 and PI/2 radians atan2(y, x) Returns the arctangent of the quotient of its arguments atanh(x) Returns the hyperbolic arctangent of x cbrt(x) Returns the cubic root of x cos(x) Returns the cosine of x (x is in radians) cosh(x) Returns the hyperbolic cosine of x exp(x) Returns the value of ex, where e is Euler\u0026rsquo;s number expm1(x) Returns the value of (ex-1), where e is Euler\u0026rsquo;s number log(x) Returns the natural logarithm (base e) of x sin(x) Returns the sine of x (x is in radians) sinh(x) Returns the hyperbolic sine of x tan(x) Returns the tangent of an angle tanh(x) Returns the hyperbolic tangent of a number ","permalink":"https://codingnconcepts.com/javascript/math-constants-and-functions-in-javascript/","tags":["Javascript Core"],"title":"Math Constants and Functions in JavaScript"},{"categories":["Spring Boot"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to map Jackson JSON request and response in Spring Boot Application with various Jackson configurations.\nJackson JSON ObjectMapper When you create a @RestController in a Spring Boot application to define API endpoints then Jackson JSON ObjectMapper is the default HTTP Converter of your web application which does two things:\nMap the Java Object to JSON Response when you return the Object from GET request method like this:-\n@GetMapping public List\u0026lt;User\u0026gt; getAllUsers() Converting the Java Object to JSON is known as Marshalling or Serialization\nMap the JSON to Java Object when you add a @RequestBody argument in POST request method like this:-\n@PostMapping public Long createUser(@RequestBody User user) Converting the JSON to Java Object is known as Unmarshalling or Deserialization\nWe will look at various configurations of serialization and deserialization using Jackson ObjectMapper with examples that can help to resolve common issues related to JSON mapping in API development.\nAPI Development Let\u0026rsquo;s quickly develop some GET and POST APIs using the Controller class UserController, we will use this to demonstrate various Jackson configurations and their impact on JSON.\n@RestController @RequestMapping(\u0026#34;/users\u0026#34;) public class UserController { @Autowired private UserService userService; @GetMapping public List\u0026lt;User\u0026gt; getAllUsers() { return userService.getAllUsers(); } @GetMapping(\u0026#34;/{id}\u0026#34;) public User getUserById(@PathVariable Long id) { return userService.getUserById(id); } @PostMapping @ResponseStatus(HttpStatus.CREATED) public Long createUser(@RequestBody User user) { return userService.createUser(user); } } public class User { private Long id; private String name; private LocalDate dateOfBirth; /* Getters and Setters */\t} Let\u0026rsquo;s look at various important configurations and their impact on API requests and responses.\nPrevent Failure on Unknown Property in JSON Request Body If there are unknown properties in JSON Request Body that cannot be mapped to User Java Object then Jackson ObjectMapper throws UnrecognizedPropertyException. This is a default behavior.\nspring.jackson.deserialization.FAIL_ON_UNKNOWN_PROPERTIES = true (default) Let\u0026rsquo;s add additional field gender in POST request body which is missing in User request mapping java Object. It will throw Json parse error Unrecognized field \u0026#34;gender\u0026#34; as in below example:-\napplication.yml spring.jackson.deserialization.FAIL_ON_UNKNOWN_PROPERTIES: true Request curl -X POST \\ http://localhost:8080/users \\ -H \u0026#39;cache-control: no-cache\u0026#39; \\ -H \u0026#39;content-type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Ashish\u0026#34;, \u0026#34;dateOfBirth\u0026#34;: \u0026#34;1986-08-22\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;male\u0026#34; }\u0026#39; Response (Error) JSON parse error: Unrecognized field \u0026#34;gender\u0026#34; (class com.example.api.model.User), not marked as ignorable; nested exception is com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException: Unrecognized field \u0026#34;gender\u0026#34; (class com.example.api.model.User), not marked as ignorable (3 known properties: \u0026#34;dateOfBirth\u0026#34;, \u0026#34;id\u0026#34;, \u0026#34;name\u0026#34;]) We can prevent this failure by setting the FAIL_ON_UNKNOWN_PROPERTIES property to false and allowing unknown properties (or additional fields) in our JSON Request Body. If you are using Spring Boot default ObjectMapper then this is the default behavior.\nspring.jackson.deserialization.FAIL_ON_UNKNOWN_PROPERTIES = false We see that POST request did not fail this time and returned id of created user.\napplication.yml spring.jackson.deserialization.FAIL_ON_UNKNOWN_PROPERTIES: false Request curl -X POST \\ http://localhost:8080/users \\ -H \u0026#39;cache-control: no-cache\u0026#39; \\ -H \u0026#39;content-type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Ashish\u0026#34;, \u0026#34;dateOfBirth\u0026#34;: \u0026#34;1986-08-22\u0026#34;, \u0026#34;gender\u0026#34;: \u0026#34;male\u0026#34; }\u0026#39; Response 1 You can also programmatically deserialize a JSON to Java Object with unknown properties using Jackson ObjectMapper like this:-\nObjectMapper mapper = new ObjectMapper().configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false); User user = mapper.readValue(jsonAsString, User.class); Don\u0026rsquo;t allow certain property in JSON Request Body Sometimes we don\u0026rsquo;t want certain properties such as id to be passed in the request body because we would be auto-generating that id in the backend. In such case, you can annotate such properties with @JsonIgnore.\n@JsonIgnore Annotation package com.example.api.model; public class User { @JsonIgnore private Long id; private String name; private LocalDate dateOfBirth; /* Getters and Setters */\t} spring.jackson.deserialization.FAIL_ON_IGNORED_PROPERTIES = false (default) By default, Jackson ObjectMapper allows all the properties to be passed in the request body even those annotated with @JsonIgnore. We see that it allows id to be passed in the request body:-\napplication.yml spring.jackson.deserialization.FAIL_ON_IGNORED_PROPERTIES: false Request curl -X POST \\ http://localhost:8080/users \\ -H \u0026#39;cache-control: no-cache\u0026#39; \\ -H \u0026#39;content-type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Ashish\u0026#34;, \u0026#34;dateOfBirth\u0026#34;: \u0026#34;1986-08-22\u0026#34; }\u0026#39; Response 1 spring.jackson.deserialization.FAIL_ON_IGNORED_PROPERTIES = true Setting FAIL_ON_IGNORED_PROPERTIES property to true will throw IgnoredPropertyException when id property which is annotated with @JsonIgnore is passed in request body:-\napplication.yml spring.jackson.deserialization.FAIL_ON_IGNORED_PROPERTIES: true Request curl -X POST \\ http://localhost:8080/users \\ -H \u0026#39;cache-control: no-cache\u0026#39; \\ -H \u0026#39;content-type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Ashish\u0026#34;, \u0026#34;dateOfBirth\u0026#34;: \u0026#34;1986-08-22\u0026#34; }\u0026#39; Response (Error) JSON parse error: Ignored field \u0026#34;id\u0026#34; (class com.example.api.model.User) encountered; mapper configured not to allow this; nested exception is com.fasterxml.jackson.databind.exc.IgnoredPropertyException: Ignored field \u0026#34;id\u0026#34; (class com.example.api.model.User) encountered; mapper configured not to allow this (3 known properties: \u0026#34;dateOfBirth\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;lastLogin\u0026#34;]) Don\u0026rsquo;t include properties with null value in JSON Response We generally do not want to include the properties with NULL values in JSON response. It does make sense to exclude them to reduce the network load.\nspring.jackson.default-property-inclusion: use_defaults (default) Jackson includes the null properties in the JSON response by default.\napplication.yml spring.jackson.default-property-inclusion: use_defaults Request curl -X GET http://localhost:8080/users Response [{\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Adam\u0026#34;,\u0026#34;dateOfBirth\u0026#34;:null}, {\u0026#34;id\u0026#34;:2,\u0026#34;name\u0026#34;:\u0026#34;Bob\u0026#34;,\u0026#34;dateOfBirth\u0026#34;:null}, {\u0026#34;id\u0026#34;:3,\u0026#34;name\u0026#34;:\u0026#34;Charlie\u0026#34;,\u0026#34;dateOfBirth\u0026#34;:null}] spring.jackson.default-property-inclusion: non_null We can set the application property spring.jackson.default-property-inclusion to non_null to not include null properties.\napplication.yml spring.jackson.default-property-inclusion: non_null Request curl -X GET http://localhost:8080/users Response [{\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Adam\u0026#34;}, {\u0026#34;id\u0026#34;:2,\u0026#34;name\u0026#34;:\u0026#34;Bob\u0026#34;}, {\u0026#34;id\u0026#34;:3,\u0026#34;name\u0026#34;:\u0026#34;Charlie\u0026#34;}] @JsonInclude Annotation For older versions of Spring boot, where the property spring.jackson.default-property-inclusion doesn\u0026rsquo;t work, you can use @JsonInclude(Include.NON_NULL) annotation at the class or field level.\npackage com.example.api.model; public class User { private Long id; private String name; @JsonInclude(JsonInclude.Include.NON_NULL) private LocalDate dateOfBirth; /* Getters and Setters */\t} Please note that field-level annotation overrides the class-level annotation, and class-level annotation overrides the application-level property.\nPretty Print JSON Response Pretty print or formatted JSON response in the development environment makes it easier to read and debug API responses.\nspring.jackson.serialization.INDENT_OUTPUT = false (default) API JSON responses are not pretty print (formatted) by default.\napplication.yml spring.jackson.serialization.INDENT_OUTPUT: false Request curl -X GET http://localhost:8080/users Response [{\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Adam\u0026#34;,\u0026#34;dateOfBirth\u0026#34;:\u0026#34;1950-01-01\u0026#34;},{\u0026#34;id\u0026#34;:2,\u0026#34;name\u0026#34;:\u0026#34;Bob\u0026#34;,\u0026#34;dateOfBirth\u0026#34;:\u0026#34;1990-10-30\u0026#34;},{\u0026#34;id\u0026#34;:3,\u0026#34;name\u0026#34;:\u0026#34;Charlie\u0026#34;,\u0026#34;dateOfBirth\u0026#34;:\u0026#34;1979-07-26\u0026#34;}] spring.jackson.serialization.INDENT_OUTPUT = true We can pretty print JSON Response by setting INDENT_OUTPUT property to true . We see that JSON response is formatted after that.\napplication.yml spring.jackson.serialization.INDENT_OUTPUT: true Request curl -X GET http://localhost:8080/users Response (Pretty Print) [ { \u0026#34;id\u0026#34; : 1, \u0026#34;name\u0026#34; : \u0026#34;Adam\u0026#34;, \u0026#34;dateOfBirth\u0026#34; : \u0026#34;1950-01-01\u0026#34; }, { \u0026#34;id\u0026#34; : 2, \u0026#34;name\u0026#34; : \u0026#34;Bob\u0026#34;, \u0026#34;dateOfBirth\u0026#34; : \u0026#34;1990-10-30\u0026#34; }, { \u0026#34;id\u0026#34; : 3, \u0026#34;name\u0026#34; : \u0026#34;Charlie\u0026#34;, \u0026#34;dateOfBirth\u0026#34; : \u0026#34;1979-07-26\u0026#34; } ] Format Date and DateTime properties in JSON Response Java Date and Time Objects such as Date, LocalDate, LocalDateTime, ZonedDateTime are converted into numeric timestamps by default during JSON serialization.\nspring.jackson.serialization.WRITE_DATES_AS_TIMESTAMPS = true (default) application.yml spring.jackson.serialization.WRITE_DATES_AS_TIMESTAMPS: true Request curl -X GET http://localhost:8080/users/1 Response { \u0026#34;id\u0026#34; : 1, \u0026#34;name\u0026#34; : \u0026#34;Adam\u0026#34;, \u0026#34;dateOfBirth\u0026#34; : [ 1950, 1, 1 ], \u0026#34;lastLogin\u0026#34; : [ 2020, 7, 3, 0, 26, 22, 211000000 ], \u0026#34;zonedDateTime\u0026#34;: 1622874073.231148 } We can disable this behavior to allow Jackson to convert Date and Time types of objects into human-readable String format. Please note that if you are using Spring Boot\u0026rsquo;s default ObjectMapper then WRITE_DATES_AS_TIMESTAMPS property is set to false by default.\nspring.jackson.serialization.WRITE_DATES_AS_TIMESTAMPS = false application.yml spring.jackson.serialization.WRITE_DATES_AS_TIMESTAMPS: false Request curl -X GET http://localhost:8080/users/1 Response { \u0026#34;id\u0026#34; : 1, \u0026#34;name\u0026#34; : \u0026#34;Adam\u0026#34;, \u0026#34;dateOfBirth\u0026#34; : \u0026#34;1950-01-01\u0026#34;, \u0026#34;lastLogin\u0026#34; : \u0026#34;2020-07-03T00:28:32.394\u0026#34; \u0026#34;zonedDateTime\u0026#34;: \u0026#34;2021-06-05T14:22:45.066295+08:00\u0026#34; } We see that date and time fields are in human-readable format in JSON response after disabling this feature.\n@JsonFormat Annotation We can further customize the Date, LocalDate, LocalDateTime, ZonedDateTime Object types by annotating them with @JsonFormat annotations in our User Object Model.\npackage com.example.api.model; public class User { private Long id; private String name; @JsonFormat(pattern=\u0026#34;dd MMM yyyy\u0026#34;) private LocalDate dateOfBirth; @JsonFormat(pattern=\u0026#34;dd MMM yyyy hh:mm:ss\u0026#34;) private LocalDateTime lastLogin; @JsonFormat(pattern = \u0026#34;yyyy-MM-dd@HH:mm:ss.SSSXXX\u0026#34;, locale = \u0026#34;en_SG\u0026#34;, timezone = \u0026#34;Asia/Singapore\u0026#34;) private ZonedDateTime zonedDateTime; /* Getters and Setters */\t} Output will be something like this after applying @JsonFormat annotations:\napplication.yml spring.jackson.serialization.WRITE_DATES_AS_TIMESTAMPS: false Request curl -X GET http://localhost:8080/users/1 Response { \u0026#34;id\u0026#34; : 1, \u0026#34;name\u0026#34; : \u0026#34;Adam\u0026#34;, \u0026#34;dateOfBirth\u0026#34; : \u0026#34;01 Jan 1950\u0026#34;, \u0026#34;lastLogin\u0026#34; : \u0026#34;03 Jul 2020 01:03:34\u0026#34;, \u0026#34;zonedDateTime\u0026#34; : \u0026#34;2020-07-03@01:03:34.467+08:00\u0026#34; } Please note that once you apply @JsonFormat annotation on Date and Time Object types, it is used in both serialization (object to JSON) and deserialization (JSON to object). That means you need to pass the date or time parameters in the same format in HTTP request body JSON if required.\n@JsonSerialize Annotation When you use @JsonFormat on LocalDate, LocalDateTime and ZonedDateTime then sometimes Jackson throw InvalidDefinitionException during serialization like this:-\nResponse (Error) com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Java 8 date/time type `java.time.LocalDate` not supported by default: add Module \u0026#34;com.fasterxml.jackson.datatype:jackson-datatype-jsr310\u0026#34; to enable handling (through reference chain: com.example.api.domain.User[\u0026#34;dateOfBirth\u0026#34;]) You can prevent this issue by adding com.fasterxml.jackson.datatype:jackson-datatype-jsr310 dependency and using @JsonSerialize annotation on date and time field types as below:-\npublic class User { Long id; String name; @JsonFormat(pattern=\u0026#34;dd MMM yyyy\u0026#34;) @JsonSerialize(using = LocalDateSerializer.class) LocalDate dateOfBirth; @JsonFormat(pattern=\u0026#34;dd MMM yyyy hh:mm:ss\u0026#34;) @JsonSerialize(using = LocalDateTimeSerializer.class) LocalDateTime lastLogin; @JsonFormat(pattern = \u0026#34;yyyy-MM-dd@HH:mm:ss.SSSXXX\u0026#34;, locale = \u0026#34;en_SG\u0026#34;, timezone = \u0026#34;Asia/Singapore\u0026#34;) @JsonSerialize(using = ZonedDateTimeSerializer.class) ZonedDateTime zonedDateTime; } Conclusion We looked at some of the useful configurations. Here is the full list of Jackson serialization and deserialization properties configurable in Spring Boot Application using application.yml, or application.properties file.\napplication.yml spring: jackson: default-property-inclusion: use_defaults/always/non-null/non-empty/non-absent/non-default serialization: CLOSE_CLOSEABLE: true/false EAGER_SERIALIZER_FETCH: true/false FAIL_ON_EMPTY_BEANS: true/false FAIL_ON_SELF_REFERENCES: true/false FAIL_ON_UNWRAPPED_TYPE_IDENTIFIERS: true/false FLUSH_AFTER_WRITE_VALUE: true/false INDENT_OUTPUT: true/false ORDER_MAP_ENTRIES_BY_KEYS: true/false USE_EQUALITY_FOR_OBJECT_ID: true/false WRAP_EXCEPTIONS: true/false WRAP_ROOT_VALUE: true/false WRITE_BIGDECIMAL_AS_PLAIN: true/false WRITE_CHAR_ARRAYS_AS_JSON_ARRAYS: true/false WRITE_DATES_AS_TIMESTAMPS: true/false WRITE_DATES_WITH_ZONE_ID: true/false WRITE_DATE_KEYS_AS_TIMESTAMPS: true/false WRITE_DATE_TIMESTAMPS_AS_NANOSECONDS: true/false WRITE_DURATIONS_AS_TIMESTAMPS: true/false WRITE_EMPTY_JSON_ARRAYS: true/false WRITE_ENUMS_USING_INDEX: true/false WRITE_ENUMS_USING_TO_STRING: true/false WRITE_ENUM_KEYS_USING_INDEX: true/false WRITE_NULL_MAP_VALUES: true/false WRITE_SELF_REFERENCES_AS_NULL: true/false WRITE_SINGLE_ELEM_ARRAYS_UNWRAPPED: true/false deserialization: ACCEPT_EMPTY_ARRAY_AS_NULL_OBJECT: true/false ACCEPT_EMPTY_STRING_AS_NULL_OBJECT: true/false ACCEPT_FLOAT_AS_INT: true/false ACCEPT_SINGLE_VALUE_AS_ARRAY: true/false ADJUST_DATES_TO_CONTEXT_TIME_ZONE: true/false EAGER_DESERIALIZER_FETCH: true/false FAIL_ON_IGNORED_PROPERTIES: true/false FAIL_ON_INVALID_SUBTYPE: true/false FAIL_ON_MISSING_CREATOR_PROPERTIES: true/false FAIL_ON_MISSING_EXTERNAL_TYPE_ID_PROPERTY: true/false FAIL_ON_NULL_CREATOR_PROPERTIES: true/false FAIL_ON_NULL_FOR_PRIMITIVES: true/false FAIL_ON_NUMBERS_FOR_ENUMS: true/false FAIL_ON_READING_DUP_TREE_KEY: true/false FAIL_ON_TRAILING_TOKENS: true/false FAIL_ON_UNKNOWN_PROPERTIES: true/false FAIL_ON_UNRESOLVED_OBJECT_IDS: true/false READ_DATE_TIMESTAMPS_AS_NANOSECONDS: true/false READ_ENUMS_USING_TO_STRING: true/false READ_UNKNOWN_ENUM_VALUES_AS_NULL:true/false READ_UNKNOWN_ENUM_VALUES_USING_DEFAULT_VALUE: true/false UNWRAP_ROOT_VALUE: true/false UNWRAP_SINGLE_VALUE_ARRAYS: true/false USE_BIG_DECIMAL_FOR_FLOATS: true/false USE_BIG_INTEGER_FOR_INTS: true/false USE_JAVA_ARRAY_FOR_JSON_ARRAY: true/false USE_LONG_FOR_INTS: true/false WRAP_EXCEPTIONS: true/false You can refer to the Spring Boot official documentation for the customization of Jackson ObjectMapper.\nPlease note that spring boot configuration supports Relaxed Binding which means properties can be in uppercase or lowercase, both are valid.\nspring.jackson.serialization.INDENT_OUTPUT: true is same as\nspring.jackson.serialization.indent_output: true That\u0026rsquo;s it for now. I\u0026rsquo;ll keep updating this post with more practical use cases as I come across them.\nDownload the source code for these examples from github/springboot-api\n","permalink":"https://codingnconcepts.com/spring-boot/jackson-json-request-response-mapping/","tags":["Spring Boot API","Jackson"],"title":"Jackson JSON Request and Response Mapping in Spring Boot"},{"categories":["Hugo"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to add sub menu (nested menu) in Hugo Website.\nOverview When you generate a website using Hugo static site generator, you generally show a menu header on website page and other pages. Sometime you need a support for nested menu structure which is not readily available with Hugo.\nYou can follow these steps to support nested menu structure:-\nUpdate config.toml file to provide menu and submenu configuration. Create layouts/partials/menu.html to generate HTML from menu and submenu configuration. Use menu partial in either layouts/partials/header.html or layouts/_defaults/baseof.html depending upon the theme you are using. Finally you need to update your style.css file with menu related CSS Follow the steps:- Update config.toml Let\u0026rsquo;s update config.toml with menu and submenu configuration to generate a nested menu structure like this:\nconfig.toml ... [menu] [[menu.main]] identifier = \u0026#34;home\u0026#34; name = \u0026#34;Home\u0026#34; url = \u0026#34;/\u0026#34; weight = 1 [[menu.main]] identifier = \u0026#34;category\u0026#34; name = \u0026#34;Category\u0026#34; url = \u0026#34;/category\u0026#34; weight = 2 [[menu.main]] identifier = \u0026#34;category1\u0026#34; name = \u0026#34;Category1\u0026#34; url = \u0026#34;/tags/category1\u0026#34; parent = \u0026#34;category\u0026#34; weight = 1 [[menu.main]] identifier = \u0026#34;category2\u0026#34; name = \u0026#34;Category2\u0026#34; url = \u0026#34;/tags/category2\u0026#34; parent = \u0026#34;category\u0026#34; weight = 2 [[menu.main]] identifier = \u0026#34;category3\u0026#34; name = \u0026#34;Category3\u0026#34; url = \u0026#34;/tags/category3\u0026#34; parent = \u0026#34;category\u0026#34; weight = 3 [[menu.main]] identifier = \u0026#34;another-category\u0026#34; name = \u0026#34;Another Category\u0026#34; url = \u0026#34;/another-category\u0026#34; weight = 3 [[menu.main]] identifier = \u0026#34;another-category1\u0026#34; name = \u0026#34;Another Category1\u0026#34; url = \u0026#34;/tags/another-category1\u0026#34; parent = \u0026#34;another-category\u0026#34; weight = 1 [[menu.main]] identifier = \u0026#34;another-category2\u0026#34; name = \u0026#34;Another Category2\u0026#34; url = \u0026#34;/tags/another-category2\u0026#34; parent = \u0026#34;another-category\u0026#34; weight = 2 [[menu.main]] identifier = \u0026#34;about\u0026#34; name = \u0026#34;About Us\u0026#34; url = \u0026#34;/about/\u0026#34; weight = 4 You should understand each field under [[menu.main]]:\nidentifier should be unique name of menu within the menu structure. It is used to identify menu, submenu and their relation by hugo internally. name should be the name which you want to display on the menu item. url is a relative URL which will be opened when you click on the menu item. weight is for sequencing the menu items. Menu items with less weight appear first in the menu. Same is applicable for submenu, submenu items with less weight appear first in the dropdown. parent is used in submenu item only to tell who is the parent of the submenu. You use the identifier of the parent menu in this field. Create/Update layouts/partials/menu.html Now we are going to use menu and submenu configuration from config.toml file to create our menu.html partial as below:\nlayouts/partials/menu.html This is the basic menu.html if you want create your own menu from scratch.\n\u0026lt;nav class=\u0026#34;nav\u0026#34;\u0026gt; \u0026lt;ul class=\u0026#34;menu\u0026#34;\u0026gt; {{ range .Site.Menus.main }} \u0026lt;li class=\u0026#34;{{ if $currentPage.HasMenuCurrent \u0026#34;main\u0026#34; . }}active{{ end }}\u0026#34;\u0026gt; \u0026lt;span\u0026gt;{{ .Name }}\u0026lt;/span\u0026gt; {{ if .HasChildren }} \u0026lt;span class=\u0026#34;drop-icon\u0026#34; for=\u0026#34;{{ .Name }}\u0026#34;\u0026gt;▾\u0026lt;/span\u0026gt; \u0026lt;ul class=\u0026#34;sub-menu\u0026#34;\u0026gt; {{ range .Children }} \u0026lt;li class=\u0026#34;{{ if $currentPage.HasMenuCurrent \u0026#34;main\u0026#34; . }}active{{ end }}\u0026#34;\u0026gt; \u0026lt;span\u0026gt;{{ .Name }}\u0026lt;/span\u0026gt; \u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; {{ end }} \u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; \u0026lt;/nav\u0026gt; This is the menu.html of my blog website created using Hugo Mainroad Theme.\n{{- if .Site.Menus.main }} \u0026lt;nav class=\u0026#34;menu\u0026#34;\u0026gt; \u0026lt;button class=\u0026#34;menu__btn\u0026#34; aria-haspopup=\u0026#34;true\u0026#34; aria-expanded=\u0026#34;false\u0026#34; tabindex=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;menu__btn-title\u0026#34; tabindex=\u0026#34;-1\u0026#34;\u0026gt;{{ T \u0026#34;menu_label\u0026#34; }}\u0026lt;/span\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;ul class=\u0026#34;menu__list\u0026#34;\u0026gt; {{- $currentNode := . }} {{- range .Site.Menus.main }} {{- if .Name }} {{- if .HasChildren }} \u0026lt;li class=\u0026#34;menu__item menu__dropdown{{ if or ($currentNode.IsMenuCurrent \u0026#34;main\u0026#34; .) ($currentNode.HasMenuCurrent \u0026#34;main\u0026#34; .) }} menu__item--active{{ end }}\u0026#34;\u0026gt; \u0026lt;a class=\u0026#34;menu__link\u0026#34; href=\u0026#34;{{ .URL }}\u0026#34;\u0026gt; {{ .Pre }} \u0026lt;span class=\u0026#34;menu__text\u0026#34;\u0026gt;{{ .Name }}\u0026lt;/span\u0026gt; \u0026lt;label class=\u0026#34;drop-icon\u0026#34; for=\u0026#34;{{ .Name }}\u0026#34;\u0026gt;▾\u0026lt;/label\u0026gt; {{ .Post }} \u0026lt;/a\u0026gt; \u0026lt;input type=\u0026#34;checkbox\u0026#34; id=\u0026#34;{{ .Name }}\u0026#34;\u0026gt; \u0026lt;ul class=\u0026#34;submenu__list\u0026#34;\u0026gt; {{ range .Children }} \u0026lt;li class=\u0026#34;menu__item{{ if or ($currentNode.IsMenuCurrent \u0026#34;main\u0026#34; .) ($currentNode.HasMenuCurrent \u0026#34;main\u0026#34; .) }} menu__item--active{{ end }}\u0026#34;\u0026gt; \u0026lt;a class=\u0026#34;menu__link\u0026#34; href=\u0026#34;{{ .URL }}\u0026#34;\u0026gt; {{ .Pre }} \u0026lt;span class=\u0026#34;menu__text\u0026#34;\u0026gt;{{ .Name }}\u0026lt;/span\u0026gt; {{ .Post }} \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; \u0026lt;/li\u0026gt; {{- else }} \u0026lt;li class=\u0026#34;menu__item{{ if or ($currentNode.IsMenuCurrent \u0026#34;main\u0026#34; .) ($currentNode.HasMenuCurrent \u0026#34;main\u0026#34; .) }} menu__item--active{{ end }}\u0026#34;\u0026gt; \u0026lt;a class=\u0026#34;menu__link\u0026#34; href=\u0026#34;{{ .URL }}\u0026#34;\u0026gt; {{ .Pre }} \u0026lt;span class=\u0026#34;menu__text\u0026#34;\u0026gt;{{ .Name }}\u0026lt;/span\u0026gt; {{ .Post }} \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; {{- end }} {{- end }} {{- end }} \u0026lt;/ul\u0026gt; \u0026lt;/nav\u0026gt; {{ else -}} \u0026lt;div class=\u0026#34;divider\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; {{- end }} Note that if you are using the theme, then you might already have menu.html partial. In that case just update the existing menu.html partial. Things to note in the above partial is use of {{- if .HasChildren }} condition to check if menu has submenu children or not. If it is having submenu children, then we loop them through using {{ range .Children }}.\nAlso, note that you might need to add classes to the HTML elements as per the theme you are using.\nOnce you create or update your layouts/partials/menu.html, import this partial in either layouts/partials/header.html or layouts/defaults/baseof.html depending upon the theme you are using, if not imported.\nCreate/Update static/css/style.css This is the most important part of nested menu structure. I struggled a lot to create the right CSS for above menu.html partial. Here is relevant part of the CSS which you need to add in your existing CSS for nested menu structure to work.\nstatic/css/style.css /* Main menu */ .no-js .menu__btn { display: none; } .menu__btn { display: block; width: 100%; padding: 0; font: inherit; color: #fff; background: #2a2a2a; border: 0; outline: 0; } .menu__btn-title { position: relative; display: block; padding: 10px 15px; padding: 0.625rem 0.9375rem; font-weight: 700; text-align: right; text-transform: uppercase; cursor: pointer; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; -o-user-select: none; user-select: none; } :focus \u0026gt; .menu__btn-title { box-shadow: inset 0 0 1px 3px #e22d30; } button:not(:-moz-focusring):focus \u0026gt; .menu__btn-title { box-shadow: none; } .menu__btn:focus, .menu__btn-title:focus { outline: 0; } .js .menu__btn--active { color: #e22d30; } .menu__list, .submenu__list { list-style: none; background: #2a2a2a; } .menu__item { transition: background-color .25s ease-out; } .menu__item:hover \u0026gt; a { color: #ffa500; } .menu__item:first-child { border: 0; } .menu__item--active { background: #e22d30; } .menu__link { display: block; padding: 10px 15px; padding: 0.625rem 0.9375rem; font-weight: 700; color: #fff; text-transform: uppercase; } .menu__list .menu__item .submenu__list { background: #2a2a2a; visibility: hidden; opacity: 0; position: absolute; max-width: 15rem; transition: all 0.5s ease; border-top: 5px solid #e22d30;\tdisplay: none;\t} .menu__item.menu__dropdown input[type=\u0026#34;checkbox\u0026#34;] { display: none; } .menu__list .menu__item:hover \u0026gt; .submenu__list, .menu__list .menu__item:focus-within \u0026gt; .submenu__list, .menu__list .menu__item .submenu__list:hover, .menu__list .menu__item .submenu__list:focus { visibility: visible; opacity: 1; display: block; } .menu__link:hover { color: #fff; } .js .menu__list { position: absolute; z-index: 1; width: 100%; visibility: hidden; -webkit-transform: scaleY(0); transform: scaleY(0); -webkit-transform-origin: top left; transform-origin: top left; } .js .menu__list--active { visibility: visible; border-top: 1px solid rgba(255, 255, 255, 0.1); border-bottom: 1px solid rgba(255, 255, 255, 0.1); -webkit-transform: scaleY(1); transform: scaleY(1); } .menu__list--transition { transition: visibility 0.15s ease, transform 0.15s ease, -webkit-transform 0.15s ease; } @media screen and (min-width: 767px) { .menu { border-bottom: 5px solid #e22d30; } .menu__btn { display: none; } .menu__list, .js .menu__list { position: relative; display: -webkit-flex; display: flex; -webkit-flex-wrap: wrap; flex-wrap: wrap; visibility: visible; border: 0; -webkit-transform: none; transform: none; } .menu__item { border-left: 1px solid rgba(255, 255, 255, 0.1); } } @media screen and (max-width: 767px) { .menu__item.menu__dropdown .drop-icon { position: absolute; right: 1rem; top: auto; width: 50%; text-align: right; } .menu__item.menu__dropdown input[type=\u0026#34;checkbox\u0026#34;] + .submenu__list { display: none; } .menu__item.menu__dropdown input[type=\u0026#34;checkbox\u0026#34;]:checked + .submenu__list { border: none; padding-left: 20px; visibility: visible; opacity: 1; display: block; position: relative; max-width: 100%; } } @media screen and (max-width: 620px) { .menu__item.menu__dropdown .drop-icon { position: absolute; right: 1rem; top: auto; } .menu__item.menu__dropdown input[type=\u0026#34;checkbox\u0026#34;] + .submenu__list { display: none; } .menu__item.menu__dropdown input[type=\u0026#34;checkbox\u0026#34;]:checked + .submenu__list { border: none; padding-left: 20px; visibility: visible; opacity: 1; display: block; position: relative; max-width: 100%; } } Import this CSS in either layouts/partials/header.html or layouts/defaults/baseof.html depending upon the theme you are using, if not imported.\n{{ $style := resources.Get \u0026#34;css/style.css\u0026#34; | resources.ExecuteAsTemplate \u0026#34;css/style.css\u0026#34; . -}} Create/Update static/js/menu.js If you don\u0026rsquo;t have menu.js javascript in your existing theme, then use this instead:-\nstatic/js/menu.js \u0026#39;use strict\u0026#39;; (function iifeMenu(document, window, undefined) { var menuBtn = document.querySelector(\u0026#39;.menu__btn\u0026#39;); var\tmenu = document.querySelector(\u0026#39;.menu__list\u0026#39;); function toggleMenu() { menu.classList.toggle(\u0026#39;menu__list--active\u0026#39;); menu.classList.toggle(\u0026#39;menu__list--transition\u0026#39;); this.classList.toggle(\u0026#39;menu__btn--active\u0026#39;); this.setAttribute( \u0026#39;aria-expanded\u0026#39;, this.getAttribute(\u0026#39;aria-expanded\u0026#39;) === \u0026#39;true\u0026#39; ? \u0026#39;false\u0026#39; : \u0026#39;true\u0026#39; ); } function removeMenuTransition() { this.classList.remove(\u0026#39;menu__list--transition\u0026#39;); } if (menuBtn \u0026amp;\u0026amp; menu) { menuBtn.addEventListener(\u0026#39;click\u0026#39;, toggleMenu, false); menu.addEventListener(\u0026#39;transitionend\u0026#39;, removeMenuTransition, false); } }(document, window)); Import this JS in either layouts/partials/header.html or layouts/defaults/baseof.html depending upon the theme you are using, if not imported.\n\u0026lt;script async defer src=\u0026#34;{{ \u0026#34;js/menu.js\u0026#34; | relURL }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Please note that I have kept CSS and JS files in static folder. This is how we specify that in config.toml configuration:-\nconfig.toml assetDir = \u0026#34;static\u0026#34; Posts to submenu mapping using Tags Now say you have created menu and submenu structure and now want to do mapping of your posts under each menu and submenu. I generally do it using tags.\nSay, I want to map my posts under Category ➞ Category1, I do that by adding tag in frontmatter like this:\nmyblogpost.md --- tags: - \u0026#34;category1\u0026#34; --- Now Hugo will map you post to the URL /tags/category1. You can use this URL to map submenu in config.toml\nconfig.toml [[menu.main]] identifier = \u0026#34;category1\u0026#34; name = \u0026#34;Category1\u0026#34; url = \u0026#34;/tags/category1\u0026#34; parent = \u0026#34;category\u0026#34; weight = 1 Conclusion Please note that I am currently using Hugo Mainroad Theme so this example should just work fine for Mainroad theme. For other themes, you need to put extra effort in terms of creating menu.html partial and right CSS for nested menu structure. I hope this tutorial will give you the basic idea in which direction you need to go.\nPlease comment in case you find any issue to follow the tutorial. Thanks for reading.\n","permalink":"https://codingnconcepts.com/hugo/nested-menu-hugo/","tags":null,"title":"Add Sub Menu in Hugo Website"},{"categories":["AWS"],"contents":"This step by step guide is very helpful for beginners to create a personal account with AWS.\nCreate your account ① Open this AWS link - Amazon Web Services home page.\n② Choose Create an AWS account. If you signed in to AWS recently, then choose Sign in to the Console.\n③ Enter your account information:\nEmail Address: Be sure that you enter your account information correctly, especially your email address. If you enter your email address incorrectly, you can\u0026rsquo;t access your account. AWS Account Name: Choose a name for your account. You can change this name in your account settings after you sign up. Password: Choose Strong password for your account. and then choose Continue\n④ Enter your personal information:\nChoose ⦿ Personal Enter your email address and phone number for a root account. ⑤ Read and accept the AWS Customer Agreement.\n⑥ Choose Create Account and Continue.\nYou receive an email to confirm that your account is created. You can sign in to your new account using the email address and password you registered with. However, you can\u0026rsquo;t use AWS services until you finish activating your account.\nAdd a payment method ⑦ On the Payment Information page, enter the information about your payment method, and then choose Verify and Add.\nYou cannot use AWS services without adding payment method. If you are new to the AWS then most of the services which you will be using like Amazon EC2, Amazon S3, and Amazon DynamoDB comes under free tier so you don\u0026rsquo;t need to worry about much in the beginning. Please go through AWS Free Tier for more details.\nNote: You can always set a Billing Alarm. If your billing goes beyond a certain amount, you will be notified via mail immediately.\nVerify your phone number ⑧ Choose your country or region code and a phone number. Enter a phone number where you can be reached via text messages or voice call. Enter the security check captcha and then choose Send SMS.\n⑨ You will receive a 4-digit verification code on your registered mobile number. Enter the verification code and choose Verify Code.\n⑩ Upon successful verification, choose Continue.\nChoose an AWS Support plan ⑪ On the Select a Support Plan page, choose one of the available Support plans. For a description of the available Support plans and their benefits, see Compare AWS Support Plans. Choose Free for Basic Plan.\nWait for account activation ⑫ you\u0026rsquo;ll receive a confirmation email once account is activated. Check your email and spam folder for the confirmation email. After you receive this email, you have full access to all AWS services.\nNote: Accounts are usually activated within a few minutes, but the process might take up to 24 hours.\nLogin as root user ⑬ Open the Amazon Web Services home page again and choose Sign in to the Console.\n⑭ Choose ⦿ Root User and login to account using your email address and password.\nCongratulations!!! You have successfully created your AWS account.\nReference: AWS Knowledge Center ","permalink":"https://codingnconcepts.com/aws/how-to-create-aws-account/","tags":["AWS"],"title":"A Step-by-Step Guide to Create an AWS Account"},{"categories":["AWS"],"contents":"This step-by-step guide is helpful for beginners trying their hands on machine learning and creating their first AWS DeepRacer model.\nPrerequisite You should have a valid AWS account. If you haven\u0026rsquo;t registered to AWS already, follow this Step-by-step guide to create an AWS Account\nWhat is AWS DeepRacer? AWS DeepRacer is an exciting way for developers to get hands-on experience with machine learning.\nIn AWS DeepRacer, you use a 1/18 scale autonomous car equipped with sensors and cameras. You can use this car in virtual simulator, to train and evaluate. The same car is used in physical AWS DeepRacer global racing league.\nIn AWS DeepRacer, You do this:\nbuild your car, or choose existing one, create your own reinforcement learning model, or choose from sample models, choose pre-defined racing track, train and evaluate your car on racing track using your model , and finally you compete with others.\nReinforcement learning Reinforcement learning (RL) is an area of machine learning concerned with how an agents should take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\nLet\u0026rsquo;s understand the reinforcement learning key terms from AWS DeepRacer perspective:\nAWS DeepRacer Key Terms\nmodel is created when you choose a car, racing track and write your reward function. agent is your Car action space is the amount of choices a Car has, for e.g., turn left, turn right, move forward at different speeds. state is position of Car on the track at a point of time. environment is the Racing Track of your Car reward function is a function written in python which returns reward in floating point number. Your car use this reward function in its training and try to accumulate more reward points training phase is when you train your car on racing track using the model you created.\nAlso known as exploration phase where your car explore the racing track, and learn from which route it can get more reward points. evaluating phase is when you evaluate your car after training is completed whether its able to complete the racing track or not. Also known as exploitation phase where your car exploit the learning from training and try to reach the convergence (finish line). The hyper parameters are reinforcement algorithm and training specific parameters which controls the car training process. You car make attempts to find the best route by accumulating reward point in training phase. Attempt starts either from the start line or any part of the track. Attempt ends when car reaches the finish line, or loose the track, or crash. Each of this attempt is called an episode.\nReward function Reward function is a python function where you give higher reward points for good behavior and lower reward points for bad behavior.\nHere is sample reward function written in python which gives higher reward points to car for keeping it wheels on track:\ndef reward_function(params): # Read input parameters all_wheels_on_track = params[\u0026#39;all_wheels_on_track\u0026#39;] if all_wheels_on_track: reward = 1.0 # higher reward for car to be on track else reward = 1e-3 # lower reward for car to go outside track return float(reward) When you train your car, it attempts to run at different angles and speed on the track and try to accumulate reward points. In each subsequent attempts, it learns from its previous attempts and try to accumulate more reward points by keeping it wheels on track.\n1. Getting Started ① Go to AWS Services ➞ AWS DeepRacer ➞ Reinforcement Learning ➞ Get Started\n② If it is your first time, you will see some errors. Don\u0026rsquo;t worry. Choose Reset Resources and everything will be fine.\nAWS DeepRacer Get Started\nThis takes about 5 minutes. AWS internally does following things for you:\nCheck IAM roles required for DeepRacer, Check AWS DeepRacer resource stack: Amazon SageMaker and AWS RoboMaker are the brain behind training your model Amazon S3 to store samples and the new models created by you Amazon Kinesis Video Streams for streaming data and simulation video, when your car is training and evaluating Amazon CloudWatch to store and analyze the logs and metrics You should be well-aware of the AWS services involved in AWS DeepRacer in order to understand the billing when AWS charge you money. Here is the AWS DeepRacer Simulator Architecture:\nAWS DeepRacer Simulator Architecture\n2. Build your Car You can either use the default Car The Original DeepRacer or build a new vehicle by following these steps:\n① Go to AWS DeepRacer ➞ Reinforcement Learning ➞ Your garage and Choose Build New Vehicle\n② Keep the default setting of Camera and Sensors. Choose Next\n③ Keep the action space settings as below. Choose Next AWS DeepRacer Action Space for Beginners\n④ Enter the name of your car and Choose color. Choose Done\nAction Space You car can turn left, turn right, move forward at different speeds. All these choice become the action spaces of the Car. Action space of a Car depends on following parameters:\nMaximum Steering Angle: Max values are between 1 and 30. Steering angle granularity: Possible values are 3, 5 and 7 Maximum speed: Select values between 0.1m/s and 4m/s. Speed granularity: Possible values are 1, 2 and 3 If you choose 5 Steering angles granularity and 3 Speeds granularity then it has total (5*3) = 15 action spaces (all combinations of angles vs speed). A car choose random action out of these 15 action spaces in its trail and errors.\nAction Space = Steering angles granularity * Speeds granularity Note that more the action spaces, more the choices a Car has to choose from. You might get the best finishing time and best time around the curve with more action spaces but it requires a lot of training and your Car might not reach the convergence (finish the racing track). Same is applicable for maximum speed.\nAs a beginner, it is advised to use:\n3 steering angles and 3 speeds to start with (3*3) = 9 action spaces maximum speed to 1m/s to train your car faster. Your primary focus while training your car should be on the accuracy and reliability of your model and not the speed or lap time of your Car. Once you have some insights, you can train your car by increasing the action spaces and max speed.\n3. Create Model There are five out of the box models available to use in AWS DeepRacer:\nName Description Status Sensors Sample-Head-to-Head Model trained with reward function for head to head racing Ready Stereo camera, Lidar Sample-Object-Avoidance Model trained with reward function that avoids objects Ready Stereo camera Sample-Time-Trial-PreventZigZag Model trained with reward function that penalizes the agent for steering too much Ready Camera Sample-Time-Trial-StayOnTrack Model trained with reward function that incentivizes the agent to stay inside the track borders Ready Stereo camera Sample-Time-Trial-FollowCenterLine Model trained with reward function that incentivizes the agent to follow the center line Ready Camera I recommend to click on each model, Go to their Training Configuration ➞ Reward Function and Action Space which will give you an idea on how to design your own model.\nLet\u0026rsquo;s create our own model:\n① Go to AWS DeepRacer ➞ Reinforcement Learning ➞ Your models and Choose Create Model\n② Enter Model name and description\n③ Choose a Racing Track. To follow with me, Choose ⦿ re:Invent 2018 and Choose Next\n④ Choose a Race Type. To follow with me, Choose ⦿ Time trial\n⑤ Choose an Agent (Car). To follow with me, Choose a car which we created earlier. Alternatively Choose default car \u0026ldquo;The Original DeepRacer\u0026rdquo;\n⑥ Clear Reward Function Code Editor. Copy and Paste following reward function in code editor and Click Validate to make sure this code is valid.\n⑦ Keep the default setting of Training Algorithm ⦿ PPO and hyperparameters.\n⑧ Enter Stop Condition ➞ Maximum time = 60 minutes.\nNote: Congratulations, You\u0026rsquo;ve configured your model. Next step is to create, train and evaluate your model. Please note that you have not charged anything till this point. Subsequent steps will charge you money based on AWS resource utilization.\n⑨ Once you choose Create Model. AWS resources will be provisioned to train your model for 60 minutes.\nReward Graph It is recommended to watch the reward graph and simulator video stream while your car is training. This was a thrilling experience for me to watch the live training of car using my first ever machine learning model.\nAWS DeepRacer Reward Graph and Simulator Video Stream\nYou should know how to read the reward graph in order to understand your model behavior:\nAverage Reward is reward points your car is accumulating in each attempt. Average reward line should go up linearly in the perfect scenario but this is rare. This is what you should look: Average Reward line should go up in the longer time period. That means your car is able to learn from its past failures and able to accumulate more reward points in subsequent attempts. If Average Reward line go down in shorter period then do not worry. That means your car is trying to attempt those directions which return less reward points. Your car would learn from those attempts and won\u0026rsquo;t try them again in subsequent attempts. There are two phases when your car is training, you will notice them if you are watching simulator video: Training Phase: Your car trains on different parts of the racing track by starting at random positions. Evaluating Phase: You car starts from the starting point of racing track and try to complete the track. Average percentage completion (Training) is percentage of track completed in training phase. Since in training phase, main aim of car is to train at different parts of the track, it doesn\u0026rsquo;t complete the track most of the time and hence the graph line remain below and doesn\u0026rsquo;t touch 100% marker. Average percentage completion (Evaluating) is percentage of track completed in evaluating phase. Since in evaluating phase, main aim of car is to complete the track, it should touch the 100% marker and able to finish the racing track. If this graph line is not touching 100% marker then there are higher chances of your model failing in actual race.\nHyperparameters Tuning the hyperparameters can improve the quality of your model but it requires a steep learning curve and a lot of trials and errors. As a beginner, it is advised to use default optimized parameters but at the same time, it is good to know how they impact the training of your Car.\nNumber of episodes per iteration: Each attempt made by car in training phase is one episode. With Default value, a Car make 20 attempts in one iteration before updating training data to the network. Batch Size: Iterations runs in batches. Batch size controls, how many iterations to be completed before updating training data to the network. Number of Epochs: Number of times to repeat the batches and send optimized training data to network. Larger number of epochs is acceptable when the batch size is large. Learning Rate: Controls the speed your car learns. Large learning rate prevents training data from reaching optimal solution whereas Small learning rate takes longer to learn. Entropy: It is a degree of randomness in the Car\u0026rsquo;s action. Larger the entropy means the more random actions a Car will take for exploration. Discount Factor: It is a factor specifies how much of the future reward contributes to the expected rewards. With larger discount factor, Car looks further into the future to consider rewards. With smaller discount factor, Car only consider immediate rewards. Take a quick look at hyperparameters:\nHyperparamter Advantage of higher values Disadvantage of higher values Default Batch Size More stable updates Slower training 64 Number of Epochs More stable updates Slower training 10 Learning Rate Car learns faster May struggle to converge 0.0003 Entropy More experimental may lead to better results May struggle to converge 0.01 Discount Factor Model looks farther out Slower training 0.999 Episodes Improves model stability Slower training 20 Personal Experience When i started making my first model, I thought that i will make my first model as master piece. I selected max out configuration i.e. max speed 4m/s and maximum action spaces and training period as 2 hours. It failed miserably. It was not able to finish the track even in 2 hours training period and the reward graphs were all going in wrong direction.\nAfter that I used the model configuration and reward function as mentioned in the post and able to finish the track in 60 min training time. I, then increased the max-speed from 1m/s to 1.5m/s and able to finish the reInvent:2018 track in ~19 seconds.\nI recommend:-\nMake you model with above example. Learn from the reward graph and simulator video. Take one step at a time say increase the max-speed from 1m/s to 1.5m/s and train again. Continue to progress\u0026hellip; Thanks for Reading. I am still in learning phase of AWS DeepRacer and keep updating this post based on my new findings.\n","permalink":"https://codingnconcepts.com/aws/aws-deepracer/","tags":["AWS","Popular Posts"],"title":"Build Your First AWS DeepRacer Model"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn the usage of Array Destructuring in JavaScript ES6 with examples.\nDestructuring Assignment is a special syntax introduced in JavaScript ES6 that allows us to extract multiple items from an array or object and assign them to variables, in a single statement.\nAlso Read: Object Destructuring\nBasic Array Destructuring We can extract multiple elements from an Array and assign them to variables using Array Destructuring in a single statement.\nArray Destructuring shorthand syntax is quite handy as compare to our traditional way of assigning elements to the variables one by one.\nconst numbers = [1, 2, 3]; //const one = numbers[0]; //Traditional Way //const two = numbers[1]; //const three = numbers[2]; const [one, two, three] = numbers; //Array Destructuring console.log(one); //1 console.log(two); //2 console.log(three); //3 We can also declare the variables before assigned:\nlet one, two, three; const numbers = [1, 2, 3]; [one, two, three] = numbers; console.log(one); //1 console.log(two); //2 console.log(three); //3 or we can declare them inline:\nconst [one, two, three] = [1, 2, 3]; console.log(one); //1 console.log(two); //2 console.log(three); //3 Default Values We can give default values to the array elements that are undefined or doesn\u0026rsquo;t exist.\nconst numbers = [1, undefined, 3]; const [one = \u0026#34;I\u0026#34;, two = \u0026#34;II\u0026#34;, three = \u0026#34;III\u0026#34;, four = \u0026#34;IV\u0026#34;] = numbers; console.log(one); //1 console.log(two); //II \u0026lt;- default value since \u0026#39;undefined\u0026#39; console.log(three); //3 console.log(four); //IV \u0026lt;- default value since doesn\u0026#39;t exist We see that second element is undefined and fourth element doesn\u0026rsquo;t exist. Default value is assigned to the variable in both the cases.\nNested Array Destructuring We can also perform nested Array Destructuring to get elements from nested array at deeper levels:\nconst numbers = [1, 2, [9, 10, [11, 12]]]; const [one, two, [nine, ten, [eleven, twelve]]] = numbers console.log(one) //1 console.log(nine) //9 console.log(eleven) //11 An alternate for nested Array Destructuring is to use ES9 Array.flat() method, which flatten the nested array into a normal one:\nconst numbers = [1, 2, [9, 10, [11, 12]]]; const flattenNumbers = numbers.flat().flat(); const [one, two, nine, ten, eleven, twelve] = flattenNumbers; console.log(one) //1 console.log(nine) //9 console.log(eleven) //11 Note that Array.flat() method flatten the nested array at one level deep. That is why we have called this method two times numbers.flat().flat() to flatten at two levels deep.\nAlso note that Array.flat() method is shorter version of Array.flatMap() method, both works same:\nconst numbers = [1, 2, [9, 10, [11, 12]]]; const flat = numbers.flat(); const flatMap = numbers.flatMap(x =\u0026gt; x); console.log(flat); // [1, 2, 9, 10, [11, 12]] console.log(flatMap); // [1, 2, 9, 10, [11, 12]] // flatten one more level deeper const flatDeep = flat.flat(); const flatMapDeep = flatMap.flatMap(x =\u0026gt; x); console.log(flatDeep); // [1, 2, 9, 10, 11, 12] console.log(flatMapDeep); // [1, 2, 9, 10, 11, 12] Skipping Elements using commas We can also skip some of the elements by using comma separator. Let\u0026rsquo;s get the first and fourth element of the array:\nlet [one, , , four] = [1, 2, 3, 4]; console.log(one); //1 console.log(four); //4 Here two extra comma separators , , in Array Destructuring skip the second and third element. Similarly let\u0026rsquo;s skip the first and third element:\nlet [ , two , , four] = [1, 2, 3, 4]; console.log(two); //2 console.log(four); //4 Rest of the Elements We can also use rest operator ... to get rest of the elements from an array like this:\nconst numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]; const [one, two, three, ...others] = numbers; console.log(one); //1 console.log(two); //1 console.log(three); //1 console.log(others); //[4, 5, 6, 7, 8, 9] Note that rest operator can always be used at last, otherwise it throws error.\nRest and Spread Operators Often we get confused with Rest and Spread operator as both use the same ... syntax. Rest and Spread operators can be used together in the Array Destructuring statement:\nRest Operator: Used at the left hand side of statement to get the rest of the elements from an array Spread Operator: Used at the right hand side of statement to copy elements to an array const tens = [10, 20, 30]; const hundreds = [100, 200]; const thousands = [1000]; // Rest operator // Spread Operator const [one, two, ten, twenty, ...others] = [1, 2, ...tens, ...hundreds, ...thousands];; console.log(one); //1 console.log(two); //2 console.log(ten); //10 console.log(twenty); //20 console.log(others); //[30, 100, 200, 1000] Destructuring Return Statement We can extract data from an array returned from a function using Array Destructuring in this way:\nfunction getNumberArray() { return [1, 2, 3, 4, 5]; } var [one, ...others] = getNumberArray(); console.log(one); //1 console.log(others); //[2, 3, 4, 5] Destructuring Function Arguments We can even pass an array into a function and then extract only the elements we want using Array Destructuring in this way:\nconst numbers = [1, 2, 3, 4, 5]; function getNumberArray( [ one, ...others ] ) { console.log(one); //1 console.log(others); //[2, 3, 4, 5] } getNumberArray(numbers); Practical Use Cases Inside for-of loop We can use Array Destructuring inside for-of loop in this way:\nconst numbers = [ [ 1, 2, 3 ], [ 4, 5, 6 ], [ 7, 8, 9 ] ]; for (let [ a, b, c ] of numbers) { console.log(a, b, c); } // 1 2 3 // 4 5 6 // 7 8 9 Inside forEach function We can iterate through Objects easily using Array Destructuring.\nconst obj = { foo: \u0026#39;bar\u0026#39;, baz: 42 }; Object.entries(obj).forEach(([key, value]) =\u0026gt; console.log(`${key}: ${value}`)); // \u0026#34;foo: bar\u0026#34;, \u0026#34;baz: 42\u0026#34; We see that Object.entries returns an Array with key-value pairs which is extracted using [key, value] destructuring.\nInside Map iteration We can also destruct the key-value pairs of a Map using [key, value] destructuring.\nvar map = new Map(); map.set(\u0026#34;one\u0026#34;, 1); map.set(\u0026#34;two\u0026#34;, 2); for (var [key, value] of map) { console.log(key + \u0026#34; is \u0026#34; + value); } // one is 1 // two is 2 Iterate over only the keys:\nfor (var [key] of map) { // ... } Or iterate over only the values:\nfor (var [,value] of map) { // ... } Swapping values This is quite interesting to swap values using Array Destructuring:\nlet one = 1; let two = 2; [one, two] = [two, one]; console.log(one); //2 console.log(two); //1 Regular Expression Regular expressions functions such as match return an array of matched items, which can be mapped to variables using Array Destructuring:\nconst [a, b, c, d] = \u0026#39;one two three\u0026#39;.match(/\\w+/g); // a = \u0026#39;one\u0026#39;, b = \u0026#39;two\u0026#39;, c = \u0026#39;three\u0026#39;, d = undefined ","permalink":"https://codingnconcepts.com/javascript/array-destructuring/","tags":["JavaScript ES6","Javascript Array"],"title":"How to use Array Destructuring in JavaScript"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn the usage of Object Destructuring in JavaScript ES6 with examples.\nDestructuring Assignment is a special syntax introduced in JavaScript ES6 that allows us to extract multiple items from an array or object and assign them to variables, in a single statement.\nAlso Read: Array Destructuring\nBasic Object Destructuring We can extract multiple properties from an Object and assign them to variables using Object Destructuring in a single statement.\nObject Destructuring shorthand syntax is quite handy as compare to our traditional way of assigning properties to the variables one by one.\nconst person = { name: \u0026#34;John\u0026#34;, company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34; }; // let name = person.name; //Traditional Way // let company = person.company; // let job = person.job; const { name, company, job } = person; //Object Destructuring We can declare the variables inline as well:\nconst { name, company, job } = { name: \u0026#34;John\u0026#34;, company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34; }; Assignment without Declaration We can declare the variables without assignment. Values can be assigned later using Object Destructuring like this:\nconst person = { name: \u0026#34;John\u0026#34;, company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34; }; let name, company, job; ({ name, company, job } = person); Note that the parentheses ( ... ); around the assignment statement are required when using object literal destructuring assignment without a declaration.\nVariable Name By default, variable name is same as object property name which you are extracting. We can always change the variable name to something else like this:\nconst person = { name: \u0026#34;John\u0026#34;, company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34; }; const { name: foo, job: bar} = person; console.log(foo); //\u0026#34;John\u0026#34; console.log(bar); //\u0026#34;Developer\u0026#34; Please note that now we can access the values using foo and bar variables only. If we try to access values using name and age variables, we would get undefined error.\nDefault Values We can also provide a default value to the variable, just in case the extracted object property is undefined or doesn\u0026rsquo;t exist.\nconst person = { name: \u0026#34;John\u0026#34;, company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34;, department: undefined }; const { name = \u0026#34;Bob\u0026#34;, department = \u0026#34;NA\u0026#34;, age = 21 } = person; console.log(name); //\u0026#34;John\u0026#34; console.log(department); //\u0026#34;NA\u0026#34; \u0026lt;- default value since \u0026#39;undefined\u0026#39; console.log(age); //21 \u0026lt;- default value since doesn\u0026#39;t exist We can also set default values with new variable name:\nconst person = { name: \u0026#34;John\u0026#34;, company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34;, department: undefined }; const { name:foo = \u0026#34;Bob\u0026#34;, department:bar = \u0026#34;NA\u0026#34;, age:baz = 21 } = person; console.log(foo); //\u0026#34;John\u0026#34; console.log(bar); //\u0026#34;NA\u0026#34; console.log(baz); //21 Dynamic Property Name We can also extract the properties with dynamic name (the property name is known at runtime) like this:\nconst prop = \u0026#34;name\u0026#34;; const person = { name: \u0026#34;John\u0026#34;, company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34;, department: undefined }; const { [prop]:foo } = person; console.log(foo); //\u0026#34;John\u0026#34; Nested Object Destructuring We can also perform nested Object Destructuring to extract properties from nested object at deeper levels:\nconst person = { name: \u0026#34;John\u0026#34;, friends : [\u0026#34;Adam\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;], hobbies: [\u0026#34;Biking\u0026#34;, \u0026#34;Cooking\u0026#34;], location: { country: \u0026#34;USA\u0026#34;, city: \u0026#34;NY\u0026#34; }, }; const { name, friends, hobbies : [firstHobby, secondHobby], location: { country, city } } = person; console.log(name); //\u0026#34;John\u0026#34; console.log(friends); //[\u0026#34;Adam\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;] console.log(firstHobby); //\u0026#34;Biking\u0026#34; console.log(secondHobby); //\u0026#34;Cooking\u0026#34; console.log(country); //\u0026#34;USA\u0026#34; console.log(city); //\u0026#34;NY\u0026#34; Remaining Object Properties The rest operator ... can be used to extract the remaining properties to a new variable which are not already extracted by the Object Destructuring.\nconst person = {name: \u0026#34;John\u0026#34;, company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34;, friends : [\u0026#34;Adam\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;]}; const {name, friends, ...others} = person; console.log(name); //\u0026#34;John\u0026#34; console.log(friends); //[\u0026#34;Adam\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;] console.log(others); //{company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34;} Rest and Spread Operators Often we get confused with Rest and Spread operator as both use the same ... syntax. Rest and Spread operators can be used together in the Object Destructuring statement:\nRest Operator: Used at the left hand side of statement to get the remaining properties from an object Spread Operator: Used at the right hand side of statement to copy properties to an object const primaryDetails = {name: \u0026#34;John\u0026#34;, company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34;} const otherDetails = {friends: [\u0026#34;Adam\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;], hobbies: [\u0026#34;Biking\u0026#34;, \u0026#34;Cooking\u0026#34;]}; // Rest operator // Spread Operator const {name, company, hobbies, ...others} = {...primaryDetails, ...otherDetails}; console.log(name); //\u0026#34;John\u0026#34; console.log(company); //\u0026#34;Google\u0026#34; console.log(hobbies); //[\u0026#34;Biking\u0026#34;, \u0026#34;Cooking\u0026#34;] console.log(others); //{job: \u0026#34;Developer\u0026#34;, friends: [\u0026#34;Adam\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;]} Destructuring Return Statement We can extract data from an object returned from a function using Object Destructuring in this way:\nfunction getPersonDetails() { return {name: \u0026#34;John\u0026#34;, company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34;}; } const {name, ...others} = getPersonDetails(); console.log(name); //\u0026#34;John\u0026#34; console.log(others); //{company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34;} Destructuring Function Arguments We can even pass an object into a function and then extract only the properties we want using Object Destructuring in this way:\nconst person = { name: \u0026#34;John\u0026#34;, company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34; }; function getPersonDetails({name, ...others }) { console.log(name); //\u0026#34;John\u0026#34; console.log(others); //{company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34;} } getPersonDetails(person); We can also set default values to the function arguments like this:\nconst person = { name: \u0026#34;John\u0026#34;, company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34; }; function getPersonDetails({name, department = \u0026#34;NA\u0026#34; }) { console.log(name); //\u0026#34;John\u0026#34; console.log(department); //\u0026#34;NA\u0026#34; } getPersonDetails(person); Common Use Cases When you write code, try to use Object Destructuring wherever possible. Some of the common use cases I came across are as follows:\nInside for-of loop We can use Object Destructuring inside for-of loop in this way:\nconst users = [{ name: \u0026#34;John\u0026#34;, company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34; }, { name: \u0026#34;Adam\u0026#34;, company: \u0026#34;Microsoft\u0026#34;, job: \u0026#34;Analyst\u0026#34; }, { name: \u0026#34;Bob\u0026#34;, company: \u0026#34;Yahoo\u0026#34;, job: \u0026#34;Data Scientist\u0026#34; }]; for (let {name, company} of users) { console.log(name, company); } // John Google // Adam Microsoft // Bob Yahoo Inside map function We can use Object Destructuring inside map function in this way:\nconst users = [{ name: \u0026#34;John\u0026#34;, company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34; }, { name: \u0026#34;Adam\u0026#34;, company: \u0026#34;Microsoft\u0026#34;, job: \u0026#34;Analyst\u0026#34; }, { name: \u0026#34;Bob\u0026#34;, company: \u0026#34;Yahoo\u0026#34;, job: \u0026#34;Data Scientist\u0026#34; }]; const userNames = users.map(({ name }) =\u0026gt; name); console.log(userNames); //[\u0026#34;John\u0026#34;, \u0026#34;Adam\u0026#34;, \u0026#34;Bob\u0026#34;] Named Export We can choose what to export from the module using Object Destructuring, and import keyword. If you have worked with Angular, React or any other JavaScript framework. You might have come across Named Export like this:\nimport React, { Component } from \u0026#39;React\u0026#39;; // Old way import React from \u0026#39;React\u0026#39;; class MyComponent extends React.Component{} // New way import React, { Component } from \u0026#39;React\u0026#39;; class MyComponent extends Component{} Same thing applies for any other package we import, we can choose only the functions we want and use them independently.\nimport { isEmail, isCreditCard } from \u0026#39;validator\u0026#39;; console.log(isEmail(\u0026#39;my@email.com\u0026#39;)); // true Console API We can destructure functions from Console API object:\nconst { log, warn, error } = console; log(\u0026#39;Hello World!\u0026#39;); // equivalent to console.log(\u0026#39;...\u0026#39;); warn(\u0026#39;Watch out!\u0026#39;); // console.warn(\u0026#39;...\u0026#39;); error(\u0026#39;Something went wrong!\u0026#39;); // console.error(\u0026#39;...\u0026#39;); Swapping Variables This is interesting to see that we can swap variables using Object Destructuring\nlet a = \u0026#34;a\u0026#34;; let b = \u0026#34;b\u0026#34;; [b, a] = [a, b]; console.log(a); // b console.log(b); // a HTML Elements We can destructure properties of HTML Elements like this:\nconst { value } = document.querySelector(\u0026#39;input\u0026#39;); This way it’s much easier to get a value property out of an input element, just as getting disabled property out of a button element.\nconst { disabled } = document.querySelector(\u0026#39;button\u0026#39;); HTML Events Let\u0026rsquo;s handle HTML Events using destructuring function arguments like this:\n\u0026lt;input type=\u0026#34;text\u0026#34; onchange=\u0026#34;handleChange(event)\u0026#34;/\u0026gt; Now we are going to extract a target object from an event object and then extract a value property from the target:\n// #1 function handleChange(event) { const { value } = event.target; console.log(value); } // # 2 function handleChange({ target }) { const { value } = target; console.log(value); } // # 3 function handleChange({ target: { value } }) { console.log(value); } Conditional Object Creation We can also create an object at runtime based on conditions using Object Destructuring\nconst isEmployed = true; const friends = [\u0026#34;Adam\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;]; const hobbies = null; const user = { name: \u0026#34;John\u0026#34;, ...(isEmployed \u0026amp;\u0026amp; { company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34; }), ...(friends \u0026amp;\u0026amp; {friends}), ...(hobbies \u0026amp;\u0026amp; {hobbies}) }; console.log(user); // prints {name: \u0026#34;John\u0026#34;, company: \u0026#34;Google\u0026#34;, job: \u0026#34;Developer\u0026#34;, friends: [\u0026#34;Adam\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;]} ","permalink":"https://codingnconcepts.com/javascript/object-destructuring/","tags":["JavaScript ES6","Javascript Object"],"title":"How to use Object Destructuring in JavaScript"},{"categories":["Javascript"],"contents":"In this quick tutorial, we\u0026rsquo;ll learn how to capitalize the first letter of a String in JavaScript.\n\u0026lsquo;capitalize\u0026rsquo; Function You can use this custom made capitalize() function to capitalize the first letter of a string:\n// es5 way function capitalize(string) { return string.charAt(0).toUpperCase() + string.slice(1); } // es6 way using destructuring const capitalize = ([first,...rest]) =\u0026gt; first.toUpperCase() + rest.join(\u0026#39;\u0026#39;); \u0026lsquo;capitalize\u0026rsquo; Function Details Let\u0026rsquo;s look at the steps involved to come up with capitalize() function:\nGet the first letter of the string using charAt() method const string = \u0026#34;string\u0026#34;; string.charAt(0); // Returns \u0026#34;s\u0026#34; Convert the first letter to uppercase using toUpperCase() method const string = \u0026#34;string\u0026#34;; string.charAt(0).toUpperCase(); // Returns \u0026#34;S\u0026#34; Get the rest of the string except first letter using slice() method const string = \u0026#34;string\u0026#34;; string.slice(1); // Returns \u0026#34;tring\u0026#34; Note that slice(1) means get a substring from index 1 to end of the string. Alternatively, You can also use substring(1). Finally, add the first uppercase letter to rest of the string var string = \u0026#34;string\u0026#34;; function capitalize(string) { return string.charAt(0).toUpperCase() + string.slice(1); } capitalize(string); // Returns \u0026#34;String\u0026#34; Add \u0026lsquo;capitalize\u0026rsquo; to String methods We can also add our custom made capitalize() function to String.prototype methods so that we can directly use that on a string.\nvar string = \u0026#34;string\u0026#34;; /* this is how methods are defined in prototype of any built-in Object */ Object.defineProperty(String.prototype, \u0026#39;capitalize\u0026#39;, { value: function () { return this.charAt(0).toUpperCase() + this.slice(1); }, writable: true, // so that one can overwrite it later configurable: true // so that it can be deleted later }); string.capitalize(); // Returns \u0026#34;String\u0026#34; Capitalize First Letter of each word in a given String We can use the capitalizeSentence function to capitalize first letter of each word in a sentence:\nfunction capitalizeSentence(sentence) { return sentence .split(\u0026#34; \u0026#34;) .map(string =\u0026gt; string.charAt(0).toUpperCase() + string.slice(1)) .join(\u0026#34; \u0026#34;); } capitalizeSentence(\u0026#34;a quick brown fox jumps over the lazy dog\u0026#34;); // \u0026#34;A Quick Brown Fox Jumps Over The Lazy Dog\u0026#34; ","permalink":"https://codingnconcepts.com/javascript/how-to-capitalize-first-letter-of-string-in-javascript/","tags":["Javascript String"],"title":"How to Capitalize First Letter of String in JavaScript"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to remove duplicates from an Array in JavaScript and return only unique values.\nHere is a quick view of different ways to remove duplicates from an Array:\n// Array: const fruits = [\u0026#39;apple\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;apple\u0026#39;]; // 1. ES6 Set [...new Set(fruits)]; // 2. filter fruits.filter((item, index) =\u0026gt; fruits.indexOf(item) == index); // 3. forEach fruits.forEach(item =\u0026gt; !uniqueFruits.includes(item) \u0026amp;\u0026amp; uniqueFruits.push(item)); // 4. reduce fruits.reduce((newarray, item) =\u0026gt; newarray.includes(item) ? newarray : [...newarray, item], []); // Result: // ▶ Set(3) {\u0026#34;apple\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;banana\u0026#34;} Use ES6 Set() If you have started using ES6 then it is recommended to use Set object.\nSet object lets you store unique values of any type, whether primitive values or object reference.\nWhen you pass an array to new Set() object, it removes any duplicate values. Let\u0026rsquo;s look at the code where two things are going on:\nFirst, we create a new Set object by passing an array which removes the duplicates. Second, we convert this object back to an array by using spread operator ... const fruits = [\u0026#39;apple\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;apple\u0026#39;]; const uniqueSet = new Set(fruits); // ▶ Set(3) {\u0026#34;apple\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;banana\u0026#34;} const backToArray = [...uniqueSet]; // ▶ (3) [\u0026#34;apple\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;banana\u0026#34;] Alternatively, you can also use Array.from to convert a Set into an array:\nconst fruits = [\u0026#39;apple\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;apple\u0026#39;]; const uniqueFruits = Array.from(new Set(fruits)); // ▶ Set(3) {\u0026#34;apple\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;banana\u0026#34;} Use Array.filter() The filter() method filter out the elements from an array based on the condition we provide. In other words,\nif the condition returns true, it will be included in filtered array if the condition returns false, it will NOT be included in filtered array const fruits = [\u0026#39;apple\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;apple\u0026#39;]; const uniqueFruits = fruits.filter((item, index) =\u0026gt; fruits.indexOf(item) == index); // ▶ Set(3) {\u0026#34;apple\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;banana\u0026#34;} Here use of Array.indexOf() method is very important which gives you the first occurrence of element in case there are duplicates. Let\u0026rsquo;s see the code:\nconst fruits = [\u0026#39;apple\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;apple\u0026#39;]; fruits.indexOf(\u0026#39;apple\u0026#39;); //0 fruits.indexOf(\u0026#39;orange\u0026#39;); //1 fruits.indexOf(\u0026#39;banana\u0026#39;); //4 In the above case, our filter() method filters out the first occurrence of each unique element.\nUse Array.forEach() The forEach() method can also be used to loop through elements of an array and push() elements to the new array which is not already there.\nconst fruits = [\u0026#39;apple\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;apple\u0026#39;]; const uniqueFruits = []; fruits.forEach(item =\u0026gt; !uniqueFruits.includes(item) \u0026amp;\u0026amp; uniqueFruits.push(item)); console.log(uniqueFruits); // ▶ Set(3) {\u0026#34;apple\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;banana\u0026#34;} Use Array.reduce() The reduce() method is used to reduce the elements of the array and combine them into a new array based on some reducer function that you pass.\nIn this case, our reducer function is checking if our new array contains the item. If it doesn’t, push that item into our new array. Otherwise, skip that element and return just our new array as is.\nReduce is always bit difficult to understand. Let\u0026rsquo;s look at the code:\nconst fruits = [\u0026#39;apple\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;apple\u0026#39;]; const uniqueFruits = fruits.reduce((newarray, item) =\u0026gt; newarray.includes(item) ? newarray : [...newarray, item], []); console.log(uniqueFruits); // ▶ Set(3) {\u0026#34;apple\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;banana\u0026#34;} Performance We have created an array with 1 million random numbers (ranging from 1 to 1000). Let\u0026rsquo;s check the performance of each method.\n// Array: var numbers = []; for ( var i = 0; i \u0026lt; 1000000; i++ ) { numbers.push(Math.floor((Math.random() * 1000) + 1)); } // 1. ES6 Set console.time(\u0026#34;set\u0026#34;); [...new Set(numbers)]; console.timeEnd(\u0026#34;set\u0026#34;); // 2. filter console.time(\u0026#34;filter\u0026#34;); numbers.filter((item, index) =\u0026gt; numbers.indexOf(item) == index); console.timeEnd(\u0026#34;filter\u0026#34;); // 3. forEach console.time(\u0026#34;forEach\u0026#34;); var uniqueNumbers = []; numbers.forEach(item =\u0026gt; !uniqueNumbers.includes(item) \u0026amp;\u0026amp; uniqueNumbers.push(item)); console.timeEnd(\u0026#34;forEach\u0026#34;); // 4. reduce console.time(\u0026#34;reduce\u0026#34;); numbers.reduce((newarray, item) =\u0026gt; newarray.includes(item) ? newarray : [...newarray, item], []); console.timeEnd(\u0026#34;reduce\u0026#34;); Here is the result when i ran the above code on MacBook Pro\u0026rsquo;s Google Chrome v83.0.4103.106 (64-bit):\nset: 23.051025390625ms filter: 1004.9609375ms forEach: 471.6630859375ms reduce: 472.902099609375ms We see that ES6 Set object is a clear winner followed by forEach loop. It is highly recommended to use ES6 Set to remove duplicates from an array because of its shorthand syntax and performance.\n","permalink":"https://codingnconcepts.com/javascript/how-to-remove-array-duplicates-in-javascript/","tags":["Javascript Array"],"title":"How to Remove Duplicates from Array in JavaScript"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to clone an Array in JavaScript and also learn the difference between shallow copy and deep copy of an Array.\nHere is a quick view of different ways to clone an array:\n/** Shallow Copy */ const fruits = [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;]; // 1. ES6 Spread Operator [...fruits]; // 2. Array.slice fruits.slice(); // 3. Array.from Array.from(fruits); /** Deep Copy */ const grocery = [[\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;], [\u0026#39;onion\u0026#39;, \u0026#39;potato\u0026#39;, \u0026#39;tomato\u0026#39;], [\u0026#39;milk\u0026#39;, \u0026#39;juice\u0026#39;]]; // 1. JSON.stringify \u0026amp; JSON.parse JSON.parse(JSON.stringify(grocery)); // 2. map grocery.map(element =\u0026gt; [...element]); Shallow Copy We\u0026rsquo;ll look at various methods to create a shallow copy of an Array in JavaScript. Shallow copy means the first level is copied and deeper levels are referenced.\nShallow copy is good enough when you are working with one-dimensional array. Let\u0026rsquo;s look at different ways to clone an array using shallow copy:\nES6 Spread Operator If you have started using ES6 then it is recommended to use spread operator to clone an array in JavaScript. It\u0026rsquo;s short syntax is very handy.\nconst fruits = [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;]; const cloneFruits = [...fruits]; // [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;] Array.slice() If you are still working old way then Array.slice can be used to clone an array:\nconst fruits = [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;]; const cloneFruits = fruits.slice(); // [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;] Array.from() We can also use Array.from method to clone an array:\nconst fruits = [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;]; const cloneFruits = Array.from(fruits); // [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;] Problem with Shallow Copy Let\u0026rsquo;s see how shallow copy behaves with multi-dimensional (nested) array:\nconst fruits = [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;]; const veggies = [\u0026#39;onion\u0026#39;, \u0026#39;potato\u0026#39;, \u0026#39;tomato\u0026#39;]; const liquids = [\u0026#39;milk\u0026#39;, \u0026#39;juice\u0026#39;]; const grocery = [fruits, veggies, liquids]; const groceryCopy = [...grocery]; grocery[2].push(\u0026#39;beer\u0026#39;); console.log(grocery); console.log(groceryCopy); // (3) [[\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;], [\u0026#39;onion\u0026#39;, \u0026#39;potato\u0026#39;, \u0026#39;tomato\u0026#39;], [\u0026#39;milk\u0026#39;, \u0026#39;juice\u0026#39;, \u0026#39;beer\u0026#39;]] // (3) [[\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;], [\u0026#39;onion\u0026#39;, \u0026#39;potato\u0026#39;, \u0026#39;tomato\u0026#39;], [\u0026#39;milk\u0026#39;, \u0026#39;juice\u0026#39;, \u0026#39;beer\u0026#39;]] // They\u0026#39;ve both been changed because they share references We see that both Array and its shallow copy have been changed because they share the reference. Remember, deeper levels are referenced in shallow copy.\nDeep copy We can use deep copy instead to solve the problem arises with shallow copy of multi-dimensional (nested) array. Let\u0026rsquo;s look at different ways to clone an array using deep copy:\nJSON.stringify() and JSON.parse() We can make a deep copy of an Array using the combination of JSON.stringify and JSON.parse\nJSON.stringify turns an object into a string. JSON.parse turns a string into an object. const fruits = [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;]; const veggies = [\u0026#39;onion\u0026#39;, \u0026#39;potato\u0026#39;, \u0026#39;tomato\u0026#39;]; const liquids = [\u0026#39;milk\u0026#39;, \u0026#39;juice\u0026#39;]; const grocery = [fruits, veggies, liquids]; const groceryCopy = JSON.parse(JSON.stringify(grocery)); grocery[2].push(\u0026#39;beer\u0026#39;); console.log(grocery); console.log(groceryCopy); // (3) [[\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;], [\u0026#39;onion\u0026#39;, \u0026#39;potato\u0026#39;, \u0026#39;tomato\u0026#39;], [\u0026#39;milk\u0026#39;, \u0026#39;juice\u0026#39;, \u0026#39;beer\u0026#39;]] // (3) [[\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;], [\u0026#39;onion\u0026#39;, \u0026#39;potato\u0026#39;, \u0026#39;tomato\u0026#39;], [\u0026#39;milk\u0026#39;, \u0026#39;juice\u0026#39;]] // These two arrays are completely separate! Array.map() You can also make a deep copy of multi-dimensional (nested) Array by iterating and make a copy at deeper levels by yourself using Array.map\nconst fruits = [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;]; const veggies = [\u0026#39;onion\u0026#39;, \u0026#39;potato\u0026#39;, \u0026#39;tomato\u0026#39;]; const liquids = [\u0026#39;milk\u0026#39;, \u0026#39;juice\u0026#39;]; const grocery = [fruits, veggies, liquids]; const groceryCopy = grocery.map(element =\u0026gt; [...element]); grocery[2].push(\u0026#39;beer\u0026#39;); console.log(grocery); console.log(groceryCopy); // (3) [[\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;], [\u0026#39;onion\u0026#39;, \u0026#39;potato\u0026#39;, \u0026#39;tomato\u0026#39;], [\u0026#39;milk\u0026#39;, \u0026#39;juice\u0026#39;, \u0026#39;beer\u0026#39;]] // (3) [[\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;], [\u0026#39;onion\u0026#39;, \u0026#39;potato\u0026#39;, \u0026#39;tomato\u0026#39;], [\u0026#39;milk\u0026#39;, \u0026#39;juice\u0026#39;]] // These two arrays are completely separate! Please note that the above example makes a deep copy of two-dimensional array. You need to iterate deeper if Array has more nested levels.\n","permalink":"https://codingnconcepts.com/javascript/how-to-clone-array-in-javascript/","tags":["Javascript Array"],"title":"How to Clone an Array in JavaScript"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to sort an Array in JavaScript by using Array.sort() method.\nSort String Array Let\u0026rsquo;s create a string array:\nconst fruits = [\u0026#39;mango\u0026#39;, \u0026#39;cherry\u0026#39;, \u0026#39;berries\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;kiwi\u0026#39;]; We can sort the array elements in ascending alphabetical (a-z, A-Z) order using sort() method:\nfruits.sort(); console.log(fruits); ▶ (6) [\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;berries\u0026#34;, \u0026#34;cherry\u0026#34;, \u0026#34;kiwi\u0026#34;, \u0026#34;mango\u0026#34;] We can also sort the array elements in descending alphabetical (z-a, Z-A) order using reverse() method:\nfruits.reverse(); console.log(fruits); ▶ (6) [\u0026#34;mango\u0026#34;, \u0026#34;kiwi\u0026#34;, \u0026#34;cherry\u0026#34;, \u0026#34;berries\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;apple\u0026#34;] Sort Number Array The built-in sort() and reverse() methods sort the array elements in alphabetical order so it is not useful when it comes to number array. Fortunately, sort() methods takes compare functions as an argument which can be used to override its default sorting behavior.\nLet\u0026rsquo;s first create our compare function:\nCompare Function function(a, b) {return a - b} When the sort() method compares two values, it sends the values to our compare function and sorts the values according to the returned value.\nIf the result is negative, a is sorted before b. If the result is positive, b is sorted before a. If the result is 0, nothing changes. Let\u0026rsquo;s sort number array in ascending order using sort() with compare function:\nconst numbers = [1, 7, 3, 5, 8, 2, 9, 4, 6]; numbers.sort(function(a, b){return a - b}); //ascending console.log(numbers); ▶ (9) [1, 2, 3, 4, 5, 6, 7, 8, 9] We need to reverse the subtraction from (a - b) to (b - a) in compare function, if we want to sort the numbers in descending order:\nconst numbers = [1, 7, 3, 5, 8, 2, 9, 4, 6]; numbers.sort(function(a, b){return b - a}); //descending console.log(numbers); ▶ (9) [9, 8, 7, 6, 5, 4, 3, 2, 1] ","permalink":"https://codingnconcepts.com/javascript/how-to-sort-array-in-javascript/","tags":["Javascript Array"],"title":"How to Sort an Array in JavaScript"},{"categories":["Interview Questions","Java"],"contents":"These Java 8 interview questions are based on my personal interview experience. Keep following this post for regular updates.\nAlso Read Core Java Interview Questions\nAlso Read Java Multithreading (Concurrency) Interview Questions\nQ1. What new features introduced in Java 8? Java 8 introduced several new features but the most significant are the following:-\nLambda Expressions is introduction of functional programming in Java to treat actions (functions) as Objects Method References is used to reference method as lambda expression using double colon :: operator Functional Interface is an interface with maximum one abstract method, implementation can be provided using a Lambda Expression Default methods give us the ability to add full implementations in interfaces besides abstract methods Stream APIs are a special iterator class that allows processing collections of objects in a functional manner Optional are special wrapper class used for expressing optionality Enhanced Date and Time API is an improved, immutable JodaTime-inspired Date API Nashorn, JavaScript Engine is Java-based engine for executing and evaluating JavaScript code Permanent Generation i.e. PermGen space which has limited memory size by default and often leads to OutOfMemory error. It is completely replaced by Metaspace which grows automatically by default and auto trigger the garbage collection when usage reach -XX:MaxMetaspaceSize Along with these new features, lots of feature enhancements are done under-the-hood, at both JVM and compiler level.\nQ2. What is a Functional Interface? A Functional Interface is an interface -\nhaving class annotated with @FunctionalInterface - which is informative and does not affect the semantics. must have one and only one abstract method can have one or more default and static methods with implementation. Function Interfaces can be implemented using lambda expressions. Let\u0026rsquo;s look at two examples:-\nExample 1 Java has some built-in functional interface such as java.lang.Runnable which is having only one run() abstract method.\n@FunctionalInterface public interface Runnable { public abstract void run(); } This is how we implement Runnable Functional Interface in general to create a new Thread :-\nThread thread = new Thread(new Runnable() { public void run() { System.out.println(\u0026#34;Hello World!\u0026#34;); } }); This is how we can implement Runnable using lambda expression:-\nThread thread = new Thread(() -\u0026gt; System.out.println(\u0026#34;Hello World!\u0026#34;)); Lambda expression is short and sweet.\nExample 2 Another functional interface is java.util.Comparator which is having one compare() abstract method, and many default and static methods.\n@FunctionalInterface public interface Comparator\u0026lt;T\u0026gt; { int compare(T o1, T o2); default Comparator\u0026lt;T\u0026gt; reversed() { return Collections.reverseOrder(this); } public static \u0026lt;T extends Comparable\u0026lt;? super T\u0026gt;\u0026gt; Comparator\u0026lt;T\u0026gt; reverseOrder() { return Collections.reverseOrder(); } } This is how we implement Comparator Functional Interface in general to sort the list :-\nList\u0026lt;String\u0026gt; a1 = Arrays.asList(\u0026#34;equity\u0026#34;, \u0026#34;stocks\u0026#34;, \u0026#34;gold\u0026#34;, \u0026#34;foreign exchange\u0026#34;, \u0026#34;fixed income\u0026#34;, \u0026#34;future\u0026#34;); a1.sort(new Comparator\u0026lt;String\u0026gt;() { @Override public int compare(String o1, String o2) { return o1.compareTo(o2); } }); This is how we can implement Comparator using lambda expression:-\nList\u0026lt;String\u0026gt; a1 = Arrays.asList(\u0026#34;equity\u0026#34;, \u0026#34;stocks\u0026#34;, \u0026#34;gold\u0026#34;, \u0026#34;foreign exchange\u0026#34;, \u0026#34;fixed income\u0026#34;, \u0026#34;future\u0026#34;); a1.sort((o1, o2) -\u0026gt; o2.compareTo(o1)); Q3. What is a default method and when do we use it? Before Java 8, an interface can have only abstract methods and no default methods.\nIn Java 8, Function Interface can have one or more default methods with implementation.\nFor example, below Functional Interface has two default methods compose and andThen:-\n@FunctionalInterface public interface Function\u0026lt;T, R\u0026gt; { R apply(T t); default \u0026lt;V\u0026gt; Function\u0026lt;V, R\u0026gt; compose(Function\u0026lt;? super V, ? extends T\u0026gt; before) { Objects.requireNonNull(before); return (V v) -\u0026gt; apply(before.apply(v)); } default \u0026lt;V\u0026gt; Function\u0026lt;T, V\u0026gt; andThen(Function\u0026lt;? super R, ? extends V\u0026gt; after) { Objects.requireNonNull(after); return (T t) -\u0026gt; after.apply(apply(t)); } static \u0026lt;T\u0026gt; Function\u0026lt;T, T\u0026gt; identity() { return t -\u0026gt; t; } } When you implement the above Functional Interface, you don\u0026rsquo;t need to implement compose and andThen default methods. You only implement apply abstract method.\nUsually, when a new abstract method is added to an interface, all implementing classes will break until they implement the new abstract method. default methods solves that problem as you do not need to implement them.\nThe default methods were introduced so that:-\nOld interfaces like Runnable and Comparator can be used with lambda expression as they have only one abstract method and many default and static methods, they qualify as Functional Interface Old interfaces like Collection can add new functionality using default method while maintaining backward compatibility with classes that are already implementing the interface. Q4. Will the following code compile? @FunctionalInterface public interface MyFunction\u0026lt;T, U, V\u0026gt; { public V apply(T t, U u); default void print(T t, U u) { System.out.println(t + u); } } Yes. The code will compile because it meets the functional interface specification having single abstract method apply and one or more default method print. Note that you don\u0026rsquo;t need to explicitly use the abstract keyword with apply method definition.\nQ5. Describe some of the Functional Interfaces in the Standard Library. There are a lot of functional interfaces in the java.util.function package, the more common ones include but not limited to:\nClass Name Description Usage Function accepts one argument and produces a result .map(s -\u0026gt; s.toUpperCase()) Consumer accepts one argument and returns no result .forEach(s -\u0026gt; System.out.println(s)) Supplier returns a result each time it invoked .collect(Collectors.toList()) Predicate accepts one argument and returns a boolean filter(s -\u0026gt; s.startsWith(\u0026quot;A\u0026quot;)) BiFunction accepts two arguments and produces a result BiConsumer accepts two arguments and returns no result BinaryOperator accepts two argument of same type and produces a result of same type .reduce(0, (a, b) -\u0026gt; a+b) Q6. What is a Method Reference? A method reference is a Java 8 construct that can be used for referencing a method without invoking it using double colon :: operator. It is used for treating methods as Lambda Expressions. They only work as syntactic sugar to reduce the verbosity of some lambdas. See below code, how method reference has reduced the verbosity over lambda expression.\nList\u0026lt;String\u0026gt; languages = Arrays.asList(\u0026#34;java\u0026#34;, \u0026#34;javascript\u0026#34;, \u0026#34;css\u0026#34;); // Lambda expression languages.forEach(str -\u0026gt; System.out.println(str)); // Method Reference languages.forEach(System.out::println); Double colon :: is basically refers to a single method, and this single method can be a\nA Static method ClassName::staticMethodName\ne.g. String::valueOf, Integer::parseInteger, Double::parseDouble An Instance method Object::instanceMethodName\ne.g. String::toString, System.out::println, String::toUpperCase A Constructor ClassName::new\ne.g. String::new A Super method super::parentMethodName Read Double Colon Operators in Java 8 for more details on method reference.\nQ7. What is a Lambda Expression and What is it used for? In very simple terms, a lambda expression is a function that can be referenced and passed around as an object.\nLambda expressions introduce functional style programming in Java and facilitate the writing of compact and easy-to-read code.\nBecause of this, lambda expressions are a natural replacement for anonymous classes as method arguments. One of their main uses is to define inline implementations of functional interfaces.\nInitialize a thread //Java 7 Thread t1 = new Thread(new Runnable(){ @Override public void run() { System.out.println(\u0026#34;Thread started in Java7\u0026#34;); } }); t1.start(); //Java 8 Lambda Expression Thread t2 = new Thread(() -\u0026gt; System.out.println(\u0026#34;Thread started in Java8\u0026#34;)); t2.start(); Sort a list List\u0026lt;String\u0026gt; a1 = Arrays.asList(\u0026#34;equity\u0026#34;, \u0026#34;stocks\u0026#34;, \u0026#34;gold\u0026#34;, \u0026#34;foreign exchange\u0026#34;,\u0026#34;fixed income\u0026#34;, \u0026#34;future\u0026#34;); //Java 7 a1.sort(new Comparator\u0026lt;String\u0026gt;() { @Override public int compare(String o1, String o2) { return o1.compareTo(o2); } }); //Java 8 Lambda Expression a1.sort((o1, o2) -\u0026gt; o2.compareTo(o1)); Q8. Explain the Syntax and Characteristics of a Lambda Expression? A lambda expression consists of two parts: the parameter part and the expressions part separated by a forward arrow as below:\n// Syntax param -\u0026gt; expression Any lambda expression has the following characteristics:\nOptional type declaration – when declaring the parameters on the left-hand side of the lambda, we don\u0026rsquo;t need to declare their types as the compiler can infer them from their values. Optional parentheses – when only a single parameter is declared, we don\u0026rsquo;t need to place it in parentheses. But when more than one parameter is declared, parentheses are required Optional curly braces – when the expressions part only has a single statement, there is no need for curly braces. But curly braces are required when there is more than one statement Optional return statement – when there is only single statement, then we can omit return keyword from return statement. // Lamda Expression Stream.of(\u0026#34;java\u0026#34;, \u0026#34;spring\u0026#34;, \u0026#34;spring boot\u0026#34;).filter((String s) -\u0026gt; { return s == \u0026#34;java\u0026#34;; }); // Removed Optional Type Declaration, Parentheses, Curly Braces and Return Keyword Stream.of(\u0026#34;java\u0026#34;, \u0026#34;spring\u0026#34;, \u0026#34;spring boot\u0026#34;).filter(s -\u0026gt; s == \u0026#34;java\u0026#34;); Q9. What is a Stream? How does it differ from a Collection? In simple terms, a stream is an iterator whose role is to accept a set of actions to apply on each of the elements it contains.\nThe stream represents a sequence of objects from a source such as a collection, which supports aggregate operations. They were designed to make collection processing simple and concise. Contrary to the collections, the logic of iteration is implemented inside the stream, so we can use methods like map and flatMap for performing a declarative processing.\nAnother difference is that the Stream API is fluent and allows pipelining:\nint sum = Arrays.stream(new int[]{1, 2, 3}) .filter(i -\u0026gt; i \u0026gt;= 2) .map(i -\u0026gt; i * 3) .sum(); And yet another important distinction from collections is that streams are inherently lazily loaded and processed.\nRead Streams in Java 8 for more examples on streams.\nQ10. What is the difference between Intermediate and Terminal Operations? Stream operations are combined into pipelines to process streams. All operations are either intermediate or terminal.\nIntermediate vs Terminal Operations Output: Output of intermediate operation is another stream whereas output of terminal operation is a collection, array or primitive. Chaining: Stream operation pipeline can have as many as intermediate operators chained together but pipeline must end with terminal operator. Lazy Evaluation: Intermediate operations are evaluated lazily whereas terminal operations are eager. The intermediate operations just remain as a pipeline, and executed only when the terminal operation is executed Pipeline: Stream operations pipeline can have many intermediate operations but only one terminal operation. Examples of Operations are as follows:-\nIntermediate Operations: filter(), map(), flatMap(), sorted(), distinct(), limit(), skip() Terminal Operations: forEach(), collect(), reduce(), min(), max(), count(), average(), sum(), anyMatch(), allMatch(), noneMatch(), findFirst(), findAny() Lazy Loading of Intermediate Operations Let\u0026rsquo;s look at the example to understand the lazy evaluation of intermediate operation:-\nSystem.out.println(\u0026#34;Stream without terminal operation\u0026#34;); Arrays.stream(new int[] { 1, 2, 3 }) .map(i -\u0026gt; { int n = i*2; System.out.println(\u0026#34;doubling \u0026#34; + i + \u0026#34;=\u0026#34; + n); return n; }); Output Stream without terminal operation We see that map() intermediate operations is not executed when there is no terminal operation.\nSystem.out.println(\u0026#34;Stream with terminal operation\u0026#34;); Arrays.stream(new int[] { 1, 2, 3 }) .map(i -\u0026gt; { int n = i*2; System.out.println(\u0026#34;doubling \u0026#34; + i + \u0026#34;=\u0026#34; + n); return n; }).sum(); Output Stream with terminal operation doubling 1=2 doubling 2=4 doubling 3=6 We see that map() intermediate operations is executed only when a terminal operation sum() exists.\nQ11. What is the difference between Map and flatMap Stream Operations? Both map and flatMap are intermediate stream operations that receive a function and apply this function to all elements of a stream.\nThe difference is that for the map, this function returns a value, but for flatMap, this function returns a stream. The flatMap operation “flattens” the streams into one.\nHere is an example of using map to get list of user\u0026rsquo;s name in uppercase and flatMap to get a flat list of user\u0026rsquo;s phone numbers:-\nMap\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; user = new HashMap\u0026lt;\u0026gt;(); user.put(\u0026#34;John\u0026#34;, Arrays.asList(\u0026#34;555-1123\u0026#34;, \u0026#34;555-3389\u0026#34;)); user.put(\u0026#34;Mary\u0026#34;, Arrays.asList(\u0026#34;555-2243\u0026#34;, \u0026#34;555-5264\u0026#34;)); user.put(\u0026#34;Steve\u0026#34;, Arrays.asList(\u0026#34;555-6654\u0026#34;, \u0026#34;555-3242\u0026#34;)); List\u0026lt;String\u0026gt; names = user.keySet().stream() .map(String::toUpperCase) .collect(Collectors.toList()); System.out.print(names); //[STEVE, JOHN, MARY] List\u0026lt;String\u0026gt; phones = user.values().stream() .flatMap(Collection::stream) .collect(Collectors.toList()); System.out.println(phones); //[555-6654, 555-3242, 555-1123, 555-3389, 555-2243, 555-5264] Q12. What is the difference between findFirst and findAny Stream Operations? findFirst operation always return the first elements of the stream irrespective of sequential or parallel stream findAny operation behavior is explicitly nondeterministic. It is free to select any element in the stream. This is to allow for maximal performance in parallel operations. List\u0026lt;String\u0026gt; users = Arrays.asList(\u0026#34;David\u0026#34;, \u0026#34;Jack\u0026#34;, \u0026#34;Duke\u0026#34;, \u0026#34;Jill\u0026#34;, \u0026#34;Dany\u0026#34;, \u0026#34;Julia\u0026#34;, \u0026#34;Peter\u0026#34;); Optional\u0026lt;String\u0026gt; findFirst = users.parallelStream().filter(s -\u0026gt; s.startsWith(\u0026#34;D\u0026#34;)).findFirst(); Optional\u0026lt;String\u0026gt; findAny = users.parallelStream().filter(s -\u0026gt; s.startsWith(\u0026#34;D\u0026#34;)).findAny(); System.out.println(findFirst.get()); //Always print David System.out.println(findAny.get()); //Print David or Duke or Dany Q13. What is Effectively Final in Java 8? Java 8 has introduced a new concept called \u0026ldquo;effectively final\u0026rdquo; variable. A non-final local variable or method parameter whose value is never changed after initialization is known as effectively final.\nIf you remember, prior to Java 8, we cannot use a local variable in an anonymous class. If you have to access a local variable in the Anonymous class, you have to make it final.\nIn Java 8, you can use non-final local variable in an anonymous class or lambda expression, if its value is never changed. Java internally consider these variables as effectively final and allow them to use.\nThe below code will throw error prior to Java 8 since message variable is not final, however it works just fine with Java 8 and above.\n// Effectively Final \u0026#34;message\u0026#34; variable String message = \u0026#34;Thread Started\u0026#34;; // Use in Anonymous Class Thread t1 = new Thread(new Runnable(){ @Override public void run() { System.out.println(message + \u0026#34; in Java7\u0026#34;); } }); t1.start(); // Use in Lambda Expression Thread t2 = new Thread(() -\u0026gt; System.out.println(message + \u0026#34; in Java8\u0026#34;)); t2.start(); Q14. What are the enhancements in Data and Time APIs in Java 8? The older date and time APIs before Java 8 are:-\njava.util.Date - represents a specific instant in time, with millisecond precision java.util.Calendar - an abstract class that provides methods for converting between a specific instant in time and a set of calendar fields java.util.Timezone - represents a time zone offset, and also figures out daylight savings The new data and time APIs in java.time.* package introduced in Java 8:-\njava.time.LocalDate - A date without time-zone in the ISO-8601 calendar system, such as 2007-12-03 java.time.LocalTime - A time without a time-zone in the ISO-8601 calendar system, such as 10:15:30 java.time.LocalDateTime - A date-time without a time-zone in the ISO-8601 calendar system, such as 2007-12-03T10:15:30 java.time.ZonedDateTime - A date-time with a time-zone in the ISO-8601 calendar system, such as 2007-12-03T10:15:30+01:00 Europe/Paris java.time.Period - A date-based amount of time in the ISO-8601 calendar system, such as \u0026lsquo;2 years, 3 months and 4 days. used to measure quantity of time in terms of years, months and days java.time.Duration - A time-based amount of time, such as \u0026lsquo;34.5 seconds\u0026rsquo;. used to measure quantity of times in terms of seconds and nanoseconds. Enhancement in Java 8 date and time APIs are as follows:-\nJava 8 date and time APIs in java.time.* package are immutable and thread-safe whereas older java.util.* package APIs are not. Java 8 date and time APIs provide many useful methods for date and time calculation, which are missing in older APIs Java 8 provides ZonedDateTime to deal with time zone specific date and time directly, which is not easy to deal in older APIs Java 8 provides many other classes such as Clock, Instant, DayOfWeek, Month, Year to deal with date and time at granular level Q15. What is Optional in Java 8? Before Java 8, there was no mechanism to check whether there is null value expected out of an API, and we had to just rely on null check.\nJava 8 introduced java.util.Optional, which is a container for an object which may or may not contain a non-null value. This class has various utility methods:-\nempty() - returns an empty Optional object of(T value) - returns an Optional with the specified present non-null value. ofNullable(T value) - Returns an Optional describing the specified value, if non-null, otherwise returns an empty Optional. isPresent() - returns true if value is present get() - returns value of the object ifPresent() - execute the block of code if value is present orElse() - return default value if value not present Optional\u0026lt;String\u0026gt; empty = Optional.empty(); empty.isPresent(); //false Optional\u0026lt;String\u0026gt; opt = Optional.of(\u0026#34;hello\u0026#34;); opt.isPresent(); //true opt.get() //hello opt.ifPresent(name -\u0026gt; System.out.println(name.length())); //5 Optional\u0026lt;String\u0026gt; nullable = Optional.ofNullable(null); nullable.isPresent(); //false nullable.orElse(\u0026#34;world\u0026#34;); //world nullable.orElseGet(() -\u0026gt; \u0026#34;world\u0026#34;) //world // usage with sptreams API User user = users.stream().findFirst().orElse(new User(\u0026#34;default\u0026#34;, \u0026#34;1234\u0026#34;)); ","permalink":"https://codingnconcepts.com/top-java-8-interview-questions/","tags":["Interview Q\u0026A","Java Q\u0026A"],"title":"Java-8 Interview Questions"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn JavaScript\u0026rsquo;s Console API methods with quick examples\u0026hellip;\nconsole.log console.log is the most frequently used method to log values to the console:-\nconst name = \u0026#39;Console!\u0026#39;; console.log(\u0026#39;hello,\u0026#39;, name); //hello, Console! We can also use String Template Literal:-\nconsole.log(`hello, ${name}`); //hello, Console! We can also log multiple values in a single statement to the console:-\nconst key = \u0026#39;value\u0026#39;; const number = 1; const fruits = [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;]; const obj = {a: 1, b: 2}; console.log(\u0026#39;key:\u0026#39;, name, \u0026#39;number:\u0026#39;, number, \u0026#39;fruits:\u0026#39;, fruits, \u0026#39;obj:\u0026#39;, obj); key: Console! number: 1 fruits: (3) [\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;mango\u0026#34;] obj: {a: 1, b: 2} Just like console.log, we have other methods to log values to the console:\nconsole.debug just like console.log with \u0026ldquo;debug\u0026rdquo; log level. Normally browser default log level is info and this won\u0026rsquo;t be displayed until you change log level to debug. console.info just like console.log with \u0026ldquo;info\u0026rdquo; log level console.warn prints a warning to the console console.error prints object to the console as an error, and include a stack trace console.debug(\u0026#39;Let me find you\u0026#39;); //won\u0026#39;t be displayed console.info(\u0026#39;Just FYI\u0026#39;); console.warn(\u0026#39;I told you !\u0026#39;); console.error(\u0026#39;I cannot do it.\u0026#39;); Just FYI ➤ ⚠️ I told you ! ➤ ⓧ I cannot do it. console.assert You can do some assertion tests using console.assert`.\nThe first argument is an expression that if evaluate to false, then assertion fails and the second argument gets printed to the console as an error.\n// this will pass, nothing will be logged console.assert(2 == \u0026#39;2\u0026#39;, \u0026#39;2 not == to \u0026#34;2\u0026#34;\u0026#39;); // this fails, \u0026#39;3 not === to \u0026#34;3\u0026#34;\u0026#39; will be logged console.assert(3 === \u0026#39;3\u0026#39;, \u0026#39;3 not === to \u0026#34;3\u0026#34;\u0026#39;); ➤ ⓧ Assertion failed: 3 not === to \u0026#34;3\u0026#34; You can also pass an object in the second argument which is printed when the assertion fails:\nconst x = 5; const y = 3; const reason = \u0026#39;x is expected to be less than y\u0026#39;; console.assert(x \u0026lt; y, {x, y, reason}); ➤ ⓧ Assertion failed: ➤ {x: 5, y: 3, reason: \u0026#34;x is expected to be less than y\u0026#34;} console.clear You can just clear the console using console.clear:\nconsole.clear(); console.count The console.count method is used to count the number of times it has been invoked with the same provided label. For example, here we have two counters, one for even values and one for odd values:\n[1, 2, 3, 4, 5].forEach(nb =\u0026gt; { if (nb % 2 === 0) { console.count(\u0026#39;even\u0026#39;); } else { console.count(\u0026#39;odd\u0026#39;); } }); odd: 1 even: 1 odd: 2 even: 2 odd: 3 The console.countReset method is used to reset the counter to 1, if we execute below after the above example,\nconsole.countReset(\u0026#39;even\u0026#39;); console.countReset(\u0026#39;odd\u0026#39;); console.count(\u0026#39;even\u0026#39;); console.count(\u0026#39;odd\u0026#39;); even: 1 odd: 1 console.dir You can print all the internal properties of an object in a formatted way using console.dir\nWe can print prototype methods of an Array or Object using console.dir,\nconsole.dir([\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;qux\u0026#34;]); //Array console.dir({a: \u0026#34;foo\u0026#34;, b: \u0026#34;bar\u0026#34;}); //Object ▼ Array(3) 0: \u0026#34;foo\u0026#34; 1: \u0026#34;bar\u0026#34; 2: \u0026#34;qux\u0026#34; length: 3 ▼ __proto__: Array(0) ➤ concat: ƒ concat() ➤ constructor: ƒ Array() ➤ ... ▼ Object a: \u0026#34;foo\u0026#34; b: \u0026#34;bar\u0026#34; ➤ __proto__: Object We can also print scopes and closures of a function using console.dir,\nvar outerFunc = function(c){ var a = 1; var innerFunc = function(d) { var b = 2; return a + b + c + d; } return innerFunc; } console.dir(outerFunc(3)); ▼ ƒ innerFunc(d) arguments: null caller: null length: 1 name: \u0026#34;innerFunc\u0026#34; ➤ prototype: {constructor: ƒ} ➤ __proto__: ƒ () ▼ [[[Scopes]]: Scopes[2] ➤ 0: Closure (outerFunc) {c: 3, a: 1} ➤ 1: Global {parent: Window, opener: null, top: Window, length: 1, frames: Window, …} console.dirxml You can print DOM element in HTML like tree structure using console.dirxml\nconsole.dirxml(document.body); ▼ \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;hello\u0026lt;/h1\u0026gt; \u0026lt;script\u0026gt; console.dirxml(document.body); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; console.group You can group the log messages together using console.group and console.groupEnd.\nconsole.group(\u0026#34;API Details\u0026#34;); console.log(\u0026#34;Scheme : HTTPS\u0026#34;); console.log(\u0026#34;Host : example.com\u0026#34;); console.groupEnd(); ▼ API Details Scheme : HTTPS Host : example.com Please note that group messages logged using console.group are initially expanded. If you\u0026rsquo;d rather have them collapsed by default, you can use console.groupCollapsed instead:\nconsole.groupCollapsed(\u0026#34;API Details\u0026#34;); console.log(\u0026#34;Scheme : HTTPS\u0026#34;); console.log(\u0026#34;Host : example.com\u0026#34;); console.groupEnd(); ▶ API Details You can further group the messages into nested levels. That allows you to print hierarchical data to the console in a cleanly formatted manner:\nconsole.group(\u0026#34;API Details\u0026#34;); console.log(\u0026#34;Scheme : HTTPS\u0026#34;); console.log(\u0026#34;Host : example.com\u0026#34;); // nesting console.group(\u0026#34;User API\u0026#34;); console.log(\u0026#34;Method : GET\u0026#34;); console.log(\u0026#34;Endpoint : /user\u0026#34;); // further nesting console.group(\u0026#34;Query Parameters\u0026#34;); console.log(\u0026#34;id : 1\u0026#34;); console.groupEnd(); console.groupEnd(); console.groupEnd(); ▼ API Details Scheme : HTTPS Host : example.com ▼ User API Method : GET Endpoint : /user ▼ Query Parameters id : 1 console.table You can print data in tabular format using console.table. This method is quite useful to visualize large objects and arrays.\nvar john = { firstName: \u0026#34;John\u0026#34;, lastName: \u0026#34;Smith\u0026#34;, age: 41 }; var jane = { firstName: \u0026#34;Jane\u0026#34;, lastName: \u0026#34;Doe\u0026#34;, age: 38 }; var emily = { firstName: \u0026#34;Emily\u0026#34;, lastName: \u0026#34;Jones\u0026#34;, age: 12 }; console.table([john, jane, emily]); ┌─────────┬───────────┬──────────┬─────┐ │ (index) │ firstName │ lastName │ age │ ├─────────┼───────────┼──────────┼─────┤ │ 0 │ \u0026#39;John\u0026#39; │ \u0026#39;Smith\u0026#39; │ 41 │ │ 1 │ \u0026#39;Jane\u0026#39; │ \u0026#39;Doe\u0026#39; │ 38 │ │ 2 │ \u0026#39;Emily\u0026#39; │ \u0026#39;Jones\u0026#39; │ 12 │ └─────────┴───────────┴──────────┴─────┘ ▶ Array(3) You can also choose to print a few properties when working with large objects in this way:\nconsole.table([john, jane, emily], [\u0026#34;firstName\u0026#34;]); ┌─────────┬───────────┐ │ (index) │ firstName │ ├─────────┼───────────┤ │ 0 │ \u0026#39;John\u0026#39; │ │ 1 │ \u0026#39;Jane\u0026#39; │ │ 2 │ \u0026#39;Emily\u0026#39; │ └─────────┴───────────┘ ▶ Array(3) Also learn more about console.table() usage in JavaScript\nconsole.time You can print the time taken by a code execution by starting a timer with console.time and ending it with console.timeEnd.\nThe timer can have an optional label, if you use a timer with label then both console.time and console.timeEnd should have same label.\nconsole.time(\u0026#39;fetching data\u0026#39;); fetch(\u0026#39;https://jsonplaceholder.typicode.com/users\u0026#39;) .then(d =\u0026gt; d.json()) .then(console.log); console.timeEnd(\u0026#39;fetching data\u0026#39;); fetching data: 0.435791015625ms ▶ (10) [{…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}] console.trace You can use console.trace() to print a stack trace of method execution flow to the console.\n1 const first = () =\u0026gt; { second(); }; 2 const second = () =\u0026gt; { third(); }; 3 const third = () =\u0026gt; { fourth(); }; 4 const fourth = () =\u0026gt; { console.trace(\u0026#34;The trace\u0026#34;); }; 5 first(); You get @ file_name:line_number in console output which you can click on to navigate to the source.\nOutput ▼ The trace fourth @ test:4 third\t@ test:3 second @ test:2 first\t@ test:1 (anonymous) @ test:5 console.log formatting console.log also provide formatting to make our log messages stand out from other messages. This gives us the ability to find our important messages in the console.\n%s format a value as a string %d or %i format a value as an integer %f format a value as a floating point number %o is used to print as expandable DOM Element %O is used to print as JavaScript object %c is used to apply CSS style rules to the output string as specified by the second parameter Let\u0026rsquo;s format the values as string, integer and floating point number:\nconsole.log(\u0026#39;The %s is turning %d in next %f months\u0026#39;, \u0026#39;kid\u0026#39;, 18, 2.5); //The kid is turning 18 in next 2.5 months Let\u0026rsquo;s see what the fuss about printing DOM as expandable HTML element tree or JSON object tree:\nconsole.log(\u0026#39;%o\u0026#39;, document.body); //Print HTML element tree console.log(\u0026#39;%O\u0026#39;, document.body); //Print JSON object tree ▶ \u0026lt;body class=\u0026#34;body\u0026#34; data-gr-c-s-loaded=\u0026#34;true\u0026#34;\u0026gt; ... \u0026lt;body\u0026gt; ▶ body.body You can print colorful formatted log messages like this:\nconst red = \u0026#39;color:red\u0026#39;; const orange = \u0026#39;color:orange\u0026#39;; const green = \u0026#39;color:green\u0026#39;; const blue = \u0026#39;color:blue\u0026#39;; console.log(\u0026#39;%cHello %cWorld %cfoo %cbar\u0026#39;, red, orange, green, blue); Hello World foo bar You can also print stylish banners to the console using multiple style properties:\nconsole.log( \u0026#39;%cThanks for reading %cCodingNConcepts :)\u0026#39;, \u0026#39;color: #000; font-weight: bold; font-size: 1.5rem; border: 1px solid black; padding: 5px; margin-right: 5px;\u0026#39;, \u0026#39;color: #e22d30; font-weight: bold; font-size: 2rem; text-shadow: 0 0 5px rgba(0,0,0,0.2);\u0026#39;, ); Thanks for reading CodingNConcepts :) ","permalink":"https://codingnconcepts.com/javascript/console-api-walkthrough/","tags":["Javascript Console"],"title":"JavaScript Console API Walkthrough"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn plenty of tips and tricks for debugging JavaScript code using Chrome DevTool, different ways of logging to the console using console APIs, and much more\u0026hellip;\nSay No to alerts alert(\u0026#34;I will pop up in browser\u0026#34;); When we use alert() for debugging value, it pops up in the browser which is blocking and annoying. Try to avoid them as there are alternate ways available for debugging, we\u0026rsquo;ll look at them subsequently.\nBrowser as Debugger Most of the modern browsers such as Chrome, Firefox, Edge, Opera, and Safari has built-in support for JavaScript debugging. Chrome as the debugger is generally the first choice among developers.\nChrome Developer Tool To open DevTool from Chrome\u0026rsquo;s main menu, select More tools ➞ Developer tools.\nShortcut keys Mac Windows, Linux, Chrome OS Elements Tab Command+Option+C Control+Shift+C Console Tab Command+Option+J Control+Shift+J Sources Tab Command+Option+I Control+Shift+I Chrome Developer Tool\nSome of the most frequently used icons of Chrome DevTools are:-\nUse Inspect icon ⬉ to select an element in the page and inspect its DOM position in Elements Tab. You can update or delete a DOM element, inspect the CSS applied on DOM element, and much more in Elements Tab. Use Device icon ⍇ to check the responsiveness of the website. It adds an additional toolbar on your browser page where you can simulate viewport for different devices, for e.g. mobile phones (such as iPhone, Samsung Galaxy, Nexus, LG, Nokia, Blackberry, etc), tablets (such as iPad etc) and laptops (with or without touch). You can see all JavaScript source code that renders the page, under Sources Tab. Generally source files are minified and it is difficult to understand them and apply breakpoints. Use Format Icon {} to format these minified files in human readable format. Line-of-code Breakpoint You can use breakpoint to pause your JavaScript code in browser. The most well known type of breakpoint is line-of-code.\nTo set a line-of-code breakpoint in DevTools:\nClick on Sources tab. Open the file and go to the line of code you want to debug. You will see line number column to the left of code. Click on it. An icon appears beside line number to indicate that the breakpoint is set. Click again on same line number if you want to remove breakpoint. Icon disappears. Please note that the breakpoint icon in DevTool may look different depending upon chrome version and OS (windows or MacOS)\nSometime line-of-code breakpoints can be inefficient to set, especially if you don\u0026rsquo;t know exactly where to look, or if you are working with a large codebase. You can save your time when debugging by knowing how and when to use the other types of breakpoints.\nBreakpoint Type Use This When You Want To Pause\u0026hellip; Line-of-code On an exact line of code. Conditional line-of-code On an exact line of code, but only when some other condition is true. DOM On the code that changes or removes a specific DOM node, or its children. XHR On the XHR send() or fetch() method When the request URL contains a string pattern. Event listener On the code that runs after an event, such as click, is fired. Exception On the line of code that is throwing a caught or uncaught exception. Function Whenever a specific function is called. Set Breakpoint from code using \u0026lsquo;debugger\u0026rsquo; statement When debugger mode is on in the browser and the code execution reaches to debugger; statement, it pause on that line just like line-of-code breakpoint.\nThe debugger statement is quite useful when you want to debug a certain part of your code in browser. Normally you find the buggy code in browser developer tool and set a breakpoint to debug. Sometimes it is not easy to find the code in browser, in that case, you can insert debugger; statement in your source code.\nfunction hello(name) { let phrase = `Hello, ${name}!`; debugger; // \u0026lt;-- the debugger pause on this line say(phrase); } Conditional Breakpoint You can use a conditional line-of-code breakpoint when you know the exact line of code that you need to investigate, but you want to pause only when some other condition is true.\nTo set a conditional line-of-code breakpoint:\nClick the Sources tab. Open the file and go to the line of code you want to debug. You will see line number column to the left of code. Right-click it. Select Edit breakpoint.., A dialog displays underneath the line of code. Enter your condition in the dialog. Press Enter to activate the breakpoint. You see that icon for conditional breakpoint is different from simple line-of-code breakpoint. Breakpoint icon in DevTool may look different depending upon chrome version and OS (windows or MacOS)\nChrome DevTool Conditional Breakpoint\nDOM Change Breakpoint Use a DOM change breakpoint when you want to pause on the code that changes a DOM node or its children.\nTo set a DOM change breakpoint:\nClick the Elements tab. Go the element that you want to set the breakpoint on. Right-click the element. Hover over Break on then select Subtree modifications, Attribute modifications, or Node removal. Types of DOM change breakpoints Subtree modifications: Triggered when a child of the currently-selected node is removed or added, or the contents of a child are changed. Not triggered on child node attribute changes, or on any changes to the currently-selected node. Attributes modifications: Triggered when an attribute is added or removed on the currently-selected node, or when an attribute value changes. Node Removal: Triggered when the currently-selected node is removed. XHR/Fetch Breakpoint If you encounter an error in AJAX request and you are not able to identify the code which is submitting this request, then XHR breakpoint is very useful to quickly find the AJAX source code.\nXHR breakpoint pause the execution of code when the request URL of an AJAX request contains a specified string. XHR breakpoint is supported for AJAX send() and fetch() methods.\nTo set an XHR breakpoint:\nClick the Sources tab. Expand the XHR Breakpoints pane. Click Add breakpoint. Enter the string which you want to break on. DevTools pauses when this string is present anywhere in an XHR\u0026rsquo;s request URL. Press Enter to confirm. Event listener Breakpoint Use event listener breakpoints when you want to pause on the event listener code that runs after an event is fired. You can select specific events, such as click under mouse events, or all events, such as cut, copy, paste under clipboard category.\nTo turn on event listener breakpoint:\nClick the Sources tab. Expand the Event Listener Breakpoints pane. DevTools shows a list of event categories, such as Animation, Canvas, Clipboard, Mouse, etc. Either check ☑ category to include all events under that category, or expand the category and check ☑ a specific event. Exception Breakpoint The Chrome Developer Tools allows you to pause execution of your JavaScript code upon throwing a caught or uncaught exception. This is particularly useful when your code is not throwing error and failing silently. This makes it possible for you to inspect the state of your application at the time when the Error object was created.\nTo turn on exception breakpoint:\nClick the Sources tab. Click Pause on exceptions icon. It turns blue when enabled. Check ☑ Pause on uncaught exceptions if you want to pause execution on caught exceptions as well. Remember to turn them off when done. Chrome DevTool XHR, Event Listener and Exception Breakpoints\nFunction Breakpoint Call debug(functionName), where functionName is the function you want to debug, when you want to pause whenever a specific function is called. You can insert debug() into your code (like a console.log() statement) or call it from the DevTools Console. debug() is equivalent to setting a line-of-code breakpoint on the first line of the function.\nMake sure the target function is in scope. DevTools throws a ReferenceError if the function you want to debug is not in scope.\nfunction sum(a, b) { let result = a + b; // DevTools pauses on this line. return result; } debug(sum); // Pass the function object, not a string. sum(); Blackbox Script files When you debug your application code in Chrome DevTool and step through your code line-by-line, the debugger sometimes jumps into a source file that\u0026rsquo;s not your focus (such as a third-party JS library). I\u0026rsquo;m sure you\u0026rsquo;ve experienced the annoyance of stepping through the library code before getting back to your own application code.\nThe Chrome DevTool provides an ability to blackbox JavaScript file(s) so that debugger will not jump into those files and ignore them when stepping through code you\u0026rsquo;re debugging.\nWhat happens when you blackbox a script? Exceptions thrown from library code will not pause (if Pause on exceptions is enabled), Stepping into/out/over bypasses the library code, Event listener breakpoints don\u0026rsquo;t break in library code, The debugger will not pause on any breakpoints set in library code. The end result is you are debugging your application code instead of third party resources. To blackbox JavaScript files:\nClick on Main Menu \u0026gt; Settings icon OR use F1 shortcut key to open settings Click on Blackboxing menu item. Click on Add Pattern\u0026hellip; button. Enter file name or pattern in text box and click Add button. Check ☑ Blackbox content script to enable blackboxing. Chrome DevTool Blackbox Script\nSnippets Generally we use browser console to execute and test some code snippets. Sometime, we need to test same code again and again. The Chrome DevTools provides the ability to save code snippets for future use.\nTo save your code snippet:\nClick the Sources tab. In the left panel of Sources Tab, Click on the Snippets sub-tab. Click on + New Snippet to create a new file to save your code snippet. When you open a snippet code. There are icons to format and execute your code. You can use snippets to store your frequently used debugging code scripts, made by you or other developers. Paul Irish has written some useful snippets - stopBefore.js, Grep.js which you can copy in your DevTool snippets. Snippets are accessible from console. Let\u0026rsquo;s go through them:-\nThe storeBefore.js snippet allows to setup a breakpoint that is triggered just before a certain function is called. For example, this will trigger a breakpoint just before the document.getElementById function is called: stopBefore(document, \u0026#39;getElementById\u0026#39;) The grep.js snippet allows searching an object and its prototypical chain for properties that match a given search criteria. For example this instruction will search for all properties matching get in the document object: grep(document, \u0026#39;get\u0026#39;); The debugAccess.js snippet allows to trigger a breakpoint when a given property is accessed. For example this will trigger a breakpoint each time document.cookie is called: debugAccess(document, \u0026#39;cookie\u0026#39;); Print Multiple Values Most of the developers use console.log() for debugging values in browser console. It is a king of debugging and this solves most of your debugging problems.\nThe thing you might not know is, console.log() can print multiple values by providing comma-separated values so you don\u0026rsquo;t need to concat values yourself.\nEach comma \u0026quot;,\u0026quot; add a space between values.\nlet x = 1; console.log(\u0026#39;x =\u0026#39;, x); let y = \u0026#34;hello\u0026#34;; let fruits = [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;]; var obj = {a: 1, b: 2}; console.log(\u0026#39;x =\u0026#39;, x, \u0026#39;y =\u0026#39;, y, \u0026#39;fruits =\u0026#39;, fruits, \u0026#39;obj =\u0026#39;, obj); Output x = 1 x = 1 y = hello fruits = (3) [\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;mango\u0026#34;] obj = {a: 1, b: 2} You can also use Template Literal to combining multiple values in single string like this\nconsole.log(`x = ${x}, y = ${y}, fruits = ${fruits}, obj = ${obj}`); Avoid logging Object Reference Don\u0026rsquo;t use console.log(obj), use console.log(JSON.parse(JSON.stringify(obj))) instead.\nWhen you log an array (or object) using console.log() and update it afterward. Many browsers show you the latest state of the array (or object) which might be misleading.\nLogging using console.log(JSON.parse(JSON.stringify(obj))) make sure, you are printing a copy of array (or object) which will log the exact state at the time of printing.\nLet\u0026rsquo;s understand this from below array example,\nlet fruits = [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;mango\u0026#39;]; console.log(fruits); console.log(JSON.parse(JSON.stringify(fruits))); //makes a copy of it fruits.push(\u0026#39;grapes\u0026#39;); Output ▼ (3) [\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;mango\u0026#34;] 0: \u0026#34;apple\u0026#34; 1: \u0026#34;banana\u0026#34; 2: \u0026#34;mango\u0026#34; 3: \u0026#34;grapes\u0026#34; length: 4 ➤ __proto__: Array(0) ▼ (3) [\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;mango\u0026#34;] 0: \u0026#34;apple\u0026#34; 1: \u0026#34;banana\u0026#34; 2: \u0026#34;mango\u0026#34; length: 3 ➤ __proto__: Array(0) We see that console.log() showing the latest state of array with length 4 whereas its length was 3 at the time of logging.\nLet\u0026rsquo;s also look at object example,\nlet person = { name: \u0026#39;adam\u0026#39;, age: 21, gender: \u0026#39;male\u0026#39; }; console.log(person); console.log(JSON.parse(JSON.stringify(person))); //makes a copy of it person.married = \u0026#39;NO\u0026#39;; Output ▼ {name: \u0026#34;adam\u0026#34;, age: 21, gender: \u0026#34;male\u0026#34;} age: 21 gender: \u0026#34;male\u0026#34; married: \u0026#34;NO\u0026#34; name: \u0026#34;adam\u0026#34; ➤ __proto__: Object ▼ {name: \u0026#34;adam\u0026#34;, age: 21, gender: \u0026#34;male\u0026#34;} age: 21 gender: \u0026#34;male\u0026#34; name: \u0026#34;adam\u0026#34; ➤ __proto__: Object We see that console.log() showing the latest state of object including married field which was missing at the time of logging.\nPrint DOM element in JSON format DOM elements are printed as HTML elements tree structure using console.log(). Instead, we can use console.dir() to see DOM elements in JSON like tree structure.\nconsole.log(document.body); console.dir(document.body); Print Constructor function\u0026rsquo;s Prototype Methods The console.dir() is quite useful to print constructor function\u0026rsquo;s internal properties such as prototype methods.\nWe see in below example that console.log() just print the name of constructor function whereas, using console.dir(), we\u0026rsquo;re able to see all prototype methods of Array function.\nconsole.log(Array); console.dir(Array); Output ƒ Array() { [native code] } ▼ ƒ Array() arguments: (...) caller: (...) length: 1 name: \u0026#34;Array\u0026#34; ▼ prototype: ƒ () length: 0 ➤ constructor: ƒ Array() ➤ concat: ƒ concat() ➤ find: ƒ find() ➤ findIndex: ƒ findIndex() ➤ lastIndexOf: ƒ lastIndexOf() ➤ pop: ƒ pop() ➤ push: ƒ push() ➤ reverse: ƒ reverse() ➤ slice: ƒ slice() ➤ sort: ƒ sort() ➤ splice: ƒ splice() ➤ includes: ƒ includes() ➤ indexOf: ƒ indexOf() ➤ join: ƒ join() ➤ toString: ƒ toString() ... Print Function\u0026rsquo;s Closure The console.dir() is quite useful to print function\u0026rsquo;s internal properties such as scopes and closures.\nWe see in below example that console.log() just print the function\u0026rsquo;s signature, using console.dir(), we’re able to see the prototype methods, scopes and most importantly closures of the function.\nvar outerFunc = function(c){ var a = 1; var innerFunc = function(d) { var b = 2; var innerMostFunc = function(e) { return a + b + c + d + e; } return innerMostFunc; } return innerFunc; } // print innerMostFunc console.log(outerFunc(3)(4)); console.dir(outerFunc(3)(4)); Output ƒ (e) { return a + b + c + d + e; } ▼ ƒ innerMostFunc(c) length: 1 name: \u0026#34;innerMostFunc\u0026#34; arguments: null caller: null ➤ prototype: {constructor: ƒ} ➤ __proto__: ƒ () [[FunctionLocation]]: ▼ [[Scopes]]: Scopes[3] ➤ 0: Closure (innerFunc) {d: 4, b: 2} ➤ 1: Closure (outerFunc) {c: 3, a: 1} ➤ 2: Global {parent: Window, opener: null, top: Window, length: 1, frames: Window, …} Print Object\u0026rsquo;s internal properties Some of the browsers print a stringify version of object when use console.log() whereas prints object JSON tree when use console.dir(). There is not much difference in printing in advance browsers such as chrome where both methods print object JSON tree.\nvar obj = {a: 1, b: 2}; console.log(obj); console.dir(obj); We see that nothing much different when print object using both methods in chrome browser,\nOutput ▼ {a: 1, b: 2} a: 1 b: 2 ➤ __proto__: Object ▼ Object a: 1 b: 2 ➤ __proto__: Object Print Objects as Table The console.table() method is quite useful to print a large set of data in tabular format to visualize it easily. This method also provides the ability to print a few fields out of those large data sets. When rendered in browser as a table, browser provides the ability to sort the columns of the table.\nLet\u0026rsquo;s look at the example where we are logging a large JSON in tabular format,\nvar personDetails = [ { \u0026#34;_id\u0026#34;: \u0026#34;5edbbb78633118f455e877fb\u0026#34;, \u0026#34;index\u0026#34;: 0, \u0026#34;guid\u0026#34;: \u0026#34;30dd1d2c-5083-4165-8580-5ae734cd0d12\u0026#34;, \u0026#34;isActive\u0026#34;: true, \u0026#34;balance\u0026#34;: \u0026#34;$1,778.03\u0026#34;, \u0026#34;picture\u0026#34;: \u0026#34;http://placehold.it/32x32\u0026#34;, \u0026#34;age\u0026#34;: 26, \u0026#34;eyeColor\u0026#34;: \u0026#34;blue\u0026#34;, \u0026#34;name\u0026#34;: { \u0026#34;first\u0026#34;: \u0026#34;Anderson\u0026#34;, \u0026#34;last\u0026#34;: \u0026#34;Sargent\u0026#34; }, \u0026#34;company\u0026#34;: \u0026#34;MAZUDA\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;anderson.sargent@mazuda.com\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;+1 (839) 437-3851\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;235 Ashland Place, Chautauqua, Minnesota, 3487\u0026#34;, \u0026#34;about\u0026#34;: \u0026#34;Pariatur nisi cillum culpa aliquip mollit veniam. Laboris in minim non dolor ut deserunt ex sit occaecat irure consequat pariatur esse. Cillum velit dolore enim non enim ipsum aliqua veniam fugiat adipisicing magna mollit occaecat.\u0026#34;, \u0026#34;registered\u0026#34;: \u0026#34;Saturday, April 8, 2017 3:02 AM\u0026#34;, \u0026#34;latitude\u0026#34;: \u0026#34;26.03084\u0026#34;, \u0026#34;longitude\u0026#34;: \u0026#34;-74.869342\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;labore\u0026#34;, \u0026#34;nulla\u0026#34;, \u0026#34;ea\u0026#34;, \u0026#34;qui\u0026#34;, \u0026#34;sunt\u0026#34; ], \u0026#34;range\u0026#34;: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \u0026#34;friends\u0026#34;: [ { \u0026#34;id\u0026#34;: 0, \u0026#34;name\u0026#34;: \u0026#34;Coleman Nunez\u0026#34; }, { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Foley Curry\u0026#34; }, { \u0026#34;id\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;Kara Glass\u0026#34; } ], \u0026#34;greeting\u0026#34;: \u0026#34;Hello, Anderson! You have 5 unread messages.\u0026#34;, \u0026#34;favoriteFruit\u0026#34;: \u0026#34;apple\u0026#34; } ] console.table(personDetails); console.table(personDetails, [\u0026#34;age\u0026#34;, \u0026#34;eyeColor\u0026#34;]); // print few fields The below output in tabular format is from chrome browser, Also learn more about console.table() usage in JavaScript\nPrint Time taken for Code Execution You can print time taken by a code execution by using console.time() before and console.timeEnd() after that particular piece of code. Few pointers:-\nconsole.time() and console.timeEnd() methods should have same labels. You can also set up multiple timers by assigning different labels to the method. console.time() and console.timeEnd() are part of the Console API (just like console.log()) Let’s see how it works:\nconsole.time(\u0026#39;Timer1\u0026#39;); console.time(\u0026#39;Timer2\u0026#39;); var items = []; for(var i = 0; i \u0026lt; 100000; i++){ items.push({index: i}); } console.timeEnd(\u0026#39;Timer1\u0026#39;); for(var j = 0; j \u0026lt; 100000; j++){ items.push({index: j}); } console.timeEnd(\u0026#39;Timer2\u0026#39;); Output Timer1: 13.088134765625ms Timer2: 26.070517578125ms Print Stack trace of Method Execution You can use console.trace() to print a stack trace of method execution flow to the console. Few pointers:-\nconsole.trace() is compatible with the snippets feature of Chrome DevTools. console.trace() is part of the Console API (just like console.log()) Let’s see how it works:\n1 const first = () =\u0026gt; { second(); }; 2 const second = () =\u0026gt; { third(); }; 3 const third = () =\u0026gt; { fourth(); }; 4 const fourth = () =\u0026gt; { console.trace(\u0026#34;The trace\u0026#34;); }; 5 first(); You get @ file_name:line_number in console output which you can click on to navigate to the source.\nOutput ▼ The trace fourth @ test:4 third\t@ test:3 second @ test:2 first\t@ test:1 (anonymous) @ test:5 Use a Unit Testing Framework There are many third-party unit-testing framework for JavaScript available with their own philosophy and syntax. Here are most widely used JavaScript testing frameworks:-\nMocha is feature-rich framework running on Node.js and in the browser, making asynchronous testing simple and fun. Mocha tests run serially, allowing for flexible and accurate reporting, while mapping uncaught exceptions to the correct test cases. JEST is popular framework maintained by Facebook. It is a preferred framework for the React based applications as it requires zero configuration. It works great with projects using Babel, TypeScript, Node, React, Angular, and Vue. Jasmine is a behavior-driven testing framework. It has no external dependencies. It does not require a DOM. And it has a clean, obvious syntax so that you can easily write tests. QUnit is a powerful, easy-to-use JavaScript unit testing framework. It\u0026rsquo;s used by the jQuery, jQuery UI and jQuery Mobile projects and is capable of testing any generic JavaScript code Thanks for reading till the end. I hope this tutorial would have helped you to find some useful tips for debugging and logging to the console, which you\u0026rsquo;d not known before.\nReference:\nGoogle Developers Chrome DevTool ","permalink":"https://codingnconcepts.com/javascript/debugging-tips-and-tricks/","tags":["Javascript Console"],"title":"JavaScript Debugging Tips and Tricks"},{"categories":["Hugo"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to setup client side search engine for Hugo Website without any backend server or additional build steps required.\nOverview When you generate a website using Hugo static site generator. You write your pages generally in markdown (.md files) so all your content lies in the markdown files.\nWe are going to follow these steps to create our search engine:-\nUpdate config.toml file to instruct Hugo to generate JSON file of entire website content Create layouts/_default/index.json to provide a template to Hugo for generating a index.json file Create static/js/search.js to use Fuse.js for searching content out of this JSON file Create content/search.md to serve a page for /search URL Create layouts/_default/search.html to provide a template to render your search page results Follow the steps:- Update config.toml Hugo provide out of the box support to generate content in multiple formats including JSON. All we need to do is to tell Hugo that we want to generate index.json file for entire website content.\nAdd the following snippet in config.toml file to instruct Hugo to generate JSON output along with HTML and RSS default outputs:\nconfig.toml ... [outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;, \u0026#34;JSON\u0026#34;] Create layouts/_default/index.json Once we instruct Hugo to generate index.json file, Hugo looks for the template to generate file. Add the following index.json template to specified folder:\nlayouts/_default/index.json {{- $.Scratch.Add \u0026#34;index\u0026#34; slice -}} {{- range .Site.RegularPages -}} {{- $.Scratch.Add \u0026#34;index\u0026#34; (dict \u0026#34;title\u0026#34; .Title \u0026#34;tags\u0026#34; .Params.tags \u0026#34;categories\u0026#34; .Params.categories \u0026#34;contents\u0026#34; .Plain \u0026#34;permalink\u0026#34; .Permalink) -}} {{- end -}} {{- $.Scratch.Get \u0026#34;index\u0026#34; | jsonify -}} Create search.js Now create a search.js file which uses jquery, fuse.js, and mark.js to parse generated index.json file, and return matching content, with highlighting.\nPlease note that initial part of the code in search.js provide config:-\nsummaryInclude: Number of pages to show in search result fuseOptions: keys is important parameter where you can increase the weight of title, contents, tags or categories values to give them high priority in matching the content in search results. static/js/search.js summaryInclude=60; var fuseOptions = { shouldSort: true, includeMatches: true, threshold: 0.0, tokenize:true, location: 0, distance: 100, maxPatternLength: 32, minMatchCharLength: 1, keys: [ {name:\u0026#34;title\u0026#34;,weight:0.8}, {name:\u0026#34;contents\u0026#34;,weight:0.5}, {name:\u0026#34;tags\u0026#34;,weight:0.3}, {name:\u0026#34;categories\u0026#34;,weight:0.3} ] }; var searchQuery = param(\u0026#34;s\u0026#34;); if(searchQuery){ $(\u0026#34;#search-query\u0026#34;).val(searchQuery); executeSearch(searchQuery); }else { $(\u0026#39;#search-results\u0026#39;).append(\u0026#34;\u0026lt;p\u0026gt;Please enter a word or phrase above\u0026lt;/p\u0026gt;\u0026#34;); } function executeSearch(searchQuery){ $.getJSON( \u0026#34;/index.json\u0026#34;, function( data ) { var pages = data; var fuse = new Fuse(pages, fuseOptions); var result = fuse.search(searchQuery); console.log({\u0026#34;matches\u0026#34;:result}); if(result.length \u0026gt; 0){ populateResults(result); }else{ $(\u0026#39;#search-results\u0026#39;).append(\u0026#34;\u0026lt;p\u0026gt;No matches found\u0026lt;/p\u0026gt;\u0026#34;); } }); } function populateResults(result){ $.each(result,function(key,value){ var contents= value.item.contents; var snippet = \u0026#34;\u0026#34;; var snippetHighlights=[]; var tags =[]; if( fuseOptions.tokenize ){ snippetHighlights.push(searchQuery); }else{ $.each(value.matches,function(matchKey,mvalue){ if(mvalue.key == \u0026#34;tags\u0026#34; || mvalue.key == \u0026#34;categories\u0026#34; ){ snippetHighlights.push(mvalue.value); }else if(mvalue.key == \u0026#34;contents\u0026#34;){ start = mvalue.indices[0][0]-summaryInclude\u0026gt;0?mvalue.indices[0][0]-summaryInclude:0; end = mvalue.indices[0][1]+summaryInclude\u0026lt;contents.length?mvalue.indices[0][1]+summaryInclude:contents.length; snippet += contents.substring(start,end); snippetHighlights.push(mvalue.value.substring(mvalue.indices[0][0],mvalue.indices[0][1]-mvalue.indices[0][0]+1)); } }); } if(snippet.length\u0026lt;1){ snippet += contents.substring(0,summaryInclude*2); } //pull template from hugo templarte definition var templateDefinition = $(\u0026#39;#search-result-template\u0026#39;).html(); //replace values var output = render(templateDefinition,{key:key,title:value.item.title,link:value.item.permalink,tags:value.item.tags,categories:value.item.categories,snippet:snippet}); $(\u0026#39;#search-results\u0026#39;).append(output); $.each(snippetHighlights,function(snipkey,snipvalue){ $(\u0026#34;#summary-\u0026#34;+key).mark(snipvalue); }); }); } function param(name) { return decodeURIComponent((location.search.split(name + \u0026#39;=\u0026#39;)[1] || \u0026#39;\u0026#39;).split(\u0026#39;\u0026amp;\u0026#39;)[0]).replace(/\\+/g, \u0026#39; \u0026#39;); } function render(templateString, data) { var conditionalMatches,conditionalPattern,copy; conditionalPattern = /\\$\\{\\s*isset ([a-zA-Z]*) \\s*\\}(.*)\\$\\{\\s*end\\s*}/g; //since loop below depends on re.lastInxdex, we use a copy to capture any manipulations whilst inside the loop copy = templateString; while ((conditionalMatches = conditionalPattern.exec(templateString)) !== null) { if(data[conditionalMatches[1]]){ //valid key, remove conditionals, leave contents. copy = copy.replace(conditionalMatches[0],conditionalMatches[2]); }else{ //not valid, remove entire section copy = copy.replace(conditionalMatches[0],\u0026#39;\u0026#39;); } } templateString = copy; //now any conditionals removed we can do simple substitution var key, find, re; for (key in data) { find = \u0026#39;\\\\$\\\\{\\\\s*\u0026#39; + key + \u0026#39;\\\\s*\\\\}\u0026#39;; re = new RegExp(find, \u0026#39;g\u0026#39;); templateString = templateString.replace(re, data[key]); } return templateString; } Create content/search.md Create search.md to create a page solely to respond to /search URL. No content from this page is rendered. As we have defined the front-matter layout as \u0026ldquo;search\u0026rdquo;, content is rendered on this page is from template layouts/_default/search.html\ncontent/search.md --- title: \u0026#34;Search Results\u0026#34; sitemap: priority : 0.1 layout: \u0026#34;search\u0026#34; --- Nothing on this page will be visible. This file exists solely to respond to /search URL. Setting a very low sitemap priority will tell search engines this is not important content. Create layouts/_default/search.html This is the page rendered when viewing /search in your browser. This example uses the template functionality to load content and all required JS files in the \u0026ldquo;main\u0026rdquo; block.\nlayouts/_default/search.html {{ define \u0026#34;main\u0026#34; }} \u0026lt;script src=\u0026#34;https://code.jquery.com/jquery-3.3.1.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.0/fuse.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{ \u0026#34;js/search.js\u0026#34; | absURL }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;section class=\u0026#34;resume-section p-3 p-lg-5 d-flex flex-column\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;my-auto\u0026#34; \u0026gt; \u0026lt;form action=\u0026#34;{{ \u0026#34;search\u0026#34; | absURL }}\u0026#34;\u0026gt; \u0026lt;input id=\u0026#34;search-query\u0026#34; name=\u0026#34;s\u0026#34;/\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;div id=\u0026#34;search-results\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;Matching pages\u0026lt;/h3\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/section\u0026gt; \u0026lt;!-- this template is sucked in by search.js and appended to the search-results div above. So editing here will adjust style --\u0026gt; \u0026lt;script id=\u0026#34;search-result-template\u0026#34; type=\u0026#34;text/x-js-template\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;summary-${key}\u0026#34;\u0026gt; \u0026lt;h4\u0026gt;\u0026lt;a href=\u0026#34;${link}\u0026#34;\u0026gt;${title}\u0026lt;/a\u0026gt;\u0026lt;/h4\u0026gt; \u0026lt;p\u0026gt;${snippet}\u0026lt;/p\u0026gt; ${ isset tags }\u0026lt;p\u0026gt;Tags: ${tags}\u0026lt;/p\u0026gt;${ end } ${ isset categories }\u0026lt;p\u0026gt;Categories: ${categories}\u0026lt;/p\u0026gt;${ end } \u0026lt;/div\u0026gt; \u0026lt;/script\u0026gt; {{ end }} You may require to put some efforts on layouts/_default/search.html file to match the search result output as per your theme. You can create any template, as long as you include the third-party libs (jquery, fuse, mark.js) before search.js, it will work.\nCustomization for Hugo Mainroad Theme I have created this blog website using Hugo static site generator and Hugo Mainroad theme. I have done some changes in above search.html template to match Mainroad theme.\nlayouts/_default/search.html {{ define \u0026#34;main\u0026#34; }} \u0026lt;script src=\u0026#34;https://code.jquery.com/jquery-3.3.1.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.0/fuse.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;{{ \u0026#34;js/search.js\u0026#34; | absURL }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;main class=\u0026#34;main list\u0026#34; role=\u0026#34;main\u0026#34;\u0026gt; {{- with .Title }} \u0026lt;header class=\u0026#34;main__header\u0026#34;\u0026gt; \u0026lt;h1 class=\u0026#34;main__title\u0026#34;\u0026gt;{{ . }} \u0026lt;span class=\u0026#34;list__lead post__lead\u0026#34; id=\u0026#34;search-string\u0026#34;\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;/header\u0026gt; {{- end }} \u0026lt;div id=\u0026#34;search-results\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/main\u0026gt; \u0026lt;!-- this template is sucked in by search.js and appended to the search-results div above. So editing here will adjust style --\u0026gt; \u0026lt;script id=\u0026#34;search-result-template\u0026#34; type=\u0026#34;text/x-js-template\u0026#34;\u0026gt; \u0026lt;article class=\u0026#34;list__item post\u0026#34; id=\u0026#34;summary-${key}\u0026#34;\u0026gt; \u0026lt;header class=\u0026#34;list__header\u0026#34;\u0026gt; \u0026lt;h3 class=\u0026#34;list__title post__title \u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;${link}\u0026#34; rel=\u0026#34;bookmark\u0026#34;\u0026gt; ${title} \u0026lt;/a\u0026gt; \u0026lt;/h3\u0026gt; \u0026lt;div class=\u0026#34;list__meta meta\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;meta__item-categories meta__item\u0026#34;\u0026gt; \u0026lt;svg class=\u0026#34;meta__icon icon icon-category\u0026#34; width=\u0026#34;16\u0026#34; height=\u0026#34;16\u0026#34; viewBox=\u0026#34;0 0 16 16\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;m7 2l1 2h8v11h-16v-13z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;span class=\u0026#34;meta__text\u0026#34;\u0026gt; ${ isset categories } ${categories}${ end } \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;meta__item-categories meta__item\u0026#34;\u0026gt; \u0026lt;svg class=\u0026#34;meta__icon icon icon-tag\u0026#34; width=\u0026#34;16\u0026#34; height=\u0026#34;16\u0026#34; viewBox=\u0026#34;0 0 32 32\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;span class=\u0026#34;meta__text\u0026#34;\u0026gt; ${ isset tags } ${tags}${ end } \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;div class=\u0026#34;content list__excerpt post__content clearfix\u0026#34;\u0026gt; ${snippet} \u0026lt;/div\u0026gt; \u0026lt;/article\u0026gt; \u0026lt;/script\u0026gt; {{ end }} I have also done some changes in search box widget for Hugo Mainroad theme as follows:-\nlayouts/partials/widgets/search.html \u0026lt;div class=\u0026#34;widget-search widget\u0026#34;\u0026gt; \u0026lt;form class=\u0026#34;widget-search__form\u0026#34; role=\u0026#34;search\u0026#34; method=\u0026#34;get\u0026#34; action=\u0026#34;{{ \u0026#34;search\u0026#34; | absURL }}\u0026#34;\u0026gt; \u0026lt;label\u0026gt; \u0026lt;input class=\u0026#34;widget-search__field\u0026#34; type=\u0026#34;search\u0026#34; placeholder=\u0026#34;{{ T \u0026#34;search_placeholder\u0026#34; }}\u0026#34; value=\u0026#34;\u0026#34; name=\u0026#34;s\u0026#34; aria-label=\u0026#34;{{ T \u0026#34;search_placeholder\u0026#34; }}\u0026#34;\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;input class=\u0026#34;widget-search__submit\u0026#34; type=\u0026#34;submit\u0026#34; value=\u0026#34;Search\u0026#34;\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/div\u0026gt; Conclusion We have learned in this tutorial how easily we can create a client side search engine for our Hugo website. You can follow first four steps blindly and require some efforts for 5th step to match search.html template as per your theme.\nOnce you are done with all the changes, checkout the search result in your dev server as follows:-\nhttp://localhost:1313/search/?s=searchquery If you want to see how search results look like for my blog website CodingNConcepts, Hit https://codingnconcepts.com/search/?s=java\nReference: gist.github.com/eddiewebb\n","permalink":"https://codingnconcepts.com/hugo/client-side-search-engine-hugo/","tags":null,"title":"Client Side Search Engine for Hugo Website"},{"categories":["Java"],"contents":"Factory Design Pattern is one of the most commonly used design patterns. It falls under the category of Creational Design Patterns.\nOverview Analogy As the name suggests, Factory Design Pattern is based on real time Factory concept. As we know that factory manufactures products as per the requirements. It does not expose the internal details of manufacturing. Also, in case of requirements for newer products, it manufactures them as well.\nSimilarly, Factory Design Pattern enables us to create objects using a factory class without exposing the creation logic to the client and thus providing abstraction with flexibility to extension.\nWhen a client needs an object it asks the factory for it by providing some information on the type of object it requires. Factory method in factory class acts as a single place to create the objects from concrete classes based on the client\u0026rsquo;s input and return them as objects of interface or abstract types.\nVirtual Constructor This pattern replaces the direct object construction calls (using new operator) from client with calls to factory method. Since it replaces direct constructor calls, it is also known as Virtual Constructor.\nGoF Design Patterns According to GoF (Gangs of Four) Design Patterns,\nFactory Design Pattern defines an interface for creating an object, but let subclasses decide which class to instantiate. The Factory method lets a class defer instantiation to subclasses.\nWhen to use Factory Design Pattern? When we need to create objects of types which may be sub-classed. When client needs to create an object based on some input parameter and need not know how the object is created Also, a good choice when we don’t know the exact types before hand when starting on a project. This would provide ease to extensibility. How to implement Factory Design Pattern? Create a super class (an interface) Create sub class(es) (requirement based): Concrete A Concrete B Concrete C Factory Class with factory method that accepts some input parameter (to decide the type of object) to return the required object as object of interface Create an Enum that would be used to define types based on which factory method would create an object of concrete class Client code(main class) which creates an object using factory method of factory class by providing some input parameter and then use it Code Example Now we know what are the classes required to build a Factory Design Pattern.\nLet\u0026rsquo;s first create an interface named Currency\npublic interface Currency { String getCode(); String getSign(); } Create concrete classes Rupee, SGDollar, and USDollar implementing Currency interface\npublic class Rupee implements Currency { @Override public String getCode() { return \u0026#34;INR\u0026#34;; } @Override public String getSign() { return \u0026#34;₹\u0026#34;; } } public class SGDollar implements Currency { @Override public String getCode() { return \u0026#34;SGD\u0026#34;; } @Override public String getSign() { return \u0026#34;S$\u0026#34;; } } public class USDollar implements Currency { @Override public String getCode() { return \u0026#34;USD\u0026#34;; } @Override public String getSign() { return \u0026#34;$\u0026#34;; } } Create an Enum Country for input parameter to factory method\npublic enum Country { INDIA, SINGAPORE, USA, CANADA } Finally create a factory class CurrencyFactory\npublic class CurrencyFactory { public static Currency createCurrency(String country) { if(country.equalsIgnoreCase(Country.SINGAPORE.toString())) { return new SGDollar(); } else if(country.equalsIgnoreCase(Country.INDIA.toString()) { return new Rupee(); } else if(country.equalsIgnoreCase(Country.USA.toString())) { return new USDollar(); } throw new IllegalArgumentException(String.format(\u0026#34;No currency found for \u0026#39;%s\u0026#39;\u0026#34;, country)); } } Let\u0026rsquo;s create a Client (Main Class) to test our Factory\npublic class Client { public static void main(String [] args) { Currency indiaCurrency = CurrencyFactory.createCurrency(Country.INDIA.toString()); Currency singaporeCurrency = CurrencyFactory.createCurrency(Country.SINGAPORE.toString()); Currency usaCurrency = CurrencyFactory.createCurrency(Country.USA.toString()); //Currency canadaCurrency = CurrencyFactory.createCurrency(Country.CANADA.toString()); //Exception in thread \u0026#34;main\u0026#34; java.lang.IllegalArgumentException: No currency found for \u0026#39;CANADA\u0026#39; System.out.println(indiaCurrency.getCode()); System.out.println(indiaCurrency.getSign()); System.out.println(singaporeCurrency.getCode()); System.out.println(singaporeCurrency.getSign()); System.out.println(usaCurrency.getCode()); System.out.println(usaCurrency.getSign()); } } Output INR ₹ SGD S$ USD $ Advantages of Factory Design Pattern Loose Coupling: Introduces loose coupling between classes. Hence, involves programming against abstract entities rather than concrete implementations. Encapsulation: Good approach to encapsulation. Factory method used to create different objects from factory encapsulates the object creation code. Abstraction: Provides abstraction between implementation and client classes through inheritance. Single Responsibility Principle: Object creation is done at one place and this separates out the object creation code from actual usage. Any changes in object creation in future would require change in only place. Open/Closed Principle: Further to extend the functionality of factory to support newer type would be a small change i.e. adding a new subclass and corresponding object creation logic in factory method. ","permalink":"https://codingnconcepts.com/java/factory-design-pattern-java/","tags":["Java Design Pattern"],"title":"Factory Design Pattern In Java"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn how we can display data in tabular format in console using console.table() advance logging method. This comes in very handy to visualize complex array and objects in JavaScript.\nconsole.table([]) We are logging array of numbers using both console.log() and console.table() to see how nicely console.table() has printed a table to display numbers array.\nvar numbers = [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;]; console.log(numbers); console.table(numbers); Output [ \u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39; ] ┌─────────┬─────────┐ │ (index) │ Values │ ├─────────┼─────────┤ │ 0 │ \u0026#39;one\u0026#39; │ │ 1 │ \u0026#39;two\u0026#39; │ │ 2 │ \u0026#39;three\u0026#39; │ └─────────┴─────────┘ console.table([ [], [], [] ]) The visualization in tabular format becomes more handy when we print array of arrays using console.table() as compare to console.log() method.\nvar numbers = [[\u0026#34;one\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;I\u0026#34;], [\u0026#34;two\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;II\u0026#34;], [\u0026#34;three\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;III\u0026#34;]]; console.log(numbers); console.table(numbers); Output [ [ \u0026#39;one\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;I\u0026#39; ], [ \u0026#39;two\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;II\u0026#39; ], [ \u0026#39;three\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;III\u0026#39; ] ] ┌─────────┬─────────┬─────┬───────┐ │ (index) │ 0 │ 1 │ 2 │ ├─────────┼─────────┼─────┼───────┤ │ 0 │ \u0026#39;one\u0026#39; │ \u0026#39;1\u0026#39; │ \u0026#39;I\u0026#39; │ │ 1 │ \u0026#39;two\u0026#39; │ \u0026#39;2\u0026#39; │ \u0026#39;II\u0026#39; │ │ 2 │ \u0026#39;three\u0026#39; │ \u0026#39;3\u0026#39; │ \u0026#39;III\u0026#39; │ └─────────┴─────────┴─────┴───────┘ console.table(Object) Similar to arrays, Objects can be printed using console.table() to visualize in tabular format.\nfunction Person(firstName, lastName, age) { this.firstName = firstName; this.lastName = lastName; this.age = age; } var john = new Person(\u0026#34;John\u0026#34;, \u0026#34;Smith\u0026#34;, 41); console.log(john); console.table(john); Output Person { firstName: \u0026#39;John\u0026#39;, lastName: \u0026#39;Smith\u0026#39;, age: 41 } ┌───────────┬─────────┐ │ (index) │ Values │ ├───────────┼─────────┤ │ firstName │ \u0026#39;John\u0026#39; │ │ lastName │ \u0026#39;Smith\u0026#39; │ │ age │ 41 │ └───────────┴─────────┘ console.table(Objects[]) The visualization in tabular format becomes more handy when we print array of objects using console.table() as compare to console.log() method.\nvar john = new Person(\u0026#34;John\u0026#34;, \u0026#34;Smith\u0026#34;, 41); var jane = new Person(\u0026#34;Jane\u0026#34;, \u0026#34;Doe\u0026#34;, 38); var emily = new Person(\u0026#34;Emily\u0026#34;, \u0026#34;Jones\u0026#34;, 12); console.log([john, jane, emily]); console.table([john, jane, emily]); Output [ Person { firstName: \u0026#39;John\u0026#39;, lastName: \u0026#39;Smith\u0026#39;, age: 41 }, Person { firstName: \u0026#39;Jane\u0026#39;, lastName: \u0026#39;Doe\u0026#39;, age: 38 }, Person { firstName: \u0026#39;Emily\u0026#39;, lastName: \u0026#39;Jones\u0026#39;, age: 12 } ] ┌─────────┬───────────┬──────────┬─────┐ │ (index) │ firstName │ lastName │ age │ ├─────────┼───────────┼──────────┼─────┤ │ 0 │ \u0026#39;John\u0026#39; │ \u0026#39;Smith\u0026#39; │ 41 │ │ 1 │ \u0026#39;Jane\u0026#39; │ \u0026#39;Doe\u0026#39; │ 38 │ │ 2 │ \u0026#39;Emily\u0026#39; │ \u0026#39;Jones\u0026#39; │ 12 │ └─────────┴───────────┴──────────┴─────┘ Restrict Column Display You can pass the second arguments as array of fields, which you want to display. Only those columns will be displayed in tabular format.\nconsole.table([john, jane, emily], [\u0026#34;firstName\u0026#34;]); Output ┌─────────┬───────────┐ │ (index) │ firstName │ ├─────────┼───────────┤ │ 0 │ \u0026#39;John\u0026#39; │ │ 1 │ \u0026#39;Jane\u0026#39; │ │ 2 │ \u0026#39;Emily\u0026#39; │ └─────────┴───────────┘ console.table(Object of Objects) Complex nested objects are very easy to visualize using console.table()\nvar family = {}; family.mother = new Person(\u0026#34;Jane\u0026#34;, \u0026#34;Smith\u0026#34;, 38); family.father = new Person(\u0026#34;John\u0026#34;, \u0026#34;Smith\u0026#34;, 41); family.daughter = new Person(\u0026#34;Emily\u0026#34;, \u0026#34;Smith\u0026#34;, 12); console.table(family); Output ┌──────────┬───────────┬──────────┬─────┐ │ (index) │ firstName │ lastName │ age │ ├──────────┼───────────┼──────────┼─────┤ │ mother │ \u0026#39;Jane\u0026#39; │ \u0026#39;Smith\u0026#39; │ 38 │ │ father │ \u0026#39;John\u0026#39; │ \u0026#39;Smith\u0026#39; │ 41 │ │ daughter │ \u0026#39;Emily\u0026#39; │ \u0026#39;Smith\u0026#39; │ 12 │ └──────────┴───────────┴──────────┴─────┘ Sort Column Display Also note that when these tabular formats are rendered in browser console. You can click on table column headers to sort by column.\n┌──────────┬─────────────┬──────────┬─────┐ │ (index) │ firstName ▲ │ lastName │ age │ ├──────────┼─────────────┼──────────┼─────┤ │ daughter │ \u0026#39;Emily\u0026#39; │ \u0026#39;Smith\u0026#39; │ 12 │ │ mother │ \u0026#39;Jane\u0026#39; │ \u0026#39;Smith\u0026#39; │ 38 │ │ father │ \u0026#39;John\u0026#39; │ \u0026#39;Smith\u0026#39; │ 41 │ └──────────┴─────────────┴──────────┴─────┘ Restrict Column Display Again, pass the second arguments as array of fields, which you want to display. Only those columns will be displayed in tabular format.\nconsole.table(family, [\u0026#34;firstName\u0026#34;, \u0026#34;age\u0026#34;]); Output ┌──────────┬───────────┬─────┐ │ (index) │ firstName │ age │ ├──────────┼───────────┼─────┤ │ mother │ \u0026#39;Jane\u0026#39; │ 38 │ │ father │ \u0026#39;John\u0026#39; │ 41 │ │ daughter │ \u0026#39;Emily\u0026#39; │ 12 │ └──────────┴───────────┴─────┘ Conclusion We learned in this tutorial that how console.table() is useful to visualize complex arrays and objects in tabular format, and also provide the ability to sort columns and restrict column display.\nThis advance logging method is supported by all modern browsers like chrome, edge, firefox, opera and safari.\nReference: MDN web docs\n","permalink":"https://codingnconcepts.com/javascript/console-table-javascript/","tags":["Javascript Console"],"title":"console.table() in JavaScript"},{"categories":["Hugo"],"contents":"If you are using Hugo as a static site generator tool then you can create a partial to show social icons like facebook, twitter, linkedin, whatsapp etc to share page links on social media.\nCreate socialshare.html Parital First of all we are going to create a partial called socialshare.html which is responsible to create social share icons for your hugo blog pages. All you need to do is copy the below code snippet and save it under $hugo/layouts/partials/socialshare.html.\nsocialshare.html \u0026lt;!-- Social Share Button HTML --\u0026gt; {{- if .Param \u0026#34;socialshare\u0026#34; }} {{ $title := .Title }} {{ $url := printf \u0026#34;%s\u0026#34; .Permalink }} {{ $body := print $title \u0026#34;, by \u0026#34; .Site.Title \u0026#34;\\n\u0026#34; .Params.description \u0026#34;\\n\\n\u0026#34; $url \u0026#34;\\n\u0026#34; }} \u0026lt;section class=\u0026#34;social-share\u0026#34;\u0026gt; \u0026lt;ul class=\u0026#34;share-icons\u0026#34;\u0026gt; \u0026lt;!-- Twitter --\u0026gt; {{ if .Site.Params.social.share.twitter }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;https://twitter.com/intent/tweet?hashtags=codingnconcepts\u0026amp;amp;url={{ .Permalink }}\u0026amp;amp;text={{ .Title }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; aria-label=\u0026#34;Share on Twitter\u0026#34; class=\u0026#34;share-btn twitter\u0026#34;\u0026gt; {{ partial \u0026#34;svg/twitter.svg\u0026#34; (dict \u0026#34;class\u0026#34; \u0026#34;widget-social__link-icon\u0026#34;) }} \u0026lt;p\u0026gt;Twitter\u0026lt;/p\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; {{ end }} \u0026lt;!-- Facebook --\u0026gt; {{ if .Site.Params.social.share.facebook }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;https://www.facebook.com/sharer.php?u={{ $url }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; aria-label=\u0026#34;Share on Facebook\u0026#34; class=\u0026#34;share-btn facebook\u0026#34;\u0026gt; {{ partial \u0026#34;svg/facebook.svg\u0026#34; (dict \u0026#34;class\u0026#34; \u0026#34;widget-social__link-icon\u0026#34;) }} \u0026lt;p\u0026gt;Facebook\u0026lt;/p\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; {{ end }} \u0026lt;!-- LinkedIn --\u0026gt; {{ if .Site.Params.social.share.linkedin }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;https://www.linkedin.com/shareArticle?mini=true\u0026amp;amp;url={{ $url }}\u0026amp;amp;source={{ $url }}\u0026amp;amp;title={{ $title }}\u0026amp;amp;summary={{ $title }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34; aria-label=\u0026#34;Share on LinkedIn\u0026#34; class=\u0026#34;share-btn linkedin\u0026#34;\u0026gt; {{ partial \u0026#34;svg/linkedin.svg\u0026#34; (dict \u0026#34;class\u0026#34; \u0026#34;widget-social__link-icon\u0026#34;) }} \u0026lt;p\u0026gt;LinkedIn\u0026lt;/p\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; {{ end }} \u0026lt;!-- WhatsApp --\u0026gt; {{ if .Site.Params.social.share.whatsapp }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;whatsapp://send?text={{ $body }}\u0026#34; target=\u0026#34;_blank\u0026#34; class=\u0026#34;share-btn whatsapp\u0026#34;\u0026gt; {{ partial \u0026#34;svg/whatsapp.svg\u0026#34; (dict \u0026#34;class\u0026#34; \u0026#34;widget-social__link-icon\u0026#34;) }} \u0026lt;p\u0026gt;Email\u0026lt;/p\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; {{ end }} \u0026lt;!-- Email --\u0026gt; {{ if .Site.Params.social.share.email }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;mailto:?subject={{ .Site.Title }} - {{ $title }}.\u0026amp;amp;body={{ $body }}\u0026#34; target=\u0026#34;_blank\u0026#34; class=\u0026#34;share-btn email\u0026#34;\u0026gt; {{ partial \u0026#34;svg/email.svg\u0026#34; (dict \u0026#34;class\u0026#34; \u0026#34;widget-social__link-icon\u0026#34;) }} \u0026lt;p\u0026gt;Email\u0026lt;/p\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/section\u0026gt; {{ end }} {{- end }} You see that in socialshare.html partial, we have used some svg partials for social share icons. Here are some social share svg partial, which you should save under $hugo/layouts/partials/svg folder\ntwitter.svg \u0026lt;svg class=\u0026#34;{{ with .class }}{{ . }} {{ end }}icon icon-twitter\u0026#34; width=\u0026#34;24\u0026#34; height=\u0026#34;24\u0026#34; viewBox=\u0026#34;0 0 384 312\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5 0-78.8 35.3-78.8 78.8 0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3-6.7 11.6-10.6 25.2-10.6 39.6 0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1 0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4 0-12.6-.4-18.8-1.1 34.9 22.4 76.3 35.4 120.8 35.4 144.9 0 224.1-120 224.1-224.1 0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; facebook.svg \u0026lt;svg class=\u0026#34;{{ with .class }}{{ . }} {{ end }}icon icon-facebook\u0026#34; width=\u0026#34;24\u0026#34; height=\u0026#34;24\u0026#34; viewBox=\u0026#34;0 0 352 352\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;m0 32v288c0 17.5 14.5 32 32 32h288c17.5 0 32-14.5 32-32v-288c0-17.5-14.5-32-32-32h-288c-17.5 0-32 14.5-32 32zm320 0v288h-83v-108h41.5l6-48h-47.5v-31c0-14 3.5-23.5 23.5-23.5h26v-43.5c-4.4-.6-19.8-1.5-37.5-1.5-36.9 0-62 22.2-62 63.5v36h-42v48h42v108h-155v-288z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; linkedin.svg \u0026lt;svg class=\u0026#34;{{ with .class }}{{ . }} {{ end }}icon icon-linkedin\u0026#34; width=\u0026#34;24\u0026#34; height=\u0026#34;24\u0026#34; viewBox=\u0026#34;0 0 352 352\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M0,40v272c0,21.9,18.1,40,40,40h272c21.9,0,40-18.1,40-40V40c0-21.9-18.1-40-40-40H40C18.1,0,0,18.1,0,40z M312,32 c4.6,0,8,3.4,8,8v272c0,4.6-3.4,8-8,8H40c-4.6,0-8-3.4-8-8V40c0-4.6,3.4-8,8-8H312z M59.5,87c0,15.2,12.3,27.5,27.5,27.5 c15.2,0,27.5-12.3,27.5-27.5c0-15.2-12.3-27.5-27.5-27.5C71.8,59.5,59.5,71.8,59.5,87z M187,157h-1v-21h-45v152h47v-75 c0-19.8,3.9-39,28.5-39c24.2,0,24.5,22.4,24.5,40v74h47v-83.5c0-40.9-8.7-72-56.5-72C208.5,132.5,193.3,145.1,187,157z M64,288h47.5 V136H64V288z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; whatsapp.svg \u0026lt;svg class=\u0026#34;{{ with .class }}{{ . }} {{ end }}icon icon-whatsapp\u0026#34; width=\u0026#34;24\u0026#34; height=\u0026#34;24\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M20.1 3.9C17.9 1.7 15 .5 12 .5 5.8.5.7 5.6.7 11.9c0 2 .5 3.9 1.5 5.6L.6 23.4l6-1.6c1.6.9 3.5 1.3 5.4 1.3 6.3 0 11.4-5.1 11.4-11.4-.1-2.8-1.2-5.7-3.3-7.8zM12 21.4c-1.7 0-3.3-.5-4.8-1.3l-.4-.2-3.5 1 1-3.4L4 17c-1-1.5-1.4-3.2-1.4-5.1 0-5.2 4.2-9.4 9.4-9.4 2.5 0 4.9 1 6.7 2.8 1.8 1.8 2.8 4.2 2.8 6.7-.1 5.2-4.3 9.4-9.5 9.4zm5.1-7.1c-.3-.1-1.7-.9-1.9-1-.3-.1-.5-.1-.7.1-.2.3-.8 1-.9 1.1-.2.2-.3.2-.6.1s-1.2-.5-2.3-1.4c-.9-.8-1.4-1.7-1.6-2-.2-.3 0-.5.1-.6s.3-.3.4-.5c.2-.1.3-.3.4-.5.1-.2 0-.4 0-.5C10 9 9.3 7.6 9 7c-.1-.4-.4-.3-.5-.3h-.6s-.4.1-.7.3c-.3.3-1 1-1 2.4s1 2.8 1.1 3c.1.2 2 3.1 4.9 4.3.7.3 1.2.5 1.6.6.7.2 1.3.2 1.8.1.6-.1 1.7-.7 1.9-1.3.2-.7.2-1.2.2-1.3-.1-.3-.3-.4-.6-.5z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; email.svg \u0026lt;svg class=\u0026#34;{{ with .class }}{{ . }} {{ end }}icon icon-mail\u0026#34; width=\u0026#34;24\u0026#34; height=\u0026#34;24\u0026#34; viewBox=\u0026#34;0 0 416 288\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;m0 16v256 16h16 384 16v-16-256-16h-16-384-16zm347 16-139 92.5-139-92.5zm-148 125.5 9 5.5 9-5.5 167-111.5v210h-352v-210z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; CSS for socialshare.html Please note that the provided CSS is just for your reference. You might need to tweak or write your own CSS based on your hugo theme.\nstyle.css .social-share { position: relative; top: -0.5em; } .social-share ul { margin: 0; } .social-share ul li p { display: none; } .social-share .share-icons li { padding: 0 !important; padding-bottom: 10px !important; } .social-share .share-btn { padding: 0.25em; width: 3em; } .social-share-nav .share-btn h3{ color: #ffffff; } ul.share-icons { cursor: default; list-style: none; padding-left: 0; margin-top: 1em; } ul.share-icons li { display: inline-block; padding: 0 1em 0 0; } ul.share-icons li:last-child { padding-right: 0; } ul.share-icons li \u0026gt; * { text-decoration: none; border: 0; } ul.share-icons li \u0026gt; *:before { -moz-osx-font-smoothing: grayscale; -webkit-font-smoothing: antialiased; font-family: FontAwesome; font-style: normal; font-weight: normal; text-transform: none !important; } ul.share-icons li \u0026gt; * .label { display: none; } .share-btn { display: inline-block; color: #ffffff; border: none; border-radius: 4px; box-shadow: 0 2px 0 0 rgba(0,0,0,0.2); outline: none; text-align: center; text-decoration: none; } .share-btn:hover { color: #ffffff !important; } .share-btn:active { position: relative; top: 2px; box-shadow: none; color: #e2e2e2; outline: none; } .share-btn .widget-social__link-icon { margin: 0; } .share-btn.twitter { background: #55acee; } .share-btn.google-plus { background: #dd4b39; } .share-btn.facebook { background: #3B5998; } .share-btn.linkedin { background: #4875B4; } .share-btn.stumbleupon { background: #EB4823; } .share-btn.pinterest { background: #BD081C; } .share-btn.reddit { background: #ff5700; } .share-btn.email { background: #444444; } .share-btn.whatsapp { background: #25d366; } .share-btn.twitter:hover { background: #4c9ad6; } .share-btn.google-plus:hover { background: #c64333; } .share-btn.facebook:hover { background: #2f4779; } .share-btn.linkedin:hover { background: #4069a2; } .share-btn.stumbleupon:hover { background: #d3401f; } .share-btn.pinterest:hover { background: #AD0000; } .share-btn.reddit:hover { background: #e54e00; } .share-btn.email:hover { background: #363636; } SocialShare front-matter You can control whether you want to display social share icons on specific page. If socialshare is true, social icons will be displayed on that page otherwise not be displayed.\nblog-page.md --- ... socialshare: true --- SocialShare config.toml You need to add following configuration in config.toml file. You can enable and disable social share icons site-wide using this configuration.\nconfig.toml [Params.social.share] facebook = true linkedin = true twitter = true whatsapp = true email = true Add socialshare.html partial to single.html You need to add socialshare partial to single.html file as below:-\nsingle.html {{ define \u0026#34;main\u0026#34; }} \u0026lt;main\u0026gt; \u0026lt;article\u0026gt; \u0026lt;header\u0026gt; ... \u0026lt;/header\u0026gt;\t... \u0026lt;footer\u0026gt; ... \u0026lt;/footer\u0026gt; \u0026lt;/article\u0026gt; \u0026lt;/main\u0026gt; ... {{ partial \u0026#34;socialshare.html\u0026#34; . }} {{ end }} ","permalink":"https://codingnconcepts.com/hugo/social-icons-hugo/","tags":null,"title":"Add Social Icons in Hugo Website Pages"},{"categories":["Hugo"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to disable or customize sitemap.xml for Hugo website.\nOverview When you generate a website using Hugo static site generator. There are few points to understand about sitemap.xml:-\nHugo out of the box creates a sitemap.xml based on the Sitemap Protocol v0.9. The sitemap.xml file contains entries for these kind of pages:- home is website\u0026rsquo;s home page e.g. https://example.com/ section is all the folders in content directory $hugo/content/*.* page is all the pages in content directory $hugo/content/*.*. taxonomyTerm e.g. url /categories , /tags taxonomy e.g. url /categories/*.*, /tags/*.* Following is a sample sitemap.xml including all kind of pages:-\nsitemap.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; standalone=\u0026#34;yes\u0026#34;?\u0026gt; \u0026lt;urlset xmlns=\u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9\u0026#34; xmlns:xhtml=\u0026#34;http://www.w3.org/1999/xhtml\u0026#34;\u0026gt; \u0026lt;url\u0026gt; \u0026lt;!-- \u0026lt;kind\u0026gt;home\u0026lt;/kind\u0026gt; --\u0026gt; \u0026lt;loc\u0026gt;https://example.com/\u0026lt;/loc\u0026gt; \u0026lt;lastmod\u0026gt;2020-06-04T00:00:00+00:00\u0026lt;/lastmod\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;url\u0026gt; \u0026lt;!-- \u0026lt;kind\u0026gt;section\u0026lt;/kind\u0026gt; --\u0026gt; \u0026lt;loc\u0026gt;https://example.com/hugo/\u0026lt;/loc\u0026gt; \u0026lt;lastmod\u0026gt;2020-06-04T00:00:00+00:00\u0026lt;/lastmod\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;url\u0026gt; \u0026lt;!-- \u0026lt;kind\u0026gt;page\u0026lt;/kind\u0026gt; --\u0026gt; \u0026lt;loc\u0026gt;https://example.com/hugo/sitemap-hugo/\u0026lt;/loc\u0026gt; \u0026lt;lastmod\u0026gt;2020-06-04T00:00:00+00:00\u0026lt;/lastmod\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;url\u0026gt; \u0026lt;!-- \u0026lt;kind\u0026gt;taxonomyTerm\u0026lt;/kind\u0026gt; --\u0026gt; \u0026lt;loc\u0026gt;https://example.com/categories/\u0026lt;/loc\u0026gt; \u0026lt;lastmod\u0026gt;2020-06-04T00:00:00+00:00\u0026lt;/lastmod\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;url\u0026gt; \u0026lt;!-- \u0026lt;kind\u0026gt;taxonomyTerm\u0026lt;/kind\u0026gt; --\u0026gt; \u0026lt;loc\u0026gt;https://example.com/tags/\u0026lt;/loc\u0026gt; \u0026lt;lastmod\u0026gt;2020-06-04T00:00:00+00:00\u0026lt;/lastmod\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;url\u0026gt; \u0026lt;!-- \u0026lt;kind\u0026gt;taxonomy\u0026lt;/kind\u0026gt; --\u0026gt; \u0026lt;loc\u0026gt;https://example.com/categories/hugo/\u0026lt;/loc\u0026gt; \u0026lt;lastmod\u0026gt;2020-06-04T00:00:00+00:00\u0026lt;/lastmod\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;url\u0026gt; \u0026lt;!-- \u0026lt;kind\u0026gt;taxonomy\u0026lt;/kind\u0026gt; --\u0026gt; \u0026lt;loc\u0026gt;https://example.com/tags/hugo-defaults/\u0026lt;/loc\u0026gt; \u0026lt;lastmod\u0026gt;2020-06-04T00:00:00+00:00\u0026lt;/lastmod\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;/urlset\u0026gt; Disable sitemap.xml There are two ways to disable generating of sitemap.xml\nDisable using config.toml This approach is recommended, when you want to disable sitemap.xml permanently everytime you build your website.\nAdd the \u0026quot;sitemap\u0026quot; value to the disableKinds configuration variable in your configuration file.\nconfig.toml title = \u0026#34;Hugo example site\u0026#34; baseurl = \u0026#34;https://www.example.com\u0026#34; disableKinds = [\u0026#34;sitemap\u0026#34;] [taxonomies] category = \u0026#34;categories\u0026#34; tag = \u0026#34;tags\u0026#34; You can turn off multiple type of pages by giving comma separated values in disableKinds configuration.\nFor example, below is an example to turn off generation of sitemap.xml , RSS feed and robots.txt files all together.\nconfig.toml title = \u0026#34;Hugo example site\u0026#34; baseurl = \u0026#34;https://www.example.com\u0026#34; disableKinds = [\u0026#34;sitemap\u0026#34;, \u0026#34;RSS\u0026#34;, \u0026#34;robotsTXT\u0026#34;] [taxonomies] category = \u0026#34;categories\u0026#34; tag = \u0026#34;tags\u0026#34; Disable using command-line This approach is recommended, when you want to disable sitemap.xml for a specific build of your website.\nTo generate a hugo website without sitemap, execute following command from terminal:\nhugo --disableKinds=sitemap To serve website in the localhost environment without a sitemap, execute following command from terminal:\nhugo server --disableKinds=sitemap You can disable multiple kind of pages like sitemap.xml , RSS feed and robots.txt by executing following command from terminal:\nhugo --disableKinds=sitemap,RSS,robotsTXT Customize sitemap.xml Hugo has a built-in Sitemap template, but if you want to customize sitemap.xml then first of all copy and paste below hugo\u0026rsquo;s default sitemap template in layouts/_default/sitemap.xml location:-\n/layouts/_default/sitemap.xml {{ printf \u0026#34;\u0026lt;?xml version=\\\u0026#34;1.0\\\u0026#34; encoding=\\\u0026#34;utf-8\\\u0026#34; standalone=\\\u0026#34;yes\\\u0026#34;?\u0026gt;\u0026#34; | safeHTML }} \u0026lt;urlset xmlns=\u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9\u0026#34; xmlns:xhtml=\u0026#34;http://www.w3.org/1999/xhtml\u0026#34;\u0026gt; {{ range .Data.Pages }} \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;{{ .Permalink }}\u0026lt;/loc\u0026gt;{{ if not .Lastmod.IsZero }} \u0026lt;lastmod\u0026gt;{{ safeHTML ( .Lastmod.Format \u0026#34;2006-01-02T15:04:05-07:00\u0026#34; ) }}\u0026lt;/lastmod\u0026gt;{{ end }}{{ with .Sitemap.ChangeFreq }} \u0026lt;changefreq\u0026gt;{{ . }}\u0026lt;/changefreq\u0026gt;{{ end }}{{ if ge .Sitemap.Priority 0.0 }} \u0026lt;priority\u0026gt;{{ .Sitemap.Priority }}\u0026lt;/priority\u0026gt;{{ end }}{{ if .IsTranslated }}{{ range .Translations }} \u0026lt;xhtml:link rel=\u0026#34;alternate\u0026#34; hreflang=\u0026#34;{{ .Language.Lang }}\u0026#34; href=\u0026#34;{{ .Permalink }}\u0026#34; /\u0026gt;{{ end }} \u0026lt;xhtml:link rel=\u0026#34;alternate\u0026#34; hreflang=\u0026#34;{{ .Language.Lang }}\u0026#34; href=\u0026#34;{{ .Permalink }}\u0026#34; /\u0026gt;{{ end }} \u0026lt;/url\u0026gt; {{ end }} \u0026lt;/urlset\u0026gt; Now let\u0026rsquo;s do changes in above template to meet our requirement:-\nExclude taxonomy pages from sitemap.xml This is most common use case where you want to skip taxonomy pages like category (/category/.) and tags (/tags/.) pages from sitemap.xml.\nNotice that we have added a condition {{ if ne .Kind \u0026quot;taxonomy\u0026quot; }} in default sitemap template to exclude taxonomy pages\n/layouts/_default/sitemap.xml {{ printf \u0026#34;\u0026lt;?xml version=\\\u0026#34;1.0\\\u0026#34; encoding=\\\u0026#34;utf-8\\\u0026#34; standalone=\\\u0026#34;yes\\\u0026#34;?\u0026gt;\u0026#34; | safeHTML }} \u0026lt;urlset xmlns=\u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9\u0026#34; xmlns:xhtml=\u0026#34;http://www.w3.org/1999/xhtml\u0026#34;\u0026gt; {{ range .Data.Pages }}{{ if ne .Kind \u0026#34;taxonomy\u0026#34; }} \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;{{ .Permalink }}\u0026lt;/loc\u0026gt;{{ if not .Lastmod.IsZero }} \u0026lt;lastmod\u0026gt;{{ safeHTML ( .Lastmod.Format \u0026#34;2006-01-02T15:04:05-07:00\u0026#34; ) }}\u0026lt;/lastmod\u0026gt;{{ end }}{{ with .Sitemap.ChangeFreq }} \u0026lt;changefreq\u0026gt;{{ . }}\u0026lt;/changefreq\u0026gt;{{ end }}{{ if ge .Sitemap.Priority 0.0 }} \u0026lt;priority\u0026gt;{{ .Sitemap.Priority }}\u0026lt;/priority\u0026gt;{{ end }}{{ if .IsTranslated }}{{ range .Translations }} \u0026lt;xhtml:link rel=\u0026#34;alternate\u0026#34; hreflang=\u0026#34;{{ .Language.Lang }}\u0026#34; href=\u0026#34;{{ .Permalink }}\u0026#34; /\u0026gt;{{ end }} \u0026lt;xhtml:link rel=\u0026#34;alternate\u0026#34; hreflang=\u0026#34;{{ .Language.Lang }}\u0026#34; href=\u0026#34;{{ .Permalink }}\u0026#34; /\u0026gt;{{ end }} \u0026lt;/url\u0026gt; {{ end }}{{ end }} \u0026lt;/urlset\u0026gt; Exclude specific pages from sitemap.xml This is also very common use case where you want to skip some pages from sitemap.xml.\nNotice that we have added a condition {{ if ne .Params.sitemap_ignore true }} in default sitemap template to exclude pages based on sitemap_ignore flag\n/layouts/_default/sitemap.xml {{ printf \u0026#34;\u0026lt;?xml version=\\\u0026#34;1.0\\\u0026#34; encoding=\\\u0026#34;utf-8\\\u0026#34; standalone=\\\u0026#34;yes\\\u0026#34;?\u0026gt;\u0026#34; | safeHTML }} \u0026lt;urlset xmlns=\u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9\u0026#34; xmlns:xhtml=\u0026#34;http://www.w3.org/1999/xhtml\u0026#34;\u0026gt; {{ range .Data.Pages }}{{ if ne .Params.sitemap_ignore true }} \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;{{ .Permalink }}\u0026lt;/loc\u0026gt;{{ if not .Lastmod.IsZero }} \u0026lt;lastmod\u0026gt;{{ safeHTML ( .Lastmod.Format \u0026#34;2006-01-02T15:04:05-07:00\u0026#34; ) }}\u0026lt;/lastmod\u0026gt;{{ end }}{{ with .Sitemap.ChangeFreq }} \u0026lt;changefreq\u0026gt;{{ . }}\u0026lt;/changefreq\u0026gt;{{ end }}{{ if ge .Sitemap.Priority 0.0 }} \u0026lt;priority\u0026gt;{{ .Sitemap.Priority }}\u0026lt;/priority\u0026gt;{{ end }}{{ if .IsTranslated }}{{ range .Translations }} \u0026lt;xhtml:link rel=\u0026#34;alternate\u0026#34; hreflang=\u0026#34;{{ .Language.Lang }}\u0026#34; href=\u0026#34;{{ .Permalink }}\u0026#34; /\u0026gt;{{ end }} \u0026lt;xhtml:link rel=\u0026#34;alternate\u0026#34; hreflang=\u0026#34;{{ .Language.Lang }}\u0026#34; href=\u0026#34;{{ .Permalink }}\u0026#34; /\u0026gt;{{ end }} \u0026lt;/url\u0026gt; {{ end }}{{ end }} \u0026lt;/urlset\u0026gt; Now any page, you want to exclude from sitemap.xml, add the following property in front-matter of that page\nblog-page.md --- ... sitemap_ignore: true --- Exclude taxonomy and specific pages from sitemap.xml Let\u0026rsquo;s combine the last two examples where you want to skip taxonomy pages and specific pages based sitemap_ignore flag. I am using the below sitemap.xml in my own website.\nNotice that we have combined the conditions of last two examples {{ if and (ne .Kind \u0026quot;taxonomy\u0026quot;) (ne .Params.sitemap_ignore true) }}\n/layouts/_default/sitemap.xml {{ printf \u0026#34;\u0026lt;?xml version=\\\u0026#34;1.0\\\u0026#34; encoding=\\\u0026#34;utf-8\\\u0026#34; standalone=\\\u0026#34;yes\\\u0026#34;?\u0026gt;\u0026#34; | safeHTML }} \u0026lt;urlset xmlns=\u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9\u0026#34; xmlns:xhtml=\u0026#34;http://www.w3.org/1999/xhtml\u0026#34;\u0026gt; {{ range .Data.Pages }}{{ if and (ne .Kind \u0026#34;taxonomy\u0026#34;) (ne .Params.sitemap_ignore true) }} \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;{{ .Permalink }}\u0026lt;/loc\u0026gt;{{ if not .Lastmod.IsZero }} \u0026lt;lastmod\u0026gt;{{ safeHTML ( .Lastmod.Format \u0026#34;2006-01-02T15:04:05-07:00\u0026#34; ) }}\u0026lt;/lastmod\u0026gt;{{ end }}{{ with .Sitemap.ChangeFreq }} \u0026lt;changefreq\u0026gt;{{ . }}\u0026lt;/changefreq\u0026gt;{{ end }}{{ if ge .Sitemap.Priority 0.0 }} \u0026lt;priority\u0026gt;{{ .Sitemap.Priority }}\u0026lt;/priority\u0026gt;{{ end }}{{ if .IsTranslated }}{{ range .Translations }} \u0026lt;xhtml:link rel=\u0026#34;alternate\u0026#34; hreflang=\u0026#34;{{ .Language.Lang }}\u0026#34; href=\u0026#34;{{ .Permalink }}\u0026#34; /\u0026gt;{{ end }} \u0026lt;xhtml:link rel=\u0026#34;alternate\u0026#34; hreflang=\u0026#34;{{ .Language.Lang }}\u0026#34; href=\u0026#34;{{ .Permalink }}\u0026#34; /\u0026gt;{{ end }} \u0026lt;/url\u0026gt; {{ end }}{{ end }} \u0026lt;/urlset\u0026gt; Summary In this tutorial, we\u0026rsquo;ve learned how to customize sitemap.xml for your hugo website. If you have any other requirements of sitemap.xml customization, or you are facing issue following the tutorial. Please comment, I\u0026rsquo;ll try to solve your problem as soon as possible.\n","permalink":"https://codingnconcepts.com/hugo/sitemap-hugo/","tags":null,"title":"Customize sitemap in Hugo Website"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn different ways to compare two strings in Java\nCompare Strings using \u0026ldquo;==\u0026rdquo; operator 1String string1 = \u0026#34;CodingNConcepts\u0026#34;; 2String string2 = \u0026#34;CodingNConcepts\u0026#34;; 3String string3 = new String(\u0026#34;CodingNConcepts\u0026#34;); 4String string4 = new String(\u0026#34;CodingNConcepts\u0026#34;); 5 6System.out.println(string1 == string2); // true 7System.out.println(string1 == string3); // false 8System.out.println(string3 == string4); // false 9 10string3 = string3.intern(); 11string4 = string4.intern(); 12 13System.out.println(string1 == string3); // true 14System.out.println(string3 == string4); // true Explanation line 6: string1 and string2 both are initialized using literal so they both are referring to same string stored in String-Pool line 7: string3 is initialized using New so it always creates a new string object in Java heap memory whereas string1 refers to string from String-Pool line 8: Since String initialized using New always create a new string object, string3 and string4 both refers to different string object in heap memory line 10,11: When you call intern() on a string, it returns a string from String-Pool if exist otherwise a new string is created in String-Pool and returned.\nSo now after executing line 10 and 11, string3 and string4 refers to same string from String-Pool line 13,14: After executing line 10 and 11, all four strings string1, string2, string3 and string4 refers to same string from String-Pool Compare Strings using equals() method String\u0026rsquo;s equals() method - returns true if the string argument is not null and both the comparing strings have the same sequence of characters in same case.\n1String string1 = \u0026#34;CodingNConcepts\u0026#34;; 2String string2 = \u0026#34;CodingNConcepts\u0026#34;; 3String string3 = new String(\u0026#34;CodingNConcepts\u0026#34;); 4String string4 = new String(\u0026#34;CODINGNCONCEPTS\u0026#34;); 5 6System.out.println(string1.equals(string2)); // true 7System.out.println(string1.equals(string3)); // true 8System.out.println(string1.equals(string4)); // false 9System.out.println(string1.equals(null)); // false Explanation line 6: string1 and string2 both have same character sequence line 7: string1 and string3 both have same character sequence line 8: string1 and string4 both have same character sequence but case is different line 9: string argument is null ","permalink":"https://codingnconcepts.com/java/compare-two-strings-in-java/","tags":["Core Java","String"],"title":"How to Compare two Strings in Java"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn different ways to iterate over a list in Java.\nPrint List using for loop Let\u0026rsquo;s print list of prime numbers using basic and enhanced for loop\nList\u0026lt;Integer\u0026gt; primeNumbers = Arrays.asList(1, 2, 3, 5, 7); // basic for loop for(int i = 0; i \u0026lt; primeNumbers.size(); i++) { System.out.println(primeNumbers.get(i)); } // enhanced for loop for (Integer number : primeNumbers) { System.out.println(number); } Print List using forEach Java 8 introduced forEach method to loop through a List which is very convenient and easy to use.\nList\u0026lt;Integer\u0026gt; primeNumbers = Arrays.asList(1, 2, 3, 5, 7); // Iterable.forEach with lambda expression primeNumbers.forEach(number -\u0026gt; System.out.println(number)); // Iterable.forEach with method reference :: primeNumbers.forEach(System.out::println); // Stream.forEach with lambda expression primeNumbers.stream().forEach(number -\u0026gt; System.out.println(number)); // Stream.forEach with method reference :: primeNumbers.stream().forEach(System.out::println); Print List using Iterator Iterator is used to iterate over the list in forward direction.\nhasNext() Returns true if the iteration has more elements. next() Returns the next element in the iteration. List\u0026lt;Integer\u0026gt; primeNumbers = Arrays.asList(1, 2, 3, 5, 7); Iterator\u0026lt;Integer\u0026gt; iterator = primeNumbers.iterator(); while(iterator.hasNext()) { System.out.println(iterator.next()); } Print List using ListIterator ListIterator is an iterator for lists that allows the programmer to traverse the list in either direction (forward or backward), and obtain the iterator\u0026rsquo;s current position in the list.\nhasNext() returns true if this list iterator has more elements when traversing the list in the forward direction. next() returns the next element in the list and advances the cursor position. hasPrevious() returns true if this list iterator has more elements when traversing the list in the reverse direction. previous() returns the previous element in the list and moves the cursor position backwards. List\u0026lt;Integer\u0026gt; primeNumbers = Arrays.asList(1, 2, 3, 5, 7); ListIterator\u0026lt;Integer\u0026gt; listIterator = primeNumbers.listIterator(); // iterate forward while(listIterator.hasNext()) { System.out.println(listIterator.next()); } // iterate backward while(listIterator.hasPrevious()) { System.out.println(listIterator.previous()); } ","permalink":"https://codingnconcepts.com/java/iterate-list-in-java/","tags":["Java Collection"],"title":"How to Iterate over a List in Java"},{"categories":["Java"],"contents":"In this article, we\u0026rsquo;ll learn how to find middle element of a linked list using multiple approach in Java.\nApproach 1: Keep track of the LinkedList size In first approach, we can keep track of size of the linked list. We can have a size counter initialized as zero. Increase or decrease the counter by 1, on addition or deletion of nodes from linked list respectively.\nIn this case, middle element\u0026rsquo;s index will be (size-1)/2\nclass LinkedList\u0026lt;T\u0026gt; { Node\u0026lt;T\u0026gt; head = null; int size = 0; public Node\u0026lt;T\u0026gt; get(int index) { Node\u0026lt;T\u0026gt; node = head; // get by index return node; } public void add(T t) { // add element size++; } public void delete(int index) { // delete element size--; } public void findMiddleElement() { get((size - 1) / 2); } } Approach 2: Traverse the LinkedList to find size Sometime we have given only the head node of the linked list and no information about the size.\nIn simplest approach, we can traverse through whole linked list starting from head till end to find the size of the linked list.\nIn this case, middle element\u0026rsquo;s index will be (size()-1)/2\nclass LinkedList\u0026lt;T\u0026gt; { Node\u0026lt;T\u0026gt; head = null; public Node\u0026lt;T\u0026gt; get(int index) { Node\u0026lt;T\u0026gt; node = head; // get by index return node; } public int size() { Node\u0026lt;T\u0026gt; node = head; int size = 1; while (node.getNext() != null) { node = node.getNext(); size++; } return size; } public void findMiddleElement() { get((size() - 1) / 2); } } If linked list is having n elements, then this approach requires n iteration to get size and n/2 iteration to get middle elements.\nTotal iteration is (n + n/2)\nApproach 3: Fast and Slow pointers This approach is also applicable when size of the linked list is unknown and only head node is given.\nIn this approach, we iterate through the linked list using two pointers. Fast pointer jumps 2 nodes in each iteration, and the slow pointer jumps only one node per iteration.\nWhen the fast pointer reaches the end of the list, the slow pointer will be at the middle element.\nclass LinkedList\u0026lt;T\u0026gt; { Node\u0026lt;T\u0026gt; head = null; public Node\u0026lt;T\u0026gt; findMiddleElement() { Node\u0026lt;T\u0026gt; slow = head; Node\u0026lt;T\u0026gt; fast = head; while (fast != null \u0026amp;\u0026amp; fast.getNext() != null) { fast = fast.getNext().getNext(); slow = slow.getNext(); } return slow; } } This is best approach when size is unknown as we are able to find the middle elements in just n/2 iterations.\nSummary We learned that it is always good to keep track of linked list size to find the middle element. Moreover, when size of the linked list is unknown then Fast and slow pointer approach is the way to go.\n","permalink":"https://codingnconcepts.com/java/middle-element-of-linked-list-java/","tags":["Java Collection"],"title":"Find Middle Element of Linked List in Java"},{"categories":["Spring Boot"],"contents":"In this article, we\u0026rsquo;ll learn how to use @ConfigurationProperties with .yml file instead of .properties file for configuration in Spring Boot Project.\napplication.yml YAML (YAML Ain\u0026rsquo;t Markup Language) which is defined as .yml file, is becoming popular for configuration over traditional .properties file because of its simplicity and readability.\n# Peson properties person: # string name: Ashish Kumar Lahoti # string occupation: programmer # int age: 33 # float gpa: 3.5 # double fav_num: 1e+10 # boolean male: true # date birthday: 1986-08-22 # empty flaws: null # array hobbies: - bike riding - watching movies - online games - cooking # array movies: [\u0026#34;Dark Knight\u0026#34;, \u0026#34;Spider Man\u0026#34;, \u0026#34;Thor\u0026#34;] # map assets: {Car: 1, Bike: 2, Home: 1} # complex map size: t-shirt: us: XL uk: L shoes: us: 8.5 uk: 6.5 # object array friends: - name: \u0026#34;eve\u0026#34; age: 28 - {name: \u0026#34;adam\u0026#34;, age: 26} - name: \u0026#34;chloe\u0026#34; age: 40 # multi line string using \u0026gt; description: \u0026gt; Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua # multi line string with line break preserved using | signature: | Thanks \u0026amp; Regards, Ashish Kumar Lahoti email - lahoti.ashish20@gmail.com In the application.yml, we can define:-\nPrimitive types - String, int, float, double, boolean Date in standard date formats like YYYY-MM-DD Properties with empty or null values Array in two ways:- Add new line for each element prefixed by - Add comma separated values like [ item1, item2, \u0026hellip;] Map - key value pairs like { Key1: value1, Key2: value2, \u0026hellip;. } Array of Objects Multiline String using prefix \u0026gt; or |. Please note that line break is preserved when you use | YAML Syntax Tips In YAML:\nSpacing matters Line Starts with hash (#) are comments and ignored. In general, you do not need to use quotation \u0026quot;\u0026quot; around string values in YAML. Quotation is requires if it:\nincludes a colon (:), hash (#), greater than (\u0026gt;), or any character that has a special meaning in YAML, looks like a number (decimal, hexadecimal, exponential E or e notation, etc.), or is one of YAML’s reserved boolean words (true, false, no, off, etc.). @ConfigurationProperties We have defined person properties in our application.yml. Let\u0026rsquo;s map these properties to a configuration class file using @ConfigurationProperties annotation at class level\n@Configuration @ConfigurationProperties(\u0026#34;person\u0026#34;) public class PersonConfig { private String name; private String occupation; private int age; private float gpa; private double favNum; private boolean male; private String birthday; private String flaws; private String[] hobbies; private List\u0026lt;String\u0026gt; movies; private Map\u0026lt;String, Integer\u0026gt; assets; private Map\u0026lt;String, Map\u0026lt;String, String\u0026gt;\u0026gt; size; private List\u0026lt;Friend\u0026gt; friends; private String description; private String signature;\t// ... getters and setters } class Friend { private String name; private int age; // ...getters and setters } We have annotated PersonConfig class with @ConfigurationProperties(\u0026quot;person\u0026quot;). Spring boot will intialize the fields of PersonConfig with values from .yml file prefixed with \u0026ldquo;person\u0026rdquo; property for e.g. PersonConfig\u0026rsquo;s name field value will be initialized from \u0026ldquo;person.name\u0026rdquo; property.\nRelaxed Binding Spring Boot @ConfigurationProperties annotation supports relaxed binding which means that\nperson.fav-num person.favNum person.fav_num person.FAV_NUM Any of the above .yml property format can be mapped to PersonConfig\u0026rsquo;s favNum field.\nSummary In this article, we learned that Spring Boot configuration using .yml file is much simpler as compared to .properties file. If we compare .yml file of above example with .properties file. It will look something like this:-\napplication.properties person.name = Ashish Kumar Lahoti person.occupation = programmer person.age = 33 person.gpa = 3.5 person.fav_num = 1e+10 person.male = true person.birthday = 1986-08-22 person.flaws = null person.hobbies[0] = bike riding person.hobbies[1] = watching movies person.hobbies[2] = online games person.hobbies[3] = cooking person.movies = [\u0026#34;Dark Knight\u0026#34;, \u0026#34;Spider Man\u0026#34;, \u0026#34;Thor\u0026#34;] person.assets = {\u0026#39;Car\u0026#39;: 1, \u0026#39;Bike\u0026#39;: 2, \u0026#39;Home\u0026#39;: 1} person.friends[0].name = \u0026#34;adam\u0026#34; person.friends[0].age = 28 person.friends[1].name = \u0026#34;ben\u0026#34; person.friends[1].age = 26 person.friends[1].name = \u0026#34;chloe\u0026#34; person.friends[1].age = 40 person.description = Lorem ipsum dolor sit amet,\\n consectetur adipiscing elit, sed\\n do eiusmod tempor incididunt ut \\n labore et dolore magna aliqua person.signature = Thanks \u0026amp; Regards, \\n Ashish Kumar Lahoti \\n email - lahoti.ashish20@gmail.com ","permalink":"https://codingnconcepts.com/spring-boot/spring-configuration-properties-using-yml/","tags":["Spring Boot Basics","YAML"],"title":"Spring @ConfigurationProperties using YAML"},{"categories":["Spring Boot"],"contents":"In this article, we\u0026rsquo;ll learn how to install Spring Tool Plugin for Eclipse IDE to create a Spring Boot Project from Scratch using Spring Initializr.\nThe Spring Tools Plugin for the Eclipse IDE is well suited for getting started with Spring Boot Project. Follow the steps for installation and usage of Spring Tool for Eclipse IDE:-\nInstall Spring Tools Plugin for Eclipse IDE You can install the Spring Tools plugin for Eclipse IDE into an existing Eclipse installation using the Eclipse Marketplace. Go to help -\u0026gt; Eclipse Marketplace and Just open Marketplace, search for Spring Tools and install the Spring Tools 3 (Standalone Edition).\nNote: Spring Tools plugin name might be different based on OS (Windows or Mac OS X) and Eclipse version you have installed.\nSpring Tools for Eclipse IDE\nOnce you click on install, next screen will be Confirm Selected Features. All features are selected by default. Click on confirm.\nNext screen will ask you to accept the terms of license agreement and click finish\nThat\u0026rsquo;s it for installation. Spring Tools will be installed in your Eclipse in few minutes. once it installed the plugin, it will ask you to restart the eclipse.\nCreate Spring Boot Project Using Spring Tools The Spring Tools for Eclipse IDE come with a direct integration of Spring Initializr.\nGo to File, select New and choose the Spring → Spring Starter Project. It will open a wizard New Spring Starter Project\nService URL has by default Spring Initializr endpoint but this wizard lets you choose in case you have custom one running within you company.\nWizard lets you choose Type (Maven, Gradle), Packaging (Jar, War), language (Java, Kotlin, Groovy), Java Version and Other Project metadata.\nSpring Initializr for Eclipse IDE\nGo to next screen and choose Spring Boot Version and required Dependencies that match your interest and click finish. You end up with a ready-to-use Spring Boot project in your workspace - in just a few seconds.\nSummary Congratulations!! You\u0026rsquo;ve learned how to quickly create Spring Boot Project in Eclipse IDE using Spring Initializr.\nIf you are not using Eclipse IDE then don\u0026rsquo;t worry, you can still create a Spring Boot Project using Spring Initializr by following this post link:-\nHow to setup Spring Boot project using Spring Initializr\n","permalink":"https://codingnconcepts.com/spring-boot/spring-initializr-for-eclipse-ide/","tags":["Spring Boot Basics","Eclipse IDE"],"title":"Spring Initializr Plugin for Eclipse IDE"},{"categories":["Spring Boot"],"contents":"In this quick article, we\u0026rsquo;ll see @Controller and @RestController annotations and their difference in Spring MVC.\n@Controller @Component public @interface Controller { @AliasFor(annotation = Component.class) String value() default \u0026#34;\u0026#34;; } If you look at the above @Controller annotation definition, you will find out that it just a stereotype version of @Component annotation which is used to annotate class as a Spring Controller. It is mainly a controller part of Spring MVC (model-view-controller) Web application. It\u0026rsquo;s been there since the evolution of Spring Framework.\n@Controller annotation typically used at class level in combination with a @RequestMapping annotation at method level to handle web requests.\n@Controller @RequestMapping(\u0026#34;/users\u0026#34;) public class UserController { @GetMapping(\u0026#34;/{id}\u0026#34;, produces = \u0026#34;application/json\u0026#34;) public @ResponseBody getUser(@PathVariable int id) { return getUserById(id); } private User getUserById(int id){ // ... } } The request handling method is annotated with @ResponseBody. This annotation enables automatic serialization of the return object into the HttpResponse.\n@RestController @Controller @ResponseBody public @interface RestController { @AliasFor(annotation = Controller.class) String value() default \u0026#34;\u0026#34;; } If you look at the above @RestController annotation definition, you will find out that it is an advance version of @Controller which includes both the @Controller and @ResponseBody annotations and as a result, simplifies the controller implementation such that now @ResponseBody isn\u0026rsquo;t required.\n@RestController @RequestMapping(\u0026#34;/users\u0026#34;) public class UserController { @GetMapping(\u0026#34;/{id}\u0026#34;, produces = \u0026#34;application/json\u0026#34;) public getUser(@PathVariable int id) { return getUserById(id); } private User getUserById(int id){ // ... } } Every request handling method of the controller class automatically serializes return objects into HttpResponse.\n@Controller vs @RestController @Controller @RestController Added in Spring 2.5 version Relatively new, added in Spring 4.0 version Stereotype version of @Component Specialized version of @Controller Required @ResponseBody on method handler @ResponseBody is not required as it is @Controller + @ResponseBody Traditional way to create Spring MVC Controller Preferred way to use in RESTFul Web Services ","permalink":"https://codingnconcepts.com/spring-boot/spring-controller-vs-restcontroller/","tags":["Spring Boot API","REST"],"title":"Spring @Controller and @RestController"},{"categories":["Java"],"contents":"In this quick article, We\u0026rsquo;ll see usage of Java Math.pow() method which takes two arguments, a and b, and returns a to the power of b i.e. ab\nSyntax public double pow(double a, double b) Where,\nParameter a is the base Parameter b is the exponent Returns ab Example // Returns 8.0 double result = (int) Math.pow(2, 3); // Cast to int, returns 8 int intResult = (int) Math.pow(2, 3); // Returns 117.29730800599916 double doubleResult = Math.pow(2.5, 5.2); // Returns 1.0 double zeroPowerResult = Math.pow(2, 0); // Returns 2.0 double sameResult = Math.pow(2, 1); // Returns NaN double nanResult = Math.pow(2, Double.NaN); Points to note:-\nBy default, returns the result in double You can cast the result to int Both arguments, base and exponent can have decimal points If second argument is zero then result will be 1.0 If second argument is 1 then result will be value of first argument If second argument is NaN then result will be NaN ","permalink":"https://codingnconcepts.com/java/math-pow-in-java/","tags":["Core Java"],"title":"Java Math.pow() method Usage"},{"categories":["Java"],"contents":"In this article, we\u0026rsquo;ll learn how to validate mobile phone number of different country\u0026rsquo;s format using Java Regex (Regular Expressions)\nPhone Number Format A typical mobile phone number has following component:\n+\u0026lt;country_code\u0026gt; \u0026lt;area_code\u0026gt; \u0026lt;subscriber_number\u0026gt; Where depending on the country,\ncountry_code is somewhere between 1 to 3 digits area_code and subscriber_number combined is somewhere between 8 to 11 digits If you simply require a regex to match all country format then here it is,\n\u0026#34;^(\\\\+\\\\d{1,3}( )?)?((\\\\(\\\\d{1,3}\\\\))|\\\\d{1,3})[- .]?\\\\d{3,4}[- .]?\\\\d{4}$\u0026#34; If you want to know how we came up with this regex? then please read this full article.\nRegex to match 10 digit Phone Number with No space This is simplest regex to match just 10 digits. We will also see here how to use regex to validate pattern:\nString regex = \u0026#34;^\\\\d{10}$\u0026#34;; Pattern pattern = Pattern.compile(regex); Matcher matcher = pattern.matcher(\u0026#34;9876543210\u0026#34;); matcher.matches(); // returns true if pattern matches, else returns false Let’s break the regex and understand,\n^ start of expression d{10} is mandatory match of 10 digits without any space $ end of expression You can make regex more flexible to match between 8 to 11 digits phone number with no space, using this regex:\nString noSpaceRegex = \u0026#34;^\\\\d{8,11}$\u0026#34;; Regex to match 10 digit Phone Number with WhiteSpaces, Hyphens or No space String spacesAndHyphenRegex = \u0026#34;^(\\\\d{3}[- ]?){2}\\\\d{4}$\u0026#34;; Let\u0026rsquo;s break the regex and understand,\n^ start of expression d{3} is mandatory match of 3 digits [- ]? is optional match of whitespace or hyphen after 3 digits {2} is to repeat the above match d{3}[- ]? two times becomes total 6 digits d{4} is mandatory match of last 4 digits $ end of expression This Pattern will match mobile phone numbers like 9876543210, 987 654 3210, 987-654-3210, 987 654-3210, 987 6543210, etc.\nRegex to match 10 digit Phone Number with Parentheses String parenthesesRegex = \u0026#34;^((\\\\(\\\\d{3}\\\\))|\\\\d{3})[- ]?\\\\d{3}[- ]?\\\\d{4}$\u0026#34;; Let\u0026rsquo;s break the regex and understand,\n^ start of expression (\\\\(\\\\d{3}\\\\))|\\\\d{3}) is mandatory match of 3 digits with or without parentheses [- ]? is optional match of whitespace or hyphen after after 3 digits \\\\d{3}[- ]? is mandatory match of next 3 digits followed by whitespace, hyphen or no space \\\\d{4} is mandatory match of last 4 digits $ end of expression This Pattern will match mobile phone numbers with spaces and hyphen, as well as numbers like (987)6543210, (987) 654-3210, (987)-654-3210 etc.\nRegex to match 10 digit Phone number with Country Code Prefix This regex is combined with regex to include parenthesis\nString countryCodeRegex = \u0026#34;^(\\\\+\\\\d{1,3}( )?)?((\\\\(\\\\d{3}\\\\))|\\\\d{3})[- .]?\\\\d{3}[- .]?\\\\d{4}$\u0026#34;; Let\u0026rsquo;s break the regex and understand,\n^ start of expression (\\\\+\\\\d{1,3}( )?)? is optional match of country code between 1 to 3 digits prefixed with + symbol, followed by space or no space. ((\\\\(\\\\d{3}\\\\))|\\\\d{3})[- .]?\\\\d{3}[- .]?\\\\d{4} is mandatory match of 10 digits with or without parenthesis, followed by whitespace, hyphen or no space $ end of expression This Pattern will match mobile phone numbers from previous examples as well as numbers like +91 (987)6543210, +111 (987) 654-3210, +66 (987)-654-3210 etc.\nRegex to match Phone Number of All Country Formats Before we start defining a regex, let\u0026rsquo;s look at some of the country phone number formats:-\nAbkhazia +995 442 123456 Afghanistan +93 30 539-0605 Australia +61 2 1255-3456 China +86 (20) 1255-3456 Germany +49 351 125-3456 Indonesia +62 21 6539-0605 Iran +98 (515) 539-0605 Italy +39 06 5398-0605 New Zealand +64 3 539-0605 Philippines +63 35 539-0605 Singapore +65 6396 0605 Thailand +66 2 123 4567 UK +44 141 222-3344 USA +1 (212) 555-3456 Vietnam +84 35 539-0605 Let\u0026rsquo;s extract some information from these numbers:-\nCountry Code prefix starts with \u0026lsquo;+\u0026rsquo; and has 1 to 3 digits Last part of the number, also known as subscriber number is 4 digits in all of the numbers Most of the countries have 10 digits phone number after excluding country code. A general observation is that all countries phone number falls somewhere between 8 to 11 digits after excluding country code. Let\u0026rsquo;s see again on regex from previous example to validate country code:\nString countryCodeRegex = \u0026#34;^(\\\\+\\\\d{1,3}( )?)?((\\\\(\\\\d{3}\\\\))|\\\\d{3})[- .]?\\\\d{3}[- .]?\\\\d{4}$\u0026#34;l The above regex is to match 10 digit phone numbers, Let\u0026rsquo;s make some changes in this and make it more flexible to match 8 to 11 digits phone numbers:-\nRegex to Match All Country Formats String allCountryRegex = \u0026#34;^(\\\\+\\\\d{1,3}( )?)?((\\\\(\\\\d{1,3}\\\\))|\\\\d{1,3})[- .]?\\\\d{3,4}[- .]?\\\\d{4}$\u0026#34;; Let\u0026rsquo;s break the regex and understand,\n^ start of expression (\\\\+\\\\d{1,3}( )?)? is optional match of country code between 1 to 3 digits prefixed with \u0026lsquo;+\u0026rsquo; symbol, followed by space or no space. ((\\\\(\\\\d{1,3}\\\\))|\\\\d{1,3} is mandatory group of 1 to 3 digits with or without parenthesis followed by hyphen, space or no space. \\\\d{3,4}[- .]? is mandatory group of 3 or 4 digits followed by hyphen, space or no space \\\\d{4} is mandatory group of last 4 digits $ end of expression Regex to match Phone Number of Specific Country Format As you saw in the previous example that we have to add some flexibility in our regex to match all countries\u0026rsquo; formats. If you want more strict validation of specific country format then here are examples of India and Singapore Phone number regex pattern:\nString indiaRegex = \u0026#34;^(\\\\+\\\\d{2}( )?)?((\\\\(\\\\d{3}\\\\))|\\\\d{3})[- .]?\\\\d{3}[- .]?\\\\d{4}$\u0026#34;; String singaporeRegex = \u0026#34;^(\\\\+\\\\d{2}( )?)?\\\\d{4}[- .]?\\\\d{4}$\u0026#34;; ","permalink":"https://codingnconcepts.com/java/java-regex-to-validate-phone-number/","tags":["Core Java","Regex"],"title":"Java Regex to Validate Phone Number"},{"categories":["Spring Boot"],"contents":"In this article, we\u0026rsquo;ll learn how to inject values for Primitives, List, Map, and Date with inline values and from property file using @Value Annotation with examples. We\u0026rsquo;ll also see its usage with Constructor-based \u0026amp; Setter-based Injection, and SpEL.\n@Value annotation is used for injecting values into fields in a spring-managed beans. The value typically come from:-\nInline Values Property Files ((.properties and .yml files)) System Properties Environment Variables Inject Inline values using @Value Annotation Let\u0026rsquo;s look at the quick examples for how to inject inline values for String, Integer, Float, Double, Boolean, and List using @Value annotation. Injecting inline values for Map, Date, LocalDate, LocalDateTime requires SpEL (Spring Expression Language) to use inside @Value annotation.\n@Configuration public class InlineConfig { @Value(\u0026#34;How to use @Value Annotation with inline values\u0026#34;) private String title; @Value(\u0026#34;30\u0026#34;) private Integer duration; @Value(\u0026#34;4.5\u0026#34;) private Float rating; @Value(\u0026#34;1e+10\u0026#34;) private Double pageViews; @Value(\u0026#34;true\u0026#34;) private Boolean isTrending; @Value(\u0026#34;Spring, Spring Boot, Annotation\u0026#34;) private List\u0026lt;String\u0026gt; tags; // SpEL expression used to initialize a Map @Value(\u0026#34;#{{\u0026#39;keyword1\u0026#39;: \u0026#39;12\u0026#39;, \u0026#39;keyword2\u0026#39;: \u0026#39;44\u0026#39;, \u0026#39;keyword3\u0026#39;: \u0026#39;85\u0026#39;, \u0026#39;keyword4\u0026#39;: \u0026#39;100\u0026#39;}}\u0026#34;) private Map\u0026lt;String, Integer\u0026gt; keywordCountMap; // Inject Date with given format using SpEL expression @Value(\u0026#34;#{new java.text.SimpleDateFormat(\u0026#39;yyyyMMdd\u0026#39;).parse(\u0026#39;20210530\u0026#39;)}\u0026#34;) private Date createdDate; // Inject LocalDate with ISO_DATE format using SpEL expression @Value(\u0026#34;#{T(java.time.LocalDate).parse(\u0026#39;2021-05-31\u0026#39;)}\u0026#34;) private LocalDate updatedDate; // Inject LocalDateTime with ISO_LOCAL_DATE_TIME format using SpEL expression @Value(\u0026#34;#{T(java.time.LocalDateTime).parse(\u0026#39;2015-08-04T10:11:30\u0026#39;)}\u0026#34;) private LocalDateTime lastAccess; } Inject values from Property file using @Value Annotation application.properties course.title = \u0026#34;How to use Spring @Value annotation\u0026#34; course.duration = 30 course.rating = 4.5 course.page_views = 1e+10 course.trending = true We can set the values of primitive fields such as String, int, float, double and boolean from property file as below:-\n@Configuration public class CourseConfig { @Value(\u0026#34;${course.title}\u0026#34;) private String title; //How to use Spring @Value annotation @Value(\u0026#34;${course.duration}\u0026#34;) private int duration; //30 @Value(\u0026#34;${course.rating}\u0026#34;) private float rating; //4.5 @Value(\u0026#34;${course.page_views}\u0026#34;) private double pageViews; //1.0E10 @Value(\u0026#34;${course.trending}\u0026#34;) private boolean isTrending; //true } Concat multiple properties in @Value Annotation Two or more properties can be concatenated from property file like below:-\n@Configuration public class CourseConfig { @Value(\u0026#34;${course.title} (${course.rating})\u0026#34;) private String titleAndRating; //How to use Spring @Value annotation (4.5) @Value(\u0026#34;${course.title} - ${course.duration} min\u0026#34;) private String titleAndDuration; //How to use Spring @Value annotation - 30 min } Inject with Default values using @Value Annotation Default values can be provided for properties that might not be defined using below syntax:\n@Value(\u0026#34;${property: defaultValue}\u0026#34;) For example, property course.review is not defined so review field will be initialized with default value i.e. “No Reviews Yet”.\n@Value(\u0026#34;${course.review: No Reviews Yet}\u0026#34;) private String review; Inject List using @Value Annotation application.properties\napplication.properties course.tags = Java, Spring, Spring Boot, Annotation Any property having comma separated values can be initialized as a List using @Value annotation in latest version of Spring.\n// Comma separated property values auto initialize a List @Value(\u0026#34;${course.tags}\u0026#34;) private List\u0026lt;String\u0026gt; tags; Using SpEL expression with List If you are using older version of Spring, then you need to use SpEL expression to initialize a List:\n@Value(\u0026#34;#{\u0026#39;${course.tags}\u0026#39;.split(\u0026#39;,\u0026#39;)}\u0026#34;) private List\u0026lt;String\u0026gt; tags; SpEL expression can also be used to get specific value from comma separated values:\n@Value(\u0026#34;#{\u0026#39;${course.tags}\u0026#39;.split(\u0026#39;,\u0026#39;)[0]}\u0026#34;) private String firstTag; Inject Map using @Value Annotation application.properties course.keyword_count = { \u0026#39;keyword1\u0026#39;: \u0026#39;12\u0026#39;, \u0026#39;keyword2\u0026#39;: \u0026#39;44\u0026#39;, \u0026#39;keyword3\u0026#39;: \u0026#39;85\u0026#39;, \u0026#39;keyword4\u0026#39;: \u0026#39;100\u0026#39;} Note*: the keys and values in the property Map must be in single quotes ' '.\nWe can initialize a Map with above property using SpEL expression inside @Value annotation\n// SpEL expression used to initialize a Map @Value(\u0026#34;#{${course.keyword_count}}\u0026#34;) private Map\u0026lt;String, Integer\u0026gt; keywordCountMap; Inject Value of specific Key from Map If we need to get the value of a specific key from Map, all we have to do is add the key\u0026rsquo;s name in the expression:\nWe can use any one out of these two ways:-\n// way 1 - .key @Value(\u0026#34;#{${course.keyword_count}.keyword1}\u0026#34;) private Integer FirstKeywordCount; // way 2 - [\u0026#39;key\u0026#39;] @Value(\u0026#34;#{${course.keyword_count}[\u0026#39;keyword2\u0026#39;]}\u0026#34;) private Integer SecondKeywordCount; If we\u0026rsquo;re not sure whether the Map contains a certain key, we should choose a safer expression that will not throw an exception but set the value to null when the key is not found:\n@Value(\u0026#34;#{${course.keyword_count}[\u0026#39;unknownKey\u0026#39;]}\u0026#34;) private Integer unknownKeywordCount; Inject Map with Default Value We can also set default values for the properties or keys that might not exist:\n@Value(\u0026#34;#{${unknownMap : {key1: \u0026#39;1\u0026#39;, key2: \u0026#39;2\u0026#39;}}}\u0026#34;) private Map\u0026lt;String, Integer\u0026gt; unknownMap; @Value(\u0026#34;#{${course.keyword_count}[\u0026#39;unknownKey\u0026#39;] ?: 100}\u0026#34;) private Integer unknownKeyWithDefaultValue; Inject Filtered Map Entries Map entries can also be filtered before injection. Let\u0026rsquo;s assume we need to get only those entries whose keyword count is greater than 50:\n@Value(\u0026#34;#{${course.keyword_count}.?[value \u0026gt; \u0026#39;50\u0026#39;]}\u0026#34;) private Map\u0026lt;String, Integer\u0026gt; keywordCountMapFiltered; Inject Date using @Value Annotation application.properties course.created_date = 20210530 course.updated_date = 2021-05-31 course.last_access = 2015-08-04T10:11:30 There is no direct support to initialize Date, LocalDate, and LocalDateTime fields using @Value annotation. We need to use SpEL (Spring Expression Language) inside @Value annotation to parse the String based property.\n// Inject Date with given format using SpEL expression @Value(\u0026#34;#{new java.text.SimpleDateFormat(\u0026#39;yyyyMMdd\u0026#39;).parse(\u0026#39;${course.created_date}\u0026#39;)}\u0026#34;) private Date createdDate; // Inject LocalDate with ISO_DATE format using SpEL expression @Value(\u0026#34;#{T(java.time.LocalDate).parse(\u0026#39;${course.updated_date}\u0026#39;)}\u0026#34;) private LocalDate updatedDate; // Inject LocalDateTime with ISO_LOCAL_DATE_TIME format using SpEL expression @Value(\u0026#34;#{T(java.time.LocalDateTime).parse(\u0026#39;${course.last_access}\u0026#39;)}\u0026#34;) private LocalDateTime lastAccess; Inject System Properties using @Value Annotation We can initialize fields from system properties using @Value annotation in a similar way as they were defined in a property file. Let\u0026rsquo;s see examples:-\n//UTF-8 @Value(\u0026#34;${file.encoding}\u0026#34;) private String fileEncoding; //Mac OS X @Value(\u0026#34;${os.name}\u0026#34;) private String osName; // /Users/ashishkumarlahoti @Value(\u0026#34;${user.home}\u0026#34;) private String userHome Inject all System Properties We can also use the @Value annotation to inject all current system properties like this:-\n@Value(\u0026#34;#{systemProperties}\u0026#34;) private Map\u0026lt;String, String\u0026gt; systemPropertiesMap; Output systemPropertiesMap: { \u0026#34;os.name\u0026#34; : \u0026#34;Mac OS X\u0026#34;, \u0026#34;os.version\u0026#34; : \u0026#34;10.15.7\u0026#34;, \u0026#34;user.name\u0026#34; : \u0026#34;ashl\u0026#34;, \u0026#34;user.country\u0026#34; : \u0026#34;SG\u0026#34;, \u0026#34;user.language\u0026#34; : \u0026#34;en\u0026#34;, \u0026#34;user.timezone\u0026#34; : \u0026#34;Asia/Singapore\u0026#34;, \u0026#34;user.home\u0026#34; : \u0026#34;/Users/ashl\u0026#34;, \u0026#34;user.dir\u0026#34; : \u0026#34;/Users/ashl/IdeaProjects/springboot-examples/springboot-demo\u0026#34;, \u0026#34;file.encoding\u0026#34; : \u0026#34;UTF-8\u0026#34;, \u0026#34;file.separator\u0026#34; : \u0026#34;/\u0026#34;, \u0026#34;line.separator\u0026#34; : \u0026#34;\\n\u0026#34;, \u0026#34;path.separator\u0026#34; : \u0026#34;:\u0026#34;, \u0026#34;java.version\u0026#34; : \u0026#34;11.0.10\u0026#34;, \u0026#34;java.vendor\u0026#34; : \u0026#34;Oracle Corporation\u0026#34;, \u0026#34;java.home\u0026#34; : \u0026#34;/Library/Java/JavaVirtualMachines/jdk-11.0.10.jdk/Contents/Home\u0026#34;, \u0026#34;catalina.home\u0026#34; : \u0026#34;/private/var/folders/6p/3ztts6bd4xbdl7nkcq6lpcg80000gn/T/tomcat.8080.8199922893426879147\u0026#34; } Inject value of specific System property from Map We can also use SpEL expressions to get the value from System Property Map:\n// /Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home/jre @Value(\u0026#34;#{systemProperties[\u0026#39;java.home\u0026#39;]}\u0026#34;) private String javaHome; //Oracle Corporation @Value(\u0026#34;#{systemProperties[\u0026#39;java.vendor\u0026#39;]}\u0026#34;) private String javaVendor; Inject System Property with default value We can also initialize some default value if System property is not available:\n@Value(\u0026#34;#{systemProperties[\u0026#39;unknown\u0026#39;] ?: \u0026#39;Default Value\u0026#39;}\u0026#34;) private String unknownSystemProperty; Constructor based Injection using @Value Annotation When we use the @Value annotation, we\u0026rsquo;re not limited to a field-based injection. We can use the @Value annotation for constructor-based injection also.\nLet\u0026rsquo;s see this in practice:\nvalues.properties priority = high @Component @PropertySource(\u0026#34;classpath:values.properties\u0026#34;) public class PriorityProvider { private String priority; @Autowired public PriorityProvider(@Value(\u0026#34;${priority:normal}\u0026#34;) String priority) { this.priority = priority; } } In the above example, we inject a property priority directly into the constructor of PriorityProvider. Note that we also provide a default value \u0026ldquo;normal\u0026rdquo; in case the priority property is not found.\nSetter based Injection using @Value Annotation Similar to field-based and constructor-based injection, we can also use @Value annotation for setter-based injection.\nvalues.properties listOfValues = first, second, third @Component @PropertySource(\u0026#34;classpath:values.properties\u0026#34;) public class CollectionProvider { private List\u0026lt;String\u0026gt; values = new ArrayList\u0026lt;\u0026gt;(); @Autowired public void setValues(@Value(\u0026#34;#{\u0026#39;${listOfValues}\u0026#39;.split(\u0026#39;,\u0026#39;)}\u0026#34;) List\u0026lt;String\u0026gt; values) { this.values.addAll(values); } } In the code above, we use the SpEL expression to inject a list of values into the setValues method.\nInject Method Arguments using @Value Annotation When the @Value annotation is found on a method, Spring context will invoke it when all the spring configurations and beans are getting loaded. If the method has multiple arguments, then every argument value is mapped from the method annotation. If we want different values for different arguments then we can use @Value annotation directly with the argument.\n@Configuration public class Config { @Value(\u0026#34;Test\u0026#34;) public void printValues(String a, String b){ System.out.println(a + \u0026#34; \u0026amp; \u0026#34; + b); // Test \u0026amp; Test } @Value(\u0026#34;Test\u0026#34;) public void printOtherValues(String a, @Value(\u0026#34;Another Test\u0026#34;) String b){ System.out.println(a + \u0026#34; \u0026amp; \u0026#34; + b); // Test \u0026amp; Another Test } } SpEL (Spring Expression Language) with @Value Annotation We have already seen usage of SpEL with @Value annotation to inject complex values such as List, Map, and Date where Spring boot doesn\u0026rsquo;t provide direct support. SpEL provides us flexibility to transform or parse the property value before injecting. Let\u0026rsquo;s see some more practical use cases:-\nInject Scheme, Host or Port from URL application.properties bootstrap.url = http://localhost:8080 @Value(\u0026#34;#{new java.net.URI(\u0026#39;${bootstrap.url}\u0026#39;).getScheme()}\u0026#34;) private String scheme; //http @Value(\u0026#34;#{new java.net.URI(\u0026#39;${bootstrap.url}\u0026#39;).getHost()}\u0026#34;) private String host; //localhost @Value(\u0026#34;#{new java.net.URI(\u0026#39;${bootstrap.url}\u0026#39;).getPort()}\u0026#34;) private Integer port; //8080 Arithmetic Operations @Value(\u0026#34;#{((1 + 2^3 - 4) * (5 % 6)) / 7 }\u0026#34;) // 3.0 private Double arithmeticOperation; @Value(\u0026#34;#{((1 + 2^3 - 4) * (5 mod 6)) div 7 }\u0026#34;) // 3.0 private Double anotherArithmeticOperation; @Value(\u0026#34;#{\u0026#39;Hello \u0026#39; + \u0026#39;World\u0026#39;}\u0026#34;) // \u0026#34;Hello World\u0026#34; private String concatString; We can use either -\ndiv or \\ for DIVIDE operation,\nmod or % for MODULO operation.\nThe + operator can also be used to concatenate strings.\nRelational Operations // @Value(\u0026#34;#{1 == 1}\u0026#34;) true @Value(\u0026#34;#{1 eq 1}\u0026#34;) // true private boolean equal; //@Value(\u0026#34;#{1 != 1}\u0026#34;) // false @Value(\u0026#34;#{1 ne 1}\u0026#34;) // false private boolean notEqual; // @Value(\u0026#34;#{1 \u0026lt; 1}\u0026#34;) // false @Value(\u0026#34;#{1 lt 1}\u0026#34;) // false private boolean lessThan; //@Value(\u0026#34;#{1 \u0026lt;= 1}\u0026#34;) // true @Value(\u0026#34;#{1 le 1}\u0026#34;) // true private boolean lessThanOrEqual; //@Value(\u0026#34;#{1 \u0026gt; 1}\u0026#34;) // false @Value(\u0026#34;#{1 gt 1}\u0026#34;) // false private boolean greaterThan; //@Value(\u0026#34;#{1 \u0026gt;= 1}\u0026#34;) // true @Value(\u0026#34;#{1 ge 1}\u0026#34;) // true private boolean greaterThanOrEqual; We can use either -\neq or == for EQUAL operation,\n!= or ne for NOT EQUAL operation,\n\u0026lt; or lt for LESS THAN operation,\n\u0026gt; or gt for GREATER THAN operation,\n\u0026lt;= or le for LESS THAN EQUAL TO operation,\n\u0026gt;= or ge for GREATER THAN EQUAL TO operation\nLogical Operations //@Value(\u0026#34;#{250 \u0026gt; 200 \u0026amp;\u0026amp; 200 \u0026lt; 4000}\u0026#34;) // true @Value(\u0026#34;#{250 \u0026gt; 200 and 200 \u0026lt; 4000}\u0026#34;) // true private boolean andOperation; //@Value(\u0026#34;#{400 \u0026gt; 300 || 150 \u0026lt; 100}\u0026#34;) // true @Value(\u0026#34;#{400 \u0026gt; 300 or 150 \u0026lt; 100}\u0026#34;) // true private boolean orOperation; //@Value(\u0026#34;#{!true}\u0026#34;) // false @Value(\u0026#34;#{not true}\u0026#34;) // false private boolean notOperation; We can use either -\n\u0026amp;\u0026amp; or and for AND operation,\n|| or or for OR operation,\n! or not for NOT operation.\nConditional Operations The ternary operator can be used inside the expression for conditional logic.\n@Value(\u0026#34;#{2 \u0026gt; 1 ? \u0026#39;a\u0026#39; : \u0026#39;b\u0026#39;}\u0026#34;) // \u0026#34;a\u0026#34; private String ternaryOperator; Most common use case of ternary operator is to check for null and return default value.\n@Autowired private SomeBean someBean; @Value(\u0026#34;#{someBean.someProperty != null ? someBean.someProperty : \u0026#39;default\u0026#39;}\u0026#34;) private String nullCheckUsingTernaryOperator; SpEL also provide support for Elvis operator ?: which is a shorthand of ternary operator for null check. Above example can be written using Elvis operator like this:-\n@Value(\u0026#34;#{someBean.someProperty ?: \u0026#39;default\u0026#39;}\u0026#34;) // Will inject provided string if someProperty is null private String nullCheckUsingElvisOperator; Summary We learnt different ways to inject values using Spring @Value annotation. We also understood the power and flexibility provided by SpEL to use expressions inside @Value annotation.\nDownload the source code for all the examples from github/springboot-config\n","permalink":"https://codingnconcepts.com/spring-boot/spring-value-annotation/","tags":["Spring Boot Basics"],"title":"Spring @Value Annotation Guide"},{"categories":["Spring Boot"],"contents":"In this article, we\u0026rsquo;ll learn how to configure a FeignClient in your Spring Boot project to consume RESTFul APIs from other services.\nOverview FeignClient is a Declarative REST Client in Spring Boot Web Application. Declarative REST Client means you just give the client specification as an Interface and spring boot takes care of the implementation for you.\nFeignClient is used to consume RESTFul API endpoints exposed by third-party or microservice.\nFeign vs RestTemplate It is an alternative to RestTemplate and has the following advantages over RestTemplate:-\nDo not need to write implementation classes to call other services, just provide specifications as an Interface Client configurations such as encoding/decoding, timeout, and logging can just be done through property files. Client configurations can be done from Java Configuration file as well. Developed by Netflix. It has great support to work with other spring-boot cloud libraries such as Hystrix, Eureka and Ribbon Spring Boot provide support for writing test cases for Feign Client using WireMock Provide support for fallback data if API call fails. Project Setup For the initial setup of your Spring Boot project, you should use Spring Initializr. Choose the OpenFeign and Spring Web as dependencies and Contract Stub Runner as a test dependency.\nMaven Project You can click the below link to generate a Maven project with pre-selected dependencies:-\nhttps://start.spring.io/#!type=maven-project\u0026amp;language=java\u0026amp;platformVersion=2.5.1.RELEASE\u0026amp;packaging=jar\u0026amp;jvmVersion=1.8\u0026amp;groupId=com.example\u0026amp;artifactId=api\u0026amp;name=api\u0026amp;description=Create%20Feign%20Client%20to%20consume%20RESTFul%20APIs\u0026amp;packageName=com.example.api\u0026amp;dependencies=cloud-feign,web,cloud-contract-stub-runner\n\u0026lt;!-- to write web layer --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- to write web client using OpenFeign --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-openfeign\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- to write test class using junit jupiter --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- to write integration test and mock stub using WireMock --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-contract-stub-runner\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Gradle Project Similarly, You can click the below link to generate a Gradle project with pre-selected dependencies:-\nhttps://start.spring.io/#!type=gradle-project\u0026amp;language=java\u0026amp;platformVersion=2.5.1.RELEASE\u0026amp;packaging=jar\u0026amp;jvmVersion=1.8\u0026amp;groupId=com.example\u0026amp;artifactId=api\u0026amp;name=api\u0026amp;description=Create%20Feign%20Client%20to%20consume%20RESTFul%20APIs\u0026amp;packageName=com.example.api\u0026amp;dependencies=cloud-feign,web,cloud-contract-stub-runner\ndependencies { // to write web layer implementation \u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39; // to write web client using OpenFeign implementation \u0026#39;org.springframework.cloud:spring-cloud-starter-openfeign\u0026#39; // to write test class using junit jupiter testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; // to write integration test and mock stub using WireMock testImplementation \u0026#39;org.springframework.cloud:spring-cloud-starter-contract-stub-runner\u0026#39; } Feign Client Implementation Setup Spring Cloud OpenFeign supports three underlying implementations for feign client:\nDefault\nThis is enabled by default when no additional configuration is provided. ApacheHttpClient\nThis is enabled when we have feign.httpclient.enabled: true property in the configuration file and io.github.openfeign:feign-httpclient in the project classpath application.yml feign.httpclient.enabled: true pom.xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.github.openfeign\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;feign-httpclient\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; build.gradle dependencies { implementation \u0026#39;io.github.openfeign:feign-httpclient\u0026#39; } OkHttpClient\nThis is enabled when we have feign.okhttp.enabled: true property in the configuration file and io.github.openfeign:feign-okhttp in the project classpath application.yml feign.okhttp.enabled: true pom.xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.github.openfeign\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;feign-okhttp\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; build.gradle dependencies { implementation \u0026#39;io.github.openfeign:feign-okhttp\u0026#39; } Enable Feign Client When you are working on a spring boot project, you have nothing much to do to enable FeignClient for your project. Make sure:-\nYou have spring-boot-starter-web and spring-cloud-starter-openfeign dependencies in your pom.xml or build.gradle You are using @SpringBootApplication and @EnableFeignClients annotations at your application starter class file ApiApplication. Spring Boot is opinionated, when it sees the web and openfeign dependencies in the classpath, it sets up all the necessary default configurations required for FeignClient and automatically scans for the classes annotated with @FeignClient\npackage com.example.api; @SpringBootApplication @EnableFeignClients public class ApiApplication { public static void main(String[] args) { SpringApplication.run(ApiApplication.class, args); } } Create Feign Client Next, we are going to create a FeignClient to consume from the RESTFul API endpoint. Let\u0026rsquo;s create a PostFeignClient interface -\nAnnotate with @FeignClient which auto scans by spring boot application to generate feign client This FeignClient consumes the APIs from this URL: https://jsonplaceholder.typicode.com/ Let\u0026rsquo;s keep the API URL in the property file (e.g. application.yml) and use that property:-\nclient: post: baseUrl: https://jsonplaceholder.typicode.com package com.example.api.client; @FeignClient(name = \u0026#34;postFeignClient\u0026#34;, url = \u0026#34;${client.post.baseUrl}\u0026#34;) public interface PostFeignClient { @GetMapping(\u0026#34;/posts\u0026#34;) List\u0026lt;Post\u0026gt; getAllPosts(); @GetMapping(\u0026#34;/posts/{postId}\u0026#34;) Post getPostById(@PathVariable Long postId); @GetMapping(\u0026#34;/posts\u0026#34;) List\u0026lt;Post\u0026gt; getPostByUserId(@RequestParam Long userId); @PostMapping(\u0026#34;/posts\u0026#34;) Post createPost(Post post); @PutMapping(\u0026#34;/posts\u0026#34;) Post updatePost(Post post); @DeleteMapping(\u0026#34;/posts/{postId}\u0026#34;) Post deletePost(@PathVariable Long postId); } Feign Client Configuration From Property file Spring boot comes with default global configurations which are applied to all the feign clients you create. The good thing is you can change these global configurations from property files such as connection timeout, read timeout and the logger level\napplication.yml feign: client: config: default: connectTimeout: 5000 readTimeout: 5000 loggerLevel: BASIC You can also configure each feign client individually from the property file using feign client name or value. Remember, we created our feign client with name @FeignClient(name = \u0026quot;postFeignClient\u0026quot;, ...)\nThe following properties can be configured for each feign client using name or value (e.g. postFeignClient):-\nfeign: client: config: postFeignClient: connectTimeout: 5000 readTimeout: 5000 loggerLevel: FULL errorDecoder: com.example.SimpleErrorDecoder retryer: com.example.SimpleRetryer requestInterceptors: - com.example.FooRequestInterceptor - com.example.BarRequestInterceptor decode404: false encoder: com.example.SimpleEncoder decoder: com.example.SimpleDecoder contract: com.example.SimpleContract From Configuration Class file We can also configure a FeignClient using a Configuration class. You have to pass this class as configuration while creating FeignClient e.g. @FeignClient(configuration = \u0026quot;FeignClientConfig.class\u0026quot;, ...)\npackage com.example.api.client; import com.example.api.config.FeignClientConfig; @FeignClient(name = \u0026#34;postFeignClient\u0026#34;, configuration = FeignClientConfig.class, url = \u0026#34;${client.post.baseUrl}\u0026#34;) public interface PostFeignClient {} In FeignClientConfig, you can create beans of Decoder, Encoder, Logger, Contract, Feign.Builder and Client to override default beans created by Spring Boot. You can also create beans of Logger.Level, Retryer, ErrorDecoder and RequestInterceptor to include these features.\npackage com.example.api.config; public class FeignClientConfig {} Spring Cloud Netflix provides the following beans by default for feign (BeanType beanName: ClassName):\nDecoder feignDecoder: ResponseEntityDecoder (which wraps a SpringDecoder) Encoder feignEncoder: SpringEncoder Logger feignLogger: Slf4jLogger Contract feignContract: SpringMvcContract Feign.Builder feignBuilder: HystrixFeign.Builder Client feignClient: if Ribbon is enabled it is a LoadBalancerFeignClient, otherwise the default feign client is used. Spring Cloud Netflix does not provide the following beans by default for feign, but still looks up beans of these types from the application context to create the feign client:\nLogger.Level Retryer ErrorDecoder Request.Options RequestInterceptor SetterFactory Request Interceptor You may come across a use case, where you need to pass Authorization Headers or Request Headers in API calls using Feign Client.\nIn such case, you need to provide a bean of type RequestInterceptor in a Feign Client Configuration class e.g. FeignClientConfig class as below:-\npackage com.example.api.config; public class FeignClientConfig { /** * Enable this bean if you want to add headers in HTTP request */ @Bean public RequestInterceptor requestInterceptor() { return requestTemplate -\u0026gt; { requestTemplate.header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;); requestTemplate.header(\u0026#34;Accept\u0026#34;, \u0026#34;application/json\u0026#34;); requestTemplate.header(\u0026#34;header_1\u0026#34;, \u0026#34;value_1\u0026#34;); requestTemplate.header(\u0026#34;header_2\u0026#34;, \u0026#34;value_2\u0026#34;); requestTemplate.header(\u0026#34;header_3\u0026#34;, \u0026#34;value_3\u0026#34;); }; } /** * Enable this bean if you want to add basic Authorization header * for e.g. Authorization: Basic dXNlcm5hbWU6cGFzc3dvcmQ= */ @Bean public BasicAuthRequestInterceptor basicAuthRequestInterceptor() { return new BasicAuthRequestInterceptor(\u0026#34;username\u0026#34;, \u0026#34;password\u0026#34;); } } Note:- Do not annotate this class with @Configuration annotation, otherwise this configuration will become global i.e. all Feign Clients will inherit this config in that case.\nOnce you apply this configuration to FeignClient, all the requests made by that FeignClient will include the common headers and basic authorization header to outgoing HTTP requests.\nProxy Setup You may come across a use case, where the Feign Client should use an HTTP proxy to make the outbound API call.\nIn such case, you need to override the default Client bean in a Feign Client Configuration class e.g. FeignClientConfig class as below:-\npackage com.example.api.config; public class FeignClientConfig { /** * Enable this bean if you want to setup HTTP proxy for Default Feign Client */ @Bean public Client feignClient() { return new Client.Proxied(null, null, new Proxy(Proxy.Type.HTTP, new InetSocketAddress(proxyHost, proxyPort))); } /** * Enable this bean if you want to setup HTTP proxy for ApacheHttpClient Feign Client */ @Bean public CloseableHttpClient feignClient() { return HttpClientBuilder.create().setProxy( new HttpHost(proxyHost, proxyPort)).build(); } /** * Enable this bean if you want to setup HTTP proxy for OkHttpClient Feign Client */ @Bean public OkHttpClient okHttpClient() { return new OkHttpClient.Builder() .proxy(new Proxy(Proxy.Type.HTTP, new InetSocketAddress(proxyHost, proxyPort))) .build(); } } Annotate this class with @Configuration annotation, if you want to apply this configuration to all Feign Clients in the project. Else do not annotate and apply this configuration to specific Feign Client, all the requests made by that FeignClient will use the given proxy to outgoing HTTP requests.\nFeign Logging A logger is created for each FeignClient by default. Feign logging only responds to the DEBUG level.\nTo enable the feign logging for all the feign clients, declare the logging level of the package name of client interfaces to DEBUG:-\nlogging: level: com.example.api.client: DEBUG To enable the feign logging for specific FeignClient, declare the logging level to that interface to DEBUG:-\nlogging: level: com.example.api.client.PostFeignClient: DEBUG Once you enable the feign logging by setting the logging level to DEBUG, you can further control the logging using loggerLevel configuration property which tells Feign how much to log per request. Choices are:\nNONE, No logging (DEFAULT). BASIC, Log only the request method and URL and the response status code and execution time. HEADERS, Log the basic information along with request and response headers. FULL, Log the headers, body, and metadata for both requests and responses. E.g. BASIC loggerLevel for all feign clients and FULL loggerLevel for postFeignClient:-\nfeign: client: config: default: loggerLevel: BASIC postFeignClient: loggerLevel: FULL Consume the Feign Client Now that we have created feign client, let\u0026rsquo;s create a service layer class PostService and its implementation PostServiceImpl to consume these APIs using the feign client\npublic interface PostService { List\u0026lt;Post\u0026gt; getAllPosts(); Post getPostById(Long postId); List\u0026lt;Post\u0026gt; getAllPostsByUserId(Long userId); Post createPost(Post post); void updatePost(Long postId, Post post); void deletePost(Long postId); } @Service public class PostServiceImpl implements PostService { @Autowired private PostFeignClient postFeignClient; @Override public List\u0026lt;Post\u0026gt; getAllPosts() { return postFeignClient.getAllPosts(); } @Override public Post getPostById(Long postId) { return postFeignClient.getPostById(postId); } @Override public List\u0026lt;Post\u0026gt; getAllPostsByUserId(Long userId) { return postFeignClient.getPostByUserId(userId); } @Override public Post createPost(Post post) { return postFeignClient.createPost(post); } @Override public void updatePost(Long postId, Post post) { postFeignClient.updatePost(post); } @Override public void deletePost(Long postId) { postFeignClient.deletePost(postId); } } Now since we have created our service class and consumed APIs using feign client. Let\u0026rsquo;s create a controller PostController to test our feign client.\n@RestController @RequestMapping(\u0026#34;/posts\u0026#34;) public class PostController { @Autowired private PostService postService; @GetMapping public List\u0026lt;Post\u0026gt; getAllPosts() { return postService.getAllPosts(); } @GetMapping(\u0026#34;/{postId}\u0026#34;) public Post getPostById(@PathVariable Long postId) { return postService.getPostById(postId); } @PostMapping @ResponseStatus(HttpStatus.CREATED) public Post createPost(Post post) { return postService.createPost(post); } @PutMapping(\u0026#34;/{postId}\u0026#34;) @ResponseStatus(HttpStatus.OK) public void updatePost(@PathVariable Long postId, Post post) { postService.updatePost(postId, post); } @DeleteMapping(\u0026#34;/{postId}\u0026#34;) @ResponseStatus(HttpStatus.OK) public void deletePost(@PathVariable Long postId) { postService.deletePost(postId); } } Let\u0026rsquo;s test our controller endpoint from the browser to see if Feign client is working. That\u0026rsquo;s it. You have successfully created and tested feign client to consume APIs from a given endpoint.\nUnit Test for Feign Client You should always write test cases for your Feign Client. Spring Boot Cloud module spring-cloud-contract lets you use WireMock in your test cases to mock the API data.\nIf your Spring Boot application is using the default Tomcat embedded server then you can add spring-cloud-starter-contract-stub-runner dependency to your maven (or gradle) and add @AutoConfigureWireMock at the class level to use Wiremock in your tests.\nWiremock runs as a stub server and you can register stub behavior using a Java API or via static JSON declarations as part of your test. The following code shows an example:\napplication-test.yml We created a test configuration file so that our FeignClient calls https://localhost:9091 to get the data\nclient: post: baseUrl: http://localhost:9091 Unit Test We tell WireMock to run a stub on local port 9091 to serve mock data. This way our FeignClient starts receiving the data from the mocked stub.\npackage com.example.api.client; @ActiveProfiles(\u0026#34;test\u0026#34;) @SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT) @AutoConfigureWireMock(port = 9091) public class PostFeignClientTest { @Autowired PostFeignClient postFeignClient; @Test public void getAllPosts_whenValidClient_returnValidResponse() throws Exception { // Using WireMock to mock client API: stubFor(get(urlEqualTo(\u0026#34;/posts\u0026#34;)) .willReturn(aResponse() .withStatus(HttpStatus.OK.value()) .withHeader(\u0026#34;Content-Type\u0026#34;, MediaType.APPLICATION_JSON_VALUE) .withBody(read(\u0026#34;stubs/posts.json\u0026#34;)))); List\u0026lt;Post\u0026gt; posts = postFeignClient.getAllPosts(); Post post = posts.get(0); // We\u0026#39;re asserting if WireMock responded properly assertThat(posts).hasSize(10); assertThat(post.getId()).isEqualTo(1); assertThat(post.getUserId()).isEqualTo(1); assertThat(post.getTitle()).isEqualTo(\u0026#34;title\u0026#34;); assertThat(post.getBody()).isEqualTo(\u0026#34;body\u0026#34;); } private String read(String location) throws IOException { return IOUtils.toString(new ClassPathResource(location).getInputStream(), StandardCharsets.UTF_8); } } Download the complete source code for the examples in this post from github/springboot-api\n","permalink":"https://codingnconcepts.com/spring-boot/configure-feign-rest-client/","tags":["Spring Boot API","REST","Feign"],"title":"Configure Feign Client in Spring Boot"},{"categories":["Java"],"contents":"This article demonstrate use of String.format() in Java with many examples\u0026hellip;\nThe String.format() method is used to produce formatted string, numeric, and date/time data, and locale specific output. It requires a format string and argument list as input.\n1. Syntax //Returns a formatted string using the specified format string and arguments. static String format​(String format, Object... args)\t//Returns a formatted string using the specified locale, format string, and arguments. static String format​(Locale l, String format, Object... args)\tLet\u0026rsquo;s look into format specifier before we look into examples\nSyntax of format specifier %[argument_index$][flags][width][.precision]conversion Where,\noptional argument_index is position of argument. The first arg is referenced by 1$, the second by 2$, etc. optional flags is set of characters to modify the output format\nIf not specified - The result will include a leading space for positive values\n-\tThe result will be left-justified.\n#\tThe result should use a conversion-dependent alternate form\n+\tThe result will always include a sign\n0\tThe result will be zero-padded\n,\tThe result will include locale-specific grouping separators\n(\tThe result will enclose negative numbers in parentheses optional width is minimum number of characters in string output optional precision is used for floating numbers to indicate numbers after decimal places. required conversion is character indicating how it should be formatted\ns or S for string\nf for floating point numbers\nd for integers\nt for date and time\nn for line break 2. String Format Examples 2.1 String Template String[] fruits = new String[] {\u0026#34;Mango\u0026#34;, \u0026#34;Grapes\u0026#34;, \u0026#34;Banana\u0026#34;, \u0026#34;Kiwi\u0026#34;}; // String Concat System.out.println(String.format(\u0026#34;My favourite fruits are %1$s, %2$s, %3$s and %4$s\u0026#34;, fruits)); //Reverse String Concat System.out.println(String.format(\u0026#34;My favourite fruits are %4$s, %3$s, %2$s and %1$s\u0026#34;, fruits)); We have used formatter %1$s, %2$s, %3$s, %4$s to specify the position of string arguments and %n for line break.\nOutput My favourite fruits are Mango, Grapes, Banana and Kiwi My favourite fruits are Kiwi, Banana, Grapes and Mango 2.2 String Line Break String format can be used to create multi line string in Java\nSystem.out.println(String.format(\u0026#34;Address:-%nUnit 505,%n32 Cross Street\u0026#34;, null)); Output Address:- Unit 505, 32 Cross Street 2.3 String Padding Following String format will add left padding of 10 characters to foo\n%13s says that output will have min width of 13 including foo length, which is 3 hence (13 - 3 =) 10 left padding space.\nSystem.out.println(String.format(\u0026#34;%13s\u0026#34;, \u0026#34;foo\u0026#34;)); Output foo 3. Number Format Examples 3.1 Float Decimal Places System.out.println(Math.PI); // Rounding off Decimal Places System.out.println(String.format(\u0026#34;%.1f\u0026#34;, Math.PI)); System.out.println(String.format(\u0026#34;%.5f\u0026#34;, Math.PI)); System.out.println(String.format(\u0026#34;%.10f\u0026#34;, Math.PI)); We have used formatter %.1f, %.5f, and %.10f to show 1, 5 and 10 decimal places of PI respectively.\nOutput 3.141592653589793 3.1 3.14159 3.1415926536 3.2 Float Padding // Min Width with leading spaces System.out.println(String.format(\u0026#34;%10f\u0026#34;, Math.PI)); System.out.println(String.format(\u0026#34;%20f\u0026#34;, Math.PI)); System.out.println(String.format(\u0026#34;%30f\u0026#34;, Math.PI)); We have used formatter %10f, %20f, and %30f to show minimum 10, 20 and, 30 characters with leading space respectively. We see that string length 2.141593 is 8 characters which result in padding of 2, 12 and, 22 characters respectively.\nOutput 3.141593 3.141593 3.141593 3.3 Negative Floating Point Numbers // +ve Floating Numbers System.out.println(String.format(\u0026#34;%+20.5f\u0026#34;, Math.PI)); // + Flag show sign System.out.println(String.format(\u0026#34;%020.5f\u0026#34;, Math.PI)); // 0 Flag for leading zeros System.out.println(String.format(\u0026#34;%(20.5f\u0026#34;, Math.PI)); // ( Flag to show -ve numbers in parentheses // -ve Floating Numbers System.out.println(String.format(\u0026#34;%+20.5f\u0026#34;, -Math.PI)); System.out.println(String.format(\u0026#34;%020.5f\u0026#34;, -Math.PI)); System.out.println(String.format(\u0026#34;%(20.5f\u0026#34;, -Math.PI)); Output +3.14159 00000000000003.14159 3.14159 -3.14159 -0000000000003.14159 (3.14159) 3.4 Float with locale //Locale Specific System.out.println(String.format(Locale.FRANCE, \u0026#34;%f\u0026#34;, Math.PI)); System.out.println(String.format(Locale.FRANCE, \u0026#34;%f\u0026#34;, -Math.PI)); Output 3,141593 -3,141593 3.5 Format Number with Commas System.out.println(String.format(\u0026#34;%,d\u0026#34;, 1234567890)); Output 1,234,567,890 4. Date Format Examples 4.1 LocalDateTime Format LocalDateTime date = LocalDateTime.parse(\u0026#34;1986-08-22T05:45:59\u0026#34;); System.out.println(String.format(\u0026#34;My Birth Date is %1$te-%1$tm-%1$tY %1tT\u0026#34;, date)); System.out.println(String.format(\u0026#34;I was born on %1$teth %1tB, %1$tY\u0026#34;, date)); Output My Birth Date is 22-08-1986 05:45:59 I was born on 22th August, 1986 4.2 LocalDate Format Format LocalDate to yyyymmdd\nLocalDate date = LocalDate.parse(\u0026#34;2020-12-25\u0026#34;); // yyyymmdd System.out.println(String.format(\u0026#34;%1$tY%1$tm%1$te\u0026#34;, date)); Output 20201225 5. Print Format Examples 5.1 System.out.printf and System.out.format If you have to just print formatted output and doesn\u0026rsquo;t require returned output as string, you can use System.out.printf or System.out.format instead.\nString print = \u0026#34;Hello World !!!\u0026#34;; System.out.println(String.format(\u0026#34;Saying - %s\u0026#34;, print)); System.out.printf(\u0026#34;Saying - %s\u0026#34;, print); System.out.format(\u0026#34;Saying - %s\u0026#34;, print); All the three System.out print the same output\nOutput Saying - Hello World !!! Saying - Hello World !!! Saying - Hello World !!! ","permalink":"https://codingnconcepts.com/java/string-format-in-java/","tags":["Core Java","String"],"title":"Java String.format() method Usage"},{"categories":["Spring Boot"],"contents":"In this article, we\u0026rsquo;ll learn to build RESTFul APIs in Spring Boot Web Application using layered architecture and test driven development approach.\nProject Setup For initial setup of your Spring Boot project, you should use Spring Initializr. Choose the Spring Web dependency.\nMaven Project You can click the below link to generate a maven project with pre-selected dependencies:-\nhttps://start.spring.io/#!type=maven-project\u0026amp;language=java\u0026amp;platformVersion=2.5.0.RELEASE\u0026amp;packaging=jar\u0026amp;jvmVersion=11\u0026amp;groupId=com.example\u0026amp;artifactId=demo\u0026amp;name=demo\u0026amp;description=Demo%20project%20for%20Spring%20Boot\u0026amp;packageName=com.example.demo\u0026amp;dependencies=web\nA typical pom.xml file for a web project look like this:-\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.0\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;api\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;Create Feign Client to consume RESTFul APIs\u0026lt;/description\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; Gradle Project Similarly, You can click the below link to generate a Gradle project with pre-selected dependencies:-\nhttps://start.spring.io/#!type=gradle-project\u0026amp;language=java\u0026amp;platformVersion=2.5.0.RELEASE\u0026amp;packaging=jar\u0026amp;jvmVersion=11\u0026amp;groupId=com.example\u0026amp;artifactId=demo\u0026amp;name=demo\u0026amp;description=Demo%20project%20for%20Spring%20Boot\u0026amp;packageName=com.example.demo\u0026amp;dependencies=web\nA typical build.gradle file for a web project look like this:-\nplugins { id \u0026#39;org.springframework.boot\u0026#39; version \u0026#39;2.5.0\u0026#39; id \u0026#39;io.spring.dependency-management\u0026#39; version \u0026#39;1.0.11.RELEASE\u0026#39; id \u0026#39;java\u0026#39; } group = \u0026#39;com.example\u0026#39; version = \u0026#39;0.0.1-SNAPSHOT\u0026#39; sourceCompatibility = \u0026#39;11\u0026#39; repositories { mavenCentral() } dependencies { implementation \u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39; testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; } test { useJUnitPlatform() } Enable Web MVC When you are working with spring boot project, you have nothing much to do to enable spring web mvc for your project. Make sure:-\nYou have spring-boot-starter-web dependency in your pom.xml or build.gradle You are using @SpringBootApplication on your application starter class file. Spring Boot is opinionated, when it sees the web dependency in the classpath, it sets up all the necessary default configuration required for API development so that you can just concentrate on your business logic.\npackage com.example.api; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class ApiApplication { public static void main(String[] args) { SpringApplication.run(ApiApplication.class, args); } } Build RESTFul APIs We use the layered architecture to build RESTFul APIs where Controller Layer is the front liners to serve API request and delegate the request to Service Layer and then to Repository or Client layer and so and so forth. Once the requested data is available, it is returned back in the same order for e.g. Repository or Client to Service to Controller layer.\nController Layer We are creating a UserController class to build RESTFul APIs for CRUD operations. Few things to understand:-\nUse @RestController at class level, it helps to bind default HTTP Converters for you, e.g. when you return a User object from controller methods, it takes care of converting them in JSON. Use @RequestMapping at class level, to map APIs to URL. Use shorthand of @RequestMapping i.e. @GetMapping, @PostMapping, @PutMapping, @DeleteMapping at method level. Use @ResponseStatus at method level for appropriate HTTP Stats Code. Delegate the work to Service Layer, here using UserService package com.example.api.controller; @RestController @RequestMapping(\u0026#34;/users\u0026#34;) public class UserController { @Autowired private UserService userService; @GetMapping public Users getAllUsers() { return userService.getAllUsers(); } @GetMapping(\u0026#34;/{id}\u0026#34;) public User getUserById(@PathVariable Long id) { return userService.getUserById(id); } @PostMapping @ResponseStatus(HttpStatus.CREATED) public Long createUser(User user) { return userService.createUser(user); } @PutMapping(\u0026#34;/{id}\u0026#34;) @ResponseStatus(HttpStatus.OK) public void updateUser(@PathVariable Long id, User user) { userService.updateUser(id, user); } @DeleteMapping(\u0026#34;/{id}\u0026#34;) @ResponseStatus(HttpStatus.OK) public void deleteUserById(@PathVariable Long id) { userService.deleteUserById(id); } } Service Layer We are creating a UserService class which takes care of business logic. Few things to understand:-\nUse @Service at class level, which auto register service class bean to Spring boot context. Service class can further fetch the data from:- Repository classes (used to fetch data from Database by extending Spring Boot JpaRepository) Client classes (used to fetch data from thirdparty APIs using FeignClient or RestTemplate) Any other data provider In This example, our UserService is using UserMockClient to fetch mock data.\npackage com.example.api.service; @Service public class UserServiceImpl implements UserService { @Autowired private UserMockClient userMockClient; @Override public List\u0026lt;User\u0026gt; getAllUsers() { return userMockClient.getAllUsers(); } @Override public User getUserById(Long id) { return userMockClient.getUserById(id); } @Override public Long createUser(User user) { return userMockClient.createUser(user); } @Override public boolean updateUser(Long id, User user) { return userMockClient.updateUser(id, user); } @Override public boolean deleteUserById(Long id) { return userMockClient.deleteUserById(id); } } That\u0026rsquo;s it. Just start your application and test your APIs.\nUnit test for Controller and Service classes It is a good practice to write unit test cases for Controller and Service classes you created.\nUnit Test for Controller Class Let\u0026rsquo;s create a class UserControllerTest in src/test/java directory and follow the same package structure as our UserController class. Things to note that:-\nWe have used @SpringBootTest annotation at class level which configure a spring boot context of your application before executing test cases. This gives us ability to bind actual spring bean using @Autowired and mock the spring bean using @MockBean annotation. We have autowired MockMvc and mocked UserService class in this example. We have used @AutoConfigureMockMvc annotation at class level which auto configure MockMvc which is used to perform HTTP operations using Mockito framework. Mockito framework is used to mock the behavior of classes. In this example, we mocked the service layer and HTTP operations.\nIn each test case of controller layer:-\nWe first mock the behavior of service layer using Mockito methods then we perform HTTP operation using MockMvc which delegate the request to mocked service layer instead of actual one. then we assert with expected response package com.example.api.controller; @SpringBootTest @AutoConfigureMockMvc public class UserControllerTest { @Autowired private MockMvc mockMvc; @MockBean private UserService userService; @Test public void getAllUsers_whenValidRequest_returnsValidResponse() throws Exception { // mock service class behavior when(userService.getAllUsers()).thenReturn(UserTestData.users()); // perform HTTP operation using MockMvc mockMvc.perform(get(\u0026#34;/users\u0026#34;)) .andExpect(status().is2xxSuccessful()) .andExpect(jsonPath(\u0026#34;$\u0026#34;, hasSize(UserTestData.users().size()))) .andExpect(jsonPath(\u0026#34;$[0].id\u0026#34;, equalTo(1))) .andExpect(jsonPath(\u0026#34;$[0].name\u0026#34;, equalTo(\u0026#34;Adam\u0026#34;))) .andExpect(jsonPath(\u0026#34;$[0].dateOfBirth\u0026#34;, equalTo(\u0026#34;22 Aug 1986\u0026#34;))); } @Test public void getAllUsers_whenServiceThrowException_returnsInternalServerError() throws Exception { when(userService.getAllUsers()).thenThrow(new RuntimeException(\u0026#34;Oops, Something went wrong!\u0026#34;)); mockMvc.perform(get(\u0026#34;/users\u0026#34;)) .andExpect(status().isInternalServerError()) .andExpect(jsonPath(\u0026#34;$.message\u0026#34;, equalTo(\u0026#34;Internal Server Error\u0026#34;))) .andExpect(jsonPath(\u0026#34;$.debugMessage\u0026#34;, equalTo(\u0026#34;Oops, Something went wrong!\u0026#34;))); } @Test public void getUserById_whenValidUserId_returnThatUser() throws Exception { when(userService.getUserById(anyLong())).thenReturn(UserTestData.user()); mockMvc.perform(get(\u0026#34;/users/1\u0026#34;)) .andExpect(status().is2xxSuccessful()) .andExpect(jsonPath(\u0026#34;$.id\u0026#34;, equalTo(1))) .andExpect(jsonPath(\u0026#34;$.name\u0026#34;, equalTo(\u0026#34;Adam\u0026#34;))) .andExpect(jsonPath(\u0026#34;$.dateOfBirth\u0026#34;, equalTo(\u0026#34;22 Aug 1986\u0026#34;))); } @Test public void createUser_whenValidUserData_createAndReturnTheUserId() throws Exception { when(userService.createUser(UserTestData.user())).thenReturn(UserTestData.user().getId()); mockMvc.perform(post(\u0026#34;/users\u0026#34;) .contentType(MediaType.APPLICATION_JSON) .content(asJsonString(UserTestData.user()))) .andExpect(status().is2xxSuccessful()); } @Test public void updateUser_whenValidUserId_updateThatUser() throws Exception { mockMvc.perform(put(\u0026#34;/users/1\u0026#34;)) .andExpect(status().is2xxSuccessful()); } @Test public void deleteUser_whenValidUserId_deleteThatUser() throws Exception { mockMvc.perform(delete(\u0026#34;/users/1\u0026#34;)) .andExpect(status().is2xxSuccessful()); } @Test public void patchUser_whenUnsupportedHttpVerb_returnsMethodNotAllowed() throws Exception { mockMvc.perform(patch(\u0026#34;/users\u0026#34;)) .andExpect(status().isMethodNotAllowed()) .andExpect(jsonPath(\u0026#34;$.message\u0026#34;, equalTo(\u0026#34;Method Not Allowed\u0026#34;))) .andExpect(jsonPath(\u0026#34;$.debugMessage\u0026#34;, equalTo(\u0026#34;Request method \u0026#39;PATCH\u0026#39; not supported\u0026#34;))); } public static String asJsonString(final Object obj) { try { return new ObjectMapper().writeValueAsString(obj); } catch (Exception e) { throw new RuntimeException(e); } } } Unit Test for Service Class Let\u0026rsquo;s create a class UserServiceTest in src/test/java directory and follow the same package structure as our UserService class.\nHere we again used @SpringBootTest annotation to bind the actual spring bean of UserService and the mocked spring bean of client layer UserMockClient.\nIn each test case of service layer:-\nWe first mock the behavior of client layer using Mockito methods then we call method of service layer which delegates the call to mocked client layer instead of actual one. then we assert with expected output package com.example.api.service; @SpringBootTest public class UserServiceTest { @MockBean private UserMockClient userMockClient; @Autowired private UserService userService; @Test public void getAllUsers_whenValidProviderResponse_returnAllUsers() { when(userMockClient.getAllUsers()).thenReturn(UserTestData.users()); List\u0026lt;User\u0026gt; users = userService.getAllUsers(); assertThat(users.size()).isEqualTo(1); assertThat(users.get(0).getId()).isEqualTo(1); assertThat(users.get(0).getName()).isEqualTo(\u0026#34;Adam\u0026#34;); assertThat(users.get(0).getDateOfBirth().toString()).isEqualTo(\u0026#34;1986-08-22\u0026#34;); } @Test public void getUserById_whenValidUserId_returnThatUser() { when(userMockClient.getUserById(anyLong())).thenReturn(UserTestData.user()); User user = userService.getUserById(1L); assertThat(user.getId()).isEqualTo(1); assertThat(user.getName()).isEqualTo(\u0026#34;Adam\u0026#34;); assertThat(user.getDateOfBirth().toString()).isEqualTo(\u0026#34;1986-08-22\u0026#34;); } @Test public void createUser_whenValidUserData_createAndReturnUserId() { when(userMockClient.createUser(any(User.class))).thenReturn(UserTestData.user().getId()); Long id = userService.createUser(UserTestData.user()); assertThat(id).isEqualTo(1L); } @Test public void updateUser_whenValidUserData_updateAndReturnStatus() { when(userMockClient.updateUser(anyLong(), any(User.class))).thenReturn(true); Boolean status = userService.updateUser(UserTestData.user().getId(), UserTestData.user()); assertThat(status).isTrue(); } @Test public void deleteUser_whenValidUserId_deleteAndReturnStatus() { when(userMockClient.deleteUserById(anyLong())).thenReturn(true); Boolean status = userService.deleteUserById(UserTestData.user().getId()); assertThat(status).isTrue(); } } Download the complete source code for this example from github/springboot-api\n","permalink":"https://codingnconcepts.com/spring-boot/build-restful-api-with-spring-boot/","tags":["Spring Boot API","REST"],"title":"Build RESTFul API with Spring Boot"},{"categories":["Spring Boot"],"contents":"In this article, we\u0026rsquo;ll learn how to create a Spring Boot project quickly from scratch use Spring Initializer.\nSpring Initializr It is highly recommended to use Spring Initializr for the initial setup of your Spring Boot Project. Some of the advantage of Spring Initializer:-\nIt is the quickest way to setup a project by selecting project build, language, spring boot version, project metadata, and required dependencies.\nYou will always be working on the latest stable spring boot version, the latest stable version is selected by default.\nAuto generates pom.xml or build.gradle for you based on your selections.\nYou can use SHARE button to generate a URL for your selected configuration, e.g.\nhttps://start.spring.io/#!type=maven-project\u0026amp;language=java\u0026amp;platformVersion=2.5.0.RELEASE\u0026amp;packaging=jar\u0026amp;jvmVersion=1.8\u0026amp;groupId=com.example\u0026amp;artifactId=api\u0026amp;name=api\u0026amp;description=Build%20RESTFul%20API%20using%20Spring%20Boot\u0026amp;packageName=com.example.api\u0026amp;dependencies=web\nThe above URL will show following configuration pre-selected in Spring Initialzr\nExample of Spring Boot RESTFul API Project\nYou can quickly review the project using EXPLORE before generating a project.\nYou can GENERATE the project. It will download a .zip file, which you can extract and import in Spring STS, Eclipse, or IntelliJ IDE Downloaded api.zip and its extracted files\nImported Spring Boot Project in Eclipse\nThat\u0026rsquo;s it. I hope this will encourage you to build a project using Spring Initalizr\nSpring Initializr for Eclipse IDE If you use Eclipse IDE for Java development then you can install plugin Spring Tool for Eclipse IDE which is very useful to quickly create a Spring Boot project within Eclipse using Spring Initializr.\nRead How to setup Spring Tool for Eclipse IDE and create Spring Boot Project\n","permalink":"https://codingnconcepts.com/spring-boot/setup-project-using-spring-initializr/","tags":["Spring Boot Basics"],"title":"Setup a Spring Boot Project using Spring Initializr"},{"categories":["Javascript"],"contents":"In this article, we\u0026rsquo;ll learn how to implement Blockchain using Javascript.\nPrerequisite Download and install NodeJs to run our blockchain code Download and install Visual Studio Code IDE for code development Don\u0026rsquo;t worry if you don\u0026rsquo;t have prior experience with JavaScript, NodeJS, and VSCode. Just follow with me and you will learn basics of Blockchain which you can later implement in any other language.\nBlockchain Blockchain is nothing but a digital, distributed, immutable, and trusted ledger which can be used to record anything from financial transactions, government records to land titles, or even purchase orders.\nEach transaction record is a block, which are linked together chronologically to form a chain of blocks. If you want to change record in a particular block, you don\u0026rsquo;t rewrite it. Instead, the change is recorded in a new block.\nAlso read Blockchain Basics and its Practical Use Cases\nBlock Let\u0026rsquo;s see what a typical block in blockchain consist of:-\nTimestamp is date and time when the transaction took place. Record typically contains the details of a transaction like sender, receiver, amount. We are going to use actual names of sender and receiver in our program but in a practical use case like bitcoin, actual names are not revealed, a digital signature of sender and receiver is used instead. Hash is a digital fingerprint that represents the transaction in the block and is completely unique. If there is any change in transaction details, the hash would also change. Generally, it\u0026rsquo;s an alphanumeric sequence generated by applying some crypto algorithm like SHA-256 on transaction details. Previous Hash is the Hash of the previous block in blockchain. This is also used to generate Hash of the block. Nonce is an abbreviation for \u0026ldquo;number only used once,\u0026rdquo; which is a number added to a block in a blockchain that, when rehashed, meets the difficulty level restrictions. We will discuss more on this later Step 1: Create Block (block.js) Now we know the what block consist of, let\u0026rsquo;s create it\nconst SHA256 = require(\u0026#39;crypto-js/sha256\u0026#39;); class Block { constructor(timestamp, previousHash, record, difficultyLevel) { this.timestamp = timestamp; this.record = record; this.previousHash = previousHash; const proofOfWork = this.proofOfWork(difficultyLevel); this.hash = proofOfWork.hash; this.nonce = proofOfWork.nonce; } /* Genesis Block */ static genesis() { return new this(new Date(), \u0026#34;\u0026#34;, \u0026#34;GENESIS\u0026#34; ); } /* Block Mining */ static mineBlock(previousBlock, record, difficultyLevel) { const timestamp = new Date(); const previousHash = previousBlock.hash; return new Block(timestamp, previousHash, record, difficultyLevel); } /* Generate Hash using SHA256 */ static computeHash(message){ return SHA256(message).toString(); } /* Proof of Work */ proofOfWork(difficultyLevel) { const message = this.timestamp + JSON.stringify(this.record) + this.previousHash; if(difficultyLevel){ const leadingZeros = \u0026#34;0\u0026#34;.repeat(difficultyLevel); let nonce = 0; while(true){ let hash = Block.computeHash(message + nonce); if(hash.substring(0, difficultyLevel) == leadingZeros){ return { hash, nonce }; } nonce++; } }else{ return { hash: Block.computeHash(message), nonce: 0 } } } } module.exports = Block; Let\u0026rsquo;s understand few more terms\nGenesis Block is the first block in blockchain which is not having any transaction details and previous hash. This is generally added at the time of creation of blockchain. Also known as Block Zero Difficulty Level is the restriction to generate hash of the block. More difficulty level, more time it takes to generate a hash. Block Mining is a process of adding a new block to the blockchain. Who adds new block is called Block Miner, also known as bitcoin miner in case of bitcoin. Anyone can register their computer to become one of the bitcoin miner in case of a public blockchain like bitcoin. All the bitcoin miners in bitcoin network get a copy of whole blockchain and each receives a notification when a new block is added to the blockchain. Proof of Work is a cryptographic hash puzzle which each Block Miner in the blockchain tries to solve. As soon as one of the Block Miner solves the puzzle, essentially means generated the hash of the block with said difficulty level, they broadcast this to the blockchain network and hash is validated by all the other Block Miners in the network. What is Proof of Work for us? Out Proof of Work is to generate a hash with leading zeros based on difficulty level.\nDifficulty level of 5 means, generate a hash with 5 leading zeros, e.g. 00000b4d7m3h1s0k2s8bw0hn382\nHow we are going to achieve? We are going to calculate the hash by applying SHA-256 algorithm on the transaction details and nonce value. We will start with nonce value of 0 and keep incrementing it until we find the hash with leading zeros of difficulty level.\nStep 2: Create Blockchain (blockchain.js) Now we have learned a lot of terms, let\u0026rsquo;s quickly create a blockchain\nconst Block = require(\u0026#39;./block\u0026#39;); class Blockchain { constructor() { this.difficultyLevel = 1; this.chain = [Block.genesis()]; } addBlock(record) { const newBlock = Block.mineBlock(this.chain[this.chain.length-1], record, this.difficultyLevel); this.chain.push(newBlock); } } module.exports = Blockchain; Step 3: Test Blockchain (server.js) Let\u0026rsquo;s create few blocks of transaction with random sender, receiver and amount details. Also, let\u0026rsquo;s increase the difficulty level after every 2 transactions.\nconst Blockchain = require(\u0026#39;./blockchain\u0026#39;) const { performance } = require(\u0026#39;perf_hooks\u0026#39;); const blockchain = new Blockchain(); const userList = [\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;, \u0026#34;David\u0026#34;, \u0026#34;Eric\u0026#34;, \u0026#34;Franklin\u0026#34;, \u0026#34;Gavin\u0026#34;, \u0026#34;Harry\u0026#34;, \u0026#34;Iris\u0026#34;, \u0026#34;Joey\u0026#34;, \u0026#34;Kate\u0026#34;, \u0026#34;Leo\u0026#34;, \u0026#34;Monica\u0026#34;, \u0026#34;Nancy\u0026#34;, \u0026#34;Oscar\u0026#34;, \u0026#34;Phoebe\u0026#34;, \u0026#34;Quinn\u0026#34;, \u0026#34;Ross\u0026#34;, \u0026#34;Sofia\u0026#34;, \u0026#34;Tyler\u0026#34;, \u0026#34;Umar\u0026#34;, \u0026#34;Victor\u0026#34;, \u0026#34;Wilson\u0026#34;, \u0026#34;Xena\u0026#34;, \u0026#34;Yasmine\u0026#34;, \u0026#34;Zara\u0026#34;]; const addNBlocks = (n) =\u0026gt; { for(let i = 0; i \u0026lt; n; i++) { blockchain.addBlock({ sender: userList[Math.floor(Math.random() * userList.length)], receiver: userList[Math.floor(Math.random() * userList.length)], amount: Math.floor(Math.random() * 1000) }); } } const t0 = performance.now(); addNBlocks(2); var t1 = performance.now() console.log(\u0026#34;[Difficulty Level 1] Added first 2 blocks in \u0026#34; + (t1 - t0) + \u0026#34; milliseconds.\u0026#34;) blockchain.difficultyLevel = 3; addNBlocks(2); var t2 = performance.now() console.log(\u0026#34;[Difficulty Level 3] Added next 2 blocks in \u0026#34; + (t2 - t1) + \u0026#34; milliseconds.\u0026#34;) blockchain.difficultyLevel = 5; addNBlocks(2); var t3 = performance.now() console.log(\u0026#34;[Difficulty Level 5] Added next 2 blocks in \u0026#34; + (t3 - t2) + \u0026#34; milliseconds.\u0026#34;) /* Print Blockchain*/ console.log(blockchain.chain); Now everything is ready, go to terminal and run following commands\nnpm install --save crypto-js node server You will see an output like this.\nOutput [Difficulty Level 1] Added first 2 blocks in 6.2153230011463165 milliseconds. [Difficulty Level 3] Added next 2 blocks in 175.92524899542332 milliseconds. [Difficulty Level 5] Added next 2 blocks in 2065.910447001457 milliseconds. [ Block { timestamp: 2020-05-18T17:05:37.501Z, record: \u0026#39;GENESIS\u0026#39;, previousHash: \u0026#39;\u0026#39;, hash: \u0026#39;9636ccb176c9f4825d24e1b8db51e3ffb5d448ba112ec0db9672e80f6dc855c3\u0026#39;, nonce: 0 }, Block { timestamp: 2020-05-18T17:05:37.504Z, record: { sender: \u0026#39;Xena\u0026#39;, receiver: \u0026#39;Umar\u0026#39;, amount: 770 }, previousHash: \u0026#39;9636ccb176c9f4825d24e1b8db51e3ffb5d448ba112ec0db9672e80f6dc855c3\u0026#39;, hash: \u0026#39;0329bb14cb5d59a2ddce485019de8041c6790b4483afbad91516ec78e21a70f4\u0026#39;, nonce: 24 }, Block { timestamp: 2020-05-18T17:05:37.509Z, record: { sender: \u0026#39;Harry\u0026#39;, receiver: \u0026#39;Wilson\u0026#39;, amount: 601 }, previousHash: \u0026#39;0329bb14cb5d59a2ddce485019de8041c6790b4483afbad91516ec78e21a70f4\u0026#39;, hash: \u0026#39;05187dd8fa18b1ebd50565f1607e43ab9ad081955a71fb6f542cb91755192b49\u0026#39;, nonce: 12 }, Block { timestamp: 2020-05-18T17:05:37.512Z, record: { sender: \u0026#39;David\u0026#39;, receiver: \u0026#39;Quinn\u0026#39;, amount: 600 }, previousHash: \u0026#39;05187dd8fa18b1ebd50565f1607e43ab9ad081955a71fb6f542cb91755192b49\u0026#39;, hash: \u0026#39;000a0f95e605831a1fa0178351a195fc6f60752fd59e251ea56b1c0d464b8920\u0026#39;, nonce: 7947 }, Block { timestamp: 2020-05-18T17:05:37.620Z, record: { sender: \u0026#39;Yasmine\u0026#39;, receiver: \u0026#39;Umar\u0026#39;, amount: 918 }, previousHash: \u0026#39;000a0f95e605831a1fa0178351a195fc6f60752fd59e251ea56b1c0d464b8920\u0026#39;, hash: \u0026#39;000e8a70d4b6673f3a10cc68fdc21e0cda7a71aa0fe56d31a13cf95832596a45\u0026#39;, nonce: 5384 }, Block { timestamp: 2020-05-18T17:05:37.686Z, record: { sender: \u0026#39;Phoebe\u0026#39;, receiver: \u0026#39;Victor\u0026#39;, amount: 336 }, previousHash: \u0026#39;000e8a70d4b6673f3a10cc68fdc21e0cda7a71aa0fe56d31a13cf95832596a45\u0026#39;, hash: \u0026#39;000005b737aaa4faa3b87b56474dec9537eee843c2881cb5f6432ca9708bd7b5\u0026#39;, nonce: 45944 }, Block { timestamp: 2020-05-18T17:05:38.178Z, record: { sender: \u0026#39;Umar\u0026#39;, receiver: \u0026#39;Oscar\u0026#39;, amount: 239 }, previousHash: \u0026#39;000005b737aaa4faa3b87b56474dec9537eee843c2881cb5f6432ca9708bd7b5\u0026#39;, hash: \u0026#39;0000057a7a4c7caea164b4e5672a3c38fcf12c6c8bfb4b1c503ca876664f2660\u0026#39;, nonce: 150796 } ] Observations from Output As we increased difficulty level from 1, 3, to 5, it took more time to generate the hash and add new blocks. Blockchain technologies like bitcoin require to solve hash puzzle with the difficulty level of somewhere starting from 18 to 30. You can imagine the time requires to solve those puzzles. The first block of blockchain is GENESIS block with no transaction info and no previous hash value See the hash generated with 1, 3, and 5 leading zeros based on difficulty level See the variable nonce value keeps increasing with difficulty level, also tells us the number of attempts our hash algorithm took to generate hash. Summary I hope you have now a basic understanding of Blockchain technology and how we can implement it. Please note that the above example is a very basic implementation of Blockchain. Real-world examples are very complicated but this is the first step to enter the Blockchain world.\nPlease find the source code for this example on github\n","permalink":"https://codingnconcepts.com/javascript/blockchain-explained-using-javascript/","tags":["Javascript Coding"],"title":"Blockchain Explained using JavaScript"},{"categories":null,"contents":"Most of us think Blockchain as the technology behind Bitcoin. While this was its original purpose, blockchain is capable of so much more\u0026hellip;\nWhat is Blockchain? Blockchain is digital, distributed, immutable and trusted ledger.\nLet\u0026rsquo;s understand each term one by one:-\nBlockchain is Digital Ledger You are familiar with the ledger, which is nothing but a book to record financial transactions. Similarly, Blockchain is a digital ledger which can be programmed to record things, and the things are not limited to financial transactions, but could be anything from government records to land titles, or even purchase orders.\nGeneral Ledger\nEach line of record in a ledger book can be assumed as a block in blockchain, which are linked together chronologically to form a chain of blocks.\nYou write a new record in the financial ledger book for any changes in previous transactions. Similarly, If you want to change record in a particular block, you don\u0026rsquo;t rewrite it. Instead, the change is recorded in a new block.\nBlockchain is Distributed Ledger Let\u0026rsquo;s assume that I am maintaining a shared ledger book with my friends where everyone records their mutual expenses. Let\u0026rsquo;s say, I lend $100 to my friend Ben and recorded in a shared ledger. Now my friends Charlie and David know the exact amount of money I lend to Ben and they\u0026rsquo;ve essentially endorsed the transaction and they made a record of it. Now when next month, Ben asked me how much money had initially lent him, we could easily go to anyone of my friends Charlie or David that have a record of that transaction. When you make new friends, they can also join you in shared ledger.\nSimilar to a shared ledger, Anyone can opt to connect their computer to the blockchain network as node. In doing so, their computer receives a copy of the blockchain that is updated automatically whenever a new block is added, sort of like a Facebook News Feed that gives a live update whenever a new status is posted.\nIn this sense, blockchain is distributed and data is shared.\nAdvantage Reconciliation is not required as data is shared and each node is having copy of transaction. High availability as thousands of distributed nodes working in the network, doesn\u0026rsquo;t matter if few goes down. Blockchain is Immutable Ledger Let\u0026rsquo;s look into the pieces which make up a Blockchain to understand its immutability\ntransaction: this is details of the transaction that occurred when the block was created\nhash: this is a digital fingerprint that represents the transaction in the block and is completely unique. If there is any change in transaction details, the hash would also change. Generally, it\u0026rsquo;s an alphanumeric sequence generated by applying some crypto algorithm on transaction details\npreviousHash: this is the hash of the previous block in the chain.\nLet\u0026rsquo;s look at the below example of blockchain. Where transaction is having sender, receiver, and amount details. For simplicity, we are generating hash by just combining transaction details, i.e.\nhash = sender + receiver + amount Please note that Block 1 is the first block in our Blockchain, also known as Genesis Block, is not having previousHash value.\nBlock 1 transaction { sender: Ash, receiver: Ben: amount: $100 } hash ashben100 previousHash Block 2 transaction { sender: Ben, receiver: Charlie: amount: $200 } hash bencharlie200 previousHash ashben100 Block 3 transaction { sender: Ash, receiver: David: amount: $300 } hash ashdavid300 previousHash bencharlie200 Let’s say a hacker attempts to edit the transaction amount in Block 2 from $200 to $999. As soon as they edit the dollar amount of the transaction, the block’s hash will change. The next block Block 3 in the chain will still contain the old previousHash, and the hacker would need to update that block in order to cover their tracks. However, doing so would change that block’s hash. And the next, and so on.\nBlock 2 (Tampered Amount) transaction { sender: Ben, receiver: Charlie: amount: $999 } hash bencharlie999 previousHash ashben100 Block 3 (Old Previous Hash) transaction { sender: Ash, receiver: David: amount: $300 } hash ashdavid300 previousHash bencharlie200 In order to change a single block, then, a hacker would need to change every single block after it on the blockchain. Recalculating all those hashes would take an enormous and improbable amount of computing power. In other words, once a block is added to the blockchain it becomes very difficult to edit and impossible to delete.\nIn this sense, Blockchains are immutable and tamper-resistant.\nBlockchain is Trusted Ledger When you transfer money to someone, you do it through a mediator, generally a bank. Now all the transaction records are centralized within the bank. There are more chances for hackers to tamper with records.\nMoreover, when you transfer a crypto currency like bitcoin, which is backed by blockchain technology, there is no mediator involved, essentially all of the nodes, millions in case of crypto currencies, in the blockchain network are mediators, having their own copy of records. If hackers want to tamper with any record, they need to do it in all those million copies of record.\nLet\u0026rsquo;s understand more on this, to become eligible for adding a new block in the blockchain, a node has to solve a very complex math problem and need to provide something called proof of work. Solving puzzle is intensive in terms of computation (time) and electricity consumption (money), which requires time as well as money.\nAs we saw earlier that hacker need to tamper with all those millions copies. If adding one block to the blockchain requires spending so much time and money, you can imagine how it will be to tamper with all those millions copies. Certainly it\u0026rsquo;s not worth the effort and more likely impossible.\nIn this sense, blockchain technology is trusted.\nBlockchain\u0026rsquo;s Use Cases 1. Bitcoin Most of the crypto currencies - Bitcoin, Dash, Ethereum, Litecoin, Ripple follows public blockchain. Public blockchains are open to everyone to view and access, also known as permissionless blockchain as you do not require authorization.\nCrypto Currencies\nThe bitcoin protocol is built on the blockchain. Introduced as digital currency, Bitcoin’s pseudonymous creator Satoshi Nakamoto referred to it as “a new electronic cash system that’s fully peer-to-peer, with no trusted third party.”\nHow Bitcoin Works? You have all these people, all over the world, who have bitcoin. There are likely many millions of people around the world who own at least a portion of a bitcoin. Let’s say one of those millions of people wants to spend their bitcoin on groceries. This is where the blockchain comes in.\nWhen one person pays another for goods using bitcoin, essentially a transaction is initiated. This transactions contains all the necessary information such as sender and receiver, but not the actual names, instead a unique digital signature sort of like a username is used along with amount, timestamp, etc. Bitcoin Block transaction: { type: BITCOIN sender: g4h4g5jnsm6bd4b8d0q6d8v4zx5k2la8d9n4c6f8, receiver: m1n2h3b4b5v6c7xz89sld8ff9d47d9dft47mkd, amount: 25 timestamp: dd-mm-yyyy hh:mm:sss } hash: ??????? previousHash: b4h84nv46sn29n4b5h6j7k8l2l3nx6b3n8l38xn40z54n2bv68c This transaction joins the other transactions that have been made on the bitcoin network. Bitcoin Miners The bitcoin blockchain is a public blockchain which means anyone can register themselves to be one of the Bitcoin Miner. Bitcoin miners are the nodes in bitcoin blockchain network. All the nodes in the network essentially having a copy of all transactions, and anytime new transactions are made they would be notified with the new block.\nAll the Bitcoin miners in the network start picking up those transactions and validate by looking through all of the transactions that happen in the Blockchain so far that those are also valid. Each Bitcoin miner try to solve the cryptographic hash puzzle, also known as proof of work algorithm. Now, this is the consensus algorithm that public and permissionless blockchains use and essentially this is how they\u0026rsquo;re able to reach a consensus on which block should be next. As soon as one of the bitcoin miner solves the puzzle, essentially means generated the hash of the next block, what they\u0026rsquo;ll do is broadcast that hash of the block to all of the other nodes in the network. All the nodes verify the authenticity of the hash and once all approve, it is added as a new block in all the nodes in the network. Bitcoin miner is also rewarded with bitcoins for their work on solving the puzzle. Solving the puzzle also referred as mining What if one of the bitcoin miner is hacker? Bitcoin miner is one of the node in bitcoin network, if bitcoin miner is a hacker and they manipulated their own copy of record then bitcoin protocol discourage the existence of such manipulated blockchain through a process called consensus. In the presence of multiple, differing copies of the blockchain, the consensus protocol will adopt the longest chain available.\nWhy cryptographic hash puzzle is time consuming? Each node try to solve hash puzzle to generate a unique hash from transaction details using SHA-256 algorithm, this might take less time. But some protocols like bitcoin protocol restrict hash to be generated with 32 leading zeros. Generating a hash with leading or trailing zeros is more complex. Similarly Generating a hash with 10 leading zeros is more computation intensive then 5 leading zeros.\nNow you will say that transaction details are fixed, it will generate hash based on these details. How we generate a hash with leading zeros? For this block is having a special field called nonce which is also included to generate hash. Each node do trial and error with keep changing the nonce value until they found the hash with specified leading zeros.\nSHA-256 ( nonce + transaction details ) = 00000h3bdju7vb3n9s73nf92n1b38d 2. Private Blockchain Blockchain can also be used for business like supply chain, banking and government sectors. They are mostly private blockchains, which are accessible to a select group of authorized users - such as your company, a group of banks, or government agencies. This is also known as Permissioned blockchain.\nPrivate blockchain also uses a consensus algorithm but they are not as complex as public blockchain due to the limited number of nodes.\nNodes in the permissioned blockchain are trusted, so they generally know each other. Besides, they don\u0026rsquo;t always just represent users but entire organizations. So, in this case, it\u0026rsquo;s very important that our privacy is one of the main tenants of a permissioned Blockchain.\n3. Supply Chain Let\u0026rsquo;s take an example, let\u0026rsquo;s say that there\u0026rsquo;s a retailer that purchases 25 items in $1000. Now the manufacturer receives the order from the retailer and says, \u0026ldquo;OK, let\u0026rsquo;s approve\u0026rdquo;. Let\u0026rsquo;s make sure we have enough in the warehouse and let\u0026rsquo;s work with the shipping company. So, they go to the shipping company and they say, \u0026ldquo;OK, let\u0026rsquo;s ship this 100 items\u0026rdquo; and it\u0026rsquo;s going to cost them $100 to make the shipment.\nBlock 1 transaction: { type: BUY company: Star Retailer Ltd., items: 25, total_cost: $1000, discount: $50 } hash: buystartcompanyltd25100050 previousHash: - Block 2 transaction: { type: SHIP company: Shipping Bird Co., items: 25, total_cost: $100, dispatch_date: DD MM YY, arrival_date: DD MM YY } hash: shipshippingbirdco25100ddmmyyddmmyy previousHash: buystartcompanyltd25100050 There are some privacy concerns here,\nManufactured should know all the information about retailer and shipment Retailer should not necessarily know about the shipping cost Shipment should not necessarily know about how many retailers spent Other retailers in the blockchain should not know about this retailer and shipment So in this case, All of the organizations are part of the Blockchain, only the retailer which is part of the transaction that took place should be able to see that information. So, privacy must be part of the Blockchain for the business being able to control who can see particular transaction details.\n4. Smart Contracts One of the most successful business applications of Blockchain technology is something called Smart Contracts. Essentially this is code that\u0026rsquo;s running on the Blockchain and whenever certain conditions are met, they are automatically executed.\nSo, in the previous example of supply chain, whenever retailer made that purchase order to the manufacturer for this amount of goods there\u0026rsquo;s probably a manufacturing agent that double-checks that order has all the necessary information in it, they then probably go to the shipping agency to make sure they can cover the shipment and if the warehouse has the correct amount of goods. They\u0026rsquo;ll then make a shipment order. Now imagine if we could automate that whole process.\nThat\u0026rsquo;s what you can do with Smart Contracts, essentially code that will make sure that all the necessary information is met, and it\u0026rsquo;ll automatically create the shipment record. If any of those conditions is not met it could automatically release a refund to the retailer. This speeds up the whole process.\nReal Estate Smart Contract can automate the process title deeds, facilitate transactions and even grant access to properties through smart keys. The technology could streamline property sale, saving buyers time and money without the need to any middleman or real estate agent.\n5. Cross Border Payments People working in other countries generally send money back home from remittance agencies which takes hours, sometime few days and also they pay a good amount as fee. Blockchain technology would eliminate those Intermediaries and cut short the transfer time as well as transfer fee.\nBlockchain innovators for Cross Border Payments\n6. Online Voting In many countries, voter has to be present at the polling booth to cast their vote. There were always concerns about the authenticity of manual voting and voting machines. Voting is carried out online also in many countries but they also have concerns about security and fraud. Blockchain can successfully eliminate these concerns as it will present a clear record of the votes that have been cast. The tamper-proof feature of a blockchain makes it difficult to hack a blockchain-enabled voting system. With blockchain, voters can cast their vote from the comfort of their home. Citizens residing outside country can also cast their vote. This may result in a significant rise in voter turnout.\nWest Virginia has implemented a secure mobile voting application using blockchain technology that allows voters to receive, vote, and return their ballots electronically, first kind of project in United States history in 2018.\nCompanies working on online blockchain voting solution - FollowMyVote, Voatz, Scytl, Clear Ballot, Polyas, Intelivote, SMARTMATIC, electionrunner\n7. Inter-bank Payments The Monetary Authority of Singapore (MAS) announced Project Ubin, which use Blockchain and Distributed Ledger Technology (DLT) for inter-bank payments partnered with a consortium of financial institutions like Bank of America Merrill Lynch, Credit Suisse, DBS Bank, HSBC, JP Morgan, Mitsubishi UFJ Financial Group, OCBC, SGX, Standard Chartered, and UOB\n8. Identity Management Identity of a person includes basic details (name, age, gender, nationality, address, contact, etc) and ID documents like birth certificate, driving license, voter card and unique identity card. These all details could be stored in distributed, secured, immutable blockchain records.\nBenefits Distributed identity management system can be used by many services at the same time across the globe. Financial Services like banks can use this to verify customer identity for KYC (Know your customer) compliance. Educational Services like schools, universities can use this to provide academic certificates to student. This solves the problem of verifying the authenticity of certificates and avoid frauds of fake certificate. Government Bodies can use this to store birth certificates, marriage certificates, land titles to avoid corruption. Summary I hope after reading this post, you have a basic understanding of blockchain technology and its practical usage. I will keep updating this post based on my findings. Thanks for reading.\nPlease read the next post if interested in some Blockchain coding - Blockchain implementation using JavaScript\n","permalink":"https://codingnconcepts.com/post/blockchain-explained/","tags":["Blockchain","Popular Posts"],"title":"Blockchain Explained"},{"categories":["Hugo"],"contents":"If you are using Hugo as a static site generator tool then you might have used hugo built-in shortcodes but here are some amazing hugo shortcodes which can be used in markdown files of your hugo website.\nShortcodes are simple snippets inside your content files calling built-in or custom templates.\nCreate Custom Shortcode To create a shortcode, place an HTML template in the layouts/shortcodes directory of your source organization. Consider the file name carefully since the shortcode name will mirror that of the file but without the .html extension. For example, layouts/shortcodes/myshortcode.html will be called with {{\u0026lt; myshortcode \u0026gt;}}\nHugo Shortcode Examples Current Year This is useful when you want to show current year in markdown.\nshortcode layouts/shortcodes/year.html\n{{ now.Format \u0026#34;2006\u0026#34; }} usage content/post/myblogpost.md\n--- # front-matter --- Top 10 things you should know in {{\u0026lt; year \u0026gt;}} result post/myblogpost.html\nTop 10 things you should know in 2025 Link to open in browser window By default when you create a link in hugo pages, it opens in same browser window. You can use this shortcode to open links in new browser window or tab.\nshortcode layouts/shortcodes/a_blank.html\n\u0026lt;a href=\u0026#34;{{ .Get \u0026#34;url\u0026#34; }}\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;{{ with .Get \u0026#34;title\u0026#34; }}{{.}}{{else}}{{.Get \u0026#34;url\u0026#34;}}{{end}}\u0026lt;/a\u0026gt; usage content/post/myblogpost.md\n--- # front-matter --- [Coding N Concepts](https://codingnconcepts.com/) opens in same browser window {{\u0026lt; a_blank title=\u0026#34;Coding N Concepts\u0026#34; url=\u0026#34;https://codingnconcepts.com/\u0026#34; \u0026gt;}} opens in new browser window. result post/myblogpost.html\nCoding N Concepts opens in same browser window\nCoding N Concepts opens in new browser window.\nAdd HTML elements in Code Block Sometime we need support to add some specific styling in our markdown file. Hugo doesn\u0026rsquo;t support adding raw HTML in markdown files by default but you can enable that by making unsafe property as true in config.toml file:\n[markup] [markup.goldmark.renderer] unsafe= true You can now use raw HTML elements in your markdown files but there is still a limitation that you can not use raw HTML in code blocks. That is where span hugo shortcode is useful. It creates a \u0026lt;/span\u0026gt; element with given style attributes.\nshortcode layouts/shortcodes/span.html\n\u0026lt;span {{ with .Get \u0026#34;style\u0026#34;}} style=\u0026#34;{{ . | safeCSS }}\u0026#34;{{ end }}\u0026gt;{{ .Get \u0026#34;text\u0026#34; }}\u0026lt;/span\u0026gt; usage content/post/myblogpost.md\n--- # front-matter --- ``` ▼ The trace first @ {{\u0026lt; span style=\u0026#34;color:red;\u0026#34; text=\u0026#34;test:1\u0026#34; \u0026gt;}} second @ {{\u0026lt; span style=\u0026#34;color:orange;font-style:italic;\u0026#34; text=\u0026#34;test:2\u0026#34; \u0026gt;}} third @ {{\u0026lt; span style=\u0026#34;color:green;background-color:yellow;\u0026#34; text=\u0026#34;test:3\u0026#34; \u0026gt;}} fourth @ {{\u0026lt; span style=\u0026#34;color:blue;border:1px solid blue;\u0026#34; text=\u0026#34;test:4\u0026#34; \u0026gt;}} (anonymous) @ {{\u0026lt; span style=\u0026#34;color:black;font-weight:bold;\u0026#34; text=\u0026#34;test:5\u0026#34; \u0026gt;}} ``` result post/myblogpost.html\n▼ The trace first @ test:1 second @ test:2 third @ test:3 fourth @ test:4 (anonymous) @ test:5 Highlighter This is useful when you want to highlight some of the content in markdown.\nshortcode layouts/shortcodes/highlight.html\n\u0026lt;mark\u0026gt;{{ with .Get 0 }}{{.}}{{end}}\u0026lt;/mark\u0026gt; usage content/post/myblogpost.md\n--- # front-matter --- This is the {{\u0026lt; highlight \u0026#34;most trending\u0026#34; \u0026gt;}} post of the decade. result post/myblogpost.html\nThis is the most trending post of the decade. Strike This is useful when you want to strike through some of the content in markdown.\nshortcode layouts/shortcodes/strike.html\n\u0026lt;strike\u0026gt;{{ with .Get 0 }}{{.}}{{end}}\u0026lt;/strike\u0026gt; usage content/post/myblogpost.md\n--- # front-matter --- This is {{\u0026lt; strike \u0026#34;not trending\u0026#34; \u0026gt;}} trending post of the decade. result post/myblogpost.html\nThis is the not trending trending post of the decade. Code Block Output This is useful when you want to show the output of certain code. It gives a beautiful title \u0026ldquo;Output\u0026rdquo;.\nshortcode layouts/shortcodes/code_output.html\n\u0026lt;div class=\u0026#34;code_output\u0026#34;\u0026gt;Output\u0026lt;/div\u0026gt; usage content/post/myblogpost.md\n--- # front-matter --- ``` {{\u0026lt; code_output \u0026gt;}} 1 2 3 ``` result post/myblogpost.html\nOutput 1 2 3 Escape Hugo Shortcode Sometime you might need to escape (i.e., prevent from executing) a shortcode in a Hugo markdown (.md) file, and display the shortcode as it is {{\u0026lt; myshortcode \u0026gt;}}\nA simple method that works at the time of writing, is adding /* after the opening double curly braces and the angle bracket or percent sign (i.e., {{\u0026lt; or {{%/*) and adding */ after the closing angle bracket or percent sign and double curly braces (i.e., \u0026gt;}} or */%}}). For example, this in markdown (.md) file\n{ {\u0026lt;/* myshortcode */\u0026gt;}} will be seen as this in generated HTML\n{{\u0026lt; myshortcode \u0026gt;}} ","permalink":"https://codingnconcepts.com/hugo/custom-shortcode-hugo/","tags":null,"title":"Useful Hugo Shortcodes"},{"categories":["Hugo"],"contents":"When you add structure data to your website, it helps google, bing or other website indexers to understand the content for your website and pages which leads to better search results for your website and eventually helps in SEO ranking.\nMark up schema is based on website type and its content. Suppose you have a blog website, then schema of your web pages will be of type \u0026ldquo;Article\u0026rdquo; or \u0026ldquo;BlogPosting\u0026rdquo;. You can go through the google\u0026rsquo;s example of adding structure data using JSON-LD for \u0026ldquo;Article\u0026rdquo; type of website pages.\nSchema of structure data for all type of websites and web pages can be found in documentation of schema.org\nMicrodata and JSON-LD are two ways to add mark up in order to structure your data. Google recommends using JSON-LD as it is more cleaner approach to express the structure of the data in JSON format embedded as a \u0026lt;script\u0026gt; in the \u0026lt;head\u0026gt; tag of your HTML. Its a clear separation of structure from HTML elements as compare to Microdata where you add special properties like itemprop, itemtype in HTML element tags everywhere.\nNow we know that JSON-LD is way to go. There are the steps to add structure data using JSON-LD in hugo website:\nCreate site_schema.html Partial First of all we are going to create a partial called site_schema.html which is responsible to generate structure data JSON-LD for hugo website and hugo blog pages. All you need to do is copy the below code snippet and save it under $hugo/layouts/partials/site_schema.html.\n{{ if .IsHome -}} \u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt; { \u0026#34;@context\u0026#34;: \u0026#34;http://schema.org\u0026#34;, \u0026#34;@type\u0026#34;: \u0026#34;WebSite\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;{{ .Site.Title }}\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;{{ .Site.BaseURL }}\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;{{ .Site.Params.description }}\u0026#34;, \u0026#34;thumbnailUrl\u0026#34;: \u0026#34;{{ .Site.Params.Logo | absURL }}\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;{{ .Site.Params.Copyright }}\u0026#34; } \u0026lt;/script\u0026gt; {{ else if .IsPage }} {{ $author := or (.Params.author) (.Site.Author.name) }} {{ $org_name := .Site.Title }} \u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt; { \u0026#34;@context\u0026#34;: \u0026#34;http://schema.org\u0026#34;, \u0026#34;@type\u0026#34;: \u0026#34;BlogPosting\u0026#34;, \u0026#34;articleSection\u0026#34;: \u0026#34;{{ .Section }}\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;{{ .Title | safeJS }}\u0026#34;, \u0026#34;headline\u0026#34;: \u0026#34;{{ .Title | safeJS }}\u0026#34;, \u0026#34;alternativeHeadline\u0026#34;: \u0026#34;{{ .Params.lead }}\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;{{ if .Description }}{{ .Description | safeJS }}{{ else }}{{if .IsPage}}{{ .Summary }}{{ end }}{{ end }}\u0026#34;, \u0026#34;inLanguage\u0026#34;: {{ .Site.LanguageCode | default \u0026#34;en-us\u0026#34; }}, \u0026#34;isFamilyFriendly\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;mainEntityOfPage\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;WebPage\u0026#34;, \u0026#34;@id\u0026#34;: \u0026#34;{{ .Permalink }}\u0026#34; }, \u0026#34;author\u0026#34; : { \u0026#34;@type\u0026#34;: \u0026#34;Person\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;{{ $author }}\u0026#34; }, \u0026#34;creator\u0026#34; : { \u0026#34;@type\u0026#34;: \u0026#34;Person\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;{{ $author }}\u0026#34; }, \u0026#34;accountablePerson\u0026#34; : { \u0026#34;@type\u0026#34;: \u0026#34;Person\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;{{ $author }}\u0026#34; }, \u0026#34;copyrightHolder\u0026#34; : \u0026#34;{{ $org_name }}\u0026#34;, \u0026#34;copyrightYear\u0026#34; : \u0026#34;{{ .Date.Format \u0026#34;2006\u0026#34; }}\u0026#34;, \u0026#34;dateCreated\u0026#34;: \u0026#34;{{ .Date.Format \u0026#34;2006-01-02T15:04:05.00Z\u0026#34; | safeHTML }}\u0026#34;, \u0026#34;datePublished\u0026#34;: \u0026#34;{{ .PublishDate.Format \u0026#34;2006-01-02T15:04:05.00Z\u0026#34; | safeHTML }}\u0026#34;, \u0026#34;dateModified\u0026#34;: \u0026#34;{{ .Lastmod.Format \u0026#34;2006-01-02T15:04:05.00Z\u0026#34; | safeHTML }}\u0026#34;, \u0026#34;publisher\u0026#34;:{ \u0026#34;@type\u0026#34;:\u0026#34;Organization\u0026#34;, \u0026#34;name\u0026#34;: {{ $org_name }}, \u0026#34;url\u0026#34;: {{ .Site.BaseURL }}, \u0026#34;logo\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;ImageObject\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;{{ .Site.Params.logo | absURL }}\u0026#34;, \u0026#34;width\u0026#34;:\u0026#34;32\u0026#34;, \u0026#34;height\u0026#34;:\u0026#34;32\u0026#34; } }, \u0026#34;image\u0026#34;: {{ if .Params.images }}[{{ range $i, $e := .Params.images }}{{ if $i }}, {{ end }}{{ $e | absURL }}{{ end }}]{{ else}}{{.Site.Params.logo | absURL }}{{ end }}, \u0026#34;url\u0026#34; : \u0026#34;{{ .Permalink }}\u0026#34;, \u0026#34;wordCount\u0026#34; : \u0026#34;{{ .WordCount }}\u0026#34;, \u0026#34;genre\u0026#34; : [ {{ range $index, $tag := .Params.tags }}{{ if $index }}, {{ end }}\u0026#34;{{ $tag }}\u0026#34; {{ end }}], \u0026#34;keywords\u0026#34; : [ {{ range $index, $keyword := .Params.keywords }}{{ if $index }}, {{ end }}\u0026#34;{{ $keyword }}\u0026#34; {{ end }}] } \u0026lt;/script\u0026gt; {{ end }} Note: I am using hugo as a static site generator for this blog website and after spending so much time here and there, i came up with this partial. You can do the changes in this partial as per your need.\nThe data to generate structure data json using this partial comes from the front-matter of the blog post and config.toml file. I have tried to feed in most of the data from front-matter and fallback as config.toml so that most of the blog post pages have their specific data.\nRelevant front-matter These are the relevant front-matter properties which are being used in generating JSON-LD schema for your blog page:-\nblog-page.md --- title: I am the name and headline of blog post structure data lead: I am alternativeHeadline of blog post structure data description: I am description of blog post structure data author: I am author, creator and accountablePerson of blog post structure data. I come from config.toml if not specified here date: I am copyrightYear, dateCreated and datePublished of blog post structure data lastmod: I am dateModified of blog post structure data. I am same as \u0026#34;date\u0026#34; if not specified. images: - \u0026#34;/img/image-1.png\u0026#34; - \u0026#34;/img/image-2.png\u0026#34; tags: - \u0026#34;genre 1 of blog post structure data\u0026#34; - \u0026#34;genre 2 of blog post structure data\u0026#34; keywords: - \u0026#34;keyword 1 of blog post structure data\u0026#34; - \u0026#34;keyword 2 of blog post structure data \u0026#34; --- Relevant config.toml These are the relevant config.toml configuration properties which are being used in generating JSON-LD schema for your website:-\nconfig.toml baseURL = \u0026#34;I am url of website structure data\u0026#34; languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;I am name of website structure data\u0026#34; [Author] name = \u0026#34;I am fallback author of blog post structure data\u0026#34; [Params] description = \u0026#34;I am description of website structure data\u0026#34; logo = \u0026#34;favicon-32x32.png\u0026#34; copyright = \u0026#34;I am license of website structure data\u0026#34; Add partial to the \u0026lt;head\u0026gt; tag of baseof.html Once you created a partial, you need to add this partial in the \u0026lt;head\u0026gt; tag of html page under _default folder which generates your hugo website and blog HTML pages.\nIf you are using any theme in hugo website then this HTML page could be $hugo/themes/layouts/_default/baseof.html. If you have designed hugo website on your own then this HTML page could be $hugo/layouts/_default/baseof.html depending upon your naming convention. Just add the partial in HTML head tag like below:\nbaseof.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; ... ... {{ partial \u0026#34;site_schema.html\u0026#34; . }} \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Generated Structure Data JSON-LD The following structure data are generated in JSON-LD format.\nWebsite Main Page \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; ... ... ... \u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt; { \u0026#34;@context\u0026#34;: \u0026#34;http://schema.org\u0026#34;, \u0026#34;@type\u0026#34;: \u0026#34;WebSite\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Coding N Concepts\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://codingnconcepts.com/\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Coding N Concepts is a technical blog for developers by developers to help you with coding problems, puzzle solving, interview preparation, learning concepts in simplified way.\u0026#34;, \u0026#34;thumbnailUrl\u0026#34;: \u0026#34;https://codingnconcepts.com/favicon-32x32.png\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;Ashish Lahoti\u0026#34; } \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Blog Post Page \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; ... ... ... \u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt; { \u0026#34;@context\u0026#34;: \u0026#34;http://schema.org\u0026#34;, \u0026#34;@type\u0026#34;: \u0026#34;BlogPosting\u0026#34;, \u0026#34;articleSection\u0026#34;: \u0026#34;hugo\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Structure Data Using JSON-LD for Hugo\u0026#34;, \u0026#34;headline\u0026#34;: \u0026#34;Structure Data Using JSON-LD for Hugo\u0026#34;, \u0026#34;alternativeHeadline\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Structure Data Using JSON-LD for Hugo\u0026#34;, \u0026#34;inLanguage\u0026#34;: \u0026#34;en-us\u0026#34;, \u0026#34;isFamilyFriendly\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;mainEntityOfPage\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;WebPage\u0026#34;, \u0026#34;@id\u0026#34;: \u0026#34;https://codingnconcepts/hugo/structure-data-json-ld\u0026#34; }, \u0026#34;author\u0026#34; : { \u0026#34;@type\u0026#34;: \u0026#34;Person\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Ashish Lahoti\u0026#34; }, \u0026#34;creator\u0026#34; : { \u0026#34;@type\u0026#34;: \u0026#34;Person\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Ashish Lahoti\u0026#34; }, \u0026#34;accountablePerson\u0026#34; : { \u0026#34;@type\u0026#34;: \u0026#34;Person\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Ashish Lahoti\u0026#34; }, \u0026#34;copyrightHolder\u0026#34; : \u0026#34;Coding N Concepts\u0026#34;, \u0026#34;copyrightYear\u0026#34; : \u0026#34;2020\u0026#34;, \u0026#34;dateCreated\u0026#34;: \u0026#34;2020-05-04T00:00:00.00Z\u0026#34;, \u0026#34;datePublished\u0026#34;: \u0026#34;2020-05-04T00:00:00.00Z\u0026#34;, \u0026#34;dateModified\u0026#34;: \u0026#34;2020-05-04T00:00:00.00Z\u0026#34;, \u0026#34;publisher\u0026#34;:{ \u0026#34;@type\u0026#34;:\u0026#34;Organization\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Coding N Concepts\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;http://localhost:1313/\u0026#34;, \u0026#34;logo\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;ImageObject\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://codingnconcepts/favicon-32x32.png\u0026#34;, \u0026#34;width\u0026#34;:\u0026#34;32\u0026#34;, \u0026#34;height\u0026#34;:\u0026#34;32\u0026#34; } }, \u0026#34;image\u0026#34;: \u0026#34;https://codingnconcepts/favicon-32x32.png\u0026#34;, \u0026#34;url\u0026#34; : \u0026#34;https://codingnconcepts/hugo/structure-data-json-ld\u0026#34;, \u0026#34;wordCount\u0026#34; : \u0026#34;857\u0026#34;, \u0026#34;genre\u0026#34; : [ \u0026#34;Hugo\u0026#34; , \u0026#34;SEO\u0026#34; ], \u0026#34;keywords\u0026#34; : [ \u0026#34;Schema markup using JSON-LD in hugo website\u0026#34;, \u0026#34;SEO optimization for hugo\u0026#34; ] } \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Verify Structure Data JSON-LD You can verify the structure data using Structured Data Testing Tool provided by Google where either you give a URL of your page or just copy paste whole HTML to verify. Look for the errors and correct them.\n","permalink":"https://codingnconcepts.com/hugo/structure-data-json-ld-hugo/","tags":null,"title":"Add Structure Data JSON-LD in Hugo Website Pages"},{"categories":["Java"],"contents":"The double colon :: operator is introduced in Java 8 for method reference. It is a shorthand syntax for lambda expression that executes one method. You can write more compact and readable code using double colon operator as compare to anonymous classes and lambda expression. We are going to understand its usage with lots of examples.\n1. Overview Prerequisite You should have a basic understanding of functional interface, lambda expressions and streams to understand these examples, if not then don\u0026rsquo;t worry, you will learn those concepts as well along the way.\nDouble colon refers to a method Double colon :: is basically refers to a single method, and this single method can be a\nA Static method ClassName::staticMethodName\ne.g. Integer.parseInteger, Double.parseDouble An Instance method Object::instanceMethodName\ne.g. System.out::println, String::toUpperCase A Constructor ClassName::new A Super method super::parentMethodName\nDouble colon returns a functional interface Double colon :: always return a functional interface. There are two ways to use this returned functional interface -\nUse this to initialize a functional interface and later execute a function.\nHere we either use java built-in functional interface like Function, Supplier, Consumer, BiFunction or we create our custom functional interface using @FunctionalInterface annotation. Use this to replace lambda expression in streams.\nDon\u0026rsquo;t worry if it\u0026rsquo;s looking too complicated. Let\u0026rsquo;s deep dive into examples to understand them better.\n2. Static Method Syntax // Lambda expression (args) -\u0026gt; ClassName.staticMethodName(args) // Method Reference ClassName::staticMethodName 2.1 Calculator Let\u0026rsquo;s create two static methods square and multiply in Calculator class.\nclass Calculator { public static double square(double num){ return Math.pow(num, 2); } public static double multiply(double num1, double num2) { return num1*num2; } } 2.1.1 Initialize Functional Interface Let’s initialize java built-in Function, BiFunction functional interface and apply it to find square and multiply respectively.\nFunction interface accepts one argument and returns one argument. Here, it takes one Double arg and return its square.\nBiFunction interface accepts two arguments and returns one argument. Here, it takes two Double args and return its multiplication.\n// Initialize functional interface Function\u0026lt;Double, Double\u0026gt; square = Calculator::square; BiFunction\u0026lt;Double, Double, Double\u0026gt; multiply = Calculator::multiply; // Execute function square.apply(3.0); multiply.apply(2.5, 5.0) Output 9.0 12.5 2.1.2 Replace Lambda Expression We generate square of list of Integer by replacing lambda expression with method reference.\nList\u0026lt;Integer\u0026gt; numbers = Arrays.asList(1,2,3); // Lambda expression numbers.stream().map(number -\u0026gt; Calculator.square(number)).forEach(number -\u0026gt; System.out.println(number)); // Method Reference numbers.stream().map(Calculator::square).forEach(System.out::println); Output 1.0 4.0 9.0 2.2 Integer::parseInt We know that parseInt is a static method in Integer class.\n2.2.1 Initialize Functional Interface Let\u0026rsquo;s initialize java built-in Function functional interface and apply it to parse String to Integer.\nFunction interface takes one argument and returns one argument. Here in example, it takes a String argument and returns Integer.\n// Initialize functional interface Function\u0026lt;String, Integer\u0026gt; parseInt = Integer::parseInt; // Execute function parseInt.apply(\u0026#34;2019\u0026#34;); Output 2019 2.2.2 Replace Lambda Expression Let\u0026rsquo;s parse a list of String to Integer using its static parseInt method. We see that how we can replace lambda expression with method reference resulting in same output.\nList\u0026lt;String\u0026gt; years = Arrays.asList(\u0026#34;2019\u0026#34;, \u0026#34;2020\u0026#34;, \u0026#34;2021\u0026#34;); // Lambda Expression years.stream().map(year -\u0026gt; Integer.parseInt(year)).forEach(year -\u0026gt; System.out.println(year)); // Method Reference years.stream().map(Integer::parseInt).forEach(System.out::println); Output 2019 2020 2021 3. Instance Method Syntax // Lambda expression (args) -\u0026gt; object.instanceMethodName(args) // Method Reference object::instanceMethodName 3.1 Calculator Let\u0026rsquo;s create two instance methods square and multiply in Calculator class.\nclass Calculator { public double square(double num){ return Math.pow(num, 2); } public double multiply(double num1, double num2) { return num1*num2; } } 3.1.1 Initialize Functional Interface Let’s initialize java built-in Function, BiFunction functional interface and apply it to find square and multiply respectively.\n// Initialize functional interface Function\u0026lt;Double, Double\u0026gt; square = new Calculator()::square; BiFunction\u0026lt;Double, Double, Double\u0026gt; multiply = new Calculator()::multiply; // Execute function square.apply(3.0); multiply.apply(2.5, 5.0) Output 9.0 12.5 3.1.2 Replace Lambda Expression We generate square of list of Integer by replacing lambda expression with method reference.\nList\u0026lt;Integer\u0026gt; numbers = Arrays.asList(1,2,3); // Lambda expression numbers.stream().map(number -\u0026gt; new Calculator().square(number)).forEach(number -\u0026gt; System.out.println(number)); // Method Reference numbers.stream().map(new Calculator()::square).forEach(System.out::println); Output 1.0 4.0 9.0 3.2 System.out::println 3.2.1 Initialize Functional Interface Let\u0026rsquo;s initialize java built-in Supplier functional interface to print a String.\nSupplier interface accepts one argument and returns nothing. Here in example, it accepts a String argument and print it.\n// Initialize functional interface Consumer\u0026lt;String\u0026gt; println = System.out::println; // Execute function println.accept(\u0026#34;Learning Method Reference a.k.a Colon Operator ::\u0026#34;); Output Learning Method Reference a.k.a Colon Operator :: 3.2.2 Replace Anonymous Class and Lambda Expression We will print a list of String and see how double colon :: method reference makes the code more concise and readable as compare to anonymous class and lambda expression.\nList\u0026lt;String\u0026gt; languages = Arrays.asList(\u0026#34;java\u0026#34;, \u0026#34;javascript\u0026#34;, \u0026#34;css\u0026#34;); // Anonymous Class languages.forEach(new Consumer\u0026lt;String\u0026gt;() { @Override public void accept(String str) { System.out.println(str); } }); // Lambda expression languages.forEach(str -\u0026gt; System.out.println(str)); // Method Reference languages.forEach(System.out::println); Output java javascript css 3.2.3 Print a list of Integer Let\u0026rsquo;s print a list of integer using lambda expression and double colon :: method reference.\nList\u0026lt;Integer\u0026gt; numbers = Arrays.asList(1,2,3); // Lambda expression numbers.forEach(number-\u0026gt;System.out.println(number)); // Method Reference numbers.forEach(System.out::println); Output 1 2 3 3.3 String::toUpperCase 3.3.1 Initialize Functional Interface Let\u0026rsquo;s initialize java built-in Function functional interface and execute it to change String to UPPERCASE.\n// Initialize functional interface Function\u0026lt;String, String\u0026gt; toUpperCase = String::toUpperCase; // Execute function toUpperCase.apply(\u0026#34;java\u0026#34;); Output JAVA 3.3.2 Replace Lambda Expression Let\u0026rsquo;s look at the example where we use multiple method references to print the String in uppercase.\nList\u0026lt;String\u0026gt; languages = Arrays.asList(\u0026#34;java\u0026#34;, \u0026#34;javascript\u0026#34;, \u0026#34;css\u0026#34;); // Lambda expression languages.stream().map(str -\u0026gt; str.toUpperCase()).forEach(str -\u0026gt; System.out.println(str)); // Method Reference languages.stream().map(String::toUpperCase).forEach(System.out::println); Output JAVA JAVASCRIPT CSS 4. Constructor Double colon :: operator can be used to create an instance by calling constructor.\nSyntax ClassName::new 4.1 Create an int[10] array // Default int[] array1 = new int[10]; // Method Reference IntFunction\u0026lt;int[]\u0026gt; arrayMaker = int[]::new; int[] array2 = arrayMaker.apply(10); 4.2 Create HashMap // Default Map map1 = new HashMap(); // Method Reference Supplier\u0026lt;Map\u0026gt; mapMaker = HashMap::new; Map map2 = mapMaker.get(); 5. Super Method Syntax super::parentMethodName We create square and multiply instance methods in Calculator class and then call those methods in our inherited AdvanceCalculator class using super::parentMethodName to create advance methods squareAndAdd and squareAndMultiply\nclass Calculator { public double square(double num){ return Math.pow(num, 2); } public double multiply(double num1, double num2) { return num1*num2; } } class AdvanceCalculator extends Calculator { public double squareAndAdd(double num1, double num2) { Function\u0026lt;Double, Double\u0026gt; square = super::square; return square.apply(num1) + square.apply(num2); } public double squareAndMultiply(double num1, double num2) { Function\u0026lt;Double, Double\u0026gt; square = super::square; BiFunction\u0026lt;Double, Double, Double\u0026gt; multiply = super::multiply; return multiply.apply(square.apply(num1), square.apply(num2)); } } 6. Real World Practical example Let\u0026rsquo;s create a Class Tutorial with properties name, duration and rating.\nclass Tutorial { private String name; private Integer duration; private Double rating; public Tutorial(String name) { this.name = name; this.duration = 0; this.rating = 0.0; } public Tutorial(String name, Integer duration) { this.name = name; this.duration = duration; this.rating = 0.0; } public Tutorial(String name, Integer duration, Double rating) { this.name = name; this.duration = duration; this.rating = rating; } public String getName() { return name; }\tpublic void setName(String name) { this.name = name; } public Integer getDuration() { return duration; } public void setDuration(Integer duration) { this.duration = duration; } public Double getRating() { return rating; } public void setRating(Double rating) { this.rating = rating; } @Override public String toString() { return \u0026#34;Tutorial[ \u0026#34; + name + \u0026#34;\\t- \u0026#34; + duration + \u0026#34;min, rating=\u0026#34; + rating + \u0026#34; ]\u0026#34;; } public static int compareByRating(Tutorial t1, Tutorial t2) { return t1.getRating().compareTo(t2.getRating()); } public static int compareByDuration(Tutorial t1, Tutorial t2) { return t1.getDuration().compareTo(t2.getDuration()); } } Let\u0026rsquo;s create a list of tutorials objects:-\n// List of tutorials List\u0026lt;Tutorial\u0026gt; tutorials = Arrays.asList(new Tutorial[] { new Tutorial(\u0026#34;Streams in Java 8\u0026#34;, 30, 4.2), new Tutorial(\u0026#34;What\u0026#39;s new in Java 11\u0026#34;, 25, 4.8), new Tutorial(\u0026#34;Core Java Concepts\u0026#34;, 45, 3.5)}); 6.1 Print the list of tutorials // Lambda Expression tutorials.forEach(tutorial -\u0026gt; System.out.println(tutorial)); // Method Reference tutorials.forEach(System.out::println); Output Tutorial[ Streams in Java 8\t- 30min, rating=4.2 ] Tutorial[ What\u0026#39;s new in Java 11\t- 25min, rating=4.8 ] Tutorial[ Core Java Concepts\t- 45min, rating=3.5 ] 6.2 Get list of tutorial names in uppercase // Lambda Expression tutorials.stream().map(tutorial -\u0026gt; tutorial.getName()).map(name -\u0026gt; name.toUpperCase()).forEach(s -\u0026gt; System.out.println(s)); // Method Reference tutorials.stream().map(Tutorial::getName).map(String::toUpperCase).forEach(System.out::println); Output STREAMS IN JAVA 8 WHAT\u0026#39;S NEW IN JAVA 11 CORE JAVA CONCEPTS 6.3 Sort Tutorials by Rating // Lambda Expression tutorials.stream().sorted((tutorial1, tutorial2) -\u0026gt; Tutorial.compareByRating(tutorial1, tutorial2)).forEach(tutorial -\u0026gt; System.out.println(tutorial)); // Method Reference tutorials.stream().sorted(Tutorial::compareByRating).forEach(System.out::println);\tOutput Tutorial[ Core Java Concepts\t- 45min, rating=3.5 ] Tutorial[ Streams in Java 8\t- 30min, rating=4.2 ] Tutorial[ What\u0026#39;s new in Java 11\t- 25min, rating=4.8 ] 6.4 Sort Tutorials by Duration // Lambda Expression tutorials.stream().sorted((tutorial1, tutorial2) -\u0026gt; Tutorial.compareByDuration(tutorial1, tutorial2)).forEach(tutorial -\u0026gt; System.out.println(tutorial)); // Method Reference tutorials.stream().sorted(Tutorial::compareByDuration).forEach(System.out::println);\tOutput Tutorial[ What\u0026#39;s new in Java 11\t- 25min, rating=4.8 ] Tutorial[ Streams in Java 8\t- 30min, rating=4.2 ] Tutorial[ Core Java Concepts\t- 45min, rating=3.5 ] 6.5 Create New Instance of Tutorial Each constructor method reference returns a Functional Interface. One-arg constructor returns Function and two-arg constructor returns BiFunction functional interface comes default in Java 8 so we are creating TriFunction functional interface on our own to call three-arg constructor.\n@FunctionalInterface interface TriFunction\u0026lt;A, B, C, R\u0026gt; { R apply(A a, B b, C c); default \u0026lt;V\u0026gt; TriFunction\u0026lt;A, B, C, V\u0026gt; andThen( Function\u0026lt;? super R, ? extends V\u0026gt; after) { Objects.requireNonNull(after); return (A a, B b, C c) -\u0026gt; after.apply(apply(a, b, c)); } } // Create an instance from one arg constructor Function\u0026lt;String, Tutorial\u0026gt; tutorial1 = Tutorial::new; Tutorial t1 = tutorial1.apply(\u0026#34;Tutorial 1\u0026#34;); // Create an instance from two arg constructor BiFunction\u0026lt;String, Integer, Tutorial\u0026gt; tutorial2 = Tutorial::new; Tutorial t2 = tutorial2.apply(\u0026#34;Tutorial 2\u0026#34;, 25); // Create an instance from three arg constructor TriFunction\u0026lt;String, Integer, Double, Tutorial\u0026gt; tutorial3 = Tutorial::new; Tutorial t3 = tutorial3.apply(\u0026#34;Tutorial 3\u0026#34;, 30, 4.9); Arrays.asList(t1, t2, t3).forEach(System.out::println); Output Tutorial[ Tutorial 1\t- 0min, rating=0.0 ] Tutorial[ Tutorial 2\t- 25min, rating=0.0 ] Tutorial[ Tutorial 3\t- 30min, rating=4.9 ] 7. Conclusion In this article, we saw how to use double colon operator introduced in Java 8. It is very useful to keep your code concise and readable specially in streams where you can replace lambda expressions with method reference using double colon operator.\nModern IDEs such as Eclipse (Quick Fix Feature) and IntelliJ IDEA (Intention Feature) provide built in support to convert lambda expression to an equivalent method reference.\n","permalink":"https://codingnconcepts.com/java/double-colon-operator-in-java-8/","tags":["Java Streams"],"title":"Double Colon (::) Operator in Java 8"},{"categories":["Spring Boot"],"contents":"In this quick tutorial, we\u0026rsquo;ll configure embedded Jetty server by replacing it with default Tomcat server in Spring Boot web application.\nAdd Jetty Dependency We need to do two things here:-\nExclude default dependency spring-boot-starter-tomcat added in spring-boot-start-web Add spring-boot-starter-jetty dependency. pom.xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-tomcat\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jetty\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; build.gradle dependencies { implementation(\u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39;) { exclude group: \u0026#39;org.springframework.boot\u0026#39;, module:\u0026#39;spring-boot-starter-tomcat\u0026#39; } implementation \u0026#39;org.springframework.boot:spring-boot-starter-jetty\u0026#39; } That\u0026rsquo;s it. You have replaced tomcat with jetty server.\nWhen you build spring boot maven project, it smartly replace tomcat jars with jetty dependency jars from maven dependency as follows:\n▼ Maven Dependencies ► tomcat-embed-core-x.y.z.jar ► tomcat-embed-el-x.y.z.jar ► tomcat-embed-websocket-x.y.z.jar ► jetty.servlets-x.y.z.jar ► jetty.continuation-x.y.z.jar ► jetty-http-x.y.z.jar ► jetty-webapp-x.y.z.jar ► jetty-servlet-x.y.z.jar ► jetty-security-x.y.z.jar ► jetty-server-x.y.z.jar Application Startup Logs When you start spring boot application, You will in the logs that jetty is serving your web application now:-\nINFO c.e.demo.SpringBootDemoApplication : Starting SpringBootDemoApplication using Java 11.0.10 on Ashishs-MBP with PID 4221 (/Users/ashl/IdeaProjects/springboot-examples/springboot-config/build/classes/java/main started by ashl in /Users/ashl/IdeaProjects/springboot-examples/springboot-config) DEBUG c.e.demo.SpringBootDemoApplication : Running with Spring Boot v2.5.0, Spring v5.3.7 INFO c.e.demo.SpringBootDemoApplication : No active profile set, falling back to default profiles: default INFO org.eclipse.jetty.util.log : Logging initialized @1343ms to org.eclipse.jetty.util.log.Slf4jLog INFO o.s.b.w.e.j.JettyServletWebServerFactory : Server initialized with port: 8080 INFO org.eclipse.jetty.server.Server : jetty-9.4.41.v20210516; built: 2021-05-16T23:56:28.993Z; git: 98607f93c7833e7dc59489b13f3cb0a114fb9f4c; jvm 11.0.10+8-LTS-162 INFO o.e.j.s.h.ContextHandler.application : Initializing Spring embedded WebApplicationContext INFO w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 708 ms INFO o.e.jetty.server.handler.ContextHandler : Started o.s.b.w.e.j.JettyEmbeddedWebAppContext@49469ffa{application,/,[file:///private/var/folders/6p/3ztts6bd4xbdl7nkcq6lpcg80000gn/T/jetty-docbase.8080.2111859430883013110/, jar:file:/Users/ashl/.gradle/caches/modules-2/files-2.1/org.webjars/swagger-ui/3.49.0/e126393ccbab7950fb618b7662fb37f71cb20841/swagger-ui-3.49.0.jar!/META-INF/resources],AVAILABLE} INFO org.eclipse.jetty.server.Server : Started @1597ms INFO o.e.j.s.h.ContextHandler.application : Initializing Spring DispatcherServlet \u0026#39;dispatcherServlet\u0026#39; INFO o.s.web.servlet.DispatcherServlet : Initializing Servlet \u0026#39;dispatcherServlet\u0026#39; INFO o.s.web.servlet.DispatcherServlet : Completed initialization in 1 ms INFO o.e.jetty.server.AbstractConnector : Started ServerConnector@712cfb63{HTTP/1.1, (http/1.1)}{0.0.0.0:8080} INFO o.s.b.web.embedded.jetty.JettyWebServer : Jetty started on port(s) 8080 (http/1.1) with context path \u0026#39;/\u0026#39; INFO c.e.demo.SpringBootDemoApplication : Started SpringBootDemoApplication in 1.813 seconds (JVM running for 2.131) Add Spring Boot Jetty Configuration Spring boot also provides Jetty server specific configuration which you can configure from application.yml or application.properties file.\napplication.yml server: jetty: connection-idle-timeout: # Time that the connection can be idle before it is closed. max-http-form-post-size: # Maximum size of the form content in any HTTP post request e.g. 200000B accesslog: enabled: # Enable access log e.g. true append: # Enable append to log e.g. true custom-format: # Custom log format file-date-format: # Date format to place in log file name filename: # Log file name, if not specified, logs redirect to \u0026#34;System.err\u0026#34; format: # Log format e.g ncsa ignore-paths: # Request paths that should not be logged retention-period: # Number of days before rotated log files are deleted e.g. 31 threads: acceptors: # Number of acceptor threads to use. When the value is -1, the default, the number of acceptors is derived from the operating environment. selectors: # Number of selector threads to use. When the value is -1, the default, the number of selectors is derived from the operating environment. min: # Minimum number of threads e.g. 8 max: # Maximum number of threads e.g. 200 max-queue-capacity: # Maximum capacity of the thread pool\u0026#39;s backing queue. A default is computed based on the threading configuration. idle-timeout: # Maximum thread idle time in millisecond e.g. 60000ms You can refer to the Spring Boot official documentation for full list of configuration.\n","permalink":"https://codingnconcepts.com/spring-boot/configure-embedded-jetty-server/","tags":["Spring Boot Basics","Jetty"],"title":"Configure embedded Jetty server in Spring Boot"},{"categories":["Spring Boot"],"contents":"When we create RESTFul APIs in spring boot application in microservices environment. It becomes essential to log incoming API request and response and push it to centralized logging system such as Splunk or ELK for debugging. Also all the logs related to one request should have some common id to relate them. In this post we are going to solve these problems.\n1. Create RESTFul API Let\u0026rsquo;s create a RestController to expose RESTFul API\n@RestController @RequestMapping(\u0026#34;/posts\u0026#34;) public class PostController { @GetMapping public List\u0026lt;Post\u0026gt; getAllPosts() { return Arrays.asList(new Post[] { new Post[] { new Post(\u0026#34;spring\u0026#34;, \u0026#34;Spring Boot\u0026#34;, \u0026#34;All about Spring boot microservice\u0026#34;), new Post(\u0026#34;java\u0026#34;, \u0026#34;Java\u0026#34;, \u0026#34;Learn Streams in Java\u0026#34;), new Post(\u0026#34;javascript\u0026#34;, \u0026#34;JavaScript\u0026#34;, \u0026#34;Whats new in ES6\u0026#34;)\t}); } } 2. Create API Logger At this point we have exposed API but we haven\u0026rsquo;t logged it yet. Let\u0026rsquo;s create an API logger which is having following configurable properties:-\napp.api.logging.enable If true then api logger will be enabled and log all api request and response app.api.logging.url-patterns If provided in a comma separated url patterns, only those api request and response will be logged. default value is \u0026rsquo;*\u0026rsquo; means all api request and response will be printed app.api.logging.requestIdParamName If provided request parameter in incoming api request will be logged across all logs to serve that request. default value is requestId. If not provided, new request id (uuid) will be generated to log across all logs. import org.springframework.beans.factory.annotation.Value; import org.springframework.boot.autoconfigure.condition.ConditionalOnExpression; import org.springframework.boot.web.servlet.FilterRegistrationBean; import org.springframework.context.annotation.Bean; import org.springframework.stereotype.Component; @Component @ConditionalOnExpression(\u0026#34;${app.api.logging.enable:true}\u0026#34;) public class ApiLoggingFilterConfig { @Value(\u0026#34;${app.api.logging.url-patterns:*}\u0026#34;) private String[] urlPatterns; @Value(\u0026#34;${app.api.logging.requestIdParamName:requestId}\u0026#34;) private String requestIdParamName; @Bean public FilterRegistrationBean\u0026lt;ApiLoggingFilter\u0026gt; loggingFilter() { FilterRegistrationBean\u0026lt;ApiLoggingFilter\u0026gt; registrationBean = new FilterRegistrationBean\u0026lt;\u0026gt;(); registrationBean.setFilter(new ApiLoggingFilter(requestIdParamName)); registrationBean.addUrlPatterns(urlPatterns); return registrationBean; } } Now we will create ApiLoggingFilter which is nothing but a Servlet Filter. This filter intercepts all api request and response and log them. It also make use of slf4j MDC to print requestId across all the logs serve that request. ApiLoggingFilter class is long. you can expand below to see code.\nClick to expand ApiLoggingFilter import java.io.BufferedReader; import java.io.ByteArrayInputStream; import java.io.ByteArrayOutputStream; import java.io.IOException; import java.io.InputStream; import java.io.InputStreamReader; import java.io.OutputStream; import java.io.PrintWriter; import java.util.Collection; import java.util.Enumeration; import java.util.HashMap; import java.util.Locale; import java.util.Map; import java.util.UUID; import javax.servlet.Filter; import javax.servlet.FilterChain; import javax.servlet.ReadListener; import javax.servlet.ServletException; import javax.servlet.ServletInputStream; import javax.servlet.ServletOutputStream; import javax.servlet.ServletRequest; import javax.servlet.ServletResponse; import javax.servlet.WriteListener; import javax.servlet.http.Cookie; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletRequestWrapper; import javax.servlet.http.HttpServletResponse; import org.apache.commons.io.output.TeeOutputStream; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.slf4j.MDC; public class ApiLoggingFilter implements Filter { private static final Logger LOGGER = LoggerFactory.getLogger(ApiLoggingFilter.class); private String requestIdParamName; ApiLoggingFilter(String requestIdParamName) { this.requestIdParamName = requestIdParamName; } @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { try { HttpServletRequest httpServletRequest = (HttpServletRequest) request; HttpServletResponse httpServletResponse = (HttpServletResponse) response; Map\u0026lt;String, String\u0026gt; requestMap = this.getTypesafeRequestMap(httpServletRequest); BufferedRequestWrapper bufferedRequest = new BufferedRequestWrapper(httpServletRequest); BufferedResponseWrapper bufferedResponse = new BufferedResponseWrapper(httpServletResponse); String requestId = requestMap.containsKey(requestIdParamName) ? requestMap.get(requestIdParamName) : UUID.randomUUID().toString(); MDC.put(\u0026#34;REQUEST_ID\u0026#34;, requestId); final StringBuilder logRequest = new StringBuilder(\u0026#34;HTTP \u0026#34;).append(httpServletRequest.getMethod()) .append(\u0026#34; \\\u0026#34;\u0026#34;).append(httpServletRequest.getServletPath()).append(\u0026#34;\\\u0026#34; \u0026#34;).append(\u0026#34;, parameters=\u0026#34;) .append(requestMap).append(\u0026#34;, body=\u0026#34;).append(bufferedRequest.getRequestBody()) .append(\u0026#34;, remote_address=\u0026#34;).append(httpServletRequest.getRemoteAddr()); LOGGER.info(logRequest.toString()); try { chain.doFilter(bufferedRequest, bufferedResponse); } finally { final StringBuilder logResponse = new StringBuilder(\u0026#34;HTTP RESPONSE \u0026#34;) .append(bufferedResponse.getContent()); LOGGER.info(logResponse.toString()); MDC.clear(); } } catch (Throwable a) { LOGGER.error(a.getMessage()); } } private Map\u0026lt;String, String\u0026gt; getTypesafeRequestMap(HttpServletRequest request) { Map\u0026lt;String, String\u0026gt; typesafeRequestMap = new HashMap\u0026lt;String, String\u0026gt;(); Enumeration\u0026lt;?\u0026gt; requestParamNames = request.getParameterNames(); while (requestParamNames.hasMoreElements()) { String requestParamName = (String) requestParamNames.nextElement(); String requestParamValue; if (requestParamName.equalsIgnoreCase(\u0026#34;password\u0026#34;)) { requestParamValue = \u0026#34;********\u0026#34;; } else { requestParamValue = request.getParameter(requestParamName); } typesafeRequestMap.put(requestParamName, requestParamValue); } return typesafeRequestMap; } private static final class BufferedRequestWrapper extends HttpServletRequestWrapper { private ByteArrayInputStream bais = null; private ByteArrayOutputStream baos = null; private BufferedServletInputStream bsis = null; private byte[] buffer = null; public BufferedRequestWrapper(HttpServletRequest req) throws IOException { super(req); // Read InputStream and store its content in a buffer. InputStream is = req.getInputStream(); this.baos = new ByteArrayOutputStream(); byte buf[] = new byte[1024]; int read; while ((read = is.read(buf)) \u0026gt; 0) { this.baos.write(buf, 0, read); } this.buffer = this.baos.toByteArray(); } @Override public ServletInputStream getInputStream() { this.bais = new ByteArrayInputStream(this.buffer); this.bsis = new BufferedServletInputStream(this.bais); return this.bsis; } String getRequestBody() throws IOException { BufferedReader reader = new BufferedReader(new InputStreamReader(this.getInputStream())); String line = null; StringBuilder inputBuffer = new StringBuilder(); do { line = reader.readLine(); if (null != line) { inputBuffer.append(line.trim()); } } while (line != null); reader.close(); return inputBuffer.toString().trim(); } } private static final class BufferedServletInputStream extends ServletInputStream { private ByteArrayInputStream bais; public BufferedServletInputStream(ByteArrayInputStream bais) { this.bais = bais; } @Override public int available() { return this.bais.available(); } @Override public int read() { return this.bais.read(); } @Override public int read(byte[] buf, int off, int len) { return this.bais.read(buf, off, len); } @Override public boolean isFinished() { return false; } @Override public boolean isReady() { return true; } @Override public void setReadListener(ReadListener readListener) { } } public class TeeServletOutputStream extends ServletOutputStream { private final TeeOutputStream targetStream; public TeeServletOutputStream(OutputStream one, OutputStream two) { targetStream = new TeeOutputStream(one, two); } @Override public void write(int arg0) throws IOException { this.targetStream.write(arg0); } public void flush() throws IOException { super.flush(); this.targetStream.flush(); } public void close() throws IOException { super.close(); this.targetStream.close(); } @Override public boolean isReady() { return false; } @Override public void setWriteListener(WriteListener writeListener) { } } public class BufferedResponseWrapper implements HttpServletResponse { HttpServletResponse original; TeeServletOutputStream tee; ByteArrayOutputStream bos; public BufferedResponseWrapper(HttpServletResponse response) { original = response; } public String getContent() { return bos.toString(); } public PrintWriter getWriter() throws IOException { return original.getWriter(); } public ServletOutputStream getOutputStream() throws IOException { if (tee == null) { bos = new ByteArrayOutputStream(); tee = new TeeServletOutputStream(original.getOutputStream(), bos); } return tee; } @Override public String getCharacterEncoding() { return original.getCharacterEncoding(); } @Override public String getContentType() { return original.getContentType(); } @Override public void setCharacterEncoding(String charset) { original.setCharacterEncoding(charset); } @Override public void setContentLength(int len) { original.setContentLength(len); } @Override public void setContentLengthLong(long l) { original.setContentLengthLong(l); } @Override public void setContentType(String type) { original.setContentType(type); } @Override public void setBufferSize(int size) { original.setBufferSize(size); } @Override public int getBufferSize() { return original.getBufferSize(); } @Override public void flushBuffer() throws IOException { tee.flush(); } @Override public void resetBuffer() { original.resetBuffer(); } @Override public boolean isCommitted() { return original.isCommitted(); } @Override public void reset() { original.reset(); } @Override public void setLocale(Locale loc) { original.setLocale(loc); } @Override public Locale getLocale() { return original.getLocale(); } @Override public void addCookie(Cookie cookie) { original.addCookie(cookie); } @Override public boolean containsHeader(String name) { return original.containsHeader(name); } @Override public String encodeURL(String url) { return original.encodeURL(url); } @Override public String encodeRedirectURL(String url) { return original.encodeRedirectURL(url); } @SuppressWarnings(\u0026#34;deprecation\u0026#34;) @Override public String encodeUrl(String url) { return original.encodeUrl(url); } @SuppressWarnings(\u0026#34;deprecation\u0026#34;) @Override public String encodeRedirectUrl(String url) { return original.encodeRedirectUrl(url); } @Override public void sendError(int sc, String msg) throws IOException { original.sendError(sc, msg); } @Override public void sendError(int sc) throws IOException { original.sendError(sc); } @Override public void sendRedirect(String location) throws IOException { original.sendRedirect(location); } @Override public void setDateHeader(String name, long date) { original.setDateHeader(name, date); } @Override public void addDateHeader(String name, long date) { original.addDateHeader(name, date); } @Override public void setHeader(String name, String value) { original.setHeader(name, value); } @Override public void addHeader(String name, String value) { original.addHeader(name, value); } @Override public void setIntHeader(String name, int value) { original.setIntHeader(name, value); } @Override public void addIntHeader(String name, int value) { original.addIntHeader(name, value); } @Override public void setStatus(int sc) { original.setStatus(sc); } @SuppressWarnings(\u0026#34;deprecation\u0026#34;) @Override public void setStatus(int sc, String sm) { original.setStatus(sc, sm); } @Override public String getHeader(String arg0) { return original.getHeader(arg0); } @Override public Collection\u0026lt;String\u0026gt; getHeaderNames() { return original.getHeaderNames(); } @Override public Collection\u0026lt;String\u0026gt; getHeaders(String arg0) { return original.getHeaders(arg0); } @Override public int getStatus() { return original.getStatus(); } } } 3. application.yml Let\u0026rsquo;s setup properties related to ApiLoggingFilterConfig file in .properties or .yml file\nWe have enabled api logger app.api.logging.enabled=true We have given comma separated url-patterns for which we want to print all api request and response app.api.logging.url-patterns=/posts/*,/users/* We have given requestIdParamName whose value will be printed in api request and response. If parameter is not there then uuid will be generated and printed. Last but not least, we have changed default logging pattern of spring boot to print REQUEST_ID\nDefault Spring Boot Logging Pattern is as follows:- \u0026#34;%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}\u0026#34; app: api: logging: enable: true url-patterns: \u0026#34;/users/*,/posts/*\u0026#34; requestIdParamName: reqId logging: level: root: INFO com.abc.demo: DEBUG pattern: console: \u0026#34;%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%8.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %X{REQUEST_ID} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}\u0026#34; 4. Logs That\u0026rsquo;s it. Let\u0026rsquo;s execute some RESTFul API and look at the logs:-\nhttp://localhost:8080/posts?reqId=1234 2020-04-23 21:09:44.107 - INFO 3240 --- ApiLoggingFilter: 1234 HTTP GET \u0026#34;/posts\u0026#34; , parameters={requestId=1234}, body=, remote_address=0:0:0:0:0:0:0:1 2020-04-23 21:09:44.110 - INFO 3240 --- ApiLoggingFilter: 1234 HTTP RESPONSE [{\u0026#34;id\u0026#34;:1,\u0026#34;title\u0026#34;:\u0026#34;Spring Boot\u0026#34;,\u0026#34;body\u0026#34;:\u0026#34;All about Spring boot microservice\u0026#34;},{\u0026#34;id\u0026#34;:2,\u0026#34;title\u0026#34;:\u0026#34;Java\u0026#34;,\u0026#34;body\u0026#34;:\u0026#34;Learn Streams in Java\u0026#34;},{\u0026#34;id\u0026#34;:3,\u0026#34;title\u0026#34;:\u0026#34;JavaScript\u0026#34;,\u0026#34;body\u0026#34;:\u0026#34;Whats new in ES6\u0026#34;}] http://localhost:8080/posts When reqId is not passed as parameter, default uuid will be printed across all logs\n2020-04-23 21:09:44.107 - INFO 3240 --- ApiLoggingFilter: 754be6a6-b00a-4c98-b681-2d0041b4f72c HTTP GET \u0026#34;/posts\u0026#34; , parameters={requestId=1234}, body=, remote_address=0:0:0:0:0:0:0:1 2020-04-23 21:09:44.110 - INFO 3240 --- ApiLoggingFilter: 754be6a6-b00a-4c98-b681-2d0041b4f72c HTTP RESPONSE [{\u0026#34;id\u0026#34;:1,\u0026#34;title\u0026#34;:\u0026#34;Spring Boot\u0026#34;,\u0026#34;body\u0026#34;:\u0026#34;All about Spring boot microservice\u0026#34;},{\u0026#34;id\u0026#34;:2,\u0026#34;title\u0026#34;:\u0026#34;Java\u0026#34;,\u0026#34;body\u0026#34;:\u0026#34;Learn Streams in Java\u0026#34;},{\u0026#34;id\u0026#34;:3,\u0026#34;title\u0026#34;:\u0026#34;JavaScript\u0026#34;,\u0026#34;body\u0026#34;:\u0026#34;Whats new in ES6\u0026#34;}] 5. Summary We saw in this post that how we can create custom logger to print API request and response. Also incoming request can have requestId parameter which can be printed across all the logs which serves that request. This parameter can be further passed down in downstream microservices to further print it in the logs. This way a request can be tracked end to end from the logs.\nPlease find the complete source code of custom logging API on github springboot-microservice\n","permalink":"https://codingnconcepts.com/spring-boot/custom-api-request-response-logging/","tags":["Spring Boot API","Log"],"title":"Custom API Request \u0026 Response Logging in spring boot."},{"categories":["Spring Boot"],"contents":"In spring boot microservices based application, where microservices talks to each other using RESTFul APIs. We can expose some important information about microservice such as name, version, description over some endpoints say /info and also realtime status or health over some endpoint say /health to create application dashboards, realtime alerts, track microservice updates and so on\u0026hellip;\nActuator Good news is that both endpoints come out of the box with Spring Boot Actuator. To enable this, just add spring-boot-starter-actuator dependency in your spring boot application.\npom.xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; When you start your application after adding actuator dependency, You will see following in the startup logs that actuator exposed two endpoints i.e. */actuator/info* and */actuator/health* ``` 2020-04-20 18:36:12.881 INFO [main] o.s.b.a.e.web.EndpointLinksResolver: Exposing 2 endpoint(s) beneath base path '/actuator' ``` /actuator/info endpoint (info endpoint with no data)\nWe see that initially /actuator/info endpoint gives an empty JSON. We will now add some properties related to app info.\napplication.yml info: app: name: spring boot microservice version: 1.0.0_RELEASE description: more details about sprint boot microservice contact-support: apisupportgroup@abc.com copyright: copyright (c) abc.com license: MIT tech-used: - name: java version: 11.x - name: spring-boot version: 2.x That's all we need to do to make this data available on the ***/actuator/info*** endpoint. Spring will automatically add all the properties prefixed with **info** to the endpoint. (info endpoint with app info)\n/actuator/info endpoint can be used to -\ncreate a dashboard of microservices info check for version update of particular microservice and so and so forth\u0026hellip; /actuator/health endpoint By default endpoint show the current status of the running application.\n(health endpoint with status up and running)\n/actuator/health endpoint can be used to -\ncreate a dashboard of microservices with realtime status check the heartbeat of microservice and so and so forth\u0026hellip; ","permalink":"https://codingnconcepts.com/spring-boot/configure-info-and-health-endpoints/","tags":["Spring Boot API"],"title":"info and health endpoints in spring boot"},{"categories":["Interview Questions"],"contents":"These Microservices based interview questions are based on my personal interview experience and feedback from other interviewees. These questions verifies your theoretical as well as practical knowledge about microservices. Keep following this post for regular updates.\nQ1. What do you understand by microservices? When you break an entire application architecture into smaller services where\nEach service is independently serves a functionality of that application Each service has their own development, build process, deployment and testing cycle This kind of setup is known as microservices based architecture.\nQ2. What are the advantage of microservices over monolithic application? Monolithic application Microservices based application Single code base for entire application so one small code change requires the build process, testing and deployment of entire application. Each microservice can be developed, build, deployed and tested independently and code change in one microservice doesn\u0026rsquo;t affect entire application. Single code base for entire application so can be written in one technology. Each Microservice can be written in different technology stack like Java, Scala, Python, NodeJs etc. Monolithic application can be scaled vertically by adding more servers. It can be scaled both vertically and horizontally by adding more servers and running multiple copies of each microservice behind a load balancer. If something goes wrong with monolithic application and its down means whole application is down. It is fault tolerant and continue to run with limited functionality if something goes wrong in few microservices and they are down. Q3. You have to migrate an existing monolithic application to microservices. What will be your approach? Migrating from monolithic application to microservices is a long term process where we gradually :-\nPick one functionality of monolithic application Copy the functionality from monolithic application in terms of source code, database, schema Create an independent microservice which serves the same functionality Integrate monolithic application with this microservice Remove the source code, database, schema etc from monolithic application when integration with microservice is successful. Repeat the above 5 steps until our entire monolithic application is converted into microservices.\nQ4. What are the best practices to design microservices based application ? These are few best practices to design microservice:-\nOne functionality one microservice Separate data source for each microservice Follow twelve factors from The Twelve-Factor App Q5. What are the challenges in microservices based application ? The challenges with microservices are as follows:-\nRequire good investment for infrastructure setup. It is a nightmare to manage development, build, test, deployment and release cycles manually. DevOps is must to implement CI/CD (Continuous integration, continuous delivery) automation. Difficult to troubleshoot or debug an issue spanning across multiple microservices. Overhead of inter communication between microservices Challenges in development and testing where two or more microservices are involved. Difficult to make configuration change across large fleet of microservices. Operation and maintenance overhead. Challenges comes with distributes system such as Network latency, fault tolerance, distributed transactions, unreliable networks, handling asynchronous operations. Q6. How do you troubleshoot an issue using logs in microservices based application ? For troubleshooting an issue using logs in microservices based application,\nWe should have a centralized logging system where each microservice push their logs to Splunk or ELK (Elastic Logstash Kibana) and we can use their built in dashboards to look at the logs for debugging. We can generate a requestId for each external request, which is passed to all the microservices which are involved in handling the request. Include this requestId in all log messages pushed to splunk or ELK. We can troubleshoot any request end to end using this requestId if something goes wrong. Also read how to create custom logger to print API request and response along with incoming requestId\nQ7. How do you manage configuration in microservices based application ? If we want to modify the configuration for a microservice that has been replicated a hundred times (one hundred processes are running). If the configuration for this microservice is packaged with the microservice itself, we’ll have to redeploy each of the one hundred instances. This can result in some instances using the old configuration, and some using the new one, at some point. Moreover, sometimes microservices use external connections which, for example, require URLs, usernames, and passwords. If you want to update these settings, it would be useful to have this configuration shared across services.\nWe use externalize configuration to solve this problem by keeping the configuration information in an external store such as github, database, filesystem, or environment variables or even a configuration server. At startup, microservices load the configuration from the external store or configuration server.\nNetflix Archaius and Spring Cloud Config Server provides ready made solution for externalize configuration.\nQ8. How do microservices communicate with each other ? Microservices often communicate with each other using RESTful APIs over HTTP. The communication can be broadly divided into two categories:-\nRestTemplate, WebClient, FeignClient can be used for synchronous communication between microservices ActiveMQ, RabbitMQ, Kafka can be used for asynchronous communication across microservices. Q9. How do you manage authentication and authorization in microservices based application ? Session based authentication works well with stateful monolith applications but token based authentication and authorization is recommended for microservices based application to maintain the statelessness. A typical flow of token based authentication is as follows:-\nUser sends a login request with username and password. If you are using Api gateway then it is responsible for generating the token and hence authentication. It achieves this by communicating with authorization and users service. User receives a token on successful login which is typically stored in browser cookies. Token holds the user\u0026rsquo;s information in the encrypted format. When user make any request of resources, this token is sent in Authorization header of each request. Microservices decrypts the token and evaluate user information to authorize for resource access and send the response accordingly. JWT JWT (Json Web Token) is widely used token based authentication mechanism. JWT consist of three parts:\nheader contains type, fixed value JWT and the hashing algorithm used by JWT { \u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34;, \u0026#34;alg\u0026#34;: \u0026#34;HS256\u0026#34; } payload typically contains user authorization related information such as id, name, roles, permissions etc. It also contains the expiry period of token. { \u0026#34;id\u0026#34;: 12345, \u0026#34;name\u0026#34;: \u0026#34;admin_user\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;admin_user@organization.com\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;admin\u0026#34;], \u0026#34;permissions\u0026#34;: [\u0026#34;can_access_resource_1\u0026#34;, \u0026#34;can_access_resource_2\u0026#34;] } signature is required to verify the authenticity of token. It consists of the encoded header, the payload and the secret key. HMACSHA256( base64UrlEncode(header) + \u0026#34;.\u0026#34; + base64UrlEncode(payload), secret ) All microservices can verify the token based on the signature so there are no further calls to the authorization server after login.\nAuthentication \u0026amp; Authorization flow using JWT User Authentication \u0026amp; Authorization flow using JWT\nQ10. How do you handle distributed transaction across microservices ? Transactions are easy to handle in monolithic application with single code base, single data source and deployment on single server whereas it becomes a challenge to handle a distributed transaction across microservices where each microservice has its own data store and deployed on different servers. There are different approach to handle distributed transactions:\n1. Avoid if at all possible First and foremost approach is to avoid them completely.\nIf we can not avoid transaction between two microservices. Just think,\nAre they meant to be together? Merge them in one microservice?\nCan we redesign them in such a way so that transaction becomes unnecessary.\n2. Two-phase commit protocol (2PC) Two phase commit protocol commits into 2 steps:\nPrepare Phase The transaction coordinator send a prepare command to each participant in the transaction Each participant then checks if they could commit the transaction. Commit Phase If that’s the case, they respond with “prepared” and the transaction coordinator sends a commit command to all participants. The transaction was successful, and all changes get committed.\nor Rollback Phase If any of the participant doesn’t answer the prepare command or responds with “failed”, the transaction coordinator sends an abort command to all participants. This rolls back all the changes performed within the transaction. Its an old, complicated and slow approach because of all the coordination things and should be avoided.\n3. The Saga pattern Saga is one of the well known pattern for distributed transactions.\nA saga is a sequence of local transactions where each transaction updates data within a single service. The first transaction is initiated by an external request corresponding to the system operation, and then each subsequent step is triggered by the completion of the previous one.\nThere are a two ways to implement a saga transaction:-\nEvents/Choreography In this approach there is no central coordination, executes a transaction and then publishes an event. This event is listened by one or more services which execute local transactions and publish (or not) new events.\nThe distributed transaction ends when the last service executes its local transaction and does not publish any events or the event published is not heard by any of the saga’s participants.\nLet’s see how it would look like in our e-commerce example:\nEvent Saga Pattern (Success Use Case)\nOrder Service saves a new order, set the state as pending and publish an event called ORDER_CREATED_EVENT. The Payment Service listens to ORDER_CREATED_EVENT, charge the client and publish the event BILLED_ORDER_EVENT. The Stock Service listens to BILLED_ORDER_EVENT, update the stock, prepare the products bought in the order and publish ORDER_PREPARED_EVENT. Delivery Service listens to ORDER_PREPARED_EVENT and then pick up and deliver the product. At the end, it publishes an ORDER_DELIVERED_EVENT Finally, Order Service listens to ORDER_DELIVERED_EVENT and set the state of the order as concluded. In the case above, if the state of the order needs to be tracked, Order Service could simply listen to all events and update its state.\nRollback For Rollback, you have to implement another operation/transaction to compensate for what has been done before.\nSuppose that Stock Service has failed during a transaction. Let’s see what the rollback would look like:\nEvent Saga Pattern (Failure Use Case)\nStock Service produces PRODUCT_OUT_OF_STOCK_EVENT; Both Order Service and Payment Service listen to the previous message: Payment Service refund the client Order Service set the order state as failed Note that it is crucial to define a common shared ID for each transaction, so whenever you throw an event, all listeners can know right away which transaction it refers to.\nIt is simple, easy to understand, does not require much effort to build.\nCommand/Orchestration when a coordinator service is responsible for centralizing the saga’s decision making and sequencing business logic.\nPlease read this post for for more details on Command/Orchestration pattern\nQ11. What is service discovery pattern in microservices ? Service discovery is a pattern in microservices which solves the problem of service registry and discovery.\nService Discovery registers entries of all of the services running under that application. Whenever a service is up and running, it registers itself with discovery server and thereafter discovery server polls those services to check their heartbeats if they are up or down.\nWhen one service (client) wants to make a request to another service, it can talk to discovery server to locate the service in two ways:-\n1. Client side service discovery Client asks the address of service from discovery server and then once client gets the address, it requests to the service directly.\nClient ask service_address ⟹ Discovery Server give service_address ⟹ Client make request ⟹ Service return response ⟹ Client\nNetflix Eureka is one of the implementation of client side service discovery pattern. 2. Server side service discovery Client request to discovery server which redirect the request to appropriate service and get the response back and redirect to client.\nClient make request ⟹ Discovery Server redirect request ⟹ Service return response ⟹ Discovery Server redirect response ⟹ Client\nQ12. What is circuit breaker pattern in microservices ? Circuit breaker is popular pattern for fault tolerance in microservices.\nProblem Services sometimes call other services to handle requests. There is always a possibility that the other service is unavailable or taking longer time to respond. Precious resources such as threads might be consumed in the caller service while waiting for the other service to respond. This might lead to resource exhaustion, which would make the calling service unable to handle other requests. The failure of one service can potentially cascade to other services throughout the application.\nSolution Thats where circuit breaker comes into play to prevent service failure from cascading to other services.\nCircuit Breaker\nConsider one service calling another service like an electric circuit breaker. When all works fine circuit breaker is closed.\nWhen service detects that something is wrong with another service, the circuit breaks (circuit breaker is open).\n✦ When to break circuit?\n✓ When m out of last n requests failed (When 3 out of last 5 requests failed)\n✓ Request is considered failed if it responds after timeout period of t seconds (say 2s)\nOnce circuit breaks, service do not attempt to call another service.\n✦ What to do when circuit breaks?\n● throw an error or\n● return a fallback \u0026ldquo;default\u0026rdquo; response or\n● serve previous responses from cache\nAfter waiting for sleep window of x seconds (say 10s), service attempts to call another service (circuit breaker is half open)\n● If request fails then circuit breaks (circuit breaker is open). It repeats step 2 and 3 again.\n● If gets a successful response back then circuit gets closed. Resume the requests again. All works fine.\nNetflix Hystrix is one of the implementation of circuit breaker pattern.\nQ13. What is API gateway pattern in microservices ? API gateway pattern is a good approach to consider when building large or complex microservices based application.\nAPI Gateway Flow API Gateway\nAPI Gateway Features API gateway provides a single entry point to access microservices and facilitate following features:-\nReverse proxy or gateway routing The API Gateway offers a reverse proxy to redirect or route HTTP requests to the endpoints of the internal microservices. The gateway provides a single endpoint or URL for the client apps and then internally maps the requests to a group of internal microservices. This routing feature helps to decouple the client apps from the microservices.\nRequest aggregation As part of the gateway pattern you can aggregate multiple client HTTP requests targeting multiple internal microservices into a single client request. This pattern is especially convenient when a client page/screen needs information from several microservices. With this approach, the client app sends a single request to the API Gateway that dispatches several requests to the internal microservices and then aggregates the results and sends everything back to the client app. The main benefit and goal of this design pattern is to reduce chattiness and round-trips between the client apps and the backend API.\nCross-cutting concerns or gateway offloading. Depending on the features offered by each API Gateway product, you can offload functionality from individual microservices to the gateway, which simplifies the implementation of each microservice by consolidating cross-cutting concerns into one tier. This is especially convenient for specialized features that can be complex to implement properly in every internal microservice, such as the following functionality:\nAuthentication \u0026amp; Authorization Authentication and Authorization is done at API gateway level and services do not need do further check.\nAPI gateway authenticates incoming request by evaluating OAuth token given by OAuth authentication server on successful authentication. This OAuth token can be used in subsequent requests. After successful authentication API gateway authorize the request using access token (eg, send in custom HTTP header). Based on authorization, gateway route the request to services. Logging, debugging API gateway logs each incoming requests and outgoing response to centralized logging system where other microservices also push their logs. API gateway generates a request_id for each incoming request which is passed through all the microservices serving that request. Any request can be traced end to end using this request_id. Response Caching\nAPI gateway can use caching mechanism for some of the request to provide response without routing request to underlying microservices. Load balancing\nAPI gateway can load balance incoming request if multiple instances of same microservice is running. Retry policies, circuit breaker API gateway can provide a fault tolerant system by implementing retry policies and circuit breaker. IP whitelisting\nAdvantage of API gateway is that you need to provide only one IP for whitelisting when distributing APIs to thirdparty if required. Encryption\nAPI gateway can provide encrypted communication to clients while underlying microservices communication remain unencrypted. API gateway decrypts the incoming request, route to microservices, encrypt the response and send to clients. SSL Certificate management\nAPI gateway communication can be secured by providing SSL certificate to clients since API gateway is exposed whereas underlying microservices can communicate without SSL certification under a secure network. API Gateway - Cross Cutting Concerns API Gateway - Cross Cutting Concerns\nNetflix Zuul is one of the implementation of API Gateway pattern.\nBackend for Frontend (BFF) When you have multiple API gateways and each API gateway provide different API tailored for different clients app then this pattern is called Backend for Frontend (BFF) pattern.\nMultiple API Gateways - Backend for Frontend (BFF)\nQ14. Could you explain a high level microservice architecture ? Microservice High Level Architecture\nThe diagram say it all, Let\u0026rsquo;s see them one by one:\nClient — Client can be a Mobile App, Dynamic Single Page Web Application using Angular, ReactJS, Vue, WebComponents or any traditional clients which renders HTML. CDN (Content Delivery Network) — CDN is a system of distributed servers or networks of servers in locations all over the world. It delivers content from the website or mobile application to people more quickly and efficiently, based on the geographic locations of the user, the origin of the webpage, and the content delivery server. Load Balancer (Hardware/Software) — If there is a high value of incoming traffic and it is affecting system performance and ultimately user experience, application traffic needs to be distributed evenly and efficiently at multiple servers in a server farm. The load balancer sits between client devices and backend servers, receiving and then distributing incoming requests to any available server based on the load balancing algorithm such as Round Robin, Weighted Round Robin, Random, Least Connection, Weighted Least Connection, etc. Web Apps — Clients like Mobile App or SPA can talk to API gateway directly whereas traditional clients can talk to API gateway through load balanced web apps which are hosted on any web server like Apache, Tomcat, Heroku, etc. API Gateway (Zuul) — It’s a server that provides single entry point to talk with microservices. It offers reverse proxy for request routing and request aggregation. It is also responsible for cross cutting concerns such as Authentication \u0026amp; Authorization, logging, Response caching, Encryption, SSL certificate management, rate limiting, spike arrest. Any request coming from clients first go through the API Gateway after that it routes requests to the appropriate microservice. Netflix Zuul provides implementation of API Gateway. Service Discovery (Eureka) — Service Discovery holds the information like IP address, running port about all the microservices under applications. Microservice registers themselves with discovery server when up and running. Netflix Eureka provides implementation of Service Discovery. Management — Management Endpoints (Actuator endpoints) allow you to monitor and interact with your application. Spring Boot actuator includes several built-in endpoints and you can also add your own. Like, the health endpoint provides basic application health information. It’s widely used by containers to check the health and other parameters of the application. Microservices — These microservices are designed around business capabilities, can be deployed independently and loosely coupled. Communication among themselves happens through rest call. Event Bus (Kafka, RabbitMQ) — Event buses are used in microservices based application to avoid messy communication network and keep the communication across microservices clean, loosely coupled, non blocking, asynchronous. Event buses are nothing but a publish/subscribe system like Kafka, RabbitMQ which are used for async tasks like notifications, alerts, background jobs etc to improve performance significantly. Logging and Monitoring (ELK, Splunk) — Microservices based applications requires a centralized logging and monitoring system. One client request could be served by many services all together. In case of any failure, we need to track the request flow end to end across microservice, and this is where logging and monitoring tools helps like ELK (Elastic Search, Logstash and Kibana), Splunk, Grafana. Q15. What is 12 Factor App ? The Twelve-Factor App is a mythology for writing microservices. Following is an easy to understand summary of those 12 factors:-\n1. Codebase One codebase, multiple deploys.\nWe should have only one repository for each microservice in our source control such as git, subversion. All the microservice deployment should be from that repository.\n2. Dependencies Explicitly declare and isolate dependencies.\nWe should use dependency manager in our microservice such as maven (pom.xml) or gradle (build.gradle) for Java. Benefit is new developer can check out code onto their machine, requiring only language runtime and dependency manager as prerequisite.\n3. Config Store config in the environment.\nWe should create a Spring Cloud Config Server to manage configurations of all microservices across all environments like dev, staging, prod.\nWe should not declare configuration inside source code because configuration varies across deployments but code does not.\n4. Backing services Treat backing services as attached resources.\nWe should integrate microservice with resources like datastores, messaging systems, caching system or other microservices from the configuration only. All the resource URL, locator, credentials should come from configuration no matter if it is a thirdparty resource or developed by your organization.\n5. Build, Release \u0026amp; Run Strictly separate build and run stages.\nWe should use release management tools like Jenkins to create pipelines to separate the build (building executable by compiling source code), release (executable with configuration) and run (deployment of release to specific environment) stages.\nRelease should have a unique id such as timestamp or version like v1.0.0 which can not be mutated. Any change must be a new release.\n6. Processes Execute the app as one or more stateless processes.\nMicroservice we build, should be stateless and should not rely on in-memory cache or filesystem to store data since it usually wiped out on restart. Any data that need to be persist must be stored in backing service like database. However distributed cache like memcache, ehcache or Redis can be used.\nWe should also never use and rely on \u0026ldquo;sticky sessions\u0026rdquo;.\n7. Port Binding Export services via port binding.\nWe should always create a standalone microservice using spring boot which is having embedded Tomcat or Jetty webserver. As soon as service starts, it is ready to serve over HTTP by binding to a port. We should not rely on creating a war and then deploying to webserver.\n8. Concurrency Scale out via the process model.\nEach microservice in application should be able to handle more load by scaling out (deploying multiple copies on microservice behind load balancer).\n9. Disposability Maximize robustness with fast startup and graceful shutdown\nWe should try to minimize the startup time of microservice and handle the shutdown gracefully. Microservice should be able to start and stop at moment\u0026rsquo;s notice to facilitate fast elastic scaling, rapid deployment of code or config changes.\n10. Dev/Prod parity Keep development, staging, and production as similar as possible.\nWe should adapt CI/CD (Continuous Integration/Continuos Delivery) by combining DevOps automation tools like Jenkins to build pipelines, Docker to containerize build with all dependencies, Chef and Puppet to automate delivery process. CI/CD process minimize the gap between development, testing, staging and production environments.\n11. Logs Treat logs as event streams\nMicroservice should not manage log files itself, instead treat it as event stream and route it to a centralized log indexing and analysis system such as Splunk or ELK (Elastic logstash Kibana) or data warehousing system such as Hadoop/Hive.\n12. Admin processes Run admin/management tasks as one-off processes\nAll admin/management tasks for a microservice like database migration should be deployed and run separately.\n","permalink":"https://codingnconcepts.com/top-microservices-interview-questions/","tags":["Interview Q\u0026A","Microservice Q\u0026A"],"title":"Top Microservices Interview Questions"},{"categories":["Spring Boot"],"contents":"In this quick article, we\u0026rsquo;ll learn how to pretty print JSON response in Spring Boot web application using Jackson property.\nOverview When you create a @RestController in a Spring Boot application to define RESTFul API endpoints then HttpMessageConverters is used to convert Java Object to JSON or XML in order to render the response.\nSpring Boot by default render JSON response using MappingJackson2HttpMessageConverter which use Jackson JSON ObjectMapper library.\nRendered JSON response is not pretty print by default but you can enable it with just one property change.\nExample We first create a @RestController class:-\n@RequestMapping(\u0026#34;/posts\u0026#34;) public class PostController { @GetMapping public List\u0026lt;Post\u0026gt; getAllPosts() { return Arrays.asList(new Post[] { new Post(1, \u0026#34;post title 1\u0026#34;, \u0026#34;post body 1\u0026#34;), new Post(2, \u0026#34;post title 2\u0026#34;, \u0026#34;post body 2\u0026#34;), new Post(3, \u0026#34;post title 3\u0026#34;, \u0026#34;post body 3\u0026#34;)\t}); } } We see that default JSON Response is not pretty print:-\nNow we add following property depending upon you are using .properties or .yml file:-\napplication.properties spring.jackson.serialization.indent_output = true application.yml spring: jackson: serialization: indent_output: true We restart the application and see that now JSON response is pretty print:-\nRelax Binding Please note that spring boot configuration support Relaxed Binding that means properties can be in uppercase or lowercase, both are valid.\nspring.jackson.serialization.INDENT_OUTPUT = true is same as\nspring.jackson.serialization.indent_output = true That\u0026rsquo;s it for now.\nDownload the source code for more Jackson related config examples from github/springboot-api\n","permalink":"https://codingnconcepts.com/spring-boot/pretty-print-json-response/","tags":["Spring Boot API","Jackson"],"title":"Pretty print JSON response in Spring Boot"},{"categories":["Spring Boot","Kafka"],"contents":"This post describes how to configure Kafka producer and consumer in spring boot application and also explains how to create service classes to send and receive Kafka messages to and from configured kafka topic respectively.\nSetup Spring Boot Project It is recommended to use Spring Initializr to generate initial project. Our project should have Web and Kafka dependencies.\nMaven Project Click on the below link to generate maven project with pre-defined configuration:-\nhttps://start.spring.io/#!type=maven-project\u0026amp;language=java\u0026amp;platformVersion=2.5.0.RELEASE\u0026amp;packaging=jar\u0026amp;jvmVersion=11\u0026amp;groupId=com.example\u0026amp;artifactId=springboot-kafka\u0026amp;name=springboot-kafka\u0026amp;description=Kafka%20producer%20and%20consumer%20configuration\u0026amp;packageName=com.example.kafka\u0026amp;dependencies=web,kafka\nA typical pom.xml file for a kafka project look like this:-\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.kafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-kafka\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.kafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-kafka-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Gradle Project Click on the below link to generate gradle project with pre-defined configuration:-\nhttps://start.spring.io/#!type=gradle-project\u0026amp;language=java\u0026amp;platformVersion=2.5.0.RELEASE\u0026amp;packaging=jar\u0026amp;jvmVersion=11\u0026amp;groupId=com.example\u0026amp;artifactId=springboot-kafka\u0026amp;name=springboot-kafka\u0026amp;description=Kafka%20producer%20and%20consumer%20configuration\u0026amp;packageName=com.example.kafka\u0026amp;dependencies=web,kafka\nA typical build.gradle file for a kafka project look like this:-\ndependencies { implementation \u0026#39;org.springframework.boot:spring-boot-starter-web\u0026#39; implementation \u0026#39;org.springframework.kafka:spring-kafka\u0026#39; testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; testImplementation \u0026#39;org.springframework.kafka:spring-kafka-test\u0026#39; } Kafka Configuration Next, we need to create Kafka producer and consumer configuration to be able to publish and read messages to and from the Kafka topic. Spring boot auto configure Kafka producer and consumer for us, if correct configuration is provided through application.yml or spring.properties file and saves us from writing boilerplate code.\nA typical Kafka producer and consumer configuration looks like this:-\napplication.yml #APP SPECIFIC CUSTOM PROPERTIES app: kafka: producer: topic: \u0026lt;PRODUCER_TOPIC_NAME\u0026gt; consumer: topic: \u0026lt;CONSUMER_TOPIC_NAME_1, CONSUMER_TOPIC_NAME_2, CONSUMER_TOPIC_NAME_3\u0026gt; #SPRING PROPERTIES spring: kafka: bootstrap-servers: localhost:9200,localhost:9300,localhost:9400 properties: #Server host name verification is disabled by setting ssl.endpoint.identification.algorithm to an empty string ssl.endpoint.identification.algorithm: ssl: protocol: SSL trust-store-location: classpath:/app/store/truststore.jks trust-store-password: \u0026lt;TURST_STORE_PASSWORD\u0026gt; key-store-location: classpath:/app/store/keystore.jks key-store-password: \u0026lt;KEY_STORE_PASSWORD\u0026gt; key-password: \u0026lt;KEY_PASSWORD\u0026gt; producer: retries: 0 acks: all key-serializer: org.apache.kafka.common.serialization.StringSerializer value-serializer: org.apache.kafka.common.serialization.StringSerializer consumer: group-id: \u0026lt;CONSUMER_GROUP_ID\u0026gt; auto-offset-reset: earliest key-deserializer: org.apache.kafka.common.serialization.StringDeserializer value-deserializer: org.apache.kafka.common.serialization.StringDeserializer Kafka SSL Configuration Please note that in the above example for Kafka SSL configuration, Spring Boot looks for key-store and trust-store (*.jks) files in the Project classpath: which works in your local environment. Generally you don\u0026rsquo;t keep these files in generated Jar and keep them outside in production environment. In such cases, refer these files using file: in the configuration.\napplication-prod.yml spring: kafka: properties: #Server host name verification is disabled by setting ssl.endpoint.identification.algorithm to an empty string ssl.endpoint.identification.algorithm: ssl: protocol: SSL trust-store-location: file:/app/store/truststore.jks trust-store-password: \u0026lt;TURST_STORE_PASSWORD\u0026gt; key-store-location: file:/app/store/keystore.jks key-store-password: \u0026lt;KEY_STORE_PASSWORD\u0026gt; key-password: \u0026lt;KEY_PASSWORD\u0026gt; It is recommended to always give absolute path in production environment to avoid any error.\n# Absolute Path file:/app/store/truststore.jks # Relative Path file:app/store/truststore.jsk ssl.endpoint.identification.algorithm The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker’s certificate.\nIf you have enabled SSL for kafka Server, then sometime Spring Boot startup throw error due to hostname verification. You can disable the server host name verification by setting the property ssl.endpoint.identification.algorithm to an empty string to avoid the error.\nKafka Full Configuration You can refer to the Spring Boot official documentation for full list of available kafka producer and consumer configuration.\nHere is a list of important auto-configuration properties:-\nProperty Description spring.kafka.bootstrap-servers Comma separated list of kafka servers (host:port) running as a cluster. Applies to both producer and consumer unless overridden. spring.kafka.producer.bootstrap-servers Kafka bootstrap server for producer. Overrides spring.kafka.bootstrap-servers spring.kafka.consumer.bootstrap-servers Kafka bootstrap server for consumer. Overrides spring.kafka.bootstrap-servers spring.kafka.client-id Client-ID to pass to the server when making requests. Used for server-side logging. spring.kafka.producer.client-id Client-ID to pass for producer. Overrides spring.kafka.client-id spring.kafka.consumer.client-id Client-ID to pass for consumer. Overrides spring.kafka.client-id spring.kafka.ssl.* Kafka SSL configuration is to provide secure communication between producer/consumer and Kafka server. You need to generate key-store and trust-store files and configure the location and password spring.kafka.properties .ssl.endpoint.identification.algorithm If you have enabled SSL for kafka Server, then host name verification can be disabled by setting this property to empty string, otherwise spring boot startup throw error spring.kafka.producer.* Kafka Producer related configurations spring.kafka.consumer.* Kafka Consumer related configurations We have also created application specific property to configure Kafka producer and consumer topics:-\nProperty Description app.kafka.producer.topic Kafka topic name to publish messages app.kafka.consumer.topic Comma separated list of Kafka topic names if you want consumer service to consume from multiple kafka topics Spring Boot Kafka Producer Create Kafka Producer Let\u0026rsquo;s create a KafkaProducerService interface and its implementation to send messages to a Kafka topic. We just autowire KafkaTemplate and use its send method to publish messages to the topic.\nPlease read more about KafkaTemplate which comes with overloaded send method to publish messages with topic, partition, key, timestamp and routing information.\npublic interface KafkaProducerService { void send(String message); } @Service public class KafkaProducerServiceImpl implements KafkaProducerService { private static final Logger logger = LoggerFactory.getLogger(KafkaProducerServiceImpl.class); @Autowired private KafkaTemplate\u0026lt;String, String\u0026gt; kafkaTemplate; @Value(\u0026#34;${app.kafka.producer.topic}\u0026#34;) private String topic; @Override public void send(String message) { logger.info(\u0026#34;message sent: {}\u0026#34;, message); kafkaTemplate.send(topic, message); } } Please note that by default KafkaTemplate use DefaultKafkaProducerFactory which is auto initialized by spring boot based on kafka producer configuration provided in application.yml or application.properties file.\nIf you wish to customized the default configuration then you need to provide a bean definition of KafkaTemplate.\nCustomize Kafka Producer Configuration Let\u0026rsquo;s create a KafkaProducerConfig class to customize the configuration. We just autowire ProducerFactory which gives us instance of DefaultKafkaProducerFactory, then we just add our customized configuration on top of it.\nFor example, below we have provided encrypted passwords for trust-store, key-store, and key in our configuration application.yml file for security purpose and we want to set decrypted password to the ProducerFactory for ssl connections.\nWe have passed this customized ProducerFactory to KafkaTemplate bean initialization. That\u0026rsquo;s it!\n@Configuration public class KafkaProducerConfig { @Autowired private ProducerFactory\u0026lt;Integer, String\u0026gt; producerFactory; @Autowired private CryptoService cryptoService; public Map\u0026lt;String, Object\u0026gt; producerConfig() { Map\u0026lt;String, Object\u0026gt; producerConfig = new HashMap\u0026lt;\u0026gt;(producerFactory.getConfigurationProperties()); decryptAndAddToConsumerConfig(producerConfig, SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG); decryptAndAddToConsumerConfig(producerConfig, SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG); decryptAndAddToConsumerConfig(producerConfig, SslConfigs.SSL_KEY_PASSWORD_CONFIG); return producerConfig; } @Bean public KafkaTemplate\u0026lt;String, String\u0026gt; kafkaTemplate() { return new KafkaTemplate\u0026lt;\u0026gt;(new DefaultKafkaProducerFactory\u0026lt;\u0026gt;(producerConfig())); } private void decryptAndAddToConsumerConfig(Map\u0026lt;String, Object\u0026gt; config, String property) { config.compute(property, (k, v) -\u0026gt; cryptoService.decrypt((String) v)); } } Spring Boot Kafka Consumer Create Kafka Consumer Let\u0026rsquo;s create a KafkaConsumerService interface and its implementation to receive messages from a Kafka topic.\nWe just use @KafkaListener annotation at method level and pass the kafka consumer topic names. Spring boot automatically binds this method to the kafka consumer instance. As soon as any message is published to those topics, this method receive them in realtime.\npublic interface KafkaConsumerService { void receive(String message); } @Service public class KafkaConsumerServiceImpl implements KafkaConsumerService { private static final Logger logger = LoggerFactory.getLogger(KafkaConsumerServiceImpl.class); @KafkaListener(topics = {\u0026#34;#{\u0026#39;${app.kafka.consumer.topic}\u0026#39;.split(\u0026#39;,\u0026#39;)}\u0026#34;}) public void receive(@Payload String message) { logger.info(\u0026#34;message received: {}\u0026#34;, message); } } Please note that @KafkaListener use ConcurrentKafkaListenerContainerFactory to create an instance of kafka consumer. This factory use the default configuration from DefaultKafkaConsumerFactory which is auto initialized by spring boot based on kafka consumer configuration provided in application.yml or application.properties file.\nIf you wish to customized the default configuration then you need to provide a bean definition of ConcurrentKafkaListenerContainerFactory by yourself.\nCustomize Kafka Consumer Configuration Let\u0026rsquo;s create a KafkaConsumerConfig class to customize the configuration. We just autowire ConsumerFactory which gives us instance of DefaultKafkaConsumerFactory, then we just add our customized configuration on top of it.\nFor example, below we have provided encrypted passwords for trust-store, key-store, and key in our configuration application.yml file for security purpose and we want to set decrypted password to the ConsumerFactory for ssl connections.\nWe have passed this newly created ConsumerFactory to ConcurrentKafkaListenerContainerFactory bean initialization. That\u0026rsquo;s it!\nWe have also used @EnableKafka annotation at class level which tells spring boot to auto detect @KafkaListener annotation applied on any method in spring boot application and use custom configuration instead.\n@EnableKafka @Configuration public class KafkaConsumerConfig { @Autowired private ConsumerFactory\u0026lt;Integer, String\u0026gt; consumerFactory; @Autowired private CryptoService cryptoService; @Bean public ConcurrentKafkaListenerContainerFactory\u0026lt;String, String\u0026gt; kafkaListenerContainerFactory() { ConcurrentKafkaListenerContainerFactory\u0026lt;String, String\u0026gt; factory = new ConcurrentKafkaListenerContainerFactory\u0026lt;\u0026gt;(); factory.setConsumerFactory(new DefaultKafkaConsumerFactory\u0026lt;\u0026gt;(consumerConfig())); return factory; } private Map\u0026lt;String, Object\u0026gt; consumerConfig() { Map\u0026lt;String, Object\u0026gt; consumerConfig = new HashMap\u0026lt;\u0026gt;(consumerFactory.getConfigurationProperties()); decryptAndAddToConsumerConfig(consumerConfig, SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG); decryptAndAddToConsumerConfig(consumerConfig, SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG); decryptAndAddToConsumerConfig(consumerConfig, SslConfigs.SSL_KEY_PASSWORD_CONFIG); return consumerConfig; } private void decryptAndAddToConsumerConfig(Map\u0026lt;String, Object\u0026gt; config, String property) { config.compute(property, (k, v) -\u0026gt; cryptoService.decrypt((String) v)); } } Summary Spring boot provides a wrapper over kafka producer and consumer implementation in Java which helps us to easily configure-\nKafka Producer using KafkaTemplate which provides overloaded send method to send messages in multiple ways with keys, partitions and routing information. Kafka Consumer using @EnableKafka annotation which auto detects @KafkaListener annotation applied to any method and that methods becomes a Kafka Listener. You can download complete source code from github and read official spring documentation Spring for Apache Kafka for further exploration.\nAlso Read How to install kafka broker on local machine for development and testing\n","permalink":"https://codingnconcepts.com/spring-boot/configure-kafka-producer-and-consumer/","tags":["Spring Boot Kafka"],"title":"Configure Kafka Producer and Consumer in spring boot"},{"categories":["Javascript"],"contents":"Async functions and Await keyword are latest additions in JavaScript as part of ECMAScript 2017 release which introduced a new way of writing asynchronous functions. In this post we will talk about why we should use async/wait, its syntax and practical usage with example.\nWhy Async/Await? In earlier days, you would have used callbacks to handle asynchronous operations. However, callbacks have limited functionality and often leads to unmanageable code if you are handling multiple async calls, it leads to heavily nested callback code which is also known as callback hell.\nLater, Promises were introduced in ES6 to overcome the problems of callback functions and improved code readability. Finally Async/Await introduced in ES2017, which are nothing but the syntactic improved version of promises. Its underlying is Promise with improved syntax which provides,\nbetter way to chain promises and pass values between chained promises more concise and readable code compare to promises debugging is easy better error handling Also read this post for more details on promises in javascript\nSyntax async When async keyword is applied before a function, it turns into asynchronous function and always return a promise object.\n1async function hello() { 2 //return Promise.resolve(\u0026#34;Hello\u0026#34;); 3 return \u0026#34;Hello\u0026#34;; 4} 5 6console.log(hello()); 7hello().then(data =\u0026gt; console.log(data)); Output Promise {\u0026lt;resolved\u0026gt;: \u0026#34;Hello\u0026#34;} Hello We see in above code snippet that when we execute async function hello(), it wraps the string value in promise object and return a resolved promise. We can also explicitly return the resolved promise object, line 2 and line 3 are same.\nWe further used then on resolved promise object to get Hello string (line 7)\nawait // works only inside async functions let value = await promise; await keyword works only inside async function and it makes the function execution wait until the returned promise settles (either resolve or reject).\n1async function hello() { 2 let promise = new Promise((resolve, reject) =\u0026gt; { 3 setTimeout(() =\u0026gt; resolve(\u0026#34;Hello\u0026#34;), 5000) 4 }); 5 6 let value = await promise; // wait until the promise resolves 7 8 return value; 9} 10 11hello().then(data =\u0026gt; console.log(data)); Please note that in above code snippet when we execute async function hello(), function execution literally waits for 5s at line 6 before returning resolved promise object. CPU resources are not utilized in this wait period and can be used for other work.\nAlso note that if you use await keyword inside non-async function, it returns SyntaxError like below:\nfunction hello() { let promise = Promise.resolve(\u0026#34;Hello\u0026#34;); let value = await promise; ⓧ Uncaught SyntaxError: await is only valid in async function return value; } Usage Let\u0026rsquo;s see one of the practical example of getting data from multiple HTTP endpoints using fetch API.\n1. Create three promise objects We have created a common function getData and used this to create three parameterized promise objects getUser, getPosts and getComments to fetch data from their respective HTTP endpoint.\n//create a common getData function let getData = (url) =\u0026gt; new Promise(function (resolve, reject ){ fetch(url) .then(response =\u0026gt; { return response.json(); }) .then(data =\u0026gt; { resolve(data); }) .catch(error =\u0026gt; { reject(error); }); }); //create multiple promises from common getData function let getUsers = getData(\u0026#39;https://jsonplaceholder.typicode.com/users\u0026#39;); let getPosts = (userId) =\u0026gt; getData(`https://jsonplaceholder.typicode.com/posts?userId=${userId}`); let getComments = (postId) =\u0026gt; getData(`https://jsonplaceholder.typicode.com/comments?postId=${postId}`); 2. Promise Chaining Our goal is to fetch all comments on first post of first user.\nWe are first fetching all users from getUsers promise and chaining it with getPost promise by passing firstUser. Further chaining it with getComments promise by passing firstPost.\n//promise chaining of multiple asynchronous calls getUsers.then(users =\u0026gt; { let firstUser = users[0]; return getPosts(firstUser.id); }).then(posts =\u0026gt; { let firstPost = posts[0]; return getComments(firstPost.id); }).then(comments =\u0026gt; { console.log(comments); }).catch(error =\u0026gt; console.error(error)); Output ▼ (5) [{…}, {…}, {…}, {…}, {…}] ➤ 0: {postId: 1, id: 1, name: \u0026#34;id labore ex et quam laborum\u0026#34;, email: \u0026#34;Eliseo@gardner.biz\u0026#34;, body: \u0026#34;laudantium enim quasi est quidem magnam voluptate …utem quasi↵reiciendis et nam sapiente accusantium\u0026#34;} ➤ 1: {postId: 1, id: 2, name: \u0026#34;quo vero reiciendis velit similique earum\u0026#34;, email: \u0026#34;Jayne_Kuhic@sydney.com\u0026#34;, body: \u0026#34;est natus enim nihil est dolore omnis voluptatem n…iatur↵nihil sint nostrum voluptatem reiciendis et\u0026#34;} ➤ 2: {postId: 1, id: 3, name: \u0026#34;odio adipisci rerum aut animi\u0026#34;, email: \u0026#34;Nikita@garfield.biz\u0026#34;, body: \u0026#34;quia molestiae reprehenderit quasi aspernatur↵aut …mus et vero voluptates excepturi deleniti ratione\u0026#34;} ➤ 3: {postId: 1, id: 4, name: \u0026#34;alias odio sit\u0026#34;, email: \u0026#34;Lew@alysha.tv\u0026#34;, body: \u0026#34;non et atque↵occaecati deserunt quas accusantium u…r itaque dolor↵et qui rerum deleniti ut occaecati\u0026#34;} ➤ 4: {postId: 1, id: 5, name: \u0026#34;vero eaque aliquid doloribus et culpa\u0026#34;, email: \u0026#34;Hayden@althea.biz\u0026#34;, body: \u0026#34;harum non quasi et ratione↵tempore iure ex volupta…ugit inventore cupiditate↵voluptates magni quo et\u0026#34;} length: 5 ➤ __proto__: Array(0) 3. async/await Let\u0026rsquo;s achieve the same goal of fetching comments using async/await,\n//async and await makes code cleaner and readable async function getCommentsOfFirstPostByFirstUser(){ let users = await getUsers; let firstUser = users[0]; let posts = await getPosts(firstUser.id); let firstPost = posts[0]; let comments = await getComments(firstPost.id); return comments; } getCommentsOfFirstPostByFirstUser().then(comments =\u0026gt; console.log(comments)); Output ▼ (5) [{…}, {…}, {…}, {…}, {…}] ➤ 0: {postId: 1, id: 1, name: \u0026#34;id labore ex et quam laborum\u0026#34;, email: \u0026#34;Eliseo@gardner.biz\u0026#34;, body: \u0026#34;laudantium enim quasi est quidem magnam voluptate …utem quasi↵reiciendis et nam sapiente accusantium\u0026#34;} ➤ 1: {postId: 1, id: 2, name: \u0026#34;quo vero reiciendis velit similique earum\u0026#34;, email: \u0026#34;Jayne_Kuhic@sydney.com\u0026#34;, body: \u0026#34;est natus enim nihil est dolore omnis voluptatem n…iatur↵nihil sint nostrum voluptatem reiciendis et\u0026#34;} ➤ 2: {postId: 1, id: 3, name: \u0026#34;odio adipisci rerum aut animi\u0026#34;, email: \u0026#34;Nikita@garfield.biz\u0026#34;, body: \u0026#34;quia molestiae reprehenderit quasi aspernatur↵aut …mus et vero voluptates excepturi deleniti ratione\u0026#34;} ➤ 3: {postId: 1, id: 4, name: \u0026#34;alias odio sit\u0026#34;, email: \u0026#34;Lew@alysha.tv\u0026#34;, body: \u0026#34;non et atque↵occaecati deserunt quas accusantium u…r itaque dolor↵et qui rerum deleniti ut occaecati\u0026#34;} ➤ 4: {postId: 1, id: 5, name: \u0026#34;vero eaque aliquid doloribus et culpa\u0026#34;, email: \u0026#34;Hayden@althea.biz\u0026#34;, body: \u0026#34;harum non quasi et ratione↵tempore iure ex volupta…ugit inventore cupiditate↵voluptates magni quo et\u0026#34;} length: 5 ➤ __proto__: Array(0) Summary We see that async/await are much easier to use as compare to promises.\n","permalink":"https://codingnconcepts.com/javascript/async-await-in-javascript/","tags":["Javascript Interview","Javascript ES6"],"title":"Async/Await in JavaScript"},{"categories":["Javascript"],"contents":"Promises are introduced natively in ES6. They are very similar to our promises. As we keep or break our promises, Javascript promises are also either resolve or reject. In this post we will talk about why we should use promises, promise syntax, promise states and its practical usage with examples using fetch API.\nWhy Promises? In earlier days, you would have used callbacks to handle asynchronous operations. However, callbacks have limited functionality and often leads to unmanageable code if you are handling multiple async calls, it leads to heavily nested callback code which is also known as callback hell.\nPromises were introduced to improve code readability and better handling of async calls and errors.\nPromise Syntax Let\u0026rsquo;s look at the syntax of simple promise object.\nlet promise = new Promise(function (resolve, reject) { // asynchronous call }); Promise takes a callback function as an argument and that callback function takes two arguments — the first is a resolve function, and the second one is a reject function. A promise can either be fulfilled with a value or rejected with a reason (error).\nPromise States A promise object has one of three states:\npending: is the initial state. fulfilled: is success state. resolve() method is called. rejected: is failed state, reject() is called. Usage We generally use async calls using fetch API to get data from the HTTP endpoints. Let\u0026rsquo;s look at the example, how promises can be used in such case.\n//create a promise object let getUsers = new Promise(function (resolve, reject ){ fetch(\u0026#39;https://jsonplaceholder.typicode.com/users\u0026#39;) .then(response =\u0026gt; { return response.json(); }) .then(data =\u0026gt; { resolve(data); }) .catch(error =\u0026gt; { reject(error); }); }); //call promise object getUsers .then((data) =\u0026gt; { console.log(data); }) .catch(error =\u0026gt; { console.log(error); }); Chained Promises Promise chaining comes into play when you have to use output of one async call as input of another async call. You can chain multiple promises in this case.\nLet’s look at the below example where we first fetch users list using getUser async call then chain it with getPosts by passing userId.\n//create a common getData function let getData = (url) =\u0026gt; new Promise(function (resolve, reject ){ fetch(url) .then(response =\u0026gt; { return response.json(); }) .then(data =\u0026gt; { resolve(data); }) .catch(error =\u0026gt; { reject(error); }); }); let getUsers = getData(\u0026#39;https://jsonplaceholder.typicode.com/users\u0026#39;); let getPosts = (userId) =\u0026gt; getData(`https://jsonplaceholder.typicode.com/posts?userId=${userId}`); //chained promises to fetch all posts by first user (userId = 1) getUsers.then((data) =\u0026gt; { const user = data[0]; return getPosts(user.id); }) .then((data) =\u0026gt; { console.log(data); }) .catch(error =\u0026gt; { console.log(error); }); Output ▼ (10) [{…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}] ➤ 0: {userId: 1, id: 1, title: \u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;, body: \u0026#34;quia et suscipit↵suscipit recusandae consequuntur …strum rerum est autem sunt rem eveniet architecto\u0026#34;} ➤ 1: {userId: 1, id: 2, title: \u0026#34;qui est esse\u0026#34;, body: \u0026#34;est rerum tempore vitae↵sequi sint nihil reprehend…aperiam non debitis possimus qui neque nisi nulla\u0026#34;} ➤ 2: {userId: 1, id: 3, title: \u0026#34;ea molestias quasi exercitationem repellat qui ipsa sit aut\u0026#34;, body: \u0026#34;et iusto sed quo iure↵voluptatem occaecati omnis e…↵molestiae porro eius odio et labore et velit aut\u0026#34;} ➤ 3: {userId: 1, id: 4, title: \u0026#34;eum et est occaecati\u0026#34;, body: \u0026#34;ullam et saepe reiciendis voluptatem adipisci↵sit … ipsam iure↵quis sunt voluptatem rerum illo velit\u0026#34;} ➤ 4: {userId: 1, id: 5, title: \u0026#34;nesciunt quas odio\u0026#34;, body: \u0026#34;repudiandae veniam quaerat sunt sed↵alias aut fugi…sse voluptatibus quis↵est aut tenetur dolor neque\u0026#34;} ➤ 5: {userId: 1, id: 6, title: \u0026#34;dolorem eum magni eos aperiam quia\u0026#34;, body: \u0026#34;ut aspernatur corporis harum nihil quis provident …s↵voluptate dolores velit et doloremque molestiae\u0026#34;} ➤ 6: {userId: 1, id: 7, title: \u0026#34;magnam facilis autem\u0026#34;, body: \u0026#34;dolore placeat quibusdam ea quo vitae↵magni quis e…t excepturi ut quia↵sunt ut sequi eos ea sed quas\u0026#34;} ➤ 7: {userId: 1, id: 8, title: \u0026#34;dolorem dolore est ipsam\u0026#34;, body: \u0026#34;dignissimos aperiam dolorem qui eum↵facilis quibus…↵ipsam ut commodi dolor voluptatum modi aut vitae\u0026#34;} ➤ 8: {userId: 1, id: 9, title: \u0026#34;nesciunt iure omnis dolorem tempora et accusantium\u0026#34;, body: \u0026#34;consectetur animi nesciunt iure dolore↵enim quia a…st aut quod aut provident voluptas autem voluptas\u0026#34;} ➤ 9: {userId: 1, id: 10, title: \u0026#34;optio molestias id quia eum\u0026#34;, body: \u0026#34;quo et expedita modi cum officia vel magni↵dolorib…it↵quos veniam quod sed accusamus veritatis error\u0026#34;} length: 10 ➤ __proto__: Array(0) Promise.all() Promise.all() is useful when you want to execute multiple async calls and wait for all of them to finish and get collective output.\nPromise.all() takes an array of promises and return an array of the results in the same sequence of promises. It throws an exception if any one of the async call fails.\nLet’s look at the below example, where we fetch the result of three async calls getUsers, getPosts and getComments all together.\n//create a common getData function let getData = (url) =\u0026gt; new Promise(function (resolve, reject ){ fetch(url) .then(response =\u0026gt; { return response.json(); }) .then(data =\u0026gt; { resolve(data); }) .catch(error =\u0026gt; { reject(error); }); }); //create multiple promises from common getData function let getUsers = getData(\u0026#39;https://jsonplaceholder.typicode.com/users\u0026#39;); let getPosts = getData(\u0026#39;https://jsonplaceholder.typicode.com/posts\u0026#39;); let getComments = getData(\u0026#39;https://jsonplaceholder.typicode.com/comments\u0026#39;); //fetch data to get users, posts and comments collectively Promise.all([getUsers, getPosts, getComments]).then(result =\u0026gt; { console.log(result); }); Output ▼ (3) [Array(10), Array(100), Array(500)] ➤ 0: (10) [{…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}] ➤ 1: (100) [{…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}] ➤ 2: (500) [{…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, …] length: 3 ➤ __proto__: Array(0) Promise.allSettled() Promise.allSettled() is similar to Promise.all() to execute multiple async calls. The only difference between these two,\nPromise.all() is either all resolve or any reject, means if any async call fails, it returns an error. Promise.allSettled() is all settle, means it doesn\u0026rsquo;t return error if any async call fails. It gives the collective output of all successful and failed async calls. Let\u0026rsquo;s look at the below example, where getPostsFails aync call returns 404 error since url endpoint doesn\u0026rsquo;t exist but you still able to fetch data for getUsers and getComments async calls.\n//create a common getData function let getData = (url) =\u0026gt; new Promise(function (resolve, reject ){ fetch(url) .then(response =\u0026gt; { if(response.ok){ return response.json(); }else{ throw `Error ${response.status}`; } }) .then(data =\u0026gt; { resolve(data); }) .catch(error =\u0026gt; { reject(error); }); }); //create multiple promises from common getData function let getUsers = getData(\u0026#39;https://jsonplaceholder.typicode.com/users\u0026#39;); let getPostsFails = getData(\u0026#39;https://jsonplaceholder.typicode.com/postsfailes\u0026#39;); let getComments = getData(\u0026#39;https://jsonplaceholder.typicode.com/comments\u0026#39;); //fetch data to get users, posts and comments collectively regardless of any error Promise.allSettled([getUsers, getPostsFails, getComments]).then(result =\u0026gt; { console.log(result); }); Output ▼ (3) [{…}, {…}, {…}] ➤ 0: {status: \u0026#34;fulfilled\u0026#34;, value: Array(10)} ➤ 1: {status: \u0026#34;rejected\u0026#34;, reason: \u0026#34;Error 404\u0026#34;} ➤ 2: {status: \u0026#34;fulfilled\u0026#34;, value: Array(500)} length: 3 ➤ __proto__: Array(0) Promise.race() Promise.race() is useful when you are interested in fetching data from any one of the async call out of multiple async calls, whichever resolve first.\nLook at the below example where we are interested in any data out of getTodos, getUsers and getComments, whichever resolves first. In our case getUsers resolved first and returned user list.\nPlease note that if you execute the same code snippet again and again, result might differ based on network connectivity and which one resolve first.\n//create a common getData function let getData = (url) =\u0026gt; new Promise(function (resolve, reject ){ fetch(url) .then(response =\u0026gt; { return response.json(); }) .then(data =\u0026gt; { resolve(data); }) .catch(error =\u0026gt; { reject(error); }); }); //create multiple promises from common getData function let getTodos = getData(\u0026#39;https://jsonplaceholder.typicode.com/todos\u0026#39;); let getUsers = getData(\u0026#39;https://jsonplaceholder.typicode.com/users\u0026#39;); let getComments = getData(\u0026#39;https://jsonplaceholder.typicode.com/comments\u0026#39;); //fetch either todos or users or comments whichever resolves first Promise.race([getTodos, getUsers, getComments]).then(result =\u0026gt; { console.log(result); }); Output ▼ (10) [{…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}] ➤ 0: {id: 1, name: \u0026#34;Leanne Graham\u0026#34;, username: \u0026#34;Bret\u0026#34;, email: \u0026#34;Sincere@april.biz\u0026#34;, address: {…}, …} ➤ 1: {id: 2, name: \u0026#34;Ervin Howell\u0026#34;, username: \u0026#34;Antonette\u0026#34;, email: \u0026#34;Shanna@melissa.tv\u0026#34;, address: {…}, …} ➤ 2: {id: 3, name: \u0026#34;Clementine Bauch\u0026#34;, username: \u0026#34;Samantha\u0026#34;, email: \u0026#34;Nathan@yesenia.net\u0026#34;, address: {…}, …} ➤ 3: {id: 4, name: \u0026#34;Patricia Lebsack\u0026#34;, username: \u0026#34;Karianne\u0026#34;, email: \u0026#34;Julianne.OConner@kory.org\u0026#34;, address: {…}, …} ➤ 4: {id: 5, name: \u0026#34;Chelsey Dietrich\u0026#34;, username: \u0026#34;Kamren\u0026#34;, email: \u0026#34;Lucio_Hettinger@annie.ca\u0026#34;, address: {…}, …} ➤ 5: {id: 6, name: \u0026#34;Mrs. Dennis Schulist\u0026#34;, username: \u0026#34;Leopoldo_Corkery\u0026#34;, email: \u0026#34;Karley_Dach@jasper.info\u0026#34;, address: {…}, …} ➤ 6: {id: 7, name: \u0026#34;Kurtis Weissnat\u0026#34;, username: \u0026#34;Elwyn.Skiles\u0026#34;, email: \u0026#34;Telly.Hoeger@billy.biz\u0026#34;, address: {…}, …} ➤ 7: {id: 8, name: \u0026#34;Nicholas Runolfsdottir V\u0026#34;, username: \u0026#34;Maxime_Nienow\u0026#34;, email: \u0026#34;Sherwood@rosamond.me\u0026#34;, address: {…}, …} ➤ 8: {id: 9, name: \u0026#34;Glenna Reichert\u0026#34;, username: \u0026#34;Delphine\u0026#34;, email: \u0026#34;Chaim_McDermott@dana.io\u0026#34;, address: {…}, …} ➤ 9: {id: 10, name: \u0026#34;Clementina DuBuque\u0026#34;, username: \u0026#34;Moriah.Stanton\u0026#34;, email: \u0026#34;Rey.Padberg@karina.biz\u0026#34;, address: {…}, …} length: 10 ➤ __proto__: Array(0) Summary It is always better to use Promises over callback functions as Promises has a lot to offer in terms of,\nResolve a successful anyc call response using Promise.resolve(response) Reject async call response based on status or data using Promise.reject(response) Better error handling using Promise.catch(onRejection) Chaining of multiple async calls using Promise.then(onFulfillment, onRejection) Get collective result of multiple async calls using Promise.all([promise1, promise2, ...]) Get collective result of multiple async calls regardless of any error using Promise.allSettled([promise1, promise2, ...]) Get any one result whichever resolves first out of many async calls using Promise.race([promise1, promise2, ...]) ","permalink":"https://codingnconcepts.com/javascript/promises-in-javascript/","tags":["Javascript Interview","Javascript ES6"],"title":"Promises in JavaScript"},{"categories":["Interview Questions","Javascript"],"contents":"This post is a collection of tricky interview questions based on different concepts in JavaScript. The difficulty level of question will increase as you read forward down the line in this post.\nNumber operators. Guess the output? console.log( 2 + \u0026#34;2\u0026#34; ); console.log( \u0026#34;2\u0026#34; + \u0026#34;2\u0026#34; ); console.log( 2 - \u0026#34;2\u0026#34; ); console.log( \u0026#34;2\u0026#34; - \u0026#34;2\u0026#34; ); console.log( \u0026#34;A\u0026#34; - \u0026#34;A\u0026#34; ); Think for a while before looking at the output.\nOutput 22 22 0 0 NaN Here is the explanation,\nJavascript + operator behaves as,\nnumber operator when both operands are number concat operator if any one or both of the operands are string. Javascript - operator always behaves as number operator. if any one or both of the operands are string, Javascript attempts to convert it to a number, if not able to convert then return NaN.\nRelational Operators. Guess the output ? console.log( 10 \u0026lt; 20 \u0026lt; 30 ); console.log( 30 \u0026gt; 20 \u0026gt; 10 ); Have you guessed true in both the cases ? No, its not correct.\nOutput true false The output is true and false. Here is the explanation,\nIn Javascript relational operators are evaluated from left to right, false equals 0, and true equals 1 for number comparisons.\nSo comparison evaluation is something like this,\n10 \u0026lt; 20 \u0026lt; 30 =\u0026gt; true \u0026lt; 30 =\u0026gt; 1 \u0026lt; 30 =\u0026gt; true 30 \u0026gt; 20 \u0026gt; 10 =\u0026gt; true \u0026gt; 10 =\u0026gt; 1 \u0026gt; 10 =\u0026gt; false Comparison Operators. Guess the output ? console.log( null == 0 ); console.log( null \u0026gt; 0 ); console.log( null \u0026gt;= 0 ); You might have guessed false in all the three cases but its not correct.\nOutput false false true In Javascript, equality check == and comparisons \u0026gt; \u0026lt; \u0026gt;= \u0026lt;= behaves differently.\nFor equality comparison,\nnull and undefined are comparable. 0, false and \u0026quot;\u0026quot; (empty string) are comparable. null and 0 are not comparable so null == 0 returns false. For number comparison,\nif one operand is number, it attempts to convert other operand to number. null becomes 0 and undefined becomes NaN for number comparison so null \u0026gt; 0 returns false whereas null \u0026gt;= 0 returns true. forEach loop. Guess the output? const elements = [1, 2, 3, 4, 5]; elements.forEach(element =\u0026gt; { console.log(element); if(element == 2){ return; //break; //continue; } }) Have you guessed it will print 1 and 2 ? Let\u0026rsquo;s look at the output,\nOutput 1 2 3 4 5 Yes, its confusing in Javascript. The reason is that we are passing a callback function in forEach loop which will be executed for each element no matter if we return.\nIf you use break or continue instead of return, you get either one of the error since these are not applicable for a callback function:\nUncaught SyntaxError: Illegal break statement Uncaught SyntaxError: Illegal continue statement From Official MDN docs:\nThere is no way to stop or break a forEach loop other than throwing an exception. If you need such behavior, the forEach method is the wrong tool. Early termination may be accomplished with:\nA simple for loop A for...of / for...in loops Variable hoisting. Guess the output? var foo = 1; function myFun(){ console.log(foo); var foo = 2; } myFun(); Have you guessed foo = 1 or foo = 2?\nIt\u0026rsquo;s neither one of them. It will print undefined. Here is the explanation,\nHoisting is a JavaScript mechanism where variables and function declarations are moved to the top of their scope before code execution.\nLet’s see how it is interpreted by the compiler in hoisting process,\nvar foo; foo = 1; function myFun(){ var foo; // var hoisted and initialized with undefined console.log(foo); // undefined foo = 2; } myFun(); Closure. Guess the output? for (var i = 0; i \u0026lt; 3; i++) { setTimeout(function() { console.log(i); }, i*1000); } Have you guessed it will print 0, 1, 2 after every 1 second ? Let\u0026rsquo;s look at the output,\nOutput 3 3 3 Actually it will print 3, 3, 3 after every 1 second. This is because of JavaScript Closure. Here is the explanation,\nclosure A JavaScript closure is when an inner function has access to its outer function\u0026rsquo;s scope. In the following line of code:\nsetTimeout(function() { console.log(i); }, i*1000); variable i is used in an inner function whereas it is actually declared in outer for loop. Inner function will be able to access the value of i through Closure.\nhoisting In Javascript hoisting process, declaration of i will be moved to the top of their scope, since i is defined using var in for loop, declaration of i will be moved to global scope in hoisting process.\nAfter three iteration of for loop, value of global scoped variable i will be 3. All three closures will refer to this same i variable from global scope.\nlet if i were defined using let instead of var in for loop, output would have been different. Why?\nlet is block scoped as opposed to var which is function scoped. Since for loop is also a block, value of i in each iteration is block scoped within that iteration and each closure has its own copy of i variable. Let\u0026rsquo;s look at the code,\nfor (let i = 0; i \u0026lt; 3; i++) { setTimeout(function() { console.log(i); }, i*1000); } Output 0 1 2 this keyword. Guess the output? var a = new Person(\u0026#34;a\u0026#34;); var b = Person var c = Person(\u0026#34;c\u0026#34;); function Person(fname) { this.fname = fname; } console.log(\u0026#34;1.\u0026#34;, fname); console.log(\u0026#34;2.\u0026#34;, a.fname); console.log(\u0026#34;3.\u0026#34;, b.fname); console.log(\u0026#34;4.\u0026#34;, c.fname); Think for a while before looking at the output.\nOutput 1. c 2. a 3. undefined Uncaught TypeError: Cannot read property \u0026#39;fname\u0026#39; of undefined Here is the explanation,\nfname will print c. When you execute Person(\u0026quot;c\u0026quot;) function, this refers to global object window and this.fname is assigned value as c. a.fname will print a. When you execute function using new keyword new Person(\u0026quot;a\u0026quot;), this refers to newly created object. b.fname will print undefined since you are just assigning a function object and not executing it. Person object is not having property named as fname function Person(\u0026quot;c\u0026quot;) doesn\u0026rsquo;t return anything so c is undefined and c.fname will throw error Also read this post to understand all about this keyword with examples\n","permalink":"https://codingnconcepts.com/tricky-javascript-interview-questions/","tags":["Interview Q\u0026A","JavaScript Q\u0026A"],"title":"Tricky Javascript Interview Questions"},{"categories":["Javascript"],"contents":"This is frequently asked question in JavaScript interview. We can compare primitive types, array and object using two comparison operators == and === available in JavaScript. This post describes the difference between these two with many examples.\n\u0026ldquo;==\u0026rdquo; operator == is also known as abstract comparison operator which compares only content of operand and not type. == attempts to convert the operands to compatible type before comparison. String and number with same content are equal. == converts the string to number before comparison. 0, false and empty string \u0026quot;\u0026quot; are comparable and equal. null and undefined are comparable and equal. Two arrays with exactly same elements are not equal because both refers to different object in memory. Similarly two objects with exactly same properties are not equals because both refers to different object in memory. \u0026ldquo;===\u0026rdquo; operator === is also known as strict comparison operator which compares both content and its type. === does not attempt to convert the operand to compatible types before comparison and compare directly. It has better performance compare to == Two strings are strictly equal when they have the same sequence of characters, same length, and same characters in corresponding positions. Two numbers are strictly equal when they are numerically equal (have the same number value). NaN is not equal to anything, including NaN. Confusing in JavaScript. Positive zero (+0) and negative zero (-0) are strictly equal. Confusing in JavaScript since content is different. Two boolean values are strictly equal only if both either true or false. Two objects are strictly equal only if they refer to the same Object. null only strictly equals to null undefined only strictly equals to undefined Compare \u0026ldquo;==\u0026rdquo; and \u0026ldquo;===\u0026rdquo; console.log(\u0026#34;cnc\u0026#34; == \u0026#34;cnc\u0026#34;); // true, same content and type console.log(\u0026#34;cnc\u0026#34; === \u0026#34;cnc\u0026#34;); // true, same content and type console.log(12345 == 12345); // true, same content and type console.log(12345 === 12345); // true, same content and type console.log(false == false); // true, same content and type console.log(false === false); // true, same content and type console.log(12345 == \u0026#34;12345\u0026#34;); // true, comparable and equal console.log(12345 === \u0026#34;12345\u0026#34;); // false, different type console.log(0 == false); // true, comparable and equal console.log(0 === false); // false, different type console.log(\u0026#34;\u0026#34; == false); // true, comparable and equal console.log(\u0026#34;\u0026#34; === false); // false, different type console.log(null == undefined); // true, comparable and equal console.log(null === undefined); // false, different type console.log([] == []); // false, both refers to different object in memory console.log([] === []); // false, both refers to different object in memory console.log([1, 2] == [1, 2]); // false, both refers to different object in memory console.log([1, 2] === [1, 2]); // false, both refers to different object in memory console.log({} == {}); // false, both refers to different object in memory console.log({} === {}); // false, both refers to different object in memory var array1 = [1, 2, 3, 4, 5]; var array2 = array1; console.log(array1 == array2); // true, both refers to same array console.log(array1 === array2); // true, both refers to same array var obj1 = { app : \u0026#34;cnc\u0026#34;}; var obj2 = obj1; console.log(obj1 == obj2); // true, both refers to same object console.log(obj1 === obj2); // true, both refers to same object console.log(+0 == -0); // true, confusing, content is different console.log(+0 === -0); // true, confusing, content is different console.log(NaN == NaN); // false, confusing, content and type is same console.log(NaN === NaN); // false, confusing, content and type is same The last two comparisons are very confusing in Javascript, Remember always\nNaN is not equal to NaN +0 is equal to -0 Also remember, === is recommended to use for comparison where ever possible as it is faster then == in terms of performance.\nObject.is() ES6 has introduced a new method Object.is() to compare two values, let\u0026rsquo;s check it out\nconsole.log(Object.is(\u0026#34;cnc\u0026#34;, \u0026#34;cnc\u0026#34;)); // true, same content and type console.log(Object.is(12345, 12345)); // true, same content and type console.log(Object.is(false, false)); // true, same content and type console.log(Object.is(12345, \u0026#34;12345\u0026#34;)); // false, different type console.log(Object.is(0, false)); // false, different type console.log(Object.is(\u0026#34;\u0026#34;, false)); // false, different type console.log(Object.is(null, undefined)); // false, different type console.log(Object.is([], [])); // false, both refers to different object in memory console.log(Object.is([1, 2], [1, 2])); // false, both refers to different object in memory console.log(Object.is({}, {})); // false, both refers to different object in memory var array1 = [1, 2, 3, 4, 5]; var array2 = array1; console.log(Object.is(array1, array2)); // true, both refers to same array var obj1 = { app : \u0026#34;cnc\u0026#34;}; var obj2 = obj1; console.log(Object.is(obj1, obj2)); // true, both refers to same object console.log(Object.is(+0, -0)); // false, es6 is good, different content console.log(Object.is(NaN, NaN)); // true, es6 is good, same content and type You see that Object.is() makes the NaN and +0/-0 comparison less confusing.\nSummary Where == compares only content, === compares both content and type of operands. Strict comparison operator === does not attempt to convert the operands to compatible types and provide better performance as compare to abstract comparison operator ==.\nLast, Object.is() introduced in ES6 works very much similar to strict operator === and also avoid confusion of NaN comparisons and +0/-0 comparisons. It is recommended to use Object.is().\n","permalink":"https://codingnconcepts.com/javascript/comparison-operators-in-javascript/","tags":["Javascript Interview","Javascript Operator"],"title":"Difference in == and === comparison operators in JavaScript"},{"categories":["Javascript"],"contents":"The this keyword in JavaScript is very important concept but at the same time very confusing to understand.\nIn JavaScript, this keyword refers to the object it belongs to. It has different values depending on where it is used:\nIn a method, this refers to the owner object where method is defined. Alone, this refers to the global object. In a function, this refers to the global object. In a function, in strict mode, this is undefined. When a function called with new keyword, this refers to new object instance In a DOM event, this refers to the element that received the event. Function prototype methods call(), apply() and bind() can be used to refer this to any object. this in a Method When you execute a method of an object, this refers to the object where method is defined.\nIn the below example this refers to the person object since fullName() method is defined inside person object.\nvar person = { firstName: \u0026#34;John\u0026#34;, lastName : \u0026#34;Doe\u0026#34;, fullName : function() { return this.firstName + \u0026#34; \u0026#34; + this.lastName; } }; console.log(person.fullName()); // John Doe Let\u0026rsquo;s see one more example below where obj1 and obj2 are executing their own increment() methods so here this refers to obj1 and obj2 respectively\nvar increment = function(){ return console.log(this.a + 1); } var obj1 = { a: 1, increment: increment }; var obj2 = { a: 2, increment: increment }; obj1.increment(); //this = obj1 obj2.increment(); //this = obj2 Output 2 3 this Alone When you use this alone, it refers to global object\nLet\u0026rsquo;s try what is the value of this when used alone:\nconsole.log(this); //this = window Output Window {parent: Window, opener: null, top: Window, length: 4, frames: Window, …} We see that this refers to Window object, which is global object of browser.\nRemember: In strict mode, when used alone, this also refers to the global object.\nthis in a Function (default) For a function (which is at top level or not inside any function), this refers to global object.\nfunction topLevelFunction(){ console.log(this); } topLevelFunction(); //this = window In the above case this refers to the global object window of browser\nthis in a Function (strict) However, for a function (which is at top level or not inside any function) in strict mode, this refers to undefined\nLook at the below example where this refers to undefined in strict mode function:\n\u0026#39;use strict\u0026#39;; function topLevelFunction(){ console.log(this); } topLevelFunction(); //this = undefined one more example, where increment() function is called in strict mode, this refers to undefined and throw error.\nvar a = 1; var increment = function(){ \u0026#39;use strict\u0026#39;; return console.log(this.a + 1); } increment(); //this = undefined Output Uncaught TypeError: Cannot read property \u0026#39;a\u0026#39; of undefined this with new When a function is called using new keyword, then the function is known as a constructor function and returns a new object. In this case, this refers to a newly created object.\nvar Person = function(firstName, lastName){ this.firstName = firstName; this.lastName = lastName; this.getFullName = function(){ console.log(`${this.firstName} ${this.lastName}`); } } let person1 = new Person(\u0026#34;Albert\u0026#34;, \u0026#34;Einstein\u0026#34;); let person2 = new Person(\u0026#34;Isaac\u0026#34;, \u0026#34;Newton\u0026#34;); person1.getFullName(); //this = person1 person2.getFullName(); //this = person2 Output Albert Einstein Isaac Newton this with Explicit binding Function has call(), apply() and bind() prototype methods which can be called on function to change the context of this explictly.\nLet\u0026rsquo;s look at these methods one by one and also see the differences\ncall() ƒ.call(this, arg1, arg2, ...) When a function is called using call() method,\nfirst argument is referred by this subsequent comma separated arguments are method arguments Remember: “call() arguments are separated by commas”.\napply() ƒ.apply(this, [arg1, arg2, ...]) When a function is called using apply() method,\nfirst argument is referred by this second argument is an array of values, are method arguments Remember: “apply() accepts arguments as an Array”\nbind() ƒ.bind(this) When a function is called using bind() method\nargument passed to bind() function is referred by this returns new bind function whose context is passed argument Remember: “bind() method doesn\u0026rsquo;t call the function. It returns a new function which can be called later.\nLet\u0026rsquo;s look at the example, how to use these three function prototype methods call(), apply() and bind()\nlet numObj1 = {num: 1}; let numObj2 = {num: 2}; let sumFn = function(...args){ console.log(this.num + args.reduce((a,b)=\u0026gt; a+b, 0)); } sumFn.call(numObj1, 1, 2, 3, 4); //this = numObj1 sumFn.call(numObj2, 1, 2, 3, 4); //this = numObj2 sumFn.apply(numObj1, [1,2,3,4]); //this = numObj1 sumFn.apply(numObj2, [1,2,3,4]); //this = numObj2 let sumBindFn1 = sumFn.bind(numObj1); // return Fn let sumBindFn2 = sumFn.bind(numObj2); // return Fn sumBindFn1(1, 2, 3, 4); //this = numObj1 sumBindFn2(1, 2, 3, 4); //this = numObj2 output: 11 12 11 12 11 12 ","permalink":"https://codingnconcepts.com/javascript/this-keyword-in-javascript/","tags":["Javascript Interview","Javascript Core"],"title":"Understand all about this keyword in JavaScript"},{"categories":["Javascript"],"contents":"In this tutorial, we\u0026rsquo;ll learn how to create classes in JavaScript.\nClass Declaration Let\u0026rsquo;s look at the example of creating class in JavaScript using function constructor and class keyword:\n// ES5 Function Constructor function Car(brand, color, price) { this.brand = brand; this.color = color; this.price = price; } // ES6 Class class Car { constructor(brand, color, price) { this.brand = brand; this.color = color; this.price = price; } } Please note that class is a type of function, so we use that to replace function. In that sense, both ways of creating class are pretty much same.\nWe can make our code even more shorter like this:\nclass Car { constructor(brand, color, price) { Object.assign(this, { brand, color, price}); } } Methods Let\u0026rsquo;s add some methods to our Car class\nGetter Setter method (instance method) is called from the instance of the class. They are defined using get and set keywords to get and set properties respectively. Prototype method (instance method) is called from the instance of the class. They are used to access instance properties and perform some operations on them. Static method (class method) is called directly from the class. They are defined using static keyword and often used to create utility functions. class Car { constructor(brand, color, price) { this._brand = brand; this._color = color; this._price = price; } // getter method get color(){ return `color is ${this._color.toUpperCase()}`; } // setter method set color(newColor){ this._color = newColor; } // prototype method drive(){ return `driving ${this._brand} ${this._color} color car`; } // static method static compareCars(car1, car2){ return `${car2._brand} is ${(car1._price \u0026gt; car2._price) ? \u0026#34;cheaper\u0026#34; : \u0026#34;costlier\u0026#34;} then ${car1._brand}` } } Examples Let\u0026rsquo;s create some objects using Car class and call their getter, setter, prototype and static methods\nlet redToyotaCar = new Car(\u0026#34;Toyota\u0026#34;, \u0026#34;red\u0026#34;, 500000); console.log(redToyotaCar); // prints Car {_brand: \u0026#34;Toyota\u0026#34;, _color: \u0026#34;red\u0026#34;, _price: 500000} console.log(redToyotaCar.color); // (getter method) // prints \u0026#39;color is RED\u0026#39; console.log(redToyotaCar.drive()); // (prototype method) // prints \u0026#39;driving Toyota red color car\u0026#39; redToyotaCar.color = \u0026#34;blue\u0026#34;; // (setter method) // set color to blue console.log(redToyotaCar.color); // (getter method) // prints \u0026#39;color is BLUE\u0026#39; console.log(redToyotaCar.drive()); // (prototype method) // prints \u0026#39;driving Toyota blue color car\u0026#39; let blackAudiCar = new Car(\u0026#34;Audi\u0026#34;, \u0026#34;black\u0026#34;, 900000); console.log(Car.compareCars(redToyotaCar, blackAudiCar)); // (static method) // prints \u0026#39;Audi is costlier then Toyota\u0026#39; More about Get and Set In the class Car, we created get and set methods with name color:-\n// getter method get color(){ return `color is ${this._color.toUpperCase()}`; } // setter method set color(newColor){ this._color = newColor; } and we call these getter and setter methods using this.color:-\nconsole.log(this.color); // (call getter method) this.color = \u0026#34;blue\u0026#34;; // (call setter method) You also see that we have created a property _color, which is initialized inside constructor. Let\u0026rsquo;s see the difference between these two:-\nthis.color is used to access getter and setter methods this._color is used to access _color property which is initialized inside constructor. If we use the same color property in constructor as well, then it will look something like this:-\nclass Car { constructor(brand, color, price) { this.brand = brand; this.color = color; this.price = price; } // getter method get color(){ return `color is ${this.color.toUpperCase()}`; //${this.color.toUpperCase()} will call get method again, cause recursive loop } // setter method set color(newColor){ this.color = newColor; // this.color will call get method again, cause recursive loop } } In such case, when you call getter or setter methods using this.color and since we are accessing this.color again inside these methods, getter method will be called recursively and cause stack overflow:-\nVM172:12 Uncaught RangeError: Maximum call stack size exceeded at Car.set color [as color] (\u0026lt;anonymous\u0026gt;:12:16) at Car.set color [as color] (\u0026lt;anonymous\u0026gt;:12:16) at Car.set color [as color] (\u0026lt;anonymous\u0026gt;:12:16) at Car.set color [as color] (\u0026lt;anonymous\u0026gt;:12:16) at Car.set color [as color] (\u0026lt;anonymous\u0026gt;:12:16) at Car.set color [as color] (\u0026lt;anonymous\u0026gt;:12:16) at Car.set color [as color] (\u0026lt;anonymous\u0026gt;:12:16) at Car.set color [as color] (\u0026lt;anonymous\u0026gt;:12:16) at Car.set color [as color] (\u0026lt;anonymous\u0026gt;:12:16) at Car.set color [as color] (\u0026lt;anonymous\u0026gt;:12:16) To avoid this, make sure you are not accessing the property with same name as getter or setter method name. You can follow any naming convention, one of the example is to add get and set suffix in getter setter method names:-\nclass Car { constructor(brand, color, price) { this.brand = brand; this.color = color; this.price = price; } // getter method get getColor(){ return `color is ${this.color.toUpperCase()}`; } // setter method set setColor(newColor){ this.color = newColor; } } Inheritance Let\u0026rsquo;s say we want to create a Toyota subclass from Car class and add some additional fields like \u0026ldquo;model\u0026rdquo; and \u0026ldquo;make\u0026rdquo;.\nclass Toyota extends Car { constructor(color, price, model, make){ super(\u0026#34;Toyota\u0026#34;, color, price); Object.assign(this, {model, make}); } drive(){ return `${super.drive()} made in ${this.make}`; } } Let\u0026rsquo;s create some objects from Toyota subclass\nlet toyotaCamery = new Toyota(\u0026#34;red\u0026#34;, 800000, \u0026#34;Camary\u0026#34;, 2010); console.log(toyotaCamery); // prints Toyota {_brand: \u0026#34;Toyota\u0026#34;, _color: \u0026#34;red\u0026#34;, _price: 800000, model: \u0026#34;Camary\u0026#34;, make: 2010} console.log(toyotaCamery.color); // prints \u0026#39;color is RED\u0026#39; console.log(toyotaCamery.drive()); // prints \u0026#39;driving Toyota red color car made in 2010\u0026#39; We see that creating a subclass using ES6 class keyword is quite handy and easy.\n","permalink":"https://codingnconcepts.com/javascript/classes-in-javascript/","tags":["Javascript Interview","Javascript ES6"],"title":"Classes in JavaScript"},{"categories":["Javascript"],"contents":"This is frequently asked question in JavaScript interview. This post describes the Function prototype methods call(), apply() and bind() with their syntax, usage and practical examples.\nWhat is Function.prototype? First of all we need to understand that all these three functions call, apply and bind are prototype of Function. What does that mean?\nLet\u0026rsquo;s print Function\u0026rsquo;s structure to understand it.\nconsole.dir(Function); ▼ ƒ Function() arguments: (...) caller: (...) length: 1 name: \u0026#34;Function\u0026#34; ▼ prototype: ƒ () arguments: (...) caller: (...) length: 0 name: \u0026#34;\u0026#34; ➤ constructor: ƒ Function() ➤ apply: ƒ apply() ➤ bind: ƒ bind() ➤ call: ƒ call() ➤ toString: ƒ toString() ➤ Symbol(Symbol.hasInstance): ƒ [Symbol.hasInstance]() ➤ get arguments: ƒ () ➤ set arguments: ƒ () ➤ get caller: ƒ () ➤ set caller: ƒ () You can see apply, bind and call are prototype functions of Function, that means you can use these three functions on any new function you define.\nWhen to use call(), bind() and apply() When you define a new function and call it from somewhere in your code, it is executed in a context. This context is called this and it refers to an object. This object can vary depending upon how you called a function.\nvar a = 1; var increment = function(){ return console.log(this.a + 1); } var obj1 = { a: 2, increment: increment }; var obj2 = { a: 3, increment: increment }; increment(); //this = window obj1.increment(); //this = obj1 obj2.increment(); //this = obj2 Output 2 3 4 We can see in above code snippet that we called same increment() function in three different context which changes the this reference and resulting into three different output.\nHere,\nfirst increment() function is called in global context where this binds to global object. window is the global object if you are working in browser. next two increment() functions are called in implicit context where this binds to the object on which function is called. This is called implicit context because function is tightly coupled with object (defined inside object) and this reference binds to object at compile time when you call a function from object like obj.ƒn() Sometime we may want to call a function in explicit context where we can control on this reference at runtime when calling a function. This is where call(), apply() and bind() comes into play. Let\u0026rsquo;s call the same increment() function using call(), apply() and bind() methods.\nvar increment = function(){ return console.log(this.a + 1); } var obj1 = { a: 4 }; var obj2 = { a: 5 }; increment.call(obj1); //this = obj1 increment.call(obj2); //this = obj2 increment.apply(obj1); //this = obj1 increment.apply(obj2); //this = obj2 var bindFn1 = increment.bind(obj1); var bindFn2 = increment.bind(obj2); bindFn1(); //this = obj1 bindFn2(); //this = obj2 Output 5 6 5 6 5 6 Few points to note from above code snippet:-\nAll these three call(), apply() and bind() are prototype of Function so you are able to use them on any function increment.call(), increment.apply() and increment.bind(). All these three call(), apply() and bind() providing different object context (obj1, obj2) at runtime and resulting into different output. Where call() and apply() execute a function immediately, bind() returns a bound function which can be executed later. Now that we have understood the basic usage of these prototype functions, let\u0026rsquo;s look at their syntax and practical usage with example.\ncall() Syntax functionName.call(thisArg, arg1, arg2, ...) When a function is called using call(),\nthis refers to thisArg Comma separated arguments arg1, arg2 ... are the arguments of function Remember: “call() arguments are separated by commas”.\nPractical Usage 1. Borrow functionality of other objects In Javascript, every object can have prototype functions which are actually meant to be executed only on that object. Using call() methods, you can borrow the functionality of these objects and execute it on different object.\nLet\u0026rsquo;s look at the Array prototype functions\nconsole.dir(Array); ▼ ƒ Array() arguments: (...) caller: (...) length: 1 name: \u0026#34;Array\u0026#34; ▼ prototype: ƒ () length: 0 ➤ constructor: ƒ Array() ➤ concat: ƒ concat() ➤ copyWithin: ƒ copyWithin() ➤ fill: ƒ fill() ➤ find: ƒ find() ➤ findIndex: ƒ findIndex() ➤ lastIndexOf: ƒ lastIndexOf() ➤ pop: ƒ pop() ➤ push: ƒ push() ➤ reverse: ƒ reverse() ➤ shift: ƒ shift() ➤ unshift: ƒ unshift() ➤ slice: ƒ slice() ➤ sort: ƒ sort() ➤ splice: ƒ splice() ➤ includes: ƒ includes() ➤ indexOf: ƒ indexOf() ➤ join: ƒ join() ➤ keys: ƒ keys() ➤ entries: ƒ entries() ➤ values: ƒ values() ➤ forEach: ƒ forEach() ➤ filter: ƒ filter() ➤ flat: ƒ flat() ➤ flatMap: ƒ flatMap() ➤ map: ƒ map() ➤ every: ƒ every() ➤ some: ƒ some() ➤ reduce: ƒ reduce() ➤ reduceRight: ƒ reduceRight() ➤ toLocaleString: ƒ toLocaleString() ➤ toString: ƒ toString() Let\u0026rsquo;s borrow the functionality of Array prototype methods using call() function:-\nArray.prototype.concat.call([1,2,3], [4,5]); Array.prototype.join.call([1,2,3,4,5], \u0026#34;:\u0026#34;) Output [1, 2, 3, 4, 5] 1:2:3:4:5 Similarly borrow the start functionality of car object to use it for aircraft\nvar car = { name: \u0026#39;car\u0026#39;, start: function() { console.log(\u0026#39;Start the \u0026#39; + this.name); }, speedup: function() { console.log(\u0026#39;Speed up the \u0026#39; + this.name) }, stop: function() { console.log(\u0026#39;Stop the \u0026#39; + this.name); } }; var aircraft = { name: \u0026#39;aircraft\u0026#39;, fly: function(){ console.log(\u0026#39;Fly\u0026#39;); } }; car.start.call(aircraft); Output Start the aircraft 2. Chaining constructors function Box(height, width) { this.height = height; this.width = width; } function Widget(height, width, color) { Box.call(this, height, width); this.color = color; } function Dialog(height, width, color, title) { Widget.call(this, height, width, color); this.title = title; } var dialog = new Dialog(\u0026#39;red\u0026#39;, 100, 200, \u0026#39;Title\u0026#39;); We can see in above code snippet how we can chain the constructors by calling parent constructor function in current this context\napply() Syntax functionName.apply(thisArg, [arg1, arg2, ...]) When a function is called using apply() method,\nthis refers to thisArg second argument is an array of values [arg1, arg2, ...], are the arguments of function Remember: “apply() accepts arguments as an Array”\nPractical Usage Practical usage of apply() function is same as call() function. The only difference between them, apply() accepts args as an array whereas call() accepts args as comma separated values.\nWe can pass arguments as an array using apply() function to call any function which accepts args as comma separated values.\nMath.min(1, 2, 3, 4, 5); //args as comma separated Math.min.apply([1, 2, 3, 4, 5]); //args as an array bind() Syntax functionName.bind(thisArg) When a function is called using bind() method,\nthis refers to thisArg returns new bound function which can be called later Remember: “bind() doesn\u0026rsquo;t call the function immediately. It returns a new bound function which can be called later.\nPractical Usage 1. Bounded context var Button = function(content) { this.content = content; }; Button.prototype.click = function() { console.log(this.content + \u0026#39; clicked\u0026#39;); }; var myButton = new Button(\u0026#39;OK\u0026#39;); myButton.click(); var looseClick = myButton.click; looseClick(); // not bound, \u0026#39;this\u0026#39; is not myButton - it is the global object var boundClick = myButton.click.bind(myButton); boundClick(); // bound, \u0026#39;this\u0026#39; is myButton Output OK clicked undefined clicked OK clicked 2. Binding functions with parameters var logProp = function(prop) { console.log(this[prop]); }; var Obj = { x : 5, y : 10 }; Obj.log = logProp.bind(Obj); Obj.logX = logProp.bind(Obj, \u0026#39;x\u0026#39;); //binding with prop x Obj.logY = logProp.bind(Obj, \u0026#39;y\u0026#39;); //binding with prop y Obj.log(\u0026#39;x\u0026#39;); Obj.logX(); Obj.log(\u0026#39;y\u0026#39;); Obj.logY(); Output 5 5 10 10 Another example,\nvar sum = function(a, b) { return a + b; }; var add5 = sum.bind(null, 5); //add5 is binding function with a = 5 console.log(add5(10)); //b =10 Output 15 Call vs Bind vs Apply Comparison between function objects, function calls, call, apply and bind:\ntime of function execution time of this binding function object ƒ future future function call ƒ() now now ƒ.call() now now ƒ.apply() now now ƒ.bind() future now Let\u0026rsquo;s look at last example of this post and use call(), apply() and bind() methods all together\nlet numObj1 = {num: 1}; let numObj2 = {num: 2}; let sumFn = function(...args){ console.log(this.num + args.reduce((a,b)=\u0026gt; a+b, 0)); } sumFn.call(numObj1, 1, 2, 3, 4); //this = numObj1 sumFn.call(numObj2, 1, 2, 3, 4); //this = numObj2 sumFn.apply(numObj1, [1,2,3,4]); //this = numObj1 sumFn.apply(numObj2, [1,2,3,4]); //this = numObj2 let sumBindFn1 = sumFn.bind(numObj1); // return Fn let sumBindFn2 = sumFn.bind(numObj2); // return Fn sumBindFn1(1, 2, 3, 4); //this = numObj1 sumBindFn2(1, 2, 3, 4); //this = numObj2 output: 11 12 11 12 11 12 ","permalink":"https://codingnconcepts.com/javascript/call-vs-bind-vs-apply/","tags":["Javascript Interview","Javascript Core"],"title":"Understand call, bind and apply methods in JavaScript"},{"categories":["Javascript"],"contents":"This post describes the Lexical Scope, Closures and Currying Function in JavaScript and their relations with examples.\nLexical Scope Variables defined using var keywords are having function scope which means new scope is created everytime you create a new function.\nLexical scope is associated with function scope. Lexical scope of variables is defined by their position in source code. JavaScript resolves the variable starting at the innermost scope and searches outwards until it finds the variable it was looking for.\nvar me = \u0026#34;global\u0026#34;; function whoami(){ var me = \u0026#34;local\u0026#34;; function func() { return me; } return func; } console.log(me); // global console.log(whoami()()); // local The value of the variable me outside is global whereas if you execute function whoami() from outside, value is local\nClosures Closure is very important concept and frequently asked in JavaScript interviews.\nA closure is an inner function having access to its outer function scope and all above scopes even when that function is executing outside of its outer function.\nWhen you define an inner function inside outer function, Closure is created at runtime for inner function bundled with outer function\u0026rsquo;s scope.\nLet\u0026rsquo;s look at the example to understand Closures\nvar outerFunc = function(c){ var a = 1; var innerFunc = function(d) { var b = 2; var innerMostFunc = function(e) { return a + b + c + d + e; } return innerMostFunc; } return innerFunc; } console.dir(outerFunc(3)); //1. innerFunc console.dir(outerFunc(3)(4)); //2. innerMostFunc console.log(outerFunc(3)(4)(5)); //3. 15 Output ▼ ƒ innerFunc(c) length: 1 name: \u0026#34;innerFunc\u0026#34; arguments: null caller: null ➤ prototype: {constructor: ƒ} ➤ __proto__: ƒ () [[FunctionLocation]]: ▼ [[Scopes]]: Scopes[2] ➤ 0: Closure (outerFunc) {c: 3, a: 1} ➤ 1: Global {parent: Window, opener: null, top: Window, length: 1, frames: Window, …} ▼ ƒ innerMostFunc(c) length: 1 name: \u0026#34;innerMostFunc\u0026#34; arguments: null caller: null ➤ prototype: {constructor: ƒ} ➤ __proto__: ƒ () [[FunctionLocation]]: ▼ [[Scopes]]: Scopes[3] ➤ 0: Closure (innerFunc) {d: 4, b: 2} ➤ 1: Closure (outerFunc) {c: 3, a: 1} ➤ 2: Global {parent: Window, opener: null, top: Window, length: 1, frames: Window, …} 15 We have three console dir/log. Let\u0026rsquo;s discuss them one by one:\ninnerFunc has a closure of variables defined or passed as argument in outerFunc\n0: Closure (outerFunc) {c: 3, a: 1} innerMostFunc has a closure of variables defined or passed as argument in in outerFunc and innerFunc i.e.\n0: Closure (innerFunc) {d: 4, b: 2} 1: Closure (outerFunc) {c: 3, a: 1} innerMostFunc returns a+b+c+d+e=15 where\nvalue of a and c is coming from Closure (outerFunc) value of b and d is coming from Closure (innerFunc) value of e is coming from passed argument They way we called outerFunc(3)(4)(5) is also known as currying\nCurrying One of the use case of Closure is currying functions.\nA currying function is a function where you break down a function that takes multiple argument one at a time instead of taking all argument at once.\nf(a, b, c) =\u0026gt; Currying =\u0026gt; f(a)(b)(c) Let\u0026rsquo;s convert this function\nvar add = function(a, b, c){ return a + b + c; } add(1, 2, 3); //6 to currying function\nvar add = function(a){ return function(b){ return function(c){ return a + b + c; } } } add(1)(2)(3); //6 One more example:-\nvar sayWhat = function(a){ return function(b){ return function(c){ console.log(`say ${a} to ${b} using ${c}`); } } } sayWhat(\u0026#34;hello\u0026#34;)(\u0026#34;friends\u0026#34;)(\u0026#34;currying function\u0026#34;); Output say hello to friends using currying function In currying function, each nested function is keeping track of arguments passed in outer function in their closure,\n","permalink":"https://codingnconcepts.com/javascript/lexical-scope-closures-and-currying/","tags":["Javascript Interview","Javascript Core"],"title":"Lexical Scope, Closures and Currying in JavaScript"},{"categories":["Java"],"contents":"This is one of the example of using recursive function in Java to find M power N\npublic class MPowerN { public static int pow(int m, int n){ return (n\u0026gt;1) ? m*pow(m, n-1) : m; } } @Test public void test() { assertEquals(pow(2,3), 8); } ","permalink":"https://codingnconcepts.com/java/m-power-n-using-recursive/","tags":["Java Algorithm","Recursive"],"title":"M power N Using Recursive function"},{"categories":["Java"],"contents":"Print all the possible combinations of a given String using Recursive function in Java\nHere we\u0026rsquo;re using two recursive functions given the string is \u0026ldquo;abcd\u0026rdquo;:\nsubstring is responsible for generating all possible substrings of given string in forward direction i.e. a, ab, abc, abcd, b, bc, bcd, c, cd, and d permutation is responsible for generating all possible permutation of substring generated by substring() method of same length for e.g. possible permutation of abc is abc, acb, bac, bca, cab, and cba public class PrintAllCombinationOfString { public static void main(String[] args) { String s = \u0026#34;abcd\u0026#34;; for (int i = 0; i \u0026lt; s.length(); i++) { substring(s, \u0026#34;\u0026#34;, i); } } public static void substring(String content, String part, int index) { if (index \u0026gt;= content.length()) { return; } String sub = part + content.charAt(index); permutation(\u0026#34;\u0026#34;, sub); substring(content, sub, index + 1); } private static void permutation(String prefix, String str) { int n = str.length(); if (n == 0) { System.out.println(prefix); } else { for (int i = 0; i \u0026lt; n; i++) { permutation(prefix + str.charAt(i), str.substring(0, i) + str.substring(i + 1, n)); } } } } Output a ab ba abc acb bac bca cab cba abcd abdc acbd acdb adbc adcb bacd badc bcad bcda bdac bdca cabd cadb cbad cbda cdab cdba dabc dacb dbac dbca dcab dcba b bc cb bcd bdc cbd cdb dbc dcb c cd dc d ","permalink":"https://codingnconcepts.com/java/print-all-string-combinations-using-recursive/","tags":["Java Algorithm","Recursive"],"title":"Print All String Combinations Using Recursive function"},{"categories":["Java"],"contents":"One of the major feature of Java 8 is addition of Stream. It also has introduced the functional programming in Java. We will discuss different stream operations available in Collection, Array, IntStream with examples. We will also discuss the difference between Intermediate and Terminal operations.\nStream Operations There are mainly three parts of any stream operations:-\n1. Create Stream These are called as Stream operation, which creates a stream from a given range or collection.\nList\u0026lt;String\u0026gt;.stream(), List\u0026lt;Object\u0026gt;.stream(), Arrays.stream(), IntStream.of(), IntStream.range()\n2. Process the Stream These are called as Intermediate operation, which converts a stream to another stream as a result. They can be chained together to form a pipeline of Stream operations.\nfilter(), map(), flatMap(), sorted(), distinct(), limit(), skip()\n3. Consume the Stream These are called as Terminal operation, which converts a stream to result or collection or void. They can not be chained together. Any Stream operation pipeline must end with terminal operation.\nforEach(), collect(), reduce(), min(), max(), count(), average(), sum(), anyMatch(), allMatch(), noneMatch(), findFirst(), findAny()\nIntermediate vs Terminal Operations 1. Output: Output of intermediate operation is another stream whereas output of terminal operation is a collection, array or primitive.\n2. Chaining: Stream operation pipeline can have as many as intermediate operators chained together but pipeline must end with terminal operator.\n3. Lazy Evaluation: Intermediate operations are evaluated lazily whereas terminal operations are eager. The intermediate operations just remain as a pipeline, and executed only when the terminal operation is executed\n4. Pipeline: Stream operations pipeline can have many intermediate operations but only one terminal operation.\nList\u0026lt;String\u0026gt;.stream() We will look at various stream operations pipelines. Let\u0026rsquo;s define a fruits List collection first\nList\u0026lt;String\u0026gt; fruits = Arrays.asList(\u0026#34;mango\u0026#34;, \u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;grapes\u0026#34;, \u0026#34;orange\u0026#34;); We now execute various examples of streams operations pipeline\nStream Operation | Intermediate Operation | ... | ... | Intermediate Operation | Terminal Operation Example 1. Filter elements from List stream() | filter() | collect() Filter out all elements except grapes from list fruits\nList\u0026lt;String\u0026gt; result = fruits.stream() // convert list to stream .filter(fruit -\u0026gt; !\u0026#34;grapes\u0026#34;.equals(fruit)) // filter out grapes .collect(Collectors.toList()); // collect the output and convert streams to a List result.forEach(System.out::println); Output mango apple banana orange Example 2. Change all elements in List stream() | map() | collect() Map all elements of List fruits to uppercase\nList\u0026lt;String\u0026gt; result = fruits.stream() // convert list to stream .map(fruit -\u0026gt; fruit.toUpperCase()) // map to uppercase .collect(Collectors.toList()); // collect the output and convert streams to a List result.forEach(System.out::println); Output MANGO APPLE BANANA GRAPES ORANGE Example 3. Sort all elements in List stream() | sorted() | collect() Sort all elements of List fruits in alphabetical order\nList\u0026lt;String\u0026gt; result = fruits.stream() // convert list to stream .sorted() // sort in alphabetical order .collect(Collectors.toList()); // collect the output and convert streams to a List result.forEach(System.out::println); Output apple banana grapes mango orange Example 4. Multiple intermediate operations stream() | filter() | map() | sorted() | collect() Multiple processing of stream of List fruits. Filter out all elements except grapes, Map them to uppercase, Sort them alphabetically.\nList\u0026lt;String\u0026gt; result = fruits.stream() // convert list to stream .filter(fruit -\u0026gt; !\u0026#34;grapes\u0026#34;.equals(fruit)) // filter out grapes .map(fruit -\u0026gt; fruit.toUpperCase()) // map to uppercase .sorted() // sort in alphabetical order .collect(Collectors.toList()); // collect the output and convert streams to a List result.forEach(System.out::println); Output APPLE BANANA MANGO ORANGE Example 5. Find elements in List stream() | filter() | findAny() | orElse() Filter mango from the List fruits, Return mango if found or else return null\nString fruit = fruits.stream() // convert list to stream .filter(fruit -\u0026gt; \u0026#34;mango\u0026#34;.equals(fruit)) // we love mango .findAny() // If `findAny` then return found .orElse(null); // If not found, return null System.out.println(fruit); Output mango List\u0026lt;Object\u0026gt;.stream() Streams works with Objects Also.\nExample 1. Find the 3 highest earning employees Let\u0026rsquo;s first do it in our usual way:-\nList\u0026lt;Employee\u0026gt; employees = getAllEmployees(); // Copy to new list to avoid mutating original array List\u0026lt;Employee\u0026gt; copy = new ArrayList\u0026lt;\u0026gt;(employees); // Sort descending copy.sort((o1, o2) -\u0026gt; o2.getSalary() - o1.getSalary()); // Get first 3 for(int i=0; i\u0026lt;3; i++){ Employee employee = copy.get(i); System.out.println(employee.getName()); } Now use the streams to do the same:-\nList\u0026lt;Employee\u0026gt; employees = getAllEmployees(); employees.stream() .sorted(Comparator.comparingInt(Employee::getSalary).reversed()) .limit(3) .map(Employee::getName) .forEach(System.out::println) Example 2. List Collectors (Terminal Operations) Let\u0026rsquo;s look at various collectors (terminal operations) available:-\nList\u0026lt;Employee\u0026gt; employees = getAllEmployees(); // to list List\u0026lt;String\u0026gt; listOfEmps = employees.stream() .limit(3) .map(Employee::getName) .collect(Collectors.toList()); // to set Set\u0026lt;String\u0026gt; setOfEmps = employees.stream() .limit(3) .map(Employee::getName) .collect(Collectors.toSet()); // to map Map\u0026lt;String, Employee\u0026gt; mapOfEmps = employees.stream() .limit(3) .collect(Collectors.toMap(e -\u0026gt; e.name, e -\u0026gt; e)); // john, amy, marcy String names = employees.stream() .limit(3) .map(Employee::getName) .collect(Collectors.joining(\u0026#34;,\u0026#34;)); // group by dept Map\u0026lt;String, List\u0026lt;Employee\u0026gt;\u0026gt; empByDept = employees.stream() .collect(Collectors.groupingBy(e -\u0026gt; e.dept)); // count employees in each dept Map\u0026lt;String, Long\u0026gt; countByDept = employees.stream() .collect(Collectors.groupingBy(Employee::getDept, Collectors.counting())); Example 3. Parallel Operations Stream operations are executed sequentially by default. You can initiate parallel stream operations by using .parallel() operation. It is recommended to user parallel operation only when List is considerably large otherwise there will be a performance hit.\n// parallel stream Map\u0026lt;String, List\u0026lt;Employee\u0026gt;\u0026gt; empMapByDept = employees.stream() .parallel() .collect(Collectors.groupingBy(e -\u0026gt; e.dept)); IntStream.of() | .range() Example 1. Run a for loop from 1 to n-1 int n = 6; IntStream.range(1, n) .forEach(System.out::println); Output 1 2 3 4 5 Example 2. Get min, max, avg, count, and sum of given Array Find the minimum of given number array:-\nint[] nums = {4, 1, 13, 90, 16, 2, 0}; int min = IntStream.of(nums) //create stream .min() .getAsInt(); System.out.println(\u0026#34;min: \u0026#34; + min); Output min: 0 getAsInt() throw exception if min cannot be found e.g. if array is empty. Alternate method is ifPresent()\nIntStream.of(new int[0]) .min() .ifPresent(min -\u0026gt; System.out.println(min)); // OR IntStream.of(new int[0]) .min() .ifPresent(System.out::println); We changed the lambda expression min -\u0026gt; System.out.println(min) to double colon System.out::println in later version. Both do the same thing.\nOther similar statistics operations are:-\nint[] nums = {4, 1, 13, 90, 16, 2, 0}; int min = IntStream.of(nums).min().getAsInt(); int max = IntStream.of(nums).max().getAsInt(); double avg = IntStream.of(nums).average().getAsDouble(); long count = IntStream.of(nums).count(); long sum = IntStream.of(nums).sum(); Alternatively you can create a stream only once do get all statistics:-\nIntSummaryStatistics stats = IntStream.of(nums).summaryStatistics(); int min = stats.getMin(); int max = stats.getMax(); double avg = stats.getAverage(); long count = stats.getCount(); long sum = stats.getSum(); Example 3. Find 3 distinct smallest numbers from given Array int[] nums = {4, 1, 13, 90, 16, 2, 0}; IntStream.of(nums) .distinct() .sorted() .limit(3) .forEach(System.out::println); Output 0 1 2 Make note that streams works on the copy of the array and do not mutate the original array.\nYou can change the terminal (last) operation say if we want sum of 3 distinct small numbers:-\nint[] nums = {4, 1, 13, 90, 16, 2, 0}; long sum = IntStream.of(nums) .distinct() .sorted() .limit(3) .sum(); Example 4. All possible operations on IntStream Create a Stream IntStream.of(numbers); // from Array IntStream.range(1, 101); // 1..100 IntStream.rangeClosed(1, 100); // 1..100 IntStream.generate(supplier()); // from supplier Process the Stream IntStream.of(numbers).distinct(); // distinct IntStream.of(numbers).sorted(); // sort IntStream.of(numbers).limit(3); // get first 3 IntStream.of(numbers).skip(3); // skip first 3 IntStream.of(numbers).filter(n -\u0026gt; n%2==0); // only even IntStream.of(numbers).map(n -\u0026gt; n*2); // double each num IntStream.of(numbers).boxed(); // convert each num to Integer Consume the Stream IntStream.of(numbers).min(); // min IntStream.of(numbers).max(); // max IntStream.of(numbers).sum(); // sum IntStream.of(numbers).average(); // average IntStream.of(numbers).count(); // count IntStream.range(1, 100).forEach(System.out::println); // print 1 to 99 IntStream.range(1, 100).toArray(); // collect into array IntStream.range(1, 100).boxed().collect(Collectors.toList()); // collect into list IntStream.of(numbers).anyMatch(n -\u0026gt; n%2==1); // is any num odd IntStream.of(numbers).allMatch(n -\u0026gt; n%2==1); // are all num odd Array.stream() Similar to IntStream, we can create a stream from an array using Array.stream() and apply similar operations\nint[] intArray = new int[]{1, 3, 5, 7, 9, 3, 5, 99}; int min = Arrays.stream(nums).min().getAsInt(); int max = Arrays.stream(nums).max().getAsInt(); double avg = Arrays.stream(nums).average().getAsDouble(); long count = Arrays.stream(nums).count(); long sum = Arrays.stream(nums).sum(); Arrays.stream(intArray) .distinct() .forEach(System.out::println); ","permalink":"https://codingnconcepts.com/java/streams-in-java-8/","tags":["Java Streams"],"title":"Streams in Java 8"},{"categories":["Java"],"contents":"Decorator design pattern is used to add a new feature on the existing object by wrapping it with a decorator class.\nIn this example, we will first create an interface Window interface and its implementation BasicWindow\ninterface Window { public String draw(); } class BasicWindow implements Window { @Override public String draw() { return \u0026#34;Basic Window\u0026#34;; } } Next, we want to decorate this Window with some border and scroll bar. We need a wrapper class for this which can wrap Window to add new features. For this, we create a class WindowDecorator which takes Window as constructor argument and also override draw() method to add new feature.\nclass WindowDecorator implements Window { protected Window window; public WindowDecorator(Window window) { this.window = window; } @Override public String draw() { return window.draw(); } } We\u0026rsquo;ll now implement our wrapper class WindowDecorator and create two decorator classes BorderDecorator and ScrollDecorator. These decorators override draw() method to add new features like border and scroll bar.\nclass BorderDecorator extends WindowDecorator { public BorderDecorator(Window window) { super(window); } @Override public String draw() { return window.draw() + addBorder(); } public String addBorder() { return \u0026#34; with Border\u0026#34;; } } class ScrollDecorator extends WindowDecorator { public ScrollDecorator(Window window) { super(window); } @Override public String draw() { return window.draw() + addScroll(); } public String addScroll() { return \u0026#34; and Scroll Bar\u0026#34;; } } Let\u0026rsquo;s test our decorator classes\n@Test public void testDecorators() { Window basicWindow = new BasicWindow(); assertEquals(basicWindow.draw(), \u0026#34;Basic Window\u0026#34;); Window borderWindow = new BorderDecorator(basicWindow); assertEquals(borderWindow.draw(), \u0026#34;Basic Window with Border\u0026#34;); Window borderWindowScrollable = new ScrollDecorator(borderWindow); assertEquals(borderWindowScrollable.draw(), \u0026#34;Basic Window with Border and Scroll Bar\u0026#34;); } Few points from test:\nWe have created a basicWindow object and decorated it using BorderDecorator Next we further decorated it using ScrollDecorator We can add as many as decorators at runtime to add new features to object. ","permalink":"https://codingnconcepts.com/java/decorator-design-pattern-using-java/","tags":["Java Design Pattern"],"title":"Decorator Design Pattern Using Java"},{"categories":["Javascript"],"contents":"This post describes the best practices to define constants and configuration values in JavaScript using const and Object.freeze() and the difference between them.\nWhen we define constants and configuration values in our JavaScript applications. They should have following characteristics:-\nAccessible across application Value of the variable should be immutable Reference of variable should be immutable Now we will try to implement these characteristics\u0026hellip;\nUse let Let\u0026rsquo;s define the variable using let and see,\nlet APP_NAME = \u0026#34;Coding N Concepts\u0026#34;; function getApplicationName() { APP_NAME = \u0026#34;Unkown App\u0026#34;; return APP_NAME; } getApplicationName(); // Unkown App In the above example, function getApplicationName() has changed the value of APP_NAME. How do we prevent changing value of a global variable?\nUse const We can use const to define a constant variable instead of let to prevent it from changing value.\nconst APP_NAME = \u0026#34;Coding N Concepts\u0026#34;; function getApplicationName() { APP_NAME = \u0026#34;Unkown App\u0026#34;; // This will throw TypeError return APP_NAME; } Trying to change value of a variable defined using const resulting into this error:\n“TypeError: Assignment to constant variable.” So this is it? Let\u0026rsquo;s find out\nconst fruits = [\u0026#39;mango\u0026#39;, \u0026#39;apple\u0026#39;]; fruits.push(\u0026#39;banana\u0026#39;); console.log(fruits); // [\u0026#39;mango\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;] const constants = { APP_NAME : \u0026#34;Coding N Concepts\u0026#34; }; constants.APP_NAME = \u0026#34;Unknown App\u0026#34;; console.log(constants.APP_NAME); // \u0026#34;Unknown App\u0026#34; You can see in the above two examples that you can change the value of an array or an object even if you are using const because:\nconst does not make the value of the variable immutable but instead makes the binding of the variable immutable.\nAll primitive types like integer, boolean and string binding data by value whereas types like array and object binding data by reference.\nNow we know that in case of array and object, we cannot change the reference but can change the value.\nSo how do we prevent changing value of array and object?\nUse Object.freeze() Here is where Object.freeze() comes into play. Object.freeze ignore the change in value of object and array.\nlet constants = Object.freeze({ APP_NAME : \u0026#34;Coding N Concepts\u0026#34; }); constants.APP_NAME = \u0026#34;Unknown App\u0026#34;; console.log(constants.APP_NAME); // \u0026#34;Coding N Concepts\u0026#34; You can see in example that it doesn\u0026rsquo;t throw any error if you try to change the value but it won\u0026rsquo;t affect the object state.\nObject.freeze() prevents from changing value of an object but we can still change the reference as below:-\nlet constants = Object.freeze({ APP_NAME : \u0026#34;Coding N Concepts\u0026#34; }); constants = { APP_NAME : \u0026#34;Unknown App\u0026#34; }; console.log(constants.APP_NAME); // \u0026#34;Unknown App\u0026#34; If we summarize:-\nconst doesn\u0026rsquo;t allow to change reference of object or array though you can change the value.\nObject.freeze() ignores the change in value of object or array\nCombining them together will prevent change in both reference and value of an object or array\nUse const and Object.freeze() together const constants = Object.freeze({ APP_NAME : \u0026#34;Coding N Concepts\u0026#34; }); constants.APP_NAME = \u0026#34;Unknown App\u0026#34;; // This is ignored constants = { APP_NAME : \u0026#34;Unknown App\u0026#34; }; // This will throw TypeError The above example shows that combining both const and Object.freeze() is very useful to define constants and configuration in JavaScript.\n","permalink":"https://codingnconcepts.com/javascript/const-vs-object-freeze/","tags":["Javascript Interview","Javascript ES6","Javascript Object"],"title":"Difference in const and Object.freeze() in JavaScript"},{"categories":["Javascript"],"contents":"This post describes the usage of various Object methods like Object.create(), Object.assign(), Object.freeze(), Object.seal() and many more available in JavaScript with examples.\nObject.create() The Object.create() method is used to create a new object with its prototype set to existing object.\nvar car = { color: \u0026#34;black\u0026#34;, brand: \u0026#34;Toyota\u0026#34;, drive: function(){ console.log(`Driving a ${this.isTopModel ? \u0026#39;top model\u0026#39; : \u0026#39;\u0026#39;} ${this.color} color ${this.brand} car`); } } var newCar = Object.create(car); newCar.brand = \u0026#34;Honda\u0026#34;; newCar.isTopModel = true; console.log(car.isPrototypeOf(newCar)); //true newCar.drive(); Output true Driving a top model black color Honda car Here newCar creates a completely new object with its prototype set to Car. We set the brand and isTopModel values whereas value of color will be inherited. When we call drive() method, it finds the drive() in the prototype chain on car and execute.\nObject.assign() Object.assign() is used to copy the properties and functions of one object in another object.\nWe can merge two objects using Object.assign() as follows:-\n// Initialize an object const favorites = { color: \u0026#39;Red\u0026#39;, number: 5 }; // Initialize another object const somemore = { fruit: \u0026#39;Mango\u0026#39;, movies: [\u0026#34;Spider Man\u0026#34;, \u0026#34;Conjuring\u0026#34;], }; // Merge the objects console.log(Object.assign(favorites, somemore)); Output {color: \u0026#34;Red\u0026#34;, number: 5, fruit: \u0026#34;Mango\u0026#34;, movies: [\u0026#34;Spider Man\u0026#34;, \u0026#34;Conjuring\u0026#34;]} Also can be used in Class constructor to assign all values to this as follows:\nclass Car { constructor(brand, color, price){ Object.assign(this, {brand, color, price}); } } Object.freeze() Object.freeze() freezes the state of an Object once this method is called. It ignores if any existing property is modified, and if any new property is added.\n// Initialize an object const user = { username: \u0026#39;admin\u0026#39;, password: \u0026#39;password@123\u0026#39; }; // Freeze the object Object.freeze(user); user.password = \u0026#39;*******\u0026#39;; user.active = true; console.log(user); Output {username: \u0026#34;admin\u0026#34;, password: \u0026#34;password@123\u0026#34;} Object.seal() Object.seal() seals the state of an Object once this method is called. It allows the modification of existing properties, but ignores if any new property is added.\n// Initialize an object const user = { name: \u0026#39;admin\u0026#39;, password: \u0026#39;password@123\u0026#39; }; // Seal the object Object.seal(user); user.password = \u0026#39;*******\u0026#39;; user.active = true; console.log(user); Output {name: \u0026#34;admin\u0026#34;, password: \u0026#34;*******\u0026#34;} Object.defineProperty() Object.defineProperty() is used to define new property in existing object.\nProperty can be defined with following options:\nvalue: defaults to undefined, if value of the property is not provided writable: deafults to false, if true means value of the property can be changed configurable: defaults to false, if true means property can be deleted enumerable: defaults to false, if true means property can be enumerated such as in a for..in loop var user = {}; Object.defineProperty(user, \u0026#34;name\u0026#34;, { value: \u0026#34;admin\u0026#34; }); console.log(user); is equivalent to\nvar user = {}; Object.defineProperty(user, \u0026#34;name\u0026#34;, { value: \u0026#34;admin\u0026#34;, writable: false, configurable: false, enumerable: false }); console.log(user); Output {name: \u0026#34;admin\u0026#34;} Object.defineProperties() Object.defineProperties() can be used to define multiple properites in existing object.\nvar user = {}; Object.defineProperties(user, { name: { value: \u0026#34;admin\u0026#34;, writable: true, enumerable: true }, password: { value: \u0026#39;password@123\u0026#39;, writable: false, enumerable: false } }); console.log(user); Output {name: \u0026#34;admin\u0026#34;, password: \u0026#34;password@123\u0026#34;} Object.keys(), Object.values() \u0026amp; Object.entries() Object.keys(), Object.values() and Object.entries() can be used iterator from the object as follows:\nlet favorites = { color: \u0026#39;Red\u0026#39;, number: 5, vegan: true, movies: [\u0026#34;Spider Man\u0026#34;, \u0026#34;Conjuring\u0026#34;] }; console.log(\u0026#34;keys\u0026#34;, Object.keys(favorites)); console.log(\u0026#34;values\u0026#34;, Object.values(favorites)); console.log(\u0026#34;entries\u0026#34;, Object.entries(favorites)); Output keys [\u0026#34;color\u0026#34;, \u0026#34;number\u0026#34;, \u0026#34;vegan\u0026#34;, \u0026#34;movies\u0026#34;] values [\u0026#34;Red\u0026#34;, 5, true, [\u0026#34;Spider Man\u0026#34;, \u0026#34;Conjuring\u0026#34;]] entries [ [\u0026#34;color\u0026#34;, \u0026#34;Red\u0026#34;], [\u0026#34;number\u0026#34;, 5], [\u0026#34;vegan\u0026#34;, true], [\u0026#34;movies\u0026#34;, [\u0026#34;Spider Man\u0026#34;, \u0026#34;Conjuring\u0026#34;] ] Object.keys() and Object.entries() can be used to iterate through the keys and values of an object as follows:-\nObject.keys(favorites).forEach(key =\u0026gt; console.log(`${key}: ${favorites[key]}`)); Object.entries(favorites).forEach(entry =\u0026gt; console.log(`${entry[0]}: ${entry[1]}`)); Object.is() ES6 has introduced a new method Object.is() to compare two values, it works very much similar to strict comparison operator === and also avoid confusion of NaN comparisons and +0/-0 comparisons. It is recommended to use Object.is() for value comparisons.\nlet\u0026rsquo;s check it out,\nconsole.log(Object.is(\u0026#34;cnc\u0026#34;, \u0026#34;cnc\u0026#34;)); // true, same content and type console.log(Object.is(12345, 12345)); // true, same content and type console.log(Object.is(false, false)); // true, same content and type console.log(Object.is(12345, \u0026#34;12345\u0026#34;)); // false, different type console.log(Object.is(0, false)); // false, different type console.log(Object.is(\u0026#34;\u0026#34;, false)); // false, different type console.log(Object.is(null, undefined)); // false, different type console.log(Object.is([], [])); // false, both refers to different object in memory console.log(Object.is([1, 2], [1, 2])); // false, both refers to different object in memory console.log(Object.is({}, {})); // false, both refers to different object in memory var array1 = [1, 2, 3, 4, 5]; var array2 = array1; console.log(Object.is(array1, array2)); // true, both refers to same array var obj1 = { app : \u0026#34;cnc\u0026#34;}; var obj2 = obj1; console.log(Object.is(obj1, obj2)); // true, both refers to same object console.log(NaN === NaN); // false, confusing, content and type is same console.log(Object.is(NaN, NaN)); // true, es6 is good, same content and type console.log(+0 === -0); // true, confusing, content is different console.log(Object.is(+0, -0)); // false, es6 is good, different content Summary Javascript Object provides very useful methods to\ncreate a new object using Object.create() copy object using Object.assign() protect object using Object.freeze() and Object.seal() define object properties using Object.defineProperty() and Object.defineProperties() iterate through object using Object.keys(), Object.values() and Object.entries() compare objects using Object.is() ","permalink":"https://codingnconcepts.com/javascript/object-methods-in-javascript/","tags":["Javascript Interview","Javascript ES6","Javascript Object"],"title":"Object methods in JavaScript"},{"categories":["Javascript"],"contents":"A nice feature addition in ES2015 (ES6) was the introduction of let and const keywords for variable declaration. You can use var, let and const keyword interchangeably for variable declaration though it makes a difference in terms of their scope, usage and hoisting. If you are not aware of these differences then please continue to read\u0026hellip;\nScope Scope basically means the accessibility of a variable. Variables can have three types of scope:-\n1. Global Scope When variables is defined outside a function and accessible everywhere. All var, let and const are global scope.\n2. Function Scope Variables defined using var are function-scoped because if it is defined inside function, their visibility is limited to the function, or nested function. When you try to use it outside of the function, you’ll get an error.\nfunction myFn() { var foo = \u0026#39;peekaboo!\u0026#39;; console.log(foo); // Prints \u0026#39;peekaboo!\u0026#39; } console.log(foo); // ReferenceError: foo is not defined 3. Block Scope Variables defined using let or const are block-scoped because if it is defined inside block of code (code between curly braces {}), their visibility is limited to the block of code, any nested block. When you try to use it outside of the block, you’ll get an error.\nif (true) { var foo = \u0026#39;peekaboo!\u0026#39;; let bar = \u0026#39;i see u\u0026#39;; const baz = \u0026#39;baby blue!\u0026#39;; console.log(foo); // Prints \u0026#39;peekaboo!\u0026#39;; console.log(bar); // Prints \u0026#39;i see u\u0026#39;; console.log(baz); // Prints \u0026#39;baby blue!\u0026#39;; } console.log(foo); // Prints \u0026#39;peekaboo!\u0026#39;; console.log(bar); // ReferenceError: bar is not defined console.log(baz); // ReferenceError: baz is not defined Usage 1. \u0026ldquo;var\u0026rdquo; var was the only keyword for variable declaration before ES6.\nvar outside of a for-loop for (var i = 0; i \u0026lt; 3; i++) { console.log(i); // Prints 0, 1 and 2 } console.log(i); // Prints 3 Since var is function-scoped. The variable i can be accessed outside for-loop (block of code)\nvar reassigning Variable using var can be reassigned with new value and also can be redefined.\nfunction myFn() { var foo = 1; foo = 30; var foo = 101; console.log(foo); // Prints 101 } myFn(); 2. \u0026ldquo;let\u0026rdquo; let outside of a for-loop for (let i = 0; i \u0026lt; 3; i++) { console.log(i); // Prints 0, 1 and 2 } console.log(i); // ReferenceError: i is not defined Since let is block-scoped. The variable i is not accessible outside for-loop (block of code)\nlet reassigning and redefining Variable using let can be reassigned with new value but can not be redefined.\nfunction myFn() { let foo = 1; foo = 30; let foo = 101; console.log(foo); } myFn(); Output Uncaught SyntaxError: Identifier \u0026#39;foo\u0026#39; has already been declared However, if the let variable is defined in different scopes, there will be no error.\nlet foo = 1; function myFn() { let foo = 2; console.log(foo); if(true){ let foo = 3; console.log(foo); } } console.log(foo); myFn(); Output 1 2 3 Why is there no error? This is because all three instances are treated as different variables since they have different scopes.\n3. \u0026ldquo;const\u0026rdquo; The keyword const is similar to let, it’s block-scoped, however, you can’t reassign the value to it.\nconst PI = 3.1415; PI = 5; // \u0026#34;TypeError: Assignment to constant variable. However, new items can still be pushed into an array constant or added to an object. The following 2 snippets work without complaining because we are not trying to reassign to the variables:\nconst someArr = [3, 4, 5]; someArr.push(6); const someObj = { dog: \u0026#39;Skip\u0026#39;, cat: \u0026#39;Caramel\u0026#39;, bird: \u0026#39;Jack\u0026#39; }; someObj.camel = \u0026#39;Bob\u0026#39;; Hoisting Hoisting is a JavaScript mechanism where variables and function declarations are moved to the top of their scope before code execution\nIn case of var variables, their values are initialized with undefined in hoisting process.\nconsole.log(foo); var foo = 1; console.log(foo); console.log(bar); bar = 2; console.log(bar); var bar; is interpreted as this\nvar foo; var bar; console.log(foo); // undefined foo = 1; console.log(foo); // 1 console.log(bar); // undefined bar = 2; console.log(bar); // 2 Can you guess what is output of below code snippet? var foo = 1; function myFun(){ console.log(foo); var foo = 2; } myFun(); Have you guessed foo = 1 or foo = 2?\nIt\u0026rsquo;s neither one of them. It will print undefined. Let\u0026rsquo;s see how it is interpreted by the compiler in hoisting process,\nvar foo; foo = 1; function myFun(){ var foo; // var hoisted and initialized with undefined console.log(foo); // undefined foo = 2; } myFun(); Just like var, let declarations also hoisted to the top but unlike var which is initialized as undefined, the let variable is not initialized. So if you try to use a let variable before declaration, you\u0026rsquo;ll get a Reference Error. const variables are similar to let variables in terms of hoisting.\nconsole.log(foo); let foo = 1; Output ReferenceError: foo is not defined Can you guess what is output of below code snippet? let foo = 1; function myFun(){ console.log(foo); let foo = 2; } myFun(); Have you guessed foo = undefined?\nNo its not. Let\u0026rsquo;s see how it is interpreted by the compiler in hoisting process,\nlet foo; foo = 1; function myFun(){ let foo; // let hoisted without initialization unlike var console.log(foo); // ReferenceError: Cannot access \u0026#39;foo\u0026#39; before initialization foo = 2; } myFun(); Summary So just in case, you missed the differences, here they are :\nScope Value Update Redefine Variable Hoisting var function scope yes yes (within scope) initialized with undefined let block scope yes no (within block scope) but yes (outside block scope) variable not initialized const block scope no no variable not initialized var variables are function scoped whereas let and const are block scoped. var variables can be updated and re-declared within its scope; let variables can be updated but not re-declared; const variables can neither be updated nor re-declared. They are all hoisted to the top of their scope but while var variables are initialized with undefined, let and const variables are not initialized. While var and let can be declared without being initialized, const must be initialized during declaration. Got any question or addition? please leave a comment.\nThanks for reading.\n","permalink":"https://codingnconcepts.com/javascript/difference-between-var-let-and-const/","tags":["Javascript Interview","Javascript ES6"],"title":"Difference between var, let and const"},{"categories":["Puzzles"],"contents":"The one light bulb and three switches puzzle has been asked several times in interviews to evaluate candidate\u0026rsquo;s analytical skills.\nPuzzle There is a light bulb 💡 inside a closed room (no windows) with one door. You cannot see the light bulb is on or off as the door is closed. However, you can assume that the light is off initially. There are three light switches outside the room. One of the switch belongs to the light bulb inside the room. You can turn on or off the switches however you want, but you are allowed to open the door only once. You are not allowed to touch the switches once you open the door.\nHow do you figure out which switch belongs to the light?\nThink for a while before looking at the solution.\nSolution We are assuming that all the switches are off initially. Turn on switch number 1 and wait for two minutes. After two minutes, turn off the switch number 1 and turn on the switch number 2 and open the door to see light bulb\nIf the light bulb is on, then switch number 2 belongs to the bulb. If the light bulb is off, then touch the bulb with your hand. If the bulb is hot, then switch number 1 belongs to the bulb otherwise switch number 3 belongs to the bulb. ","permalink":"https://codingnconcepts.com/puzzle/1-light-bulb-and-3-switches/","tags":["Interview Puzzle"],"title":"1 Light Bulb and 3 Switches Puzzle"},{"categories":["Puzzles"],"contents":"The Fruit basket puzzle comes under basic puzzles being asked in the interview.\nPuzzle There are 3 baskets. One basket has apples 🍎🍎 only, one has oranges 🍊🍊 only and the other has mixture of apples and oranges 🍎🍊. All the three baskets are wrongly labelled as APPLES, ORANGES and MIXTURE that means if basket is labelled as ORANGES, it is having either mixture or apples. You have to pick only one fruit from any of the one basket and correctly label all the three baskets.\nHow do you do it?\nThink for a while before looking at the solution.\nSolution Pick a fruit from the one labeled MIXTURE which have two possible outcomes:-\nIf that fruit is orange then correct label for this basket is ORANGES. The one which is fake labelled as APPLES must be either ORANGES or MIXTURE. Since we have already found our ORANGES basket. APPLES basket is actually a MIXTURE and the one fake labelled as ORANGES is APPLES.\nFake Label Correct Label MIXTURE ORANGES APPLES MIXTURE ORANGES APPLES If that fruit is apple then correct label for this basket is APPLES. The one which is fake labelled as ORANGES must be either APPLES or MIXTURE. Since we have already found our APPLES basket. ORANGES basket is actually a MIXTURE and the one fake labelled as APPLES is ORANGES.\nFake Label Correct Label MIXTURE APPLES ORANGES MIXTURE APPLES ORANGES ","permalink":"https://codingnconcepts.com/puzzle/fruit-basket-puzzle/","tags":["Interview Puzzle"],"title":"Fruit Basket Puzzle"},{"categories":["Puzzles"],"contents":"This riddle is from famous Die Hard movie where Bruce Willis and Jackson have to diffuse a bomb by placing exactly 4 gallon water on a scale using 3 gallon and 5 gallon water jug.\nRiddle You have a 3-gallon and a 5-gallon jug that you can fill multiple times from a tap 🚰. The problem is to measure exactly 4 gallons of water.\nHow do you do it?\nThink for a while before looking at the solution.\nSolution Firstly you will try to measure 4 gallons of water by estimation. For e.g. adding 3 gallons of water to 1/3 of the 3 gallon jug. But this riddle is asking for a precise measurement so this won\u0026rsquo;t work.\nWe can quick realize this:-\n5 – 3 = 2 and 5 – (3 – 2) = 4\nHere is one of the solution of this riddle:\nAction 3-gallon jug 5-gallon jug Fill up 5-gallon jug - 5 Fill up 3-gallon jug with water from 5-gallon jug 3 2 Pour out 3-gallon jug 0 2 Transfer water from 5-gallon jug to 3-gallon jug 2 0 Fill up 5 gallon jug again 2 5 Transfer water from 5-gallon jug to 3-gallon jug 3 4 That’s not the only solution. There are multiple solutions possible for this riddle.\nIncidentally, the reason we can find a solution is because the two numbers 5 and 3 are prime numbers (i.e. they have no common divisors). We can actually generate any volume of water from 1 to sum of these two prime numbers (i.e. 5 + 3 = 8) (in fact, we did get measurements of 2, 3, 4, 5 and 7 along the way in our solutions).\nWe have measured 2 and 3 in 3-gallon jug along the way in our solution We have measured 4 and 5 in 5-gallon jug along the way in our solution We have measured 7 in last two actions summing up gallons in both jugs we can measure exactly 8-gallons by filling up both 5-gallon and 3-gallon jugs ","permalink":"https://codingnconcepts.com/puzzle/the-water-jug-riddle/","tags":["Die Hard Puzzle","Maths Puzzle"],"title":"The Die Hard Water Jug Riddle (Solved)"},{"categories":["Puzzles"],"contents":"The 2 Eggs and 100 Floors problem is frequently asked in interview to evaluate candidate\u0026rsquo;s understanding on linear search and binary search. I have tried to explain this problem as easy as possible.\nProblem Statement There is a building of 100 floors. One of the floors is the highest floor (say Nth floor) an egg can be dropped from without breaking.\nIf an egg is dropped from that floor (N) or above (N+1, N+2, \u0026hellip;), it will break. If it is dropped from any floor below (N-1, N-2, \u0026hellip;), it will not break and you can drop the egg again. Given two eggs 🥚🥚, find the floor (N) an egg can be dropped from without breaking in minimum number of drops.\nThe question is, What strategy should you adopt to minimize the number egg drops it takes to find the solution?. (And what is the worst case for the number of drops it will take?)\nThink about the problem for a few minutes, and then read on.\nSolution 1 (Liner Search) First solution is linear search strategy which is very straightforward if we don\u0026rsquo;t care about number of drops. We can start dropping egg from floor 1, floor 2, floor 3 and continue till floor 100 until we find the floor N. In worst case it will take 100 drops if egg breaks only at the last (100th) floor.\nLinear search - worst case - 100 drops Can we improve worst case using binary search strategy? Lets see in Solution 2.\nSolution 2 (Binary Search) Second solution is binary search strategy where we start from the middle (50th) floor.\nIf egg breaks on 50th floor that means our N is below 50. If egg doesn\u0026rsquo;t break that means our N is above 50. Binary search strategy say that If egg breaks on 50th floor then we should check now from 25th floor but wait, we have a problem here. We have already lost 1 egg and left with only 1 egg now so we are left with linear search only but in this case only from 1st to 50th floor in worst case. So our drops came down to half compare to solution 1\nBinary Search - worst case - 50 drops Can we still improve the worst case? Let see in Solution 3\nSolution 3 (Divide and Conquer) Let\u0026rsquo;s apply our learning from previous solutions and apply a mix of linear and binary approach.\nThis time we start off by dropping an egg at floor 10, floor 20, floor 30 \u0026hellip;increasing floor by 10\nIf first egg breaks at floor 10, then we use second egg to linear search from 1 to 9\nIf first egg breaks at floor 20, then we use second egg to linear search from 11 to 19\n\u0026hellip;.\nIn worst case if egg breaks at last (100th) floor then we use second egg to linear search from 91 to 99. In worst case it will take 19 drops (10th, 20th, 30th \u0026hellip;\u0026hellip; 70th 80th 90th 91th \u0026hellip;.99th)\nDivide and Conquer - worst case - 19 drops Now when we came to an understanding, lets see Solution 4\nSolution 4 (Equation) What we need is a solution that minimizes our maximum drops. The solution above hint towards what we need is a strategy that tries to make solutions to all possible answers the same depth (same number of drops). The way to reduce the worst case is to attempt to make all cases take the same number of drops.\nAs I hope you can see by now, if the solution lays somewhere in a floor low down, then we have extra-headroom when we need to step by singles, but, as we get higher up the building, we’ve already used drop chances to get there, so there we have less drops left when we have to switch to going floor-by-floor.\nLet’s break out some algebra.\nImagine we drop our first egg from floor n, if it breaks, we can step through the previous (n-1) floors one-by-one.\nIf it doesn’t break, rather than jumping up another n floors, instead we should step up just (n-1) floors (because we have one less drop available if we have to switch to one-by-one floors), so the next floor we should try is floor n + (n-1)\nSimilarly, if this drop does not break, we next need to jump up to floor n + (n-1) + (n-2), then floor n + (n-1) + (n-2) + (n-3) …\nWe keep reducing the step by one each time we jump up, until that step-up is just one floor, and get the following equation for a 100 floor building:\nn + (n-1) + (n-2) + (n-3) + (n-4) + … + 1 \u0026gt;= 100\nThis summation, as many will recognize, is the formula for triangular numbers (which kind of makes sense, since we’re reducing the step by one each drop we make) and can be simplified to:\nn (n+1) / 2 \u0026gt;= 100\nThis is a quadratic equation, with the positive root of 13.651 (Which we have to round up to 14)\nThis means we want to start dropping from floor 14, jump up 13 floors to drop from floor 27, jump up 12 floors to drop from floor 39, and so on. Our worst case scenario is then a drop count total of 14.\nEquation - worst case - 14 drops Drop Floor #1 14 #2 27 #3 39 #4 60 #6 69 #7 77 #8 84 #9 90 #10 95 #11 99 #12 100 ","permalink":"https://codingnconcepts.com/puzzle/2-eggs-and-100-floors/","tags":["Interview Puzzle","Google Interview Puzzle"],"title":"2 Eggs and 100 Floors Problem (Solved)"},{"categories":["Puzzles"],"contents":"The 25 Horses 5 Tracks Problem is a famous Google interview question that has been asked in many companies to evaluate candidate\u0026rsquo;s analytical ability and problem solving approach.\nProblem Statement You have 25 horses\n🐎🐎🐎🐎🐎\n🐎🐎🐎🐎🐎\n🐎🐎🐎🐎🐎\n🐎🐎🐎🐎🐎\n🐎🐎🐎🐎🐎\nand you need to identify the fastest 3 horses 🐎🐎🐎 out of those 25. Only 5 horses can run in each race at the same time as there are only 5 tracks.\nWhat is the minimum number of races required to identify the fastest 3 horses without using a stopwatch or timer?\nSolution This is an interesting puzzle. Let\u0026rsquo;s look at the limitations first.\nWe don’t have a stopwatch or timer so we can’t record the finish time of each horse otherwise we could have found out the fastest 3 horses by 5 races of 5 horses in each race. Only 5 horses can run in each race otherwise we could have found out the fastest 3 horses by just one race of 25 horses.\nFirst 5 Races So at this point, we are certain that we need a minimum of 5 races.\n25 horses / 5 horses per race = 5 races Visualization is very important for such puzzles. Draw a matrix of horses for the first 5 races.\nRace-1 🐎R1H5 🐎R1H4 🐎R1H3 🐎R1H2 🐎R1H1 Race-2 🐎R2H5 🐎R2H4 🐎R2H3 🐎R2H2 🐎R2H1 Race-3 🐎R3H5 🐎R3H4 🐎R3H3 🐎R3H2 🐎R3H1 Race-4 🐎R4H5 🐎R4H4 🐎R4H3 🐎R4H2 🐎R4H1 Race-5 🐎R5H5 🐎R5H4 🐎R5H3 🐎R5H2 🐎R5H1 Where R is the Race and H is the Horse, 🐎R1H5 is the fifth horse of Race-1.\nResults of first 5 races After the first 5 races, we have a winner for each race:-\nRace-1 🐎R1H5 🐎R1H4 🐎R1H3 🐎R1H2 🐎R1H1 Race-2 🐎R2H5 🐎R2H4 🐎R2H3 🐎R2H2 🐎R2H1 Race-3 🐎R3H5 🐎R3H4 🐎R3H3 🐎R3H2 🐎R3H1 Race-4 🐎R4H5 🐎R4H4 🐎R4H3 🐎R4H2 🐎R4H1 Race-5 🐎R5H5 🐎R5H4 🐎R5H3 🐎R5H2 🐎R5H1 Results:- Slowest Second Last Third Winner Second Winner Winner For simplicity let\u0026rsquo;s assume that the first horse of each race is a winner i.e. 🐎R1H1, 🐎R2H1, 🐎R3H1, 🐎R4H4, 🐎R5H5.\nRace-6 (Race of winners of first 5 races) We have 5 winners and the fastest horse will be in one of them.\nLet\u0026rsquo;s make a 6th race of all the winners of the first 5 races i.e. 🐎R1H1, 🐎R2H1, 🐎R3H1, 🐎R4H4, 🐎R5H5.\nResult of Race-6 We got the fastest horse after the 6th race:-\nRace-6 🐎R5H1 🐎R4H1 🐎R3H1 🐎R2H1 🐎R1H1 Results:- Slowest Second Last Third Winner Second Winner Winner For simplicity let\u0026rsquo;s assume that 🐎R1H1 is the winner of the 6th race.\nNow we know that 🐎R1H1 is the fastest horse out of all 25 horses but we don\u0026rsquo;t know the 2nd and 3rd fastest horse yet. They could be from the Race-1 because the fastest horse belongs that race or from the winners of the first 5 races? Let\u0026rsquo;s find out.\nRace-7 (Race of the Chosen Five) To find the 2nd and 3rd fastest horse, we need to align all the horses as per their ranking in each race and eliminate the horses which are definitely cannot be 2nd and 3rd fastest.\nRace-1 🐎R1H5 🐎R1H4 🐎R1H3 🐎R1H2 🐎R1H1 Race-2 🐎R2H5 🐎R2H4 🐎R2H3 🐎R2H2 🐎R2H1 Race-3 🐎R3H5 🐎R3H4 🐎R3H3 🐎R3H2 🐎R3H1 Race-4 🐎R4H5 🐎R4H4 🐎R4H3 🐎R4H2 🐎R4H1 Race-5 🐎R5H5 🐎R5H4 🐎R5H3 🐎R5H2 🐎R5H1 Results:- Slowest Second Last Third Winner Second Winner Winner Let\u0026rsquo;s think and eliminate:-\nThe first winner of Race-6 is from Race-1 i.e. 🐎R1H1. If it is the fastest horse then the next 2 horses 🐎🐎 in the Race-1 can be the 2nd and 3rd fastest i.e.🐎R1H2 and 🐎R1H3. Eliminate rest. Eliminate the fastest horse 🐎R1H1 as well since we are looking for 2nd and 3rd fastest. The second winner of Race-6 is from Race-2 i.e. 🐎R2H1 so it can be the 2nd fastest horse. If it is the 2nd fastest then the next horse in the Race-2 🐎R2H2 can be 3rd fastest. Eliminate rest. The third winner of Race-6 is from Race-3 i.e. 🐎R3H1** so it can be the 3rd fastest horse. Eliminate rest. The fourth winner of Race-6 is from Race-4. Since we have to find the 2nd and 3rd fastest horse. Eliminate all. The fifth winner of Race-6 is from Race-5. Since we have to find the 2nd and 3rd fastest horse. Eliminate all. Now that we’ve eliminated all of the horses that can’t possibly be the 2nd or 3rd, we are left with the chosen 5 horses, let\u0026rsquo;s do the 7th race of 🐎R1H3, 🐎R1H2, 🐎R2H2, 🐎R2H1, and 🐎R3H1\nFinal Result The 1st and 2nd winners of the 7th race will be the 2nd and 3rd fastest horses out of all 25 horses. So you need a minimum of 7 Races to identify the fastest 3 horses.\n5 races (of all 25) + Race-6 (winners of first 5 races) + Race-7 (the chosen five) = 7 races\nConclusion I have tried to explain the solution as easy as possible. Please let me know by writing in comment section if you still find it difficult to understand.\n","permalink":"https://codingnconcepts.com/puzzle/25-horses-puzzle/","tags":["Interview Puzzle","Google Interview Puzzle"],"title":"25 Horses 5 Tracks Problem (Solved)"},{"categories":["Puzzles"],"contents":"Burning Rope Puzzle comes under basic puzzles asked in the interview.\nPuzzle You have two ropes of varying thickness and length. Each rope burns 🔥 in exactly 60 minutes. How can you measure 45 mins using only these two ropes.\nNote: You can’t cut the rope in half because of varying thickness and you don\u0026rsquo;t have a timer or watch to measure the burning time.\nSolution Burning Rope (Step 1)\nBurn first rope at both the ends and the second rope at one end. The first rope burns completely after half an hour since you burned it from both end. At this point, burn the other end of the second rope so now it will take 15 mins more to burn completely. Phat! total time is 30 + 15 = 45 mins.\n","permalink":"https://codingnconcepts.com/puzzle/burning-rope/","tags":["Interview Puzzle"],"title":"Burning Rope Puzzle (Solved)"},{"categories":["Puzzles"],"contents":"The legend says that this problem was created by Albert Einstein in the last century. Einstein said that only 2% of the world could solve it.\nRiddle There are no tricks, just pure logic, so good luck and don’t give up.\nIn a street there are five houses, painted five different colors. In each house lives a man of different nationality These five homeowners each drink a different kind of beverage, smoke different brand of cigar and keep a different pet. The question is \u0026ldquo;Who owns the fish?\u0026rdquo;\nClues:\nThe British man lives in a red house. The Swedish man keeps dogs as pets. The Danish man drinks tea. The Green house is next to, and on the left of the White house. The owner of the Green house drinks coffee. The person who smokes Pall Mall rears birds. The owner of the Yellow house smokes Dunhill. The man living in the center house drinks milk. The Norwegian lives in the first house. The man who smokes Blends lives next to the one who keeps cats. The man who keeps horses lives next to the man who smokes Dunhill. The man who smokes Blue Master drinks beer. The German smokes Prince. The Norwegian lives next to the blue house. The Blends smoker lives next to the one who drinks water. Solution It\u0026rsquo;s the German.\nHow do you solve it?\nWell, we know from examining the clues and the question that:\nThe possible nationalities are: Norwegian British Swedish Danish German\nThe possible house colors are: Red Green White Yellow Blue\nThe possible beverages are: Tea Coffee Milk Beer Water\nThe possible cigars are: Pall Mall Dunhill Blends BlueMaster Prince\nThe possible pets are: Dogs Birds Cats Horses Fish\nWell, we know there are five houses. We\u0026rsquo;ll assume they\u0026rsquo;re all in a row, and are numbered from left to right. We know the Norwegian is in the first house:\nHouse #1 #2 #3 #4 #5 Color ? ? ? ? ? Nationality Norwegian ? ? ? ? Beverage ? ? ? ? ? Smokes ? ? ? ? ? Pet ? ? ? ? ? Since the British lives in the red house, the Norwegian can\u0026rsquo;t. We also know the Norwegian lives next to the blue house, so his house isn\u0026rsquo;t blue. We also know that the green house is to the left of the white house; the Norwegian can\u0026rsquo;t live in the white house since there is no house to the left, and can\u0026rsquo;t live in the green house because his only neighbor, the one to the right, is known to live in the blue house. Therefore, the Norwegian lives in the yellow house.\nWe also know the owner of the yellow house smokes Dunhill, and that the Norwegian has a neighbor with a blue house (the Norwegian only has one neighbor, to the right.)\nSo here\u0026rsquo;s what our matrix looks like now:\nHouse #1 #2 #3 #4 #5 Color Yellow Blue ? ? ? Nationality Norwegian ? ? ? ? Beverage ? ? ? ? ? Smokes Dunhill ? ? ? ? Pet ? ? ? ? ? The man who keeps horses lives next to he man who smokes Dunhill; so the horse owner lives in the blue house. The center house\u0026rsquo;s owner drinks milk, the green house\u0026rsquo;s owner drinks coffee, and the green house is to the left of the white house. Since we know the left two houses are the yellow and blue houses, the only position for the green and white are green as the fourth and white as the fifth, since the middle (third) drinks milk and the owner of the green house drinks coffee. The middle house has to be red, and therefore is the Brit\u0026rsquo;s. So now this is what we know:\nHouse #1 #2 #3 #4 #5 Color Yellow Blue Red Green White Nationality Norwegian ? British ? ? Beverage ? ? Milk Coffee ? Smokes Dunhill ? ? ? ? Pet ? Horse ? ? ? The owner who smokes BlueMaster drinks beer; since we know what houses #3 and #4 drink [and neither are beer] and we know what house #1 smokes [and its not BlueMaster], the only possibilities are houses #2 and #5. Keep this information in mind. Since it is evident house #1 cannot drink beer (only house #2 or #5 can), the only possible beverages for house #1 are water and tea, but since the Dane drinks tea, house #1 drinks water. The man who smokes Blends lives next to someone who drinks water; the only house next to #1 (the water-drinking house) is #2. The man who smokes Blends lives next to the one who has cats; so the cat-house is #1 or #3.\nHouse #1 #2 #3 #4 #5 Color Yellow Blue Red Green White Nationality Norwegian ? British ? ? Beverage ? Beer? Milk Coffee Beer? Smokes Dunhill Blends ? ? ? Pet Cat? Horse Cat? ? ? Since the Dane drinks tea, he must live in either house #2 or #5. The Swede and German could live in house #2, #4 or #5.\nHouse #1 #2 #3 #4 #5 Color Yellow Blue Red Green White Nationality Norwegian D/S/G? British S/G? D/S/G? Beverage ? Beer? Milk Coffee Beer? Smokes Dunhill Blends ? ? ? Pet Cat? Horse Cat? ? ? We know the beer-drinker smokes BlueMaster. The only houses that could drink beer are #2 and #5, but since we know that #2 smokes Blends, #5 must be the house which drinks beer and smokes BlueMaster, and #2 has to be the house that drinks tea and the house of the Dane. We can eliminate the possibility of the Dane\u0026rsquo;s residence being house #5.\nHouse #1 #2 #3 #4 #5 Color Yellow Blue Red Green White Nationality Norwegian Danish British S/G? S/G? Beverage Water Tea Milk Coffee Beer Smokes Dunhill Blends ? ? BlueMaster Pet Cat? Horse Cat? ? ? We know the German smokes Prince. Therefore, he could not live at house #5 and therefore has to live at house #4. The Swede must live at house #5; we also know house #5 raises dogs since we know the Swede raises dogs, and that house #4 smokes Prince since the German smokes Prince.\nHouse #1 #2 #3 #4 #5 Color Yellow Blue Red Green White Nationality Norwegian Danish British German Swedish Beverage Water Tea Milk Coffee Beer Smokes Dunhill Blends ? Prince BlueMaster Pet Cat? Horse Cat? ? Dogs The only possibility for house #3\u0026rsquo;s smokes is Pall Mall; all of the others are taken. We know that whoever smokes Pall Mall raises birds; so house #3 raises birds, and house #1 therefore has cats, since the only houses which could have had cats were #1 and #3, and #3 has been eliminated.\nHouse #1 #2 #3 #4 #5 Color Yellow Blue Red Green White Nationality Norwegian Danish British German Swedish Beverage Water Tea Milk Coffee Beer Smokes Dunhill Blends Pall Mall Prince BlueMaster Pet Cat Horse Birds ? Dogs The only remaining pet is the fish, which must be owned by the German. We now know who owns the fish, and have solved the puzzle.\nThe completed matrix of data is as follows:\nHouse #1 #2 #3 #4 #5 Color Yellow Blue Red Green White Nationality Norwegian Danish British German Swedish Beverage Water Tea Milk Coffee Beer Smokes Dunhill Blends Pall Mall Prince BlueMaster Pet Cat Horse Birds Fish Dogs ","permalink":"https://codingnconcepts.com/puzzle/einsteins-riddle/","tags":["Einstein Riddle"],"title":"Einstein's Riddle"},{"categories":["Puzzles"],"contents":"This is a pure mathematical puzzle which evaluate your ability to solve maths equation. This is a great problem for building number sense.\nPuzzle The challenge is to make each below equation true using common mathematical operations. You cannot introduce any new digits (so the cube root ∛ is not allowed since it has a 3).\nHint: common mathematical operations are + - x / ! √\n0 0 0 = 6 1 1 1 = 6 2 2 2 = 6 3 3 3 = 6 4 4 4 = 6 5 5 5 = 6 6 6 6 = 6 7 7 7 = 6 8 8 8 = 6 9 9 9 = 6 Solution Note: Please try on your own before looking at the solution. Also some of the equations has multiple solutions.\nLet’s start with some easier ones.\n2 + 2 + 2 = 6 6 + 6 – 6 = 6 6 × (6/6) = 6 7 – 7/7 = 6 5 + 5/5 = 6 Here are a few solutions for 3.\n3 × 3 – 3 = 6 3! + 3 – 3 = 6 3! × (3/3) = 6 √(3 × 3) + 3 = 6 For the number 9, we can use a trick. Since √(9) = 3, we can take the square root of each number, so the problem is equivalent to solving 3 3 3 = 6, which we have just solved! So we can use any of those solutions, or we can find others too.\n√9 × √9 – √9 = 6 (√9)! + √9 – √9 = 6 (√9)! × √9/√9 = 6 (√9 × √9/√9)! = 6 We can do a similar trick for 4. Since √4 = 2, we can use the solution for 2 2 2 = 6. But there are other solutions too.\n√4 + √4 + √4 = 6 (4 – 4/4)! = 6 (√4 + 4/4)! = 6 Now we just have a couple more to solve and we will use 3! = 6 in many of the answers. We can solve for 10 as:\n(√(10 – 10/10))! = 6 Then we have the 1 solution:\n(1 + 1 + 1)! = 6 To solve 0, we use the fact that 0! = 1, and then we have reduced the problem to solving 1 1 1 = 6, which was previously solved.\n(0! + 0! + 0!)! = 6 We just have one more to solve, which many people consider to be the hardest. One way to solve uses nested square roots.\n8 – √(√(8 + 8)) = 6 The other method uses 3! = 6.\n(√(8 + 8/8))! = 6 And we are done! Here are the above solutions listed in numerical order. This is not a comprehensive list. you might have found another way too!\n(0! + 0! + 0!)! = 6 (1 + 1 + 1)! = 6 2 + 2 + 2 = 6 3 × 3 – 3 = 6 3! + 3 – 3 = 6 3! × (3/3) = 6 √(3 × 3) + 3 = 6 √4 + √4 + √4 = 6 (4 – 4/4)! = 6 (√4 + 4/4)! = 6 5 + 5/5 = 6 6 + 6 – 6 = 6 6 ×(6/6) = 6 7 – 7/7 = 6 8 – √(√(8 + 8)) = 6 (√(8 + 8/8))! = 6 √9 × √9 – √9 = 6 (√9)! + √9 – √9 = 6 (√9)! × √9/√9 = 6 (√9 × √9/√9)! = 6 (√(10 – 10/10))! = 6 ","permalink":"https://codingnconcepts.com/puzzle/the-6s-math-puzzle/","tags":["Maths Puzzle"],"title":"The 6s Math Puzzle"},{"categories":["Spring Boot"],"contents":"Swagger library is useful if you are creating REST services in spring boot web application. Swagger user interface allows you to view REST services and execute GET, POST, PUT, DELETE HTTP endpoints. This is helpful since you do not need to use Postman or some other tool to test REST Apis.\nFollow these steps to configure swagger in your spring boot application:-\nAdd Maven Dependencies If you are using maven, then add following swagger dependencies in your pom.xml\npom.xml \u0026lt;properties\u0026gt; \u0026lt;swagger.version\u0026gt;2.9.2\u0026lt;/swagger.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- swagger for api documentation and rest client generation --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springfox-swagger2\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${swagger.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- swagger-ui for api documentation web ui --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springfox-swagger-ui\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${swagger.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springfox-data-rest\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${swagger.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; Define Configuration Properties Instead of hard coding values in swagger configuration we are going to define properties in configuration file. We\u0026rsquo;re using application.yml to define properties. You may also use application.properties file.\napplication.yml app: name: spring boot application api: title: Spring Boot APIs version: 1.0.0 description: Spring Boot APIs description base-package: com.abc.controller contact-name: apisupportgroup contact-email: apisupportgroup@abc.com swagger: enable: true Note that we\u0026rsquo;ve added a property app.api.swagger.enable to enable or disable swagger from our Sprint boot project.\nLet\u0026rsquo;s use all these properties and define our SwaggerConfig class file.\nDefine SwaggerConfig.java Let\u0026rsquo;s create SwaggerConfig class to configure Swagger in our Spring boot project.\n1@Configuration 2@EnableSwagger2 3@ConfigurationProperties(\u0026#34;app.api\u0026#34;) 4@ConditionalOnProperty(name=\u0026#34;app.api.swagger.enable\u0026#34;, havingValue = \u0026#34;true\u0026#34;, matchIfMissing = false) 5public class SwaggerConfig { 6 7\tprivate String version; 8\tprivate String title; 9\tprivate String description; 10\tprivate String basePackage; 11\tprivate String contactName; 12\tprivate String contactEmail; 13\t14\t@Bean 15\tpublic Docket api() { 16\treturn new Docket(DocumentationType.SWAGGER_2) 17\t.select() 18\t.apis(RequestHandlerSelectors.basePackage(basePackage)) 19\t.paths(PathSelectors.any()) 20\t.build() 21\t.directModelSubstitute(LocalDate.class, java.sql.Date.class) 22\t.directModelSubstitute(LocalDateTime.class, java.util.Date.class) 23\t.apiInfo(apiInfo()); 24\t} 25\t26\tprivate ApiInfo apiInfo() { 27\treturn new ApiInfoBuilder() 28\t.title(title) 29\t.description(description) 30\t.version(version) 31\t.contact(new Contact(contactName, null, contactEmail)) 32\t.build(); 33\t} 34 35\t/** 36\t... 37\t... 38\tGetters 39\tSetters 40\t... 41\t... 42\t**/ 43} Note some important points of our SwaggerConfig class file:\n@Configuration annotation is used to auto scan this class file. @EnableSwagger2 annotation is used to help Spring boot project to add necessary dependency for Swagger @ConfigurationProperties annotation is used to read properties from application.yml and initialize fields @ConditionalOnProperty annotation is used to initialize SwaggerConfig bean based on app.api.swagger.enable property flag. Line 21, 22 is correct way in swagger to map \u0026ldquo;Date\u0026rdquo; and \u0026ldquo;DateTime\u0026rdquo; to their corresponding swagger types: Substitute \u0026ldquo;Date\u0026rdquo; types (java.util.LocalDate, org.joda.time.LocalDate) by java.sql.Date. Substitute \u0026ldquo;DateTime\u0026rdquo; types (java.util.ZonedDateTime, org.joda.time.LocalDateTime, …​) by java.util.Date. Verify Swagger That\u0026rsquo;s it. Now define some controllers to expose REST api endpoints and start your spring boot application. You will be able to see swagger UI something like this:-\nURL for Swagger API User Interface\nhttp://localhost:8080/swagger-ui.html URL for Swagger API Docs Json\nhttp://localhost:8080/v2/api-docs Turn off Swagger in Production Swagger user interface is very convenient for development purpose. However we generally turn off the Swagger in production environment due to security concerns.\nYou can disable the Swagger in production if you are using the same SwaggerConfig class file from previous step.\nDisable from the property file if you have environment specific property file application-prod.properties app.api.swagger.enable = false application-prod.yml app: api: swagger: enable: false You can also disable from the command-line parameter command-line parameter $ java -jar -Dapp.api.swagger.enable=false spring-boot-app-1.0.jar OR\n$ java -jar spring-boot-app-1.0.jar --app.api.swagger.enable=false Hide Endpoints from Swagger Documentation While creating Swagger documentation, we often need to hide endpoints from being exposed to end-users. The most common scenario to do so is when an endpoint is not ready yet. Also, we could have some private endpoints which we don\u0026rsquo;t want to expose.\nWe can hide such endpoints from Swagger Docs by annotating controller class or its method with @ApiIgnore annotation.\nHide Specific Endpoint If you want to hide specific endpoint of a controller class, apply the @ApiIgnore annotation at method level.\n@RestController public class UserController { @ApiIgnore @GetMapping(\u0026#34;/getUser\u0026#34;) public String getUser() { return \u0026#34;Ashish Lahoti\u0026#34;; } } Hide All Endpoints If you want to hide all endpoints of a controller class, apply the @ApiIgnore annotation at class level.\n@ApiIgnore @RestController public class UserController { // regular code } Generate REST Client with Swagger Codegen Swagger provides a swagger-codegen-cli utility jar that allows us to generate REST clients for various programming languages and multiple frameworks.\nDownload Jar file Download the latest swagger-codegen-cli.jar from here\nGenerate REST Client Let\u0026rsquo;s generate our REST client by executing the command from command-line:\njava -jar swagger-codegen-cli.jar generate \\ -i http://localhost:8080/v2/api-docs \\ --api-package com.example.client.api \\ --model-package com.example.client.model \\ --invoker-package com.example.client.invoker \\ --group-id com.example \\ --artifact-id spring-swagger-codegen-api-client \\ --artifact-version 0.0.1-SNAPSHOT \\ -l java \\ -o spring-swagger-codegen-api-client The provided arguments consist of:\n-i is source swagger file URL or path –api-package, –model-package, –invoker-package are package names for generated classes –group-id, –artifact-id, –artifact-version are Maven project properties for generated client project -l is the programming language of the generated client -o is the output directory name To list all Java-related options, type the following command:\njava -jar swagger-codegen-cli.jar config-help -l java Download the complete source code from github\n","permalink":"https://codingnconcepts.com/spring-boot/how-to-configure-swagger/","tags":["Spring Boot API","Swagger"],"title":"How to configure Swagger in spring boot"},{"categories":["Spring Boot"],"contents":"Spring boot application comes with default banner which shows up first when you start your application.\n. ____ _ __ _ _ /\\\\ / ___\u0026#39;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | \u0026#39;_ | \u0026#39;_| | \u0026#39;_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) \u0026#39; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.1.3.RELEASE) You can replace above banner with your self made custom banner in following two easy steps:-\nAlso see how to disable spring boot default banner\n1. Creating text banner First we need to create a custom banner for the application. Its very difficult to create a spring like banner manually. You can use any of the following tools to create banner:-\ndevops.datenkollektiv.de patrorjk.com www.network-science.de For example, I created following banner for my spring boot application\n____ _ _ _ ____ _ / ___|___ __| (_)_ __ __ _ | \\ | | / ___|___ _ __ ___ ___ _ __ | |_ ___ | | / _ \\ / _` | | \u0026#39;_ \\ / _` | | \\| | | | / _ \\| \u0026#39;_ \\ / __/ _ \\ \u0026#39;_ \\| __/ __| | |__| (_) | (_| | | | | | (_| | | |\\ | | |__| (_) | | | | (_| __/ |_) | |_\\__ \\ \\____\\___/ \\__,_|_|_| |_|\\__, | |_| \\_| \\____\\___/|_| |_|\\___\\___| .__/ \\__|___/ |___/ |_| 2. Configure text banner in Spring boot application Now we have created text banner, save this in file with name banner.txt under src/main/resources. Spring Boot by default picks up the content from the banner.txt file under resources folder and display it on the startup.\nYou can also change the location or file name by configuring in application.properties or application.yml file\napplication.properties spring.banner.location=classpath:/path/to/custom-banner.txt application.yml spring: banner: location: classpath:/path/to/custom-banner.txt That\u0026rsquo;s it, Restart your application to see your self made banner in console output as below:-\n","permalink":"https://codingnconcepts.com/spring-boot/how-to-create-custom-banner/","tags":["Spring Boot Basics"],"title":"How to create and configure custom banner in spring boot"},{"categories":["Spring Boot"],"contents":"Spring Boot Application converts any command line arguments starting with --, such as --spring.profiles.active=dev to a property by default and adds them to the Spring Environment. Command line properties always take precedence over other property sources.\nIf you do not want command line properties to be added to the Environment, you can disable them from SpringBootApplication main method as follows:-\n@SpringBootApplication public class SpringBootDemoApplication { public static void main(String[] args) { SpringApplication app = new SpringApplication(SpringBootDemoApplication.class); app.setAddCommandLineProperties(false); app.run(args); } } ","permalink":"https://codingnconcepts.com/spring-boot/how-to-disable-command-line-properties/","tags":["Spring Boot Basics"],"title":"How to disable command line properties in spring boot"},{"categories":["Spring Boot"],"contents":"Spring boot web application using embedded server by default runs on port 8080. Following are the ways to change default server port from 8080 to say 9090\nFollow any of the given five ways to change server port:-\n1. application.properties server.port = 9090 2. application.yml server: port: 9090 3. command-line parameter $ java -jar -Dserver.port=9090 spring-boot-app-1.0.jar OR\n$ java -jar spring-boot-app-1.0.jar --server.port=9090 4. SpringBootApplication main method @SpringBootApplication public class SpringBootDemoApplication { public static void main(String[] args) { SpringApplication app = new SpringApplication(SpringBootDemoApplication.class); app.setDefaultProperties(Collections.singletonMap(\u0026#34;server.port\u0026#34;, \u0026#34;9090\u0026#34;)); app.run(args); } } 5. implement WebServerFactoryCustomizer interface @Component public class ServerPortCustomizer implements WebServerFactoryCustomizer\u0026lt;ConfigurableWebServerFactory\u0026gt; { @Override public void customize(ConfigurableWebServerFactory factory) { factory.setPort(9090); } } ","permalink":"https://codingnconcepts.com/spring-boot/how-to-change-server-port/","tags":["Spring Boot Basics"],"title":"How to change server port in spring boot"},{"categories":["Spring Boot"],"contents":"You would have seen below spring boot banner when starting a spring boot application\n. ____ _ __ _ _ /\\\\ / ___\u0026#39;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | \u0026#39;_ | \u0026#39;_| | \u0026#39;_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) \u0026#39; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.1.3.RELEASE) There are multiple ways to disable spring boot banner as follows:\n1. SpringBootApplication main method @SpringBootApplication public class SpringBootDemoApplication { public static void main(String[] args) throws Exception { SpringApplication app = new SpringApplication(SpringBootDemoApplication.class); app.setBannerMode(Banner.Mode.OFF); app.run(args); } } Three possible values of Banner.Mode are as follows:- OFF – Disable printing of the banner. CONSOLE – Print the banner to System.out. LOG – Print the banner to the log file 2. application.properties spring.main.banner-mode=off 3. application.yml spring: main: banner-mode:\u0026#34;off\u0026#34; 4. command-line parameter $ java -jar -Dspring.main.banner-mode=off spring-boot-app-1.0.jar OR\n$ java -jar spring-boot-app-1.0.jar --spring.main.banner-mode=off ","permalink":"https://codingnconcepts.com/spring-boot/how-to-disable-banner/","tags":["Spring Boot Basics"],"title":"How to disable default banner in spring boot"},{"categories":["Interview Questions","Spring Boot"],"contents":"Java developers are now switching from Spring Framework to Spring Boot for enterprise application development in microservice architecture. Spring Boot has become a trending topic to be asked in interviews from Java backend developers in 2025.\nI have spent quite some time to prepare a very comprehensive list of questions and answers being asked in spring boot interviews. I hope, this will benefit both freshers as well as experienced developers in their interview preparation.\nQ1. What is Spring Boot and why we need this? Spring framework has become complicated over the years with lot of features. Initial Spring project setup takes time due to following :-\nInclude all required spring modules and related thirdparty libraries Take care of version compatibility of spring module with other thirdparty jars Understanding of writing spring context xml Setup tomcat and web.xml for web application deployment Build process to generate jar/war and deployment process Best practices to use spring modules ✰ Spring boot solves all this problems and help to create stand-alone, production-grade Spring based applications that you can just run.\nSpring Boot take an opinionated view of the Spring platform and third-party libraries so you can get started with minimum fuss. Most Spring Boot applications need very little Spring configuration.\nFeatures Create stand-alone Spring applications Embedded server (like Tomcat, Jetty or Undertow) to avoid complexity in application (WAR files) deployment Provide opinionated \u0026lsquo;starter\u0026rsquo; dependencies to simplify your build and application configuration Automatically configure Spring and 3rd party libraries whenever possible Provide production-ready features such as metrics, health checks and externalized configuration Absolutely no code generation and no requirement for XML configuration\nQ2. What @SpringBootApplication annotation do? @SpringBootApplication comprises of three annotations which are widely used:-\n@EnableAutoConfiguration enable Spring Boot’s auto-configuration mechanism @ComponentScan enable @Component scan on the package where the application is located @Configuration allow to register extra beans in the context or import additional configuration classes\nQ3. What happens in the background when a spring boot application runs? Looks for active profile and initialize properties and beans based on profile Automatically launch an embedded tomcat server if it is a web application means project has spring-boot-starter-web dependency in pom.xml Q4. What is Spring Initializer? We can quickly generate spring boot project by choosing language (Java, Kotlin, Groovy), builder (Maven,, Gradle), spring boot version, project metadata (group, artifact, name, description, package name, java version) and spring boot dependencies (spring-boot-starter-web, spring-boot-starter-jpa etc.) using Spring Initializer\nQ5. What is spring-boot-starter-parent? The spring-boot-starter-parent is spring boot starter project. It can be used as a parent in our project\u0026rsquo;s pom.xml:\n\u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.6.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; parent project provide following things:-\nDefault Java compiler version UTF-8 source encoding Dependency management inherited from spring-boot-dependencies POM which allow us to omit version tag for common dependencies. Resource filtering only import required thirdparty libraries based on application.properties or application.yml Default configuration for maven plugins such as maven-failsafe-plugin, maven-jar-plugin, maven-surefire-plugin, maven-war-plugin.\nQ6. What is default embedded server and port in spring boot? How to change server port? Spring boot provides support for Tomcat, Jetty and Undertow embedded servers. Default embedded server is Tomcat and default port is 8080.\nDefault server port can be changed using server.port property or command line argument -Dserver.port.\nAlso read different ways to change server port in spring boot\nYou can always change the default server Tomcat to another embedded server such as Jetty or Undertow by:-\nexcluding the dependency of tomcat spring-boot-starter-tomcat from spring-boot-starter-web and, adding the dependency of other server like jetty spring-boot-starter-jetty. \u0026lt;!-- Exclude Spring Boot\u0026#39;s Default Tomcat Server --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-tomcat\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Add Jetty Server Dependency --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jetty\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Q7. What is default logging in Sprint Boot? How to change default logging? Spring boot provides logback as default logging.\nIf you want to change default logging to log4j2 then:-\nexclude spring-boot-starter-logging from spring-boot-starter and, add spring-boot-starter-log4j2 dependency \u0026lt;!-- Exclude Spring Boot\u0026#39;s Default Logback Logging --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-logging\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Add Log4j2 Dependency --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-log4j2\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Q8. How to generate deployable JAR and WAR file in Spring Boot? Spring boot provides a maven plugin spring-boot-maven-plugin to generate JAR and WAR files which we can add in pom.xml as follows:\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; JAR When this plugin is in place, we get a fat executable JAR after executing maven package phase. This JAR contains all the necessary dependencies, including an embedded server. Thus, we do not need to worry about configuring an external server.\nWAR When we want to generate a WAR file, we change packaging to war and change scope of embedded server as provided in pom.xml\n\u0026lt;!-- Set packaging to war, default value is jar if not provided --\u0026gt; \u0026lt;packaging\u0026gt;war\u0026lt;/packaging\u0026gt; \u0026lt;!-- Change the scope of embedded server as provided, so that WAR doesn\u0026#39;t contain embedded server in its package --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-tomcat\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Q9. How to configure properties in Spring Boot? Spring Boot provides support for external configuration, allowing us to run the same application in various environments. We can use properties files, YAML files, environment variables, system properties, and command-line option arguments to specify configuration properties.\nWe can then gain access to those properties using the @Value annotation, a bound object via the @ConfigurationProperties annotation, or the Environment abstraction.\nHere are the most common sources of external configuration:\nCommand-line properties: Command-line option arguments are program arguments starting with a double hyphen, such as –-server.port=8080. Spring Boot converts all the arguments to properties and adds them to the set of environment properties. Application properties: Application properties are those loaded from the application.properties file or its YAML counterpart. By default, Spring Boot searches for this file in the current directory, classpath root, or their config subdirectory. Profile-specific properties: Profile-specific properties are loaded from the application-{profile}.properties file or its YAML counterpart. The {profile} placeholder refers to an active profile. These files are in the same locations as, and take precedence over, non-specific property files.\nQ10. What is Spring Boot Actuator? Spring Boot Actuator provides production-ready features for monitoring and managing spring boot application by exposing many useful HTTP endpoints related to application health, info etc.\nTo Enable Actuator in spring boot application, we just need to include spring-boot-starter-actuator dependency in the pom.xml file:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Some of the most common built-in endpoints Actuator provides:\ninfo: Displays arbitrary application information health: Shows application health information auditevents: Exposes audit events information env: Exposes environment properties httptrace: Displays HTTP trace information metrics: Shows metrics information loggers: Shows and modifies the configuration of loggers in the application mappings: Displays a list of all @RequestMapping paths scheduledtasks: Displays the scheduled tasks in your application threaddump: Performs a thread dump\nQ11. What is Spring Boot Devtools? Spring Boot DevTools, is a set of tools making the development process easier. To Enable DevTools in spring boot application, we just need to add a dependency to the pom.xml file:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-devtools\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; The spring-boot-devtools module is automatically disabled if the application runs in production. The repackaging of archives also excludes this module by default. Hence, it won\u0026rsquo;t bring any overhead to our final product.\nBy default, DevTools applies properties suitable to a development environment. These properties disable template caching, enable debug logging for the web group, and so on. As a result, we have this sensible development-time configuration without setting any properties.\nApplications using DevTools restart whenever a file on the classpath changes. This is a very helpful feature in development, as it gives quick feedback for modifications.\nBy default, static resources, including view templates, don\u0026rsquo;t set off a restart. Instead, a resource change triggers a browser refresh. Notice this can only happen if the LiveReload extension is installed in the browser to interact with the embedded LiveReload server that DevTools contains.\nQ12. What does it mean that Spring Boot supports relaxed binding? Relaxed binding in Spring Boot is applicable to the type-safe binding of configuration properties.\nWith relaxed binding, the key of an environment property doesn\u0026rsquo;t need to be an exact match of a property name. Such an environment property can be written in camelCase, kebab-case, snake_case, or in uppercase with words separated by underscores.\nFor example, if a property in a bean class with the @ConfigurationProperties annotation is named myProp, it can be bound to any of these environment properties: myProp, my-prop, my_prop, or MY_PROP.\n","permalink":"https://codingnconcepts.com/top-spring-boot-interview-questions/","tags":["Interview Q\u0026A","Spring Q\u0026A"],"title":"Top Spring Boot Interview Questions"},{"categories":["Interview Questions"],"contents":"Spring Framework is widely used by Java developers for enterprise application development and most frequent topic to be asked in interviews from Java backend developers in 2025.\nI have spent quite some time to prepare a very comprehensive list of questions and answers being asked in spring framework interview. I hope, this will benefit both freshers as well as experienced developers in their interview preparation.\nQ1. What is Spring Framework? The Spring Framework is a Java platform that provides comprehensive infrastructure support for developing Java applications. Spring handles the infrastructure so you can focus on your application.\nThe Spring Framework is most widely used framework across the globe which provides the best practices to use design patterns such as Singleton, Prototype, Factory, Abstract Factory, Builder, Decorator, Service Locator, and many more out of the box.\nQ2. What are the Benefits of using Spring Framework? Spring framework targets to make Java EE development easier. Here are the advantages of using it:\nLightweight: there is a slight overhead of using the framework in development Inversion of Control (IoC): Spring container takes care of wiring dependencies of various objects, instead of creating or looking for dependent objects Aspect Oriented Programming (AOP): Spring supports AOP to separate business logic from system services IoC container: it manages Spring Bean life cycle and project specific configurations MVC framework: that is used to create web applications or RESTful web services, capable of returning XML/JSON responses Transaction management: reduces the amount of boiler-plate code in JDBC operations, file uploading, etc., either by using Java annotations or by Spring Bean XML configuration file Exception Handling: Spring provides a convenient API for translating technology-specific exceptions into unchecked exceptions\nQ3. What are the Modules available in Spring Framework? Spring Framework Overview\n1. Core Container spring-bean and spring-core modules provide the fundamental parts of the framework, including the IoC and Dependency Injection features. spring-context module builds on the solid base provided by the Core and Beans modules. It provides a way to access java object as beans and manage their life-cycle. It also supports internationalization and Java EE features such as EJB, JMX, and basic remoting. spring-expression module provides a powerful Expression Language for querying and manipulating an object graph at runtime. It is an extension of the unified expression language (unified EL) as specified in the JSP 2.1 specification. The language supports setting and getting property values, property assignment, method invocation, accessing the content of arrays, collections and indexers, logical and arithmetic operators, named variables, and retrieval of objects by name from Spring’s IoC container. 2. AOP and Instrumentation spring-aop module is used to decouple code for cross-cutting concerns such as logging by using method-interceptors and pointcuts. spring-aspect module provide integration with AspectJ spring-instrument module provides class instrumentation support and classloader implementations to be used in certain application servers. spring-instrument-tomcat module contains Spring’s instrumentation agent for Tomcat. 3. Data Access/Integration spring-jdbc module provides a JDBC-abstraction layer that removes the need to do tedious JDBC coding and parsing of database-vendor specific error codes. spring-tx module supports programmatic and declarative transaction management for classes that implement special interfaces and for all your POJOs (Plain Old Java Objects). spring-orm module provides integration layers for popular object-relational mapping APIs, including JPA and Hibernate. Using the spring-orm module you can use these O/R-mapping frameworks in combination with all of the other features Spring offers, such as the simple declarative transaction management feature mentioned previously. spring-oxm module provides an abstraction layer that supports Object/XML mapping implementations such as JAXB, Castor, JiBX and XStream. spring-jms module (Java Messaging Service) contains features for producing and consuming messages. 4. Web spring-web module provides basic web-oriented integration features such as multipart file upload functionality and the initialization of the IoC container using Servlet listeners and a web-oriented application context. It also contains an HTTP client and the web-related parts of Spring’s remoting support. spring-webmvc module (also known as the Web-Servlet module) contains Spring’s model-view-controller (MVC) and REST Web Services implementation for web applications. Spring’s MVC framework provides a clean separation between domain model code and web forms and integrates with all of the other features of the Spring Framework. spring-websocket module provides WebSocket and SockJS infrastructure, including STOMP messaging support 5. Test spring-test module supports the unit testing and integration testing of Spring components with JUnit or TestNG. It provides consistent loading of Spring ApplicationContexts and caching of those contexts. It also provides mock objects that you can use to test your code in isolation.\nQ4. Name Some of the Design Patterns used in the Spring Framework? Singleton Pattern: Singleton-scoped beans (default scope of spring beans) Prototype Pattern: Prototype-scoped beans Factory Pattern: BeanFactory, ApplicationContext Adapter Pattern: Spring Web and Spring MVC Proxy Pattern: Spring Aspect Oriented Programming support Template Method Pattern: RestTemplate, JmsTemplate, JdbcTemplate, JpaTemplate, HibernateTemplate Front Controller: Spring MVC DispatcherServlet Data Access Object: Spring DAO support Model View Controller: Spring MVC\nQ5. What is Inversion of Control? Inversion of Control (IoC) is a programming principle which inverts the flow of control compare to traditional control flow.\nA Java application consists of objects which are dependent on each other and work together to run the application. In a traditional java application, JRE instantiate classes and dependent objects at compile time hance objects are tightly coupled with each other.\nHow we can use IoC in Java application context?\nWe can use various patterns such as Factory, Abstract Factory, Builder \u0026hellip; to instantiate classes and dependent objects at runtime instead of compile time. You see that we have inverted the control from compile time to runtime, which is IoC.\nHow Spring Framework IoC works?\nThe Spring Framework IoC container provides the implementation of Factory, Abstract Factory, Builder, and many more patterns out of the box with best design practices used which you can integrate into your own application(s).\nQ6. What is Dependency Injection? Dependency injection is a technique in which an object receives other objects that it depends on.\nDependency Injection, is a form of IoC, is a general concept stating that you do not create your objects manually but instead describe how they should be created. An IoC container will instantiate required classes if needed.\nHow Spring Framework DI works?\nThe Spring Framework DI provides us the way to describe the object and its dependencies in the the form of XML or Java annotations. Spring IoC container takes care of wire them up together.\nQ7. What are different ways of Dependency Injection in Spring Framework? Spring framework provides three ways for dependency injection:-\n1. Constructor-Based Dependency Injection Spring use constructor to inject dependency. It is recommended to use for mandatory dependencies.\n@Configuration public class AppConfig { @Bean public Item item1() { return new ItemImpl1(); } @Bean public Store store() { return new Store(item1()); } } \u0026lt;bean id=\u0026#34;item1\u0026#34; class=\u0026#34;com.abc.ItemImpl1\u0026#34; /\u0026gt; \u0026lt;bean id=\u0026#34;store\u0026#34; class=\u0026#34;com.abc.Store\u0026#34;\u0026gt; \u0026lt;constructor-arg type=\u0026#34;ItemImpl1\u0026#34; index=\u0026#34;0\u0026#34; name=\u0026#34;item\u0026#34; ref=\u0026#34;item1\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; 2. Setter-Based Dependency Injection Spring use setter method to inject dependency. It is recommended to use for optional dependencies.\n@Bean public Store store() { Store store = new Store(); store.setItem(item1()); return store; } \u0026lt;bean id=\u0026#34;store\u0026#34; class=\u0026#34;com.abc.Store\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;item\u0026#34; ref=\u0026#34;item1\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; 3. Field-Based Dependency Injection Spring looks for fields annotated with @Autowired and inject them using reflection.\npublic class Store { @Autowired private Item item; } Q8. Differentiate between Constructor and Setter Based Dependency Injection? Constructor vs Setter Based Dependency Injection Partial dependency injection is possible in setter-based, say we have 3 dependencies int, string, boolean, if we inject first two, then third boolean will be initialized with default value. Since constructor-based injection call the constructor, we need to pass all the arguments, and so partial injection is not possible. In case of injecting the same dependency using both setter-based and constructor-based then setter-based injection override the constructor-based dependency injection. It\u0026rsquo;s obvious because constructor is called first before setter methods in bean life cycle. It is recommended to user constructor-based injection for mandatory dependencies as it fails if you do not pass all the required dependency, whereas, setter-based injection is recommended for optional dependencies as you can inject partial dependencies. If Object A and B are dependent on each other and you are trying constructor-based injection then Spring throws ObjectCurrentlyInCreationException while instantiating Objects because A object cannot be initialized until B is ready and vice-versa. This circular dependency issue can be solved using setter-based injection.\nQ9. How many types of IOC Containers are there in Spring Framework? BeanFactory is an interface representing a container that provides and manages bean instances. The default implementation instantiates beans lazily when getBean() is called.\nApplicationContext is an interface representing a container holding all information, metadata, and beans in the application. It is built on top of BeanFactory interface but the default implementation instantiates beans eagerly when the application starts. This behavior can be overridden for individual beans.\nThree most commonly used implementation of ApplicationContext are:-\nClassPathXmlApplicationContext loads the XML configuration file from the classpath ApplicationContext context = new ClassPathXmlApplicationContext(“bean.xml”); FileSystemXmlApplicationContext loads the XML configuration file from the file system ApplicationContext context = new FileSystemXmlApplicationContext(“bean.xml”); XmlWebApplicationContext loads the XML configuration file from the /WEB-INF/applicationContext.xml location by default\nQ10. Differentiate between BeanFactory and ApplicationContext? BeanFactory vs ApplicationContext BeanFactory ApplicationContext It is an interface defined in org.springframework .beans.factory package It is an interface defined in org.springframework .context package It uses Lazy initialization It uses Eager initialization by default It explicitly provides a resource object using the syntax It creates and manages resource objects on its own It doesn’t supports internationalization It supports internationalization It doesn’t supports annotation based dependency It supports annotation based dependency Q11. What is a Spring Bean? The Spring Beans are Java Objects that are instantiated, configured, wired, and managed by the Spring IoC container. Spring Bean definition and metadata is provided by XML configuration or Java annotations, which is used by Spring IoC container. Q12. Explain the Spring Bean Life Cycle? Spring Bean Life Cycle\nSpring bean follow the following sequence in its life cycle:-\nDefault constructor will be called. All properties setter methods will be called. If class implements BeanNameAware then setBeanName method will be called. If class implements BeanFactoryAware then setBeanFactory method will be called. If class implements ApplicationContextAware then setApplicationContext method will be called. If class implements BeanPostProcessor then its postProcessBeforeInitialization will be called. If class implements InitializingBean then afterPropertiesSet method will be called. If class has custom init method defined then it will be called. If class implements BeanPostProcessor then its postProcessAfterInitialization will be called. If class implements DisposableBean then destroy method will be called. If class has custom destroy method defined then it will be called. custom-init and custom-destroy example \u0026lt;bean id=\u0026#34;store\u0026#34; class=\u0026#34;com.abc.Store\u0026#34; init-method=\u0026#34;myCustomInit\u0026#34; destroy-method=\u0026#34;myCustomDestroy\u0026#34;\u0026gt; Q13. What are different ways to configure Spring Beans? There are three ways to define Beans configuration in Spring Framework:-\nXML-based Configuration Java-based configuration Annotation-based configuration\nQ14. What is XML-based Spring Configuration? In XML-Based configuration, all the bean definitions and application specific configurations are defined in an XML file in a specific format. Parent XML element is \u0026lt;/beans\u0026gt; and individual bean is defined using \u0026lt;/bean\u0026gt; element.\n\u0026lt;!-- Spring Configurations --\u0026gt; \u0026lt;bean name=\u0026#34;viewResolver\u0026#34; class=\u0026#34;org.springframework.web.servlet.view.BeanNameViewResolver\u0026#34;/\u0026gt; \u0026lt;bean name=\u0026#34;jsonTemplate\u0026#34; class=\u0026#34;org.springframework.web.servlet.view.json.MappingJackson2JsonView\u0026#34;/\u0026gt; \u0026lt;bean id=\u0026#34;restTemplate\u0026#34; class=\u0026#34;org.springframework.web.client.RestTemplate\u0026#34;/\u0026gt; \u0026lt;!-- Bean Definition --\u0026gt; \u0026lt;bean id=\u0026#34;tutorial\u0026#34; class=\u0026#34;com.abc.TutorialBean\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;CodingNConcepts\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; Q15. What is Java-based Spring Configuration? Spring configuration can also be defined using @Configuration annotated classes and @Bean annotated methods where,\n@Configuration annotated class represents an XML configuration file @Bean annotated methods represent the element and responsible to provide a bean definition. @Configuration public class AppConfig { @Bean public MyService myService() { return new MyServiceImpl(); } } is equivalent of following XML configuration\n\u0026lt;beans\u0026gt; \u0026lt;bean id=\u0026#34;myService\u0026#34; class=\u0026#34;com.abc.services.MyServiceImpl\u0026#34;/\u0026gt; \u0026lt;/beans\u0026gt; To instantiate such config, you will need the help of AnnotationConfigApplicationContext class.\npublic static void main(String[] args) { ApplicationContext ctx = new AnnotationConfigApplicationContext(AppConfig.class); MyService myService = ctx.getBean(MyService.class); myService.doStuff(); } or alternatively you can enable component scanning, to instantiate config automatically.\n@Configuration @ComponentScan(basePackages = \u0026#34;com.abc\u0026#34;) public class AppConfig { ... } In the example above, the com.abc package will be scanned and look for @Component or other stereotypes annotated classes, and those classes will be instantiated and managed by as Spring container automatically.\nQ16. What is Annotation-based Spring Configuration? Annotation-based container configuration is an alternative of XML-based configuration and is mostly used by developers. Rather than using XML for describing a bean wiring, the developer moves the configuration to the classes by using annotations on the class, field, or method declaration.\nBy default, annotation config is turned off. It needs to be turned on explicitly by adding \u0026lt;context:annotation-config/\u0026gt; element in Spring Configuration XML file.\n\u0026lt;beans\u0026gt; \u0026lt;context:annotation-config/\u0026gt; \u0026lt;!-- bean definitions go here --\u0026gt; \u0026lt;/beans\u0026gt; Alternatively, you can include AutowiredAnnotationBeanPostProcessor in bean configuration file.\n\u0026lt;beans\u0026gt; \u0026lt;bean class=\u0026#34;org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor\u0026#34;/\u0026gt; \u0026lt;/beans\u0026gt; Q17. Can We Have Multiple Spring Configuration Files in One Project? Yes, in large projects, having multiple Spring configurations is recommended to increase maintainability and modularity.\nYou can load multiple Java-based configuration files:\n@Configuration @Import({MainConfig.class, SchedulerConfig.class}) public class AppConfig { Or load one XML file that will contain all other configs:\nApplicationContext context = new ClassPathXmlApplicationContext(\u0026#34;spring-all.xml\u0026#34;); And inside this XML file you\u0026rsquo;ll have:\n\u0026lt;import resource=\u0026#34;main.xml\u0026#34;/\u0026gt; \u0026lt;import resource=\u0026#34;other.xml\u0026#34;/\u0026gt; Q18. How to define Scope of a Spring Bean? Spring Bean\u0026rsquo;s scope can be defined either using @Scope annotation or scope attribute in XML configuration files. There are five supported scopes:\n\u0026lt;bean id=\u0026#34;mySingleton\u0026#34; class=\u0026#34;com.abc.MySingleton\u0026#34; scope=\u0026#34;singleton\u0026#34;/\u0026gt; singleton: bean scope provides single instance per Spring IoC container. This means everytime spring application context returns same instance when we ask.\nPoints to remember about Singleton bean:-\nSingleton is the default bean scope\nSingleton beans are not thread safe\nprototype: bean scope creates a new instance each and every time a bean is requested from spring container\nrequest: bean scope creates a new instance on every incoming HTTP-request.\nsession: bean scope creates a new instance per HTTP-session.\nglobal-session: bean scope creates a new instance per Global HTTP-session. It is useful for Portlet based applications where bean is used to store global info used by all portlets. This scope works same as session scope in Servlet based applications.\nThe last three are available only if the users use a web-aware ApplicationContext.\nQ19. What are inner beans in Spring? In Spring framework, whenever a bean is used for only one particular property, it is advised to declare it as an inner bean. The inner bean is supported both in setter injection property and constructor injection constructor-arg.\nFor example, let’s say we have Store class having reference of Item class. In our application, we will be creating only one instance of Item class, and use it inside Store.\npublic class Store { private Item item; //Setters and Getters } public class Item { private String name; private long quantity; //Setters and Getters } Now inner bean declaration will look like this:\n\u0026lt;bean id=\u0026#34;StoreBean\u0026#34; class=\u0026#34;com.abc.Store\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;item\u0026#34;\u0026gt; \u0026lt;!-- This is inner bean --\u0026gt; \u0026lt;bean class=\u0026#34;com.abc.Item\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;Fruits\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;quantity\u0026#34; value=\u0026#34;100\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; Q20. Are Singleton beans thread safe in Spring Framework? Spring framework does not do anything under the hood concerning the multi-threaded behavior of a singleton bean. It is the developer’s responsibility to deal with concurrency issue and thread safety of the singleton bean.\nWhile practically, most spring beans have no mutable state (e.g. Service and DAO clases), and as such are trivially thread safe. But if your bean has mutable state (e.g. View Model Objects), so you need to ensure thread safety. The most easy and obvious solution for this problem is to change bean scope of mutable beans from singleton to prototype.\nQ21. How can you inject a Java Collection in Spring? Give example? Spring offers four types of collection configuration elements which are as follows:\n\u0026lt;list\u0026gt; : This helps in wiring ie injecting a list of values, allowing duplicates. \u0026lt;set\u0026gt; : This helps in wiring a set of values but without any duplicates. \u0026lt;map\u0026gt; : This can be used to inject a collection of name-value pairs where name and value can be of any type. \u0026lt;props\u0026gt; : This can be used to inject a collection of name-value pairs where the name and value are both Strings. Let’s see example of each type.\n\u0026lt;beans\u0026gt; \u0026lt;!-- Definition for javaCollection --\u0026gt; \u0026lt;bean id=\u0026#34;javaCollection\u0026#34; class=\u0026#34;com.abc.JavaCollection\u0026#34;\u0026gt; \u0026lt;!-- java.util.List --\u0026gt; \u0026lt;property name=\u0026#34;customList\u0026#34;\u0026gt; \u0026lt;list\u0026gt; \u0026lt;value\u0026gt;India\u0026lt;/value\u0026gt; \u0026lt;value\u0026gt;Singapore\u0026lt;/value\u0026gt; \u0026lt;value\u0026gt;USA\u0026lt;/value\u0026gt; \u0026lt;value\u0026gt;UK\u0026lt;/value\u0026gt; \u0026lt;/list\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- java.util.Set --\u0026gt; \u0026lt;property name=\u0026#34;customSet\u0026#34;\u0026gt; \u0026lt;set\u0026gt; \u0026lt;value\u0026gt;India\u0026lt;/value\u0026gt; \u0026lt;value\u0026gt;Singapore\u0026lt;/value\u0026gt; \u0026lt;value\u0026gt;USA\u0026lt;/value\u0026gt; \u0026lt;value\u0026gt;UK\u0026lt;/value\u0026gt; \u0026lt;/set\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- java.util.Map --\u0026gt; \u0026lt;property name=\u0026#34;customMap\u0026#34;\u0026gt; \u0026lt;map\u0026gt; \u0026lt;entry key=\u0026#34;1\u0026#34; value=\u0026#34;India\u0026#34;/\u0026gt; \u0026lt;entry key=\u0026#34;2\u0026#34; value=\u0026#34;Singapore\u0026#34;/\u0026gt; \u0026lt;entry key=\u0026#34;3\u0026#34; value=\u0026#34;USA\u0026#34;/\u0026gt; \u0026lt;entry key=\u0026#34;4\u0026#34; value=\u0026#34;UK\u0026#34;/\u0026gt; \u0026lt;/map\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- java.util.Properties --\u0026gt; \u0026lt;property name=\u0026#34;customProperies\u0026#34;\u0026gt; \u0026lt;props\u0026gt; \u0026lt;prop key=\u0026#34;admin\u0026#34;\u0026gt;admin@myorg.com\u0026lt;/prop\u0026gt; \u0026lt;prop key=\u0026#34;support\u0026#34;\u0026gt;support@myorg.com\u0026lt;/prop\u0026gt; \u0026lt;/props\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; Q22. How to inject a java.util.Properties into a Spring Bean? You can initialize properties using \u0026lt;props\u0026gt; as below.\n\u0026lt;bean id=\u0026#34;adminUser\u0026#34; class=\u0026#34;com.abc.common.Customer\u0026#34;\u0026gt; \u0026lt;!-- java.util.Properties --\u0026gt; \u0026lt;property name=\u0026#34;emails\u0026#34;\u0026gt; \u0026lt;props\u0026gt; \u0026lt;prop key=\u0026#34;admin\u0026#34;\u0026gt;admin@myorg.com\u0026lt;/prop\u0026gt; \u0026lt;prop key=\u0026#34;support\u0026#34;\u0026gt;support@myorg.com\u0026lt;/prop\u0026gt; \u0026lt;/props\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; Alternatively you can use \u0026lt;util:properties\u0026gt; to define properties from a properties file, and use bean reference for setter injection.\n\u0026lt;util:properties id=\u0026#34;emails\u0026#34; location=\u0026#34;classpath:com/foo/emails.properties\u0026#34; /\u0026gt; Q23. Explain Spring Bean Autowiring? Autowiring allows the Spring container to automatically resolve dependencies between collaborating beans by inspecting the beans that have been defined. Spring Bean Autowiring can be done in three ways:-\nXML-based configuration using \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;store\u0026#34; class=\u0026#34;com.abc.Store\u0026#34; autowire=\u0026#34;byType\u0026#34; /\u0026gt; Annotation-based configuration using @Autowired on properties, setter-methods or constructor public class Store { @Autowired private Item item; } Java-based configuration using @Bean @Bean(autowire = Autowire.BY_TYPE) public class Store { private Item item; public setItem(Item item){ this.item = item; } } Q24. Explain different Modes of Spring Bean Autowiring? There are five autowiring modes in spring framework. Lets discuss them one by one.\nno: This option is default for spring framework and it means that autowiring is OFF. You have to explicitly set the dependencies using tags in bean definitions. byName: This option enables the dependency injection based on bean names. When autowiring a property in bean, property name is used for searching a matching bean definition in configuration file. If such bean is found, it is injected in property. If no such bean is found, a error is raised. byType: This option enables the dependency injection based on bean types. When autowiring a property in bean, property’s class type is used for searching a matching bean definition in configuration file. If such bean is found, it is injected in property. If no such bean is found, a error is raised. constructor: Autowiring by constructor is similar to byType, but applies to constructor arguments. In autowire enabled bean, it will look for class type of constructor arguments, and then do a autowire by type on all constructor arguments. Please note that if there isn’t exactly one bean of the constructor argument type in the container, a fatal error is raised. autodetect: Autowiring by autodetect uses either of two modes i.e. constructor or byType modes. First it will try to look for valid constructor with arguments, If found the constructor mode is chosen. If there is no constructor defined in bean, or explicit default no-args constructor is present, the autowire byType mode is chosen.\nQ25. Explain @Required annotation with example? This annotation simply indicates that the affected bean property must be populated at configuration time, through an explicit property value in a bean definition or through autowiring. The container throws BeanInitializationException if the affected bean property has not been populated.\nThis annotation is used to overcome the problem arise with setter-based injection where spring container doesn\u0026rsquo;t warn or throw any exception if all the required properties are not populated.\nWe can use @Required annotation over setter-method of bean property in class file to indicate it is a mandatory property,\npublic class Store { private Item item; public Item getItem() { return item; } @Required public void setItem(Item item) { this.item = item; } } Q26. Explain @Autowired annotation with example? This annotation provides more fine-grained control over where and how autowiring should be accomplished. The @Autowired annotation can be used to autowire bean on the setter method just like @Required annotation, constructor, a property or methods with arbitrary names and/or multiple arguments.\nE.g. You can use @Autowired annotation on setter methods to get rid of the \u0026lt;property\u0026gt; element in XML configuration file. When Spring finds an @Autowired annotation used with setter methods, it tries to perform byType autowiring on the method.\nYou can apply @Autowired to constructors as well. A constructor @Autowired annotation indicates that the constructor should be autowired when creating the bean, even if no \u0026lt;constructor-arg\u0026gt; elements are used while configuring the bean in XML file.\npublic class TextEditor { private SpellChecker spellChecker; @Autowired public TextEditor(SpellChecker spellChecker){ System.out.println(\u0026#34;Inside TextEditor constructor.\u0026#34; ); this.spellChecker = spellChecker; } public void spellCheck(){ spellChecker.checkSpelling(); } } And it’s configuration without constructor arguments.\n\u0026lt;beans\u0026gt; \u0026lt;context:annotation-config/\u0026gt; \u0026lt;!-- Definition for textEditor bean without constructor-arg --\u0026gt; \u0026lt;bean id=\u0026#34;textEditor\u0026#34; class=\u0026#34;com.howtodoinjava.TextEditor\u0026#34;\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;!-- Definition for spellChecker bean --\u0026gt; \u0026lt;bean id=\u0026#34;spellChecker\u0026#34; class=\u0026#34;com.howtodoinjava.SpellChecker\u0026#34;\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; Q27. Explain @Qualifier annotation with example? There may be a situation when you create two or more beans of the same type and want to wire only one of them with a property, in such case you can use @Qualifier annotation along with @Autowired to remove the confusion by specifying which exact bean will be wired.\nSee below example, it will autowire a item bean into Store class,\npublic class Store { @Autowired private Item item; } And we have two bean definitions for Item class.\n\u0026lt;bean id=\u0026#34;store\u0026#34; class=\u0026#34;com.abc.Store\u0026#34; /\u0026gt; \u0026lt;bean id=\u0026#34;itemA\u0026#34; class=\u0026#34;com.abc.Item\u0026#34; \u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;Fruit\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;itemB\u0026#34; class=\u0026#34;com.abc.Item\u0026#34; \u0026gt; \u0026lt;property name=\u0026#34;name\u0026#34; value=\u0026#34;Biscuits\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; If you run the above example spring will throw an exception because it doesn\u0026rsquo;t know which bean to autowire in Store class, itemA or itemB?\nCaused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No unique bean of type [com.abc.Item] is defined: expected single matching bean but found 2: [itemA, itemB] We can use @Qualifier annotation to wire specific itemA bean like this:-\npublic class Store { @Autowired @Qualifier(\u0026#34;itemA\u0026#34;) private Item item; } Q28. What are Spring stereotype annotations? @component vs @service vs @repository vs @controller All the stereotype annotations are used at annotate classes auto-detection and bean configuration. There are mainly 4 types of stereotypes:-\n@Component is a generic stereotype to annotate classes at any layer @Controller is used to annotate classes at API layer, mainly used in Spring MVC applications to define API endpoint and used in conjunction with @RequestMapping annotation which is used to annotate method to maps the request URL. @Service is used to annotate classes at the service layer @Repository is used to annotate classes at the persistence layer, which will act as a database repository\nQ29. What are JSR-250 annotations? @PostConstruct − This annotation can be used as an alternate of initialization callback. @PreDestroy − This annotation can be used as an alternate of destruction callback. @Resource − This annotation can be used on fields or setter methods. The @Resource annotation takes a \u0026rsquo;name\u0026rsquo; attribute which will be interpreted as the bean name to be injected. You can say, it follows by-name autowiring semantics.\nQ30. What happens when you define two beans with same id or same name ? beans.xml \u0026lt;bean id=\u0026#34;foo\u0026#34; name=\u0026#34;sameName\u0026#34; class=\u0026#34;com.abc.Foo\u0026#34; /\u0026gt; \u0026lt;bean id=\u0026#34;bar\u0026#34; name=\u0026#34;sameName\u0026#34; class=\u0026#34;com.abc.Bar\u0026#34; /\u0026gt; When you use two bean with same name (or same id) in single configuration file then \u0026ldquo;BeanDefinitionParsingException – Bean name \u0026lsquo;sameName\u0026rsquo; is already used in this file\u0026rdquo; is thrown by the spring container at the time of loading.\nWhere things get a little interesting is when bean definitions are spread out across many configuration files. Say, for example, the foo bean is defined in beans1.xml while the bar bean is defined in beans2.xml.\nbeans1.xml \u0026lt;bean id=\u0026#34;foo\u0026#34; name=\u0026#34;sameName\u0026#34; class=\u0026#34;com.abc.Foo\u0026#34; /\u0026gt; beans2.xml \u0026lt;bean id=\u0026#34;bar\u0026#34; name=\u0026#34;sameName\u0026#34; class=\u0026#34;com.abc.Bar\u0026#34; /\u0026gt; Now, if the two beans have the same name (or id), surprisingly no exception is thrown by the container!!!\nFileSystemXmlApplicationContext context = new FileSystemXmlApplicationContext( new String[] { \u0026#34;beans1.xml\u0026#34;, \u0026#34;beans2.xml\u0026#34; }); Bar f = (Bar) context.getBean(\u0026#34;sameName\u0026#34;); // works fine The documentation says \u0026ldquo;the last bean definition with the same name (or id) wins, respectively to the order of the xml files.\u0026rdquo; In the example Java code above, beans2.xml is loaded in the last. Therefore, bean class com.abc.Bar will be loaded.\nyou can always turn off this default behavior and make cause bean ids/names to be unique across all configurations of the container. Look into the DefaultListableBeanFactory setAllowBeanDefinitionOverriding() method.\nQ31. Explain Spring MVC Workflow? Spring MVC Workflow\nWhen you request from spring MVC application then it follow the following sequence:-\n1. Dispatcher Servlet Dispatcher Servlet which is also know as front controller is the core of Spring MVC Workflow and responsible for handling all HTTP request and response. If you are using tomcat as web application server then Dispatcher Servlet need to define in web.xml\nThe DispatcherServlet receives the entry of handler mapping from the configuration file and forwards the request to the controller. The controller then returns an object of Model And View. The DispatcherServlet checks the entry of view resolver in the configuration file and calls the specified view component.\n2. Handler Mapping Handler Mapping is a configuration for url and controller mapping.\nFollowing are the different implementation of HandlerMapping:-\n1) BeanNameUrlHandlerMapping This is a default spring handler mapping. Name of bean considered as URL.\n\u0026lt;bean name=\u0026#34;/welcome.htm\u0026#34; class=\u0026#34;com.abc.controller.WelcomeController\u0026#34;/\u0026gt; 2) SimpleUrlHandlerMapping Map with key value pair of URL and controller bean name.\n\u0026lt;bean class=\u0026#34;org.springframework.web.servlet.handler.SimpleUrlHandlerMapping\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;mappings\u0026#34;\u0026gt; \u0026lt;props\u0026gt; \u0026lt;prop key=\u0026#34;/welcome.htm\u0026#34;\u0026gt;welcomeController\u0026lt;/prop\u0026gt; \u0026lt;/props\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;welcomeController\u0026#34; class=\u0026#34;com.abc.controller.WelcomeController\u0026#34; /\u0026gt; 3) DefaultAnnotationHandlerMapping This is the most popular implementation of HandlerMapping where each class annotated with @Controller maps one or more request to methods that process and execute the request with provided inputs.\n@RequestMapping annotation is used at both class and method level to map the URL. In addition to simple use cases, we can use it for mapping of HTTP headers, binding parts of the URI with @PathVariable, and working with URI parameters and the @RequestParam annotation.\npackage com.abc.controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.servlet.ModelAndView; @Controller @RequestMapping(\u0026#34;/welcome\u0026#34;) public class WelcomeController{ @RequestMapping(method = RequestMethod.GET) public ModelAndView helloWorld(){ ModelAndView model = new ModelAndView(\u0026#34;WelcomePage\u0026#34;); model.addObject(\u0026#34;msg\u0026#34;, \u0026#34;hello world\u0026#34;); return model; } } 3. Controller Controller calls service layer to execute business logic and return ModelAndView, which is wrapper for model object and view name.\nPopular Controller Types:\nAbstractController\nAbstractCommandController\nSimpleFormController\n4. View Resolver View Resolver look for appropriate view JSP/HTML based on view name.\nPopular Implementations of ViewResolver:\nInternalResourceViewResolver\n5. View Dispatcher Servlet return the view JPS/HTML component to user\nQ32. Differentiate between @Controller and @RestController? @Controller vs @RestController Even though both are used to indicate that a Spring bean is a Controller in Spring MVC setup, @RestController is better when you are developing RESTful web services using Spring MVC framework. It\u0026rsquo;s a combination of @Controller + @ResponseBody annotation which allows the controller to directly write the response and bypassing the view resolution process, which is not required for RESTful web service.\nIt also instructs DispatcherServlet to use different HttpMessageConverters to represent the response in the format client is expecting e.g. HttpMessageJackson2Convert to represent response in JSON format and JAXB based message converts to generate XML response.\nQ33. What Is Spring Security? Spring Security is a separate module of the Spring framework that focuses on providing authentication and authorization methods in Java applications. It also takes care of most of the common security vulnerabilities such as CSRF attacks.\nTo use Spring Security in web applications, you can get started with a simple annotation: @EnableWebSecurity.\nQ34. What Is Aspect-Oriented Programming? Aspect-oriented programming or AOP is a programming technique which allows programmers to modularize crosscutting concerns or behavior that cuts across the typical divisions of responsibility. Examples of cross-cutting concerns can be logging and transaction management. The core of AOP is an aspect. It encapsulates behaviors that can affect multiple classes into reusable modules. Q35. What Are Aspect, Advice, Pointcut, and Joinpoint in Aop? Aspect: a class that implements cross-cutting concerns, such as transaction management Advice: the methods that get executed when a specific JoinPoint with matching Pointcut is reached in the application Pointcut: a set of regular expressions that are matched with JoinPoint to determine whether Advice needs to be executed or not JoinPoint: a point during the execution of a program, such as the execution of a method or the handling of an exception\nQ36. What are the different types of Advices? Different types of Advices in Spring AOP are:\nBefore: These types of advices execute before the joinpoint methods and are configured using @Before annotation mark. After returning: These types of advices execute after the joinpoint methods completes executing normally and are configured using @AfterReturning annotation mark. After throwing: These types of advices execute only if joinpoint method exits by throwing an exception and are configured using @AfterThrowing annotation mark. After (finally): These types of advices execute after a joinpoint method, regardless of the method’s exit whether normally or exceptional return and are configured using @After annotation mark. Around: These types of advices execute before and after a joinpoint and are configured using @Around annotation mark.\nQ37. What is cross-cutting concerns in Spring AOP? The concern is the behavior we want to have in a particular module of an application. It can be defined as a functionality we want to implement.\nThe cross-cutting concern is a concern which is applicable throughout the application. This affects the entire application. For example, logging, security and data transfer are the concerns needed in almost every module of an application, thus they are the cross-cutting concerns.\nSpring AOP - Cross Cutting Concerns\nQ38. What is Weaving? According to the official docs, weaving is a process that links aspects with other application types or objects to create an advised object. This can be done at compile time, load time, or at runtime. Spring AOP, like other pure Java AOP frameworks, performs weaving at runtime.\nQ39. What is Spring Dao? Spring Data Access Object is Spring\u0026rsquo;s support provided to work with data access technologies like JDBC, Hibernate, and JPA in a consistent and easy way.\nThis also allows to switch between the persistence technologies easily. It also allows you to code without worrying about catching exceptions that are specific to each of these technology.\nQ40. What is Spring Jdbctemplate Class and How to Use it? The Spring JDBC template is the primary API through which we can access database operations logic that we’re interested in:\ncreation and closing of connections executing statements and stored procedure calls iterating over the ResultSet and returning results To use it, we\u0026rsquo;ll need to define the simple configuration of DataSource:\n@Configuration @ComponentScan(\u0026#34;com.abc.jdbc\u0026#34;) public class SpringJdbcConfig { @Bean public DataSource mysqlDataSource() { DriverManagerDataSource dataSource = new DriverManagerDataSource(); dataSource.setDriverClassName(\u0026#34;com.mysql.jdbc.Driver\u0026#34;); dataSource.setUrl(\u0026#34;jdbc:mysql://localhost:3306/springjdbc\u0026#34;); dataSource.setUsername(\u0026#34;guest_user\u0026#34;); dataSource.setPassword(\u0026#34;guest_password\u0026#34;); return dataSource; } } Q41. How Would You Enable Transactions in Spring and What Are Their Benefits? There are two distinct ways to configure Transactions – with annotations or by using Aspect Oriented Programming (AOP) – each with their advantages.\nThe benefits of using Spring Transactions, according to the official docs, are:\nProvide a consistent programming model across different transaction APIs such as JTA, JDBC, Hibernate, JPA, and JDO Support declarative transaction management Provide a simpler API for programmatic transaction management than some complex transaction APIs such as JTA Integrate very well with Spring\u0026rsquo;s various data access abstractions\nQ42. What is Spring Boot? Spring boot solves all this problems that comes with Spring Framework and help to create stand-alone, production-grade Spring based applications that you can just run.\nFollow the post for Top Spring Boot Interview Questions\nSource Spring 5.0.0.RC3 Official Documentation Baeldung Spring Interview Questions Edureka Spring Interview Questions HowToDoInJava Spring Interview Questions ","permalink":"https://codingnconcepts.com/top-spring-interview-questions/","tags":["Interview Q\u0026A","Spring Q\u0026A"],"title":"Top Spring Interview Questions"},{"categories":["Interview Questions","Java"],"contents":"Comprehensive List of Java Interview Questions based on my personal interview experience over the last few years. Sequence of Questions in each section are in the order of recent interview trends. Keep following this post link for regular updates.\nAlso Read Java 8 Interview Questions\nAlso Read Java Multithreading (Concurrency) Interview Questions\nJava Design Patterns Q1. What is Singleton Pattern and how do you implement it? Singleton design pattern ensures that a class has only one instance and provides a global point of access to it.\nThis is one of the most commonly asked question in interviews. Then interviewer asks to implement a Singleton Design Pattern Using Java.\nFollow up questions in singleton pattern are as follows:\nHow to make singleton class thread safe?\nusing double checked locking How to prevent deserialization to create new object of singleton class?\nusing readResolve method to return same instance How to prevent cloning to create a new object of singleton class?\noverride clone method to return same instance How to prevent reflexion to create a new object of singleton class?\nthrow exception from private constructor if instance already exist Singleton Class public class Singleton implements Serializable, Cloneable{ private static final long serialVersionUID = 1L; private static Singleton instance = null; private static Object DUMMY_OBJECT = new Object(); private Singleton(){ /*To prevent object creation using reflection*/ if(instance!=null){ throw new InstantiationError( \u0026#34;Singleton Object is already created.\u0026#34; ); } } public static Singleton getInstance(){ /*Double checked locking*/ if(instance == null){ synchronized (DUMMY_OBJECT) { if(instance == null){ instance = new Singleton(); } } } return instance; } public static void print(){ System.out.println(\u0026#34;I am a singleton class.\u0026#34;); } /*To prevent object creation using deserialization*/ private Object readResolve() throws ObjectStreamException{ return instance; } /*To prevent object creation using cloning*/ @Override protected Object clone() throws CloneNotSupportedException { return instance; } } Q2. What is Immutable Object? How do you write an Immutable Class? Immutable Class means that once an object is initialized from this Class, we cannot change the state of that object.\nIn other words, An immutable object can’t be modified after it has been created. When a new value is needed, the accepted practice is to make a copy of the object that has the new value.\nIn order to create an Immutable Class in Java, you should keep following points in mind:-\nDeclare the class as final so that it cannot be extended and subclasses will not be able to override methods. Make all the fields as private so direct access in not allowed Make all the fields as final so that value cannot be modified once initialized Provide no setter methods — setter methods are those methods which modify fields or objects referred to by fields. Initialize all the final fields through a constructor and perform a deep copy for mutable objects. If the class holds a mutable object: Don\u0026rsquo;t provide any methods that modify the mutable objects. Always return a copy of mutable object from getter method and never return the actual object reference. Let’s apply all the above points and create our immutable class ImmutablePerson\nImmutablePerson.java /** * Immutable class should mark as final so it can not be extended. * Fields should mark as private so direct access is not allowed. * Fields should mark as final so value can not be modified once initialized. **/ public final class ImmutablePerson { // String - immutable private final String name; // Integer - immutable private final Integer weight; // Date - mutable private final Date dateOfBirth; /** * All the final fields are initialized through constructor * Perform a deep copy of immutable objects */ public ImmutablePerson(String name, Integer weight, Date dateOfBirth){ this.name = name; this.weight = weight; this.dateOfBirth = new Date(dateOfBirth.getTime()); } /********************************************** ***********PROVIDE NO SETTER METHODS ********* **********************************************/ /** * String class is immutable so we can return the instance variable as it is **/ public String getName() { return name; } /** * Integer class is immutable so we can return the instance variable as it is **/ public Integer getWeight() { return weight; } /** * Date class is mutable so we need a little care here. * We should not return the reference of original instance variable. * Instead a new Date object, with content copied to it, should be returned. **/ public Date getDateOfBirth() { return new Date(dateOfBirth.getTime()); } @Override public String toString() { return \u0026#34;Person { name: \u0026#34; + name + \u0026#34;, weight: \u0026#34; + weight + \u0026#34;, dateOfBirth: \u0026#34; + new SimpleDateFormat(\u0026#34;dd-MM-yyyy\u0026#34;).format(dateOfBirth) + \u0026#34;}\u0026#34;; } } Q3. Give example of 5-6 design patterns being used in Java classes. Factory\njava.util.Calendar#getInstance() Static Factory\nLong#valueOf() Abstract Factory\njavax.xml.parsers.DocumentBuilderFactory#newInstance()\njavax.xml.transform.TransformerFactory#newInstance()\njavax.xml.xpath.XPathFactory#newInstance()\njava.util.Collections#emptyMap()\njava.util.Collections#emptyList()\njava.util.Collections#emptySet() Builder\njava.lang.StringBuilder#append()\njava.lang.StringBuffer#append() Decorator\nAll subclasses of java.io.InputStream, OutputStream, Reader and Writer. Chain of responsibility\njavax.servlet.Filter#doFilter() Iterator\njava.util.Iterator Observer\nJMS message listener Singleton\njava.lang.Runtime#getRuntime() Adapter java.util.Arrays#asList()\njava.io.InputStreamReader(InputStream) (returns a Reader)\njava.io.OutputStreamWriter(OutputStream) (returns a Writer) Immutable\nString, Integer, Byte, Long, Float, Double, Character, Boolean and Short Q4. Anti patterns in Java. What is god class? Anti patterns in java is:-\nAny pattern or coding habit which is considered as bad practice Which are against SOLID design principles God class is one of the Anti pattern example. A god class is a class which is having too many properties or methods or dependencies and hence lots of responsibilities which violates single-responsibility SOLID design principle. Writing thousands lines of code in single class becomes a nightmare to debug, unit-test and document.\nJava DataStructure Q1. How HashMap Works? This is also most commonly asked question in collections. Interviewer start this question asking about collections and what are the data structure/collection you have used in development. Candidate generally answers HashMap, LinkedList, ArrayList, HashSet. Then interviewer check your knowledge of equals, hashcode and hashing algorithm. You should be aware of HashMap class internal implementation.\nRead Design your own HashMap in Java for HashMap implementation. Follow up questions in HashMap are as follows:\nHow to use HashMap in multi threading environment?\nYou should know that HashTable is thread safe. You can make HashMap thread safe by using Collections.synchronizedMap(Map) What is Concurrent HashMap? How it is better then thread safe HashMap in multi threading environment?\nSynchronized HashMap locks the whole map when one thread is accessing it resulting into other thread to wait. Concurrent HashMap provides the better performance by locking only the specific segment (similar to bucket in hashmap) rather than blocking whole map. By default, there are 16 segments, means 16 locks, means 16 concurrency level, means 16 threads can access 16 different segment of the map at the same time.\nQ2. What is the difference in ArrayList and LinkedList? ArrayList vs LinkedList ArrayList LinkedList 1. ArrayList implements List and RandomAccess interfaces LinkedList implements List and Deque interface 2. ArrayList internally uses a dynamic array to store the elements. LinkedList internally uses a doubly linked list to store the elements. 3. ArrayList elements are stored in contiguous memory location where each memory location represents an index Each LinkedList element can be stored at different memory location where each element has address of it previous and next element. 4. Insertion and Deletion of elements is slow in ArrayList as these operations require all the elements to shift by one index to either make the space or fill the empty space Insertion and Deletion of elements is faster in LinkedList as these operations just require to change the addresses in previous and next block 5. Read operation of random element is much faster in ArrayList due to indices and support of fast random access Read operation of random element is slower in LinkedList as need to traverse the list of find element 6. Recommended when read operation is more than the number of insertions and deletions Recommended when insertion and deletion rate is higher then the read operation methods ArrayList LinkedList get(int index) O(1) O(n) with n/4 steps on average getFirst(), getLast() NA O(1) add(int index, E element) O(n) with n/2 steps on average O(n) with n/4 steps on average addFirst(), addLast() NA O(1) remove(int index) O(n) with n/2 steps on average O(n) with n/4 steps on average removeFirst(), removeLast() NA O(1) Iterator.remove() O(n) with n/2 steps on average O(1) ListIterator.add(E element) O(n) with n/2 steps on average O(1) ArrayList: Many of the operations need n/2 steps on average, constant number of steps in the best case (end of list), n steps in the worst case (start of list) LinkedList: Many of the operations need n/4 steps on average, constant number of steps in the best case (e.g. index = 0), and n/2 steps in worst case (middle of list)\nQ3. How to find middle element of Linked List? It is always better to keep track of Linked list size by increasing or decreasing the counter by 1, on addition or deletion of nodes from linked list respectively. In this way, middle element\u0026rsquo;s index will be (size -1)/2 When size of the linked list is unknown and only head node is given then you should use Fast and Slow pointer approach.\nIn this approach, we iterate through the linked list using two pointers. Fast pointer jumps 2 nodes in each iteration, and the slow pointer jumps only one node per iteration.\nWhen the fast pointer reaches the end of the list, the slow pointer will be at the middle element. Also read How to find middle element of LinkedList in Java for more details\nQ4. How to find a loop in LinkedList? If you answer it then follow up question might be asked:\nHow to find the starting point of loop in LinkedList? How to find the length of the loop in LinkedList?\nQ5. Difference between ArrayList and Vector. These is legacy interview questions which is not asked frequently now a days but good to know.\nArrayList vs Vector ArrayList Vector 1. ArrayList\u0026rsquo;s methods are not synchronized Vector\u0026rsquo;s methods are synchronized 2. ArrayList is fast as it’s non-synchronized. Vector is slow because it is synchronized 3. If internal array runs out of room, ArrayList increases its size by 50% Vector defaults to doubling size of its array 4. ArrayList can only use Iterator for traversing Vector can use both Enumeration and Iterator for traversing over elements Java String Q1. What is String Constant Pool? String Constant Pool is a pool of Strings (String literals) stored in Java Heap Memory. A dedicated space is provided for pool in Heap Memory.\nWhen you ask for a string from this pool, it provides the same string if already exist, otherwise it creates a new string in the pool and returns that string. This process is also called as string intern which makes the string literals reusable.\nQ2. Difference between String Literal and New String Object. String literal vs New String Object String Literals New String Object String literals are maintained in String Constant Pool New String objects are created in Heap Memory String literals are interned by default New String objects can be interned explicitly using intern() method // String Literal String a = \u0026#34;abc\u0026#34;; String b = \u0026#34;abc\u0026#34;; System.out.println(a == b); // true // String Object String c = new String(\u0026#34;abc\u0026#34;); String d = new String(\u0026#34;abc\u0026#34;); System.out.println(c == d); // false System.out.println(a == d); // false System.out.println(a == d.intern()); // true System.out.println(c.intern() == d.intern()); //true Q3. Difference between String, StringBuilder and StringBuffer. String vs StringBuilder vs StringBuffer String object is immutable whereas StringBuffer and StringBuilder objects are mutable. String\u0026rsquo;s concat \u0026ldquo;+\u0026rdquo; operator internally uses StringBuffer or StringBuilder class. StringBuilder vs StringBuffer StringBuilder and StringBuffer provides similar methods such as substring(), insert(), append(), delete() StringBuffer is thread safe and synchronized whereas StringBuilder is not, thats why StringBuilder is more faster than StringBuffer. Most of the time you will be using StringBuilder. Use StringBuffer if you really are trying to share a buffer between threads.\nQ4. Why char[] array is used over String to store password in Java? Strings are immutable in Java and any change in a String object produces a new String. When you store passwords in a String object, they stays in memory until Garbage collector clear it, which is a security threat as anyone with access to the memory dump retrieve the password from memory.\nMoreover, String use String Pool memory for reusability, which even stays in memory for long duration as compare to other objects.\nWhereas if password is stored in char[] array, we can explicitly wipe the password after completing the work. The array can be overwritten and password will no longer be there in memory.\nJava OOPS Java OOPS concepts are very frequently asked interview questions for Java beginners.\nQ1. What is Polymorphism in Java? Polymorphism means \u0026ldquo;many forms\u0026rdquo;, and it allows us to perform same action on an object in different ways.\nThere are two types of polymorphism in java:\nStatic Polymorphism also known as compile time polymorphism - example is Method Overloading Dynamic Polymorphism also known as runtime polymorphism - example is Method Overriding Q2. Explain Overloading in Java? Method Overloading is a Compile time polymorphism. In method overloading, method name must be same, but method arguments must be different by number or type or both. In method overloading, return type can be same or different, it doesn\u0026rsquo;t affect the overloading.\npublic class Calculator { // base-method public int add(int a, int b) { return a+b; } // throw compile time error - duplicate method // we can not overload base-method by just changing return type public long add(int a, int b) { return a+b; } // overloaded method - number of arguments (3) are different than base-method (2) public int add(int a, int b, int c) { return a+b+c; } // overloaded method - argument types are different than base-method public String add(String a, String b) { return a+b; } } Q3. Explain Overriding in Java? Method Overriding is a Run time polymorphism. In method overriding, derived (child) class provides the specific implementation of the method that is already provided by the base (parent) class. In method overriding, method signature must be same i.e. method name, arguments and return type.\npackage com.example.core.oops; public class Overiding { public static void main(String[] args) { Animal animal = new Animal(); animal.makeSound(); // prints \u0026#34;Animal making sound\u0026#34; Animal dog = new Dog(); dog.makeSound(); // prints \u0026#34;Bow Bow\u0026#34; Animal cat = new Cat(); cat.makeSound(); // prints \u0026#34;Meow Meow\u0026#34; } } class Animal { public void makeSound() { System.out.println(\u0026#34;Animal making sound\u0026#34;); } } class Dog extends Animal { public void makeSound() { System.out.println(\u0026#34;Bow Bow\u0026#34;); } } class Cat extends Animal { public void makeSound() { System.out.println(\u0026#34;Meow Meow\u0026#34;); } } Q4. Can you Override a static method and private method ? No, you cannot override static method since static method is associated with a class whereas overriding is an object instance feature. No, you cannot override private method since private method is not visible outside of a class.\nQ5. What is multiple Inheritance? Is it supported by Java? If a child class inherits the property from multiple classes is known as multiple inheritance. Java does not allow to extend multiple classes.\nThe problem with multiple inheritance is that if multiple parent classes have the same method name, then at runtime it becomes difficult for the compiler to decide which method to execute from the child class.\nTherefore, Java doesn’t support multiple inheritance. The problem is commonly referred to as Diamond Problem.\nJava Basics Java Basics interview questions are very frequently asked from freshers.\nQ1. Explain each keyword in main method i.e. public static void main(String[] args). public class MyClass { public static void main(String[] args) {} } public method have public access and can be executed from outside the class. It has to be public so that java runtime can execute this method. static associates the method with class rather than object. This helps JVM to call main method without class instantiation. void is a return type of main method which returns nothing main is name of java main method and it is fixed. When you execute a class fine, it looks for main method. String[] args is used to pass Java command line arguments and is an array of type String Q2. What is final keyword in Java? The final keyword is used as non-access modifier in Java. A final keyword can be used in different contexts such as:\nValue of final variable cannot be changed once assigned.\nA final method cannot be overridden by inheriting child classes in Java.\nA final class cannot be extended by other classes in Java.\n/** final class cannot be extended */ final class A { /** final variable value cannot be changed */ private final var; /** final method cannot be overridden */ public void final method() {} } Q3. Difference between final, finally and finalize? final is used to apply restrictions on class, methods and variables. final class can\u0026rsquo;t be inherited, final method can\u0026rsquo;t be overridden and final variable value can\u0026rsquo;t be changed once assigned. finally is used along with try and catch block for exception handling. finally block is always executed whether an exception is handled or not. finally block is generally used to clean up the resources such as closing the IO streams and connections. finalize is an Object\u0026rsquo;s class protected method which is called by the garbage collector on an object that isn\u0026rsquo;t referenced anymore and have been selected for garbage collection. The method is called before the object is released from the memory by GC. We cannot control when garbage control happens and should not depend on the finalize() method to perform clean up activities. class FinalizeObject { @Override protected void finalize() throws Throwable { System.out.println(\u0026#34;Execute finalize method\u0026#34;); super.finalize(); } public static void main(String[] args) throws Exception { FinalizeObject object = new FinalizeObject(); object = null; /** Request JVM to perform garbage collection */ System.gc(); } } Q4. Explain JDK, JRE and JVM JDK JRE JVM It stands for Java Development Kit It stands for Java Runtime Environment It stands for Java Virtual Machine JDK = JDK + JRE + JVM JRE = JRE + JVM JVM = JVM It contains JRE, JVM, interpreter/loader(java), compiler(javac), debugger, archiver(jar), document generator(JavaDoc) It contains JVM, Class libraries and other supporting files It is a virtual machine contains ClassLoader, Method Area, Heap, Stack memory You can compile, document, debug, or archive a Java Program with JDK You can execute a Java Program with only JRE, You don\u0026rsquo;t need JDK JVM translates the bytecode to native machine code Q5. Why Java is Platform Independent? Java is platform independent. Let\u0026rsquo;s understand this\nWhen you write a program in Java (.java file) and compile it using javac. It is converted to ByteCode (.class file). This ByteCode is not something which any machine or OS can understand. You need an interpreter to execute this ByteCode and that interpreter is JVM (Java Virtual Machine) in case of Java. JVM is platform dependent. You need to install platform specific JVM based on Operating System you are using Mac OS X, Windows or Linux. You can execute the same ByteCode (.class file) on any platform\u0026rsquo;s JVM regardless of its Operating System. So we can say, JVM is platform dependent but Java is platform independent from its magic ByteCode.\nQ6. How many ways to create an Object Instance in Java ? New Keyword Cloning using Object.clone() method Reflection using Class.newInstance() method Reflection using Class.getConstructor().newInstance() method Object Serialization and Deserialization public class ObjectInstance { public static void main(String[] args) throws CloneNotSupportedException, InstantiationException, IllegalAccessException, IllegalArgumentException, InvocationTargetException, NoSuchMethodException, SecurityException, IOException, ClassNotFoundException { // New Keyword ObjectInstance instance1 = new ObjectInstance(); // Cloning ObjectInstance instance2 = (ObjectInstance) instance1.clone(); // Reflection ObjectInstance instance3 = ObjectInstance.class.newInstance(); ObjectInstance instance4 = ObjectInstance.class.getConstructor().newInstance(); // Object Serialization and Deserialization serialize(instance1); ObjectInstance instance5 = deserialize(); } private static void serialize(ObjectInstance objInstance) throws IOException { FileOutputStream f = new FileOutputStream(\u0026#34;resources/objectinstance.ser\u0026#34;); try (ObjectOutputStream outputStream = new ObjectOutputStream(f)) { outputStream.writeObject(objInstance); outputStream.flush(); } } private static ObjectInstance deserialize() throws IOException, ClassNotFoundException { FileInputStream f = new FileInputStream(\u0026#34;resources/objectinstance.ser\u0026#34;); try (ObjectInputStream inputStream = new ObjectInputStream(f)) { return (ObjectInstance) inputStream.readObject(); } } } Q7. What is Marker Interface? A Marker interface is an empty interface (having no fields and methods). Examples of Marker interface are:-\njava.io.Serializable,\njava.lang.Cloneable,\njava.rmi.Remote,\njava.util.RandomAccess etc.\nMarker Interface provides runtime information about objects to compiler and JVM. Now annotations are used for the same purpose instead of Marker Interface.\nLet\u0026rsquo;s create our own marker interface Deletable to indicate whether an object can be removed from the database.\npublic interface Deletable {} public class Entity implements Deletable { // implementation details } public class ShapeDao { // other dao methods public boolean delete(Object object) { if (!(object instanceof Deletable)) { return false; } // delete implementation details return true; } } Q8. What is Autoboxing and Unboxing in Java? Autoboxing is automatic conversion of primitive type to its object wrapper class such as int to Integer by Java Compiler //Autoboxing char to Character Character c = \u0026#39;a\u0026#39;; //Autoboxing int to Integer List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); list.add(1); Unboxing is automatic conversion of object wrapper class to its primitive type such as Double to Double by Java Compiler //Unboxing Double to double List\u0026lt;Double\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); list.add(3.1416); double pi = list.get(0); Q9. Why use Generics? Generics enables types to be parameterized when defining classes and methods so that we can re-use the same code with different types.\nLet\u0026rsquo;s understand this by creating a generic class Element and methods set and get\npublic class Element\u0026lt;T\u0026gt; { // T stands for \u0026#34;Type\u0026#34; private T t; public void set(T t) { this.t = t; } public T get() { return t; } } Now can re-use the same code with different types such as Integer and String as follows:-\nElement\u0026lt;Integer\u0026gt; num = new Element\u0026lt;\u0026gt;(); num.set(2); System.out.println(num.get()); Element\u0026lt;String\u0026gt; str = new Element\u0026lt;\u0026gt;(); str.set(\u0026#34;generics\u0026#34;); System.out.println(str.get()); Q10. What are the restrictions on Generics? Cannot instantiate Generic classes with primitive type List\u0026lt;int\u0026gt; num = new List\u0026lt;\u0026gt;(); // compile-time error Cannot create instance of Type parameter public static \u0026lt;E\u0026gt; void append(List\u0026lt;E\u0026gt; list) { E elem = new E(); // compile-time error list.add(elem); } Cannot declare parametrized static field in Generic class public class List\u0026lt;T\u0026gt; { // static field type cannot be parametrized since it belongs to class not class-object of type T private static T os; } Cannot use casts or instanceof with Generic Class public static \u0026lt;E\u0026gt; void rtti(List\u0026lt;E\u0026gt; list) { if (list instanceof ArrayList\u0026lt;Integer\u0026gt;) { // compile-time error // ... } } Cannot create Array of Generic class List\u0026lt;Integer\u0026gt;[] arrayOfLists = new List\u0026lt;Integer\u0026gt;[2]; // compile-time error Cannot overload a method using same Generic class with different types public class Example { public void print(Set\u0026lt;String\u0026gt; strSet) { } public void print(Set\u0026lt;Integer\u0026gt; intSet) { } // compile-time error } Java Memory Q1. What is the difference between Heap and Stack Memory in Java? Features Stack Heap Memory Stack memory is used only by one thread of execution. Heap memory is used by all the parts of the application. Access Stack memory can’t be accessed by other threads. Objects stored in the heap are globally accessible. Memory Management Follows LIFO manner to free memory. Memory management is based on the generation associated with each object. Lifetime Exists until the end of execution of the thread. Heap memory lives from the start till the end of application execution. Usage Stack memory only contains local primitive and reference variables to objects in heap space. Whenever an object is created, it’s always stored in the Heap space. Q2. What is Java Memory Model? JVM Memory is having separate space for Heap and Permanent Generation Memory\n┌------------------- Heap ------------------┐ ┌-- Non-Heap --┐ ┌------------- New -----------┐ ┌--- Old ---┐ ┌―――――――――┬―――――――――┬――――――――――┬―――――――――――――┬―――――――――――――――┐ │ Eden │ S0 │ S1 │ Tenured │ Permanent │ └―――――――――┴―――――――――┴――――――――――┴―――――――――――――┴―――――――――――――――┘ Heap Memory Heap is divided into 2 parts - New (Young) Generation and Old Generation. Heap space is allocated as per -Xms (Initial Heap Size) when JVM starts up and it can increase upto -Xmx (Max Heap Size) New (Young) Generation New Generation includes 3 parts - Eden Space and two Survivor Space (S0, S1) New Generation space is allocated as per -XX:NewSize when JVM starts up and it can increase upto -XX:MaxNewSize All newly created objects first live here, and GC\u0026rsquo;ed if they are unused (most die young) Minor GC is performed when Eden Space is full Mark Copy (Minor GC) Young Generation ┌―――――――――――――――――――――――――――――――――――――┐ │ Eden Space │ │――――――――――――――――――┬――――――――――――――――――│ │ Survivor Space 1 │ Survivor Space 2 │ └――――――――――――――――――┴――――――――――――――――――┘ 1st Minor GC 1. Keep allocating in Eden space 2. Once full, trigger minor GC 3. Mark all live objects in Eden 4. Copy it over to empty survivor space (S1) 5. Reclaim whole Eden space as empty 2nd Minor GC 1. Keep allocating in Eden space 2. Once full, trigger minor GC 3. Mark all live objects in Eden 4. Copy it over to empty survivor space (S2) 5. Copy live objects of S1 to S2 6. Reclaim both Eden and S1 as empty 3rd Minor GC ... 5. Copy live objects of S2 to S1 6. Reclaim both Eden and S2 as empty Old Generation Object moved here after surviving multiple Minor GC OR If Objects are too big for young generation. Major GC is performed when Old Generation is full. Mark Sweep Compact (Major GC) Old Generation ┌――――――――――――――――――――――――――――┐ │ ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ | | | | | │ 1. trigger major GC (aka full GC), once threshold reached └――――――――――――――――――――――――――――┘ ┌――――――――――――――――――――――――――――┐ │ ☑ | ☑ | | ☑☑ | ☑ | | | | | │ 2. mark all live objects └――――――――――――――――――――――――――――┘ ┌――――――――――――――――――――――――――――┐ │ ▇ | ▇ | | ▇▇ | ▇ | | | | | │ 3. sweep/reclaim all dead objects └――――――――――――――――――――――――――――┘ ┌――――――――――――――――――――――――――――┐ │ ▇▇▇▇▇▇ | | | | | | | | | | │ 4. move all live objects together aka compaction └――――――――――――――――――――――――――――┘ Permanent Generation Perm Gen stores String Constant Pool, Per-Class Structure, field and method data Perm Gen space is allocated as per -XX:PermSize when JVM starts up and it can increase upto -XX:MaxPermSize Q3. What are Different Types of Garbage Collectors? Collector Pros Cons Threads Use-Case SerialGC Smallest Footprint Slow Use Single Thread for both Minor and Major GC Programs with small memory, or running on shared CPU ParallelGC High Throughput High STW Pauses Use multiple threads for minor and single thread for major GC Batch applications ParallelOldGC High Throughput High STW Pauses Use multiple threads for both minor and major GC Batch applications ConcurrentMarkSweepGC Low STW Pauses (Concurrent Mark) Reduced Throughput Use one or more threads for both minor and major GC General Application G1GC Region Based, Can set STW Pause threshold, Low Latency, High Throughput High Footprint Use multiple threads Big Applications with large heap *Footprint:- Amount of memory required by GC algo to run\n*STW (Stop the world):- GC doesn\u0026rsquo;t run with application. When GC is running, all application processes are stopped.\nSerial vs Parallel GC Serial and Parallel both cause stop-the-world during GC operation Serial collector uses only one GC thread whereas Parallel collector uses multiple GC threads. Parallel collector provide high throughput due to multiple threads Parallel vs CMS GC ConcurrentMarkSweep (CMS) performs following Steps:-\ninitial mark (STW with multiple GC threads) concurrent tracing (one or more GC threads runs with application) remark (STW with multiple GC threads) concurrent sweep (one or more GC threads runs with application) Parallel collector stop-the-world throughout GC operation means 1 high STW pause. CMS stop-the-world only during initial mark and remark phase, during concurrent mark and sweep phase, CMS threads run along with application\u0026rsquo;s thread means 2 low STW pauses. G1GC Divides the heap space into multiple regions (Eden regions, Survivor regions, Old Regions, and Humongous regions)\n-XX:G1HeapRegionSize=n sets the size of a G1 region. The value will be a power of two and can range from 1MB to 32MB. The goal is to have around 2048 regions based on the minimum Java heap size. -XX:MaxGCPauseMillis=200 sets a target value for desired maximum pause time. The default value is 200 milliseconds. Q4. Can you force the JVM to perform garbage collection from a Java program? You cannot force the JVM to perform garbage collection. you can only suggest or indicate to the JVM to perform garbage collection using System.gc().\nQ5. How to do JVM Performance tuning? Heap and Stack Memory: -Xms512m sets the initial heap size to 512MB -Xmx512m sets the max heap size to 512MB, recommended to same as -Xms -Xss thread stack size -XX:NewSize=128m new generation size typically set 1/4th the size of heap -XX:MaxNewSize=128m max new generation size, recommended to same as -XX:NewSize -XX:NewRatio=2 old to young ratio means the old generation occupies 2/3 of the heap while the young generation occupies 1/3 -XX:SurvivorRatio=6 eden to survivor ratio) means each survivor occupies 1/8 of young generation while eden occupies 6/8 Permanent Generation Memory -XX:PermSize -XX:MaxPermSize Garbage Collection -XX:+UseSerialGC, -XX:+UseParallelGC (-XX:ParallelGCThreads=\u0026lt;N\u0026gt;), -XX:+UseParallelOldGC, -XX:+UseConcMarkSweepGC (-XX:ParallelCMSThreads=\u0026lt;N\u0026gt;), -XX:+UseG1GC JVM Performance Tuning using Parameters\n*Permanent Generation is replaced by Metaspace from Java 8 onwards\nQ6. What are Java Profilers? Java profilers are used to monitor and debug performance and memory leak issues in development and production environment.\nJava VisualVM is a profiling tool which provide a GUI to profile Java applications. It comes bundled with JDK and relies on other standalone tools provided in the JDK, such as JConsole, jstat, jstack, jinfo, and jmap.\nOther famous JVM profilers are JProfiler and YourKit.\nSome of the features of JVM profilers are:-\nCan use for debugging both local and remote applications Tracking all the JVM details (CPU Usage, Threads, Memory, ClassLoader) Allow to manually run the Garbage Collection Allow to take Heap Dump and Thread Dump CPU Profiling: Debug method execution with stack strace and time taken by each method in stack trace for any performance issue Memory Profiling: Debug the heap and stack memory usage for any possible memory leak. The JVM Profilers such as VisualVM, JProfiler, and YourKit requires direct connection to the monitored JVM and slow down the application. That is the reason they are mostly used in development environment and not favorable for production use.\nAPM (Application Performance Monitoring) tools such as AppDynamics, New Relic, DynaTrace, and Grafana are used in production environment instead. APM tools works as Java Agent Profiler which use Java Instrumentation API to inject code into your application. This provides granular level access of the application without affecting the performance.\nJava Errors \u0026amp; Exceptions Q1. ClassNotFoundException vs NoClassDefFoundError? ClassNotFoundException NoClassDefFoundError It is an exception. It is of type java.lang.Exception. It is an error. It is of type java.lang.Error. It occurs when an application tries to load a class at run time which is not updated in the classpath. It occurs when java runtime system doesn’t find a class definition, which is present at compile time, but missing at run time. It is thrown by the application itself. It is thrown by the methods like Class.forName(), loadClass() and findSystemClass(). It is thrown by the Java Runtime System. It occurs when classpath is not updated with required JAR files. It occurs when required class definition is missing at runtime. Q2. Error vs Exception? Error Exception Recovering from Error is not possible. We can recover from exceptions by either using try-catch block or throwing exceptions back to the caller. All errors in java are unchecked type. Exceptions include both checked as well as unchecked type. Errors are mostly caused by the environment in which program is running. Program itself is responsible for causing exceptions. Errors can occur at compile time as well as run time. Compile Time: Syntax Error, Run Time: Logical Error All exceptions occurs at runtime but checked exceptions are known to the compiler while unchecked are not. StackOverflowError, OutOfMemoryError Checked Exceptions: SQLException, IOException Unchecked Exceptions: ArrayIndexOutOfBoundException, NullPointerException, ArithmeticException. Java Theory Q1. S.O.L.I.D. (5 class design principle) in Java? These 5 design principle are know as SOLID principles:-\nSingle Responsibility Principle - One class should have one and only one responsibility. We should not write thousands lines of code in a single class (a.k.a. God Class), instead we should refactor and layer our classes such as Controllers (API), Services (Business Logic), Repositories (Database Connection) for application development. Open Close Principle - Software entities (Classes and methods) should be open for extension, but closed for modification. We should not change the code of existing class or method to add new functionality, instead we should inherit the Class or overload/override the Method. Liskov Substitution - Child class should be able to substitute Parent class during runtime polymorphism. Interface Segregation - Clients should not be forced to implement unnecessary methods which they will not use Dependency Inversion - Depend on abstractions (interfaces and abstract classes), instead of concrete implementations (classes).\nQ2. Does Singleton Pattern Follow S.O.L.I.D. Principle? Singleton pattern do not follow SOLID principle.\nSingleton class has additional responsibility of providing single instance along with other responsibility such as e.g. Database connection, Configuration Manager. Singleton class is not open for extension, not inheritable.\nQ3. How classes are related to each other through association, aggregation and composition. Association (bidirectional one to one, one to many, many to one or many to many association, represented by line with arrow in UML) for e.g. Teacher and Student. Multiple students can associate with a single teacher and a single student can associate with multiple teachers but there is no ownership between the objects and both have their own lifecycle. Aggregation (Has-a relationship, unidirectional association, parent and child can survive individually, represented by line with diamond in UML) for e.g. Car and Wheel. Car can have multiple wheels but wheel can not belong to multiple cars and if we delete the car, wheel need not to be destroyed and used in another car. Composition (Part-of relationship, unidirectional association, child can not survive without parent, represented by line with filled diamond in UML) for e.g. House and Rooms. House can contain multiple rooms there is no independent life of room and any room can not belongs to two different house if we delete the house room will automatically delete. ","permalink":"https://codingnconcepts.com/top-java-interview-questions/","tags":["Interview Q\u0026A","Java Q\u0026A"],"title":"Core Java Interview Questions"},{"categories":["Interview Questions"],"contents":"These CSS and CSS3 interview questions are based on my personal interview experience and feedback from other interviewees. Keep following this post for regular updates.\nWhat do you understand from CSS? CSS stands for Cascading Style Sheet, which is used to beautify and paint HTML elements. It tells the browser about HTML element\u0026rsquo;s style such as color, size, margin, padding etc.\nCascading means that multiple styles can be applied to same HTML element. These styles can be cascaded down to multiple style sheets.\nIn practice, styling on a single HTML elements can be applied from:-\nBrowser\u0026rsquo;s default style sheet Imported styles sheets from thirdparty (for e.g. bootstrap.css) Imported style sheets from application\u0026rsquo;s theme (for e.g. dark-theme.css) Styles defined in the same HTML document using \u0026lt;style\u0026gt; element Element\u0026rsquo;s inline styles using style attribute Which style to be applied on HTML element is decided based on CSS Precedence Rules and CSS Specificity.\nWhat are different ways to apply CSS styles? You will, of course, need to know how to add CSS to a page, and there are three main ways:\nUsing the inline style attribute on an element\n\u0026lt;div style=\u0026#34;width:100%; padding:10px; font-size:1.2em; text-align:center;\u0026#34;\u0026gt; CSS Tutorial \u0026lt;/div\u0026gt; Using a \u0026lt;style\u0026gt; block in the \u0026lt;head\u0026gt; section of your HTML\n\u0026lt;head\u0026gt; \u0026lt;style\u0026gt; div { width: 100%; padding: 10px; } .heading { font-size: 1.2em; text-align: center; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;heading\u0026#34;\u0026gt;CSS Tutorial\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; Loading an external CSS file using the tag\n\u0026lt;head\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;/css/styles.css\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;heading\u0026#34;\u0026gt;CSS Tutorial\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; /css/styles.css\ndiv { width: 100%; padding: 10px; } .heading { font-size: 1.2em; text-align: center; } The first two are useful but you’ll almost always be loading external CSS files. It’s more maintainable to keep your styles in separate files, not to mention it’s a nice separation of concerns.\nWhat are different ways to use CSS selectors? There are mainly three ways to use CSS selector to specify element to apply CSS.\nElement Selector use the HTML element tag name in css like h1, p, div Class Selector define a Class using class=\u0026quot;heading\u0026quot; attribute on an element and use like .heading Id Selector define an Id using id=\u0026quot;title\u0026quot; attribute on an element and use like #title Universal Selector used as a wildcard character * applied to all the elements on the page. Group Selector multiple selectors using comma separated Elements, Classes, Ids. \u0026lt;head\u0026gt; \u0026lt;style\u0026gt; /* Element Selector */ div { width: 100%; padding: 10px; } /* Class Selector */ .heading { font-size: 1.2em; text-align: center; } /* Id Selector */ #title { color: red; } /* Universal Selector */ * { color: black; font-family: Arial, sans-serif; } /* Group Selector */ h1, h2, .heading, #title { text-align: center; color: blue; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;title\u0026#34; class=\u0026#34;heading\u0026#34;\u0026gt;CSS Tutorial\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; What are the CSS Precedence Rules and CSS Specificity? CSS specificity is the set of rules applied to determine which style is applied to an element. Order of css specificity from highest to lowest is as follows:-\nA css rule with !important always takes precedence. Element\u0026rsquo;s inline style overrides all other styles (1,0,0,0) Css selector using element\u0026rsquo;s #id (0,1,0,0) Css selector using element\u0026rsquo;s .class, pseudo class or attribute (0,0,1,0) Css selector using element name h1, p, div (0,0,0,1) Css selector appear later in the code override earlier one if both have the same specificity Good read to understand how to calculate specificity\nShould you use !importance As we know now that !importance has highest specificity, nothing can beat that, apart from another !importance. Sometimes there is no way escaping this one when you are struggling with specificity but then it makes debugging difficult. You should avoid it wherever possible.\nHow to specify element position using CSS selector? div, p - Selects all \u0026lt;div\u0026gt; elements and all \u0026lt;p\u0026gt; elements div p - Selects all \u0026lt;p\u0026gt; elements that are anywhere inside a \u0026lt;div\u0026gt; element div \u0026gt; p - Selects all \u0026lt;p\u0026gt; elements where the immediate parent is a \u0026lt;div\u0026gt; element div + p - Selects \u0026lt;p\u0026gt; element that is immediate sibling of \u0026lt;div\u0026gt; and come afterwards div ~ p - Selects all \u0026lt;p\u0026gt; elements that are siblings of \u0026lt;div\u0026gt; element and come afterwards Let\u0026rsquo;s understand them looking at the example:-\n\u0026lt;style\u0026gt; div p { background-color: yellow; } div \u0026gt; p { color: red; } div + p { color: blue; } div ~ p { background-color: orange; } \u0026lt;/style\u0026gt; \u0026lt;h6\u0026gt;Element Position CSS Selector Example\u0026lt;/h6\u0026gt; \u0026lt;div\u0026gt; \u0026lt;p\u0026gt;I am immediate child of div\u0026lt;/p\u0026gt; \u0026lt;section\u0026gt; \u0026lt;p\u0026gt;I am grand child of div\u0026lt;/p\u0026gt; \u0026lt;section\u0026gt; \u0026lt;p\u0026gt;I am great grand child of div\u0026lt;/p\u0026gt; \u0026lt;/section\u0026gt; \u0026lt;/section\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;p\u0026gt;I am immediate sibling of div\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;I am another sibling of div\u0026lt;p\u0026gt; Element Position CSS Selector Example I am immediate child of div\nI am grand child of div\nI am great grand child of div\nI am immediate sibling of div\nI am another sibling of div What are pseudo elements? Pseudo elements are used to style particular parts of an element, rather than the whole thing. For example, you can use it to style the first line or first letter of a paragraph, text you’ve selected, or you can use it to insert text or shapes before or after an element.\nThey always start with a double colon ::, although a single colon : is still allowed for backwards compatibility. Few examples:-\np::first-line {} span::first-letter {} p::selection {} .header::after {} .tooltip::before {} What are pseudo classes? Pseudo classes are similar to pseudo elements, but instead of styling a part of an element, they apply styles when an element is in a certain state. For example, you could style a button differently based on whether the user has their mouse pointer over it, or when they click the button.\nAnother common use case is to style only certain occurrences of elements in a row. For example, styling the first tab in a series of tabs, or every second tab.\nThey all start with a single colon ::. Few Examples:-\nbutton:hover {} a:link {} /* Unvisited links */ a:active {} /* Active links */ a:visited {} /* Visited links */ input:focus {} input:disabled {} input:invalid {} input:required {} input:read-only {} input[type=\u0026#34;checkbox\u0026#34;]:checked {} tr:first-child {} /* First row of table */ tr:last-child {} /* Last row of table */ tr:nth-child(2) {} /* Second row of table */ tr:nth-child(2n) {} /* All even number rows */ tr:nth-last-child(2) {} /* Second last row */ Explain CSS Box Model? The CSS box model is essentially a box that wraps around every HTML element such as div. It consists of following:-\nContent - The content of the box, where text and images appear Padding - A transparent area outside the content Border - A border that goes around the padding and content Margin - A transparent area outside the border Margin Border Padding Content CSS box model example A typical css to create a box model\ndiv { width: 100px; height: 100px; margin: 20px; border: 5px solid red; padding: 10px; } CSS Tricks to apply margin and padding Single value margin and padding applies to all sides top, right, bottom and left. Following are different ways to apply margin or padding to individual side or in pairs or in groups.\n/* same padding on all 4 sides */ div { padding: 10px; } /* padding in pair, top/bottom (10px), left/right (20px) */ div { padding: 10px 20px; } /* three values padding, top (10px), bottom (20px) and left/right (30px) */ div { padding: 10px 20px 30px; } /* individual padding , in TRBL (top-right-bottom-left) order */ div { /* top right bottom left */ padding: 10px 20px 30px 40px; } /* individual padding css */ div { padding-top: 10px; padding-right: 20px; padding-bottom: 30px; padding-left: 40px; } CSS box-sizing By default width and height property of a box only includes content and do not consider border and padding. This is default css box-sizing:content-box. In this case actual width of box would become 100px (width) + 10px (padding) + 5px (border) = 115px;\nIf you do not want padding and border to change the width of the box, box-sizing:border-box is your css. This css adjust the width of the content to to include padding and border in same width. In this case actual width of the box would remain 100px (width) and content width will be adjusted to 100px (width) - 10px (padding) - 5px (border) = 85px;\nDifference between visibility:hidden and display:none? visibility:hidden; display:none; Hides the element Hides the element Occupy Space Do not occupy space Affect the layout Do not affect the layout How would you use media query in CSS3 ? We can not avoid media queries in responsive web applications. We can provide different styling for different devices such as Mobile, Tablet, Laptop and Desktop using media queries.\nThe most common approach is mobile first where all styles targeted for mobile devices and we progressively change styles for other devices using media queries.\nCSS media query example /* Mobile */ body { font-size: 1em; } /* Tablet */ @media only screen and (min-width: 768px) { body { font-size: 1.25em; } } /* Desktop and Laptop */ @media only screen and (min-width : 1224px) { body { font-size: 1.5em; } } /* Large Screen */ @media only screen and (min-width : 1824px) { body { font-size: 1.75em; } } Difference between inline, inline-block and block display? inline inline-bock block Start on a new line No No Yes Can set hight and width No Yes Yes Can set left/right margin \u0026amp; padding Yes Yes Yes Can set top/bottom margin \u0026amp; padding No Yes Yes Example display:inline; inline inline inline display:inline-block; inline-block inline-block inline-block display:block; block block What do you understand from Static, Relative, Absolute and Fixed position? Static - this is the default value, the element is positioned according to normal flow of the page. Relative - the element is positioned relative to its normal position. Absolute - the element is positioned absolutely to its first positioned parent. Fixed - the element is positioned related to the browser window. Sticky - the element is positioned based on the user\u0026rsquo;s scroll position. /* block 1, 2, 4, and 6 are at their default static normal position */ div.block-1, div.block-2, div.block-4, div.block-6 { position: static; } /* block 3 is relative to its normal position b/w block 2 and 4. Consume the space of normal position b/w 2 and 4 */ div.block-3 { position: relative; top: 20px; left: 20px; } /* block 5 is absolute to its parent container (red border) */ div.block-5 { position: absolute; top: 80px; left: 0px; } /* block 7 sticks to the top when you scroll down in parent container (red border) */ div.block-7 { position: sticky; top: 0px; } 1 2 3 4 5 6 7 We see absolute 5, relative 3 and sticky 7\nWhich unit of measurement you use for font-size? Something that has almost always come up for me is the way you size your text, mainly focused on the units you use. You can of course use pixels (px), but there’s also em, rem, %, vs and vh, along with a few others. Some people still don’t like using pixels, but browsers have improved and they’re generally handled pretty well.\nDefining your font sizes in em allows you to change the size of your text based on the size defined at a higher level. For example, if a container has specified a font-size of 2em, and you specify a font-size of 2em on an element inside that container, that element has an effective font-size of 4em! However, this can be a little confusing as you might not always see the size you expect!\n.container { font-size: 2em; } .container \u0026gt; p { font-size: 2em; /* this is 2em x 2em = 4em! */ } The rem unit was created to remedy that confusion. It scales well in the browser, just like em and px, but it uses a base size. From that, all further rem values are calculated. For example, if your base rem value is equal to 16px, then 1rem will always be equal to 16px, 2rem will always be equal to 32px, and so on.\nNote: While I’ve explained these units using font-sizes, the same rules apply to any dimensions where you use px, em or rem.\nAlso read CSS units for font-size: px, em and rem\nDo you know about Normalize CSS and Reset CSS? Each browser like Chrome, Firefox, IE has their own default styling for HTML elements also known as browser\u0026rsquo;s user agent styles.If you render a pure HTML page like below without any styling, page will look different in different browser as their user agent styling is applied.\n\u0026lt;h1\u0026gt;H1 Title\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;H2 Title\u0026lt;/h2\u0026gt; \u0026lt;h3\u0026gt;H3 Title\u0026lt;/h3\u0026gt; \u0026lt;h4\u0026gt;H4 Title\u0026lt;/h4\u0026gt; \u0026lt;h5\u0026gt;H5 Title\u0026lt;/h5\u0026gt; \u0026lt;h6\u0026gt;H6 Title\u0026lt;/h6\u0026gt; \u0026lt;a href=\u0026#34;https://codingnconcepts.com/\u0026#34;\u0026gt;Coding N Concepts\u0026lt;/a\u0026gt; \u0026lt;code\u0026gt;CSS\u0026lt;/code\u0026gt; Reset CSS Reset CSS resets all the default styles of browser. It says we don\u0026rsquo;t need browser\u0026rsquo;s default at all, we will define as per our need. One of the example is Eric Meyer’s reset.css\nreset.css /* http://meyerweb.com/eric/tools/css/reset/ v2.0 | 20110126 License: none (public domain) */ html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, img, ins, kbd, q, s, samp, small, strike, strong, sub, sup, tt, var, b, u, i, center, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td, article, aside, canvas, details, embed, figure, figcaption, footer, header, hgroup, menu, nav, output, ruby, section, summary, time, mark, audio, video { margin: 0; padding: 0; border: 0; font-size: 100%; font: inherit; vertical-align: baseline; } /* HTML5 display-role reset for older browsers */ article, aside, details, figcaption, figure, footer, header, hgroup, menu, nav, section { display: block; } body { line-height: 1; } ol, ul { list-style: none; } blockquote, q { quotes: none; } blockquote:before, blockquote:after, q:before, q:after { content: \u0026#39;\u0026#39;; content: none; } table { border-collapse: collapse; border-spacing: 0; } Normalize CSS Normalize CSS is an alternate for CSS reset. Instead of wiping out all styles, provides cross-browser consistency in the default styling of HTML elements.\nThat means, that if we look at the W3C standards of the styles applied by the browsers, and there is an in inconsistency in one of the browsers, the normalize.css styles will fix the browser style that has the difference. Some part of Necolas normalize.css\nnormalize.css /* Add the correct font weight in Chrome, Edge, and Safari. */ b, strong { font-weight: bolder; } /* Remove the border on images inside links in IE 10. */ img { border-style: none; } /* Correct the padding in Firefox. */ fieldset { padding: 0.35em 0.75em 0.625em; } What are CSS sprites and why would use them? If a web page has a large number of images that take a longer time to load because each image separately sends out an HTTP request. The concept of CSS sprites is used to reduce the loading time for a web page because it combines the various small images into one image. It reduces the number of HTTP requests and hence the loading time.\nSay you have to show flags of 100 countries on web page, instead of making 100 HTTP requests from browser, combine all the flags in one image known as sprite flags.png and load them in single HTTP request. Each flag will be shown on the HTML page by displaying a part of sprite image using background-position CSS property.\nCSS Sprite Example .flags-canada, .flags-london, .flags-usa { background-image: url(\u0026#39;../images/flags.png\u0026#39;); background-repeat: no-repeat; } .flags-canada { height: 128px; background-position: -5px -5px; } .flags-usa { height: 135px; background-position: -5px -143px; } .flags-london { height: 147px; background-position: -5px -288px; } What is the z-index in CSS? The z-index helps to specify the stack order of positioned elements that may overlap one another.\nHigher the z-index of an element means,\nhigher stack order, more visibility, cover elements with smaller z-index if overlapping. CSS Syntax z-index: auto | number | initial | inherit Property Values auto: Sets the stack order equal to its parents. This is default. number: Can have zero, positive or negative number. Higher the number, higher stack order. initial: Sets this property to its default value. inherit: Inherits this property from its parent element. ","permalink":"https://codingnconcepts.com/top-css-interview-questions/","tags":["Interview Q\u0026A","CSS Q\u0026A"],"title":"Top CSS Interview Questions"},{"categories":["Interview Questions","Javascript"],"contents":"These JavaScript interview questions are based on my personal interview experience and feedback from other interviewees. These questions verifies your theoretical as well as practical knowledge about JavaScript. Keep following this post for regular updates.\nQ1. What is Closure in JavaScript? Definition: A closure is an inner function having access to its outer function scope and all above scopes even when that function is executing outside of its outer function.\nWhen you define an inner function inside outer function, a closure is created at runtime for inner function bundled with outer function\u0026rsquo;s scope. Let\u0026rsquo;s look at the example to understand Closures\nvar outerFunc = function(c){ var a = 1; var innerFunc = function(d) { var b = 2; var innerMostFunc = function(e) { return a + b + c + d + e; } return innerMostFunc; } return innerFunc; } console.dir(outerFunc(3)); //1. innerFunc console.dir(outerFunc(3)(4)); //2. innerMostFunc console.log(outerFunc(3)(4)(5)); //3. 15 Output ▼ ƒ innerFunc(c) length: 1 name: \u0026#34;innerFunc\u0026#34; arguments: null caller: null ➤ prototype: {constructor: ƒ} ➤ __proto__: ƒ () [[FunctionLocation]]: ▼ [[Scopes]]: Scopes[2] ➤ 0: Closure (outerFunc) {c: 3, a: 1} ➤ 1: Global {parent: Window, opener: null, top: Window, length: 1, frames: Window, …} ▼ ƒ innerMostFunc(c) length: 1 name: \u0026#34;innerMostFunc\u0026#34; arguments: null caller: null ➤ prototype: {constructor: ƒ} ➤ __proto__: ƒ () [[FunctionLocation]]: ▼ [[Scopes]]: Scopes[3] ➤ 0: Closure (innerFunc) {d: 4, b: 2} ➤ 1: Closure (outerFunc) {c: 3, a: 1} ➤ 2: Global {parent: Window, opener: null, top: Window, length: 1, frames: Window, …} 15 We have three console dir/log. Let\u0026rsquo;s discuss them one by one:\ninnerFunc has a closure of variables defined or passed as argument in outerFunc\n0: Closure (outerFunc) {c: 3, a: 1} innerMostFunc has a closure of variables defined or passed as argument in in outerFunc and innerFunc i.e.\n0: Closure (innerFunc) {d: 4, b: 2} 1: Closure (outerFunc) {c: 3, a: 1} innerMostFunc returns a+b+c+d+e=15 where\nvalue of a and c is coming from Closure (outerFunc) value of b and d is coming from Closure (innerFunc) value of e is coming from passed argument Q2. What are Promises and why do we use it? Promises are introduced natively in ES6. They are very similar to our promises. As we keep or break the promise, Javascript promises are either resolve or reject.\nIn earlier days, you would have used callbacks to handle asynchronous operations. However, callbacks have limited functionality and often leads to unmanageable code if you are handling multiple async calls also known as callback hell. Promises were introduced to improve code readability and better handling of async calls and errors.\nSyntax of simple promise object.\nlet promise = new Promise(function (resolve, reject) { // asynchronous call }); Also read this post for more details on promises in javascript\nQ3. How to use async and await and what problem does it solve? Async/Await introduced in ECMAScript2017 is a new way to handle asynchronous operation. Its underlying is Promise with improved syntax to write concise, readable and easy to debug code as compare to promises and callbacks.\nWhen Async keyword is applied before a function, it turns into asynchronous function and always return a promise object.\nawait keyword works only inside async function and it makes the function execution wait until the returned promise settles (either resolve or reject).\nUsage of async/await:\n1async function hello() { 2 let promise = new Promise((resolve, reject) =\u0026gt; { 3 setTimeout(() =\u0026gt; resolve(\u0026#34;Hello\u0026#34;), 5000) 4 }); 5 6 let value = await promise; // wait until the promise resolves 7 8 return value; 9} 10 11hello().then(data =\u0026gt; console.log(data)); Please note that in above code snippet when we execute async function hello(), function execution literally waits for 5s at line 6 before returning resolved promise object. CPU resources are not utilized in this wait period and can be used for other work.\nAlso read this post for more details on async/await in javascript\nQ4. What is a difference between call, apply and bind in JavaScript? All three call, apply and bind are prototype methods of Function that means you can execute these three methods on any function. All three call, apply and bind methods are used to execute the function in explicit context where this refers to the first argument passed in method. call() - accepts the function arguments as comma separated values arg1, arg2, \u0026hellip; functionName.call(thisArg, arg1, arg2, ...) apply() - accepts the function arguments as array of values [arg1, arg2, \u0026hellip;] functionName.apply(thisArg, [arg1, arg2, ...]) bind() - returns a new bounded function which can be executed later. functionName.bind(thisArg) call() and apply() methods are executed immediately whereas bind() method return a new bounded function which can be executed later. Also read this post for more details on difference between call, bind and apply with their practical usage\nQ5. What is Arrow function and how it is different from normal functions? Arrow functions are introduced in ES6 and allow us to write shorter function syntax. var helloES5 = function() { return \u0026#34;Hello World\u0026#34;; } var helloES6 = () =\u0026gt; \u0026#34;Hello World\u0026#34;; In normal function this keyword represents the object that called the function but in arrow function this keyword always represents the object that defined the arrow function. Q6. What is a difference between == and === operator? == is abstract comparison operator which only compares the content and not its type 1 == 1 // true, content and its type is same \u0026#39;1\u0026#39; == 1 // true, auto type conversion, string converted into number null == undefined // true, because null is equivalent of undefined 0 == false // true, because false is equivalent of 0 \u0026#34;\u0026#34; == false // true, because false is equivalent of empty string === is strict comparison operator which compare both content and its type 1 === 1 true \u0026#39;1\u0026#39; === 1 false null === undefined false 0 === false false \u0026#34;\u0026#34; === false false === is faster then == because == converts the operands to the compatible type before comparison whereas === compares directly without any conversion. Also read this post for more details on == and === operators with many examples\nQ7. What is a difference between undefined and null ? undefined means a variable has been declared but not assigned a value yet. null is an assignment value. It can be assigned to a variable that represents null, empty or non-existent value. undefined is type of undefined whereas null is an object. var a; console.log(typeof(a)); // undefined console.log(typeof(null)); // object Q8. What is a difference between var, let and const keywords? var declarations are globally scoped or function scoped while let and const are block scoped. var variables can be updated and re-declared within its scope; let variables can be updated but not re-declared; const variables can neither be updated nor re-declared. They are all hoisted to the top of their scope but while var variables are initialized with undefined, let and const variables are not initialized. While var and let can be declared without being initialized, const must be initialized during declaration. Also read this post for more details on difference between var, let and const\nQ9. What is variable Hoisting in JavaScript? Hoisting is JavaScript\u0026rsquo;s default behavior of moving all declarations to the top of the current scope (to the top of the current script or the current function).\nVariables defined using var, let or const are all hoisted.\nIn hoisting process, var variable is initialized with undefined by default. If you try to access it before declaration then you will get undefined.\nIn hoisting process, let and const variables are not initialized. If you try to access them before declaration then you will get ReferenceError.\nconsole.log(x); // undefined console.log(y); // ReferenceError console.log(z); // ReferenceError var x = 1; let y = 2; const z = 3; Q10. What are the Object methods available in Javascript? Object.create() method is used to create a new object with its prototype set to existing object. Object.assign() method is used to copy the properties and functions of one object in another object. Object.freeze() freezes the state of an Object once this method is called. It ignores if any existing property is modified, and if any new property is added. Object.seal() seals the state of an Object once this method is called. It allows the modification of existing properties, but ignores if any new property is added. Object.defineProperty() is used to define new property in existing object or modifying existing property. Property can be marked writable, configurable and enumerable. Object.is() is used to compare two values. Return true or false. Also read this post for more details on Object methods with examples\nQ11. What is this keyword in Javascript? In JavaScript, this keyword refers to the object it belongs to. It has different values depending on where it is used:\nIn a method, this refers to the owner object where method is defined. Alone, this refers to the global object. In a function, this refers to the global object. In a function, in strict mode, this is undefined. When a function called with new keyword, this refers to new object instance. In a DOM event, this refers to the element that received the event. Function prototype methods call(), apply() and bind() can be used to refer this to any object. Also read this post to understand all about this keyword with examples\nQ12. What is the difference between throttle and debounce? Debounce and throttle are two similar (but different!) techniques to control how many times we allow a function to be executed over time.\nThrottle If throttle interval is 100ms then a throttled function will be executed only once in 100ms, no matter how often you invoke that function.\nIf is useful for rate-limiting events that occur faster than you can keep up with. For e.g. update position at every 1s time interval in infinite scrolling.\nvar throttled = _.throttle(updatePosition, 100); $(window).scroll(throttled); Debounce If debounce interval is 100ms then a debounced function will be executed after 100ms passed since its last execution.\nIt is mainly used to group a sudden burst of behavior and react afterwards. For e.g., recalculating a layout 1s after the window has stopped being resized, send event 2s after keystrokes has stopped on an input field, and so on.\nvar lazyLayout = _.debounce(calculateLayout, 300); $(window).resize(lazyLayout); Underscore and Lodash libraries have built in function for both.\nQ13. What is constructor and prototype? Do you know about prototypal inheritance. We can define classes in JavaScript using function constructor.\n// ES5 Function Constructor var Person = function(firstName, lastName){ this.firstName = firstName; this.lastName = lastName; } We can add methods to our class using prototype.\nPerson.prototype = { getFullName(){ return `${this.firstName.toUpperCase()} ${this.lastName.toUpperCase()}`; } } We can create a subclass Student of Person class using prototypal inheritance.\nvar Student = function(firstName, lastName, studentId){ Person.call(this, firstName, lastName); this.studentId = studentId; } Student.prototype = Object.create(Person.prototype); Student.prototype.constructor = Student; We can further add methods in our subclass using prototype\nStudent.prototype.toString = function(){ return `Student {${this.lastName} ${this.firstName} - ${this.studentId}}` } Let\u0026rsquo;s create an object from Student subclass\nvar student = new Student(\u0026#34;Ashish\u0026#34;, \u0026#34;Lahoti\u0026#34;, 123456); console.log(student.getFullName()); // prints \u0026#39;ASHISH LAHOTI\u0026#39; console.log(student.toString()); // prints \u0026#39;Student {Lahoti Ashish - 123456}\u0026#39; Q14. How do you define Class in ES6? We define class using class keyword in ES6 which is a replacement of function constructor.\n// ES6 Class class Car { constructor(brand, color, price) { this._brand = brand; this._color = color; this._price = price; } // getter method get color(){ return `color is ${this._color.toUpperCase()}`; } // setter method set color(newColor){ this._color = newColor; } // prototype method drive(){ return `driving ${this._brand} ${this._color} color car`; } // static method static compareCars(car1, car2){ return `${car2._brand} is ${(car1._price \u0026gt; car2._price) ? \u0026#34;cheaper\u0026#34; : \u0026#34;costlier\u0026#34;} then ${car1._brand}` } } We can define getter, setter, prototype and static methods in classes. We can also create subclasses using extend keyword.\nclass Toyota extends Car { constructor(color, price, model, make){ super(\u0026#34;Toyota\u0026#34;, color, price); Object.assign(this, {model, make}); } } Also read this post for more details on classes in JavaScript\nQ15. What is global, function and block scope in Javascript? Q16. Currying functions in javascript?","permalink":"https://codingnconcepts.com/top-javascript-interview-questions/","tags":["Interview Q\u0026A","JavaScript Q\u0026A"],"title":"Top Javascript Interview Questions"},{"categories":["Java"],"contents":"In this tutorial, We\u0026rsquo;ll learn how to create a Singleton Design Pattern using Java.\nOverview Singleton Design Pattern is one of the Gangs of Four Design patterns and comes in the Creational Design Pattern category. Singleton design pattern ensures that a class has only one instance and provides a global point of access to it.\nSingleton pattern is one of the simplest but most controversial design patterns. This is one of the reason why it is frequently asked in technical interviews.\nLet\u0026rsquo;s create a Singleton class and also answer these follow up questions:-\nHow to make singleton class thread safe?\nusing double checked locking How to prevent deserialization to create new object of singleton class?\nusing readResolve method to return same instance How to prevent cloning to create a new object of singleton class?\noverride clone method to return same instance How to prevent reflexion to create a new object of singleton class (assume one instance already exist)?\nthrow exception from private constructor if instance already exist Implementation Singleton Class package com.abc; import java.io.FileInputStream; import java.io.FileNotFoundException; import java.io.FileOutputStream; import java.io.IOException; import java.io.ObjectInputStream; import java.io.ObjectOutputStream; import java.io.ObjectStreamException; import java.io.Serializable; /** * @author aklahoti * */ public class Singleton implements Serializable, Cloneable{ private static final long serialVersionUID = 1L; private static Singleton instance = null; private static Object DUMMY_OBJECT = new Object(); private Singleton(){ /*To prevent object creation using reflection*/ if(instance!=null){ throw new InstantiationError( \u0026#34;Singleton Object is already created.\u0026#34; ); } } public static Singleton getInstance(){ /*Double checked locking*/ if(instance == null){ synchronized (DUMMY_OBJECT) { if(instance == null){ instance = new Singleton(); } } } return instance; } public static void print(){ System.out.println(\u0026#34;I am a singleton class.\u0026#34;); } /*To prevent object creation using deserialization*/ private Object readResolve() throws ObjectStreamException{ return instance; } /*To prevent object creation using cloning*/ @Override protected Object clone() throws CloneNotSupportedException { return instance; } } ","permalink":"https://codingnconcepts.com/java/singleton-design-pattern-using-java/","tags":["Java Design Pattern"],"title":"Singleton Design Pattern Using Java"},{"categories":["Kafka"],"contents":"Hi Readers,\nIf you are planning or preparing for Apache Kafka Certification then this is the right place for you.There are many Apache Kafka Certifications are available in the market but CCDAK (Confluent Certified Developer for Apache Kafka) is the most known certification as Kafka is now maintained by Confluent.\nCCDAK vs CCOAK Confluent has introduced CCOAK certification recently. CCOAK is mainly for devOps engineer focusing on build and manage Kafka cluster. CCDAK is mainly for developers and Solution architects focusing on design, producer and consumer. If you are still not sure, I recommend to go for CCDAK as it is more comprehensive exam as compared to CCOAK. These exam notes are very helpful for both CCDAK and CCOAK certifications.\nFrom here onward, we will talk about how to prepare for CCDAK.\nFrequently Asked Questions Prepare well for the exam as it verifies your theoretical as well as practical understanding of Kafka. At least 40-50 hours of preparation is required. You can register online and schedule exam on the Examity site. I suggest to set a goal of 1 to 2 months for exam preparation and register accordingly. Confluent kafka certification price cost at $150 for one attempt. You need to pay the fee again in order to retake exam after a gap of at least 14 days. You need to answer 60 multiple-choice questions in 90 minutes from your laptop (with webcam) under the supervision of online proctor. There is no negative scoring so try to answer as many questions as possible. There is no mention of number of questions need to be correct in order to pass the exam. Result will be shown immediately (PASS or FAIL) at the end of exam. No scoring or percentage is provided. You will receive a certificate similar to my CCDAK certificate after passing the exam. What an achievement !!! The certification expires after two years but you can still brag about it ;) Don\u0026rsquo;t waste time searching for CCDAK Certification Dumps. There ain\u0026rsquo;t any. Confluent has launched FREE Fundamentals Accreditation Exam which you can signup for free to better understand the CCDAK exam. Exam Preparation I have prepared for CCDAK using following:\nApache Kafka Documentation Confluent Kafka Documentation Confluent Kafka Definitive Guide PDF Udemy Apache Kafka Series - Learn Apache Kafka for Beginners v2 Udemy CCDAK 150 Practice Exam Questions You should prepare well for following topics. It is recommended to study the topics in the same sequence.\n① Kafka Architecture\nRead Confluent Kafka Definitive Guide PDF and Apache Kafka Documentation. Once you read both, revise using Kafka Architecture section in this post.\n② Kafka CLI\nRead Confluent Kafka Definitive Guide PDF and revise using KAFKA CLI section of this post.\n③ Kafka Streams\nRead Confluent Kafka Definitive Guide PDF and revise using Kafka Streams section of this post.\n④ Kafka Security\nRead Apache Kafka Documentation Security Section\n⑤ Kafka APIs\nRead Apache Kafka Documentation API section and revise using KAFKA API section of this post.\n⑥ Kafka Monitoring (Metrics)\nRead Confluent Kafka Definitive Guide PDF and Apache Kafka Documentation for important metrics. Read Confluent Kafka Documentation as well.\n⑦ Confluent Schema Registry\nRead Confluent Kafka Documentation and revise using Confluent Schema Registry section of this post.\n⑧ Confluent KSQL\nRead Confluent Kafka Documentation KSQL and Kafka Streams section\n⑨ Confluent REST Proxy\nRead Confluent Kafka Documentation Rest Proxy Section\nSample Exam Questions Please note that these are not the actual questions from the CCDAK exam but most likelihood to ask in exam.\n1. Kafka Theory Kafka is a \u0026hellip;. ?\npub-sub system Mostly Kafka is written in which language?\nScala Which errors are retriable from Kafka Producer?\nLEADER_NOT_AVAILABLE, NOT_LEADER_FOR_PARTITION, UNKNOWN_TOPIC_OR_PARTITION What is a generic unique id which can be used for a messages received from a consumer?\nTopic + Partition + Offset Read Kafka Architecture section of this post for more questions and answers\n2. Kafka Streams To transform data from a Kafka topic to another one, You should use?\nKafka Streams Which of the Kafka Stream operators are stateful? Which of the Kafka Stream operators are stateless? Which window is not having gap? Which Kafka Stream joins doesn\u0026rsquo;t require co-partition of data?\nKStream-to-GlobalKTable Which Kafka Stream joins is always windowed join?\nKStream-to-KStream What is the output of KStream-to-KTable join?\nKStream Read Kafka Streams section of this post for answers\n3. Confluent Schema Registry Which of the following is not a primitive type of Avro? Which of the following in not a complex type of Avro? Which of the following is not a required field in Avro Schema? Delete a field without default value in Avro schema is \u0026hellip;\u0026hellip; compatibility?\nbackward Adding a field to record without default value is \u0026hellip;\u0026hellip; schema evolution?\nforward In Avro, removing or adding a field that has a default value is a \u0026hellip;\u0026hellip; schema evolution?\nfull What client protocols are supported for the schema registry?\nHTTP, HTTPS Where are Avro schema stored in Confluent Schema Registry?\n_schemas Kafka topic Read Confluent Schema Registry section of this post for answers\n4. Confluent KSQL is KSQL ANSI SQL Compliant?\nNo What Java library is KSQL based on?\nKafka Streams 5. Kafka Security What are the valid authentication mechanism in KAFKA?\nSSL\nSASL/GSSAPI (Kerberos)\nSASL/PLAIN\nSASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512\nSASL/OAUTHBEARER Kafka Architecture ☛ I have spent so much time preparing these notes. You can expect most of the questions related to Kafka architecture from these notes. ☛ Moreover, If you are preparing for interviews then you can also expect most of the Kafka interview questions from these notes.\nCluster Cluster typically consist of mutliple Kafka brokers. All Kafka brokers within a Cluster are managed and coordinated by Zookeeper. Rack Rack is a logical grouping of Kafka brokers within a Cluster. Cluster can have multiple racks. Rack can have multiple brokers. Broker belongs to a rack when property broker.rack=\u0026lt;rack-id\u0026gt; is specified at broker level. This enables the rack awareness feature which spreads replicas of the same partition across different racks. Let\u0026rsquo;s say you have 6 brokers and 2 racks. Brokers 1, 2, 3 are on the rack_1, and brokers 4, 5, 6 are on rack_2.\nNow when you create a topic with 6 partition, instead of assigning broker to partition in order from 1, 2, 3, 4, 5, 6, each partition is assigned to each rack repeatedly i.e. 1, 4, 2, 5, 3, 6. ┌――――――――――――┐ ┌――――――――――――┐ | rack_1 | | rack_2 | |――――――――――――| |――――――――――――| | Broker 1 |◁― Partition 1 | Broker 4 |◁― Partition 2 | Broker 2 |◁― Partition 3 | Broker 5 |◁― Partition 4 | Broker 3 |◁― Partition 5 | Broker 6 |◁― Partition 6 └――――――――――――┘ └――――――――――――┘ Broker Every broker in Kafka is a bootstrap server which knows about all brokers, topics and partitions (metadata) that means Kafka client (e.g. producer,consumer etc) only need to connect to one broker in order to connect to entire cluster. At all times, only one broker should be the controller, and one broker must always be the controller in the cluster Topic Kafka takes bytes as input without even loading them into memory (that\u0026rsquo;s called zero copy) Brokers have defaults for all the topic configuration parameters Partition Topic can have one or more partition. Each partition can have one or more replica decided by Replication Factor. Replication Factor = 3 means One leader partition and two replicas. It is not possible to delete a partition of topic once created. Order is guaranteed within the partition and once data is written into partition, its immutable! If producer writes at 1 GB/sec and consumer consumes at 250MB/sec then requires 4 partition! Segment Partitions are made of segments (*.log files) At a time only one segment is active in a partition Segment stores the actual Kafka messages along with offset, timestamp, compression etc. log.segment.bytes = 1 GB (default), Max size in bytes to close the segment and roll over to new segment. log.segment.ms = 1 Week (default), Max time in ms to wait before closing the segment even if it is not full or reached Max Size. Every segment also has two indexes (files):- An offset to position index (*.index file) - Allows kafka where to read to find a message A timestamp to offset index (*.timeindex file) - Allows kafka to find a message with a timestamp Log Retention and Cleanup policies Log cleanup happen on partition segments (.log files). Smaller or more segments means the log cleanup will happen more often! Old segments are cleaned up based on time-based and size-based log retention policies whichever happens first. Time Based Retention: The segment older by configured retention time eligible for Cleanup. You can use any of the three configuration log.retention.ms, log.retention.minutes, or log.retention.hours to specify retention period in ms, minutes or hours. If ms is not set, minutes is used. If minutes is not set, hours is used. Default retention time is 1 Week. Size Based Retention: The older segment are cleaned up when the max configured size of a topic partition (includes all segments), is reached. You can use log.retention.bytes=-1 (default is infinite) to configure max size in bytes. Cleanup process checks any logs to cleanup in every log.cleaner.backoff.ms=15 seconds (default). Cleanup Policy: Logs are deleted, compacted, or both based on log.cleanup.policy. Delete policy discard the old segments when their retention time or size is reached. Compact policy delete the old messages per key and keep the latest copy for that key. Deleted records can still be seen by consumers for a period of delete.retention.ms=24 hours (default) Offset Each Topic Partition is having its own offset starting from 0. Topic Replication Replication factor = 3 and partition = 2 means there will be total 6 partition distributed across Kafka cluster. Each partition will be having 1 leader and 2 ISR (in-sync replica). Broker contains leader partition called leader of that partition and only leader can receive and serve data for partition. Replication factor can not be greater then number of broker in the kafka cluster. If topic is having a replication factor of 3 then each partition will live on 3 different brokers. Producer Kafka Producer automatically recover from following retriable errors:\nLEADER_NOT_AVAILABLE\nNOT_LEADER_FOR_PARTITION Kafka Producer throw error for following non-retriable errors:\nOFFSET_OUT_OF_RANGE\nBROKER_NOT_AVAILABLE\nMESSAGE_TOO_LARGE\nINVALID_TOPIC_EXCEPTION If you send a message of size 3 MB to a topic with default message size configuration. Then producer will throw MessageSizeTooLarge exception immediately since it is not a retriable exception. When produce to a topic which doesn\u0026rsquo;t exist and auto.create.topic.enable=true then kafka creates the topic automatically with the broker/topic settings num.partition and default.replication.factor Producer Acknowledgment acks Producer\u0026hellip; Data Loss acks=0 do not wait for acknowledgement possible data loss acks=1 wait for leader acknowledgement limited data loss acks=all wait for leader+replica acknowledgement no data loss acks=all must be used in conjunction with min.insync.replicas which can be set at broker or topic level.\n*( assuming that replicas are distributed across 3 brokers for below points )\nmin.insync.replica only matters if acks=all acks=all, min.insync.replica=2 implies that at least 2 brokers that are ISR (including leader) must acknowledge acks=all, min.insync.replica=1 implies that at least 1 brokers that is ISR (including leader) must acknowledge A kafka topic with replication.factor=3, acks=all, min.insync.replicas=2 can only tolerate 1 broker going down, otherwise the producer will receive an exception NOT_ENOUGH_REPLICAS on send. A kafka topic with replication.factor=3, acks=all, min.insync.replicas=1, can tolerate maximum number of 2 brokers going down, so that a producer can still produce to the topic. Producer Configuration Mandatory properties to configure Kafka producer is as follows:\nbootstrap.servers\nkey.serializer\nvalue.serializer Safe Producer Configuration min.insync.replicas=2 (set at broker or topic level) retries=MAX_INT number of reties by producer in case of transient failure/exception. (default is 0) max.in.flight.per.connection number=5 number of producer request can be made in parallel (default is 5) acks=all enable.idempotence=true producer send producerId with each message to identify for duplicate msg at kafka end. When kafka receives duplicate message with same producerId which it already committed. It do not commit it again and send ack to producer (default is false) High Throughput Producer using compression and batching compression.type=snappy value can be none(default), gzip, lz4, snappy. Compression is enabled at the producer level and doesn\u0026rsquo;t require any config change in broker or consumer Compression is more effective in case of bigger batch of messages being sent to kafka linger.ms=20 Number of millisecond a producer is willing to wait before sending a batch out. (default 0). Increase linger.ms value increase the chance of batching. batch.size=32KB or 64KB Maximum number of bytes that will be included in a batch (default 16KB). Any message bigger than the batch size will not be batched Message Key Producer can choose to send a message with key. If key=null, data is sent in round robin fashion If key!=null and has some value, then all the message for that key always go to the same partition. This can be used to order the messages for a specific key since order is guaranteed in same partition. Adding a partition to the topic will loose the guarantee of same key go to same partition. Keys are hashed using murmur2 algorithm by default. Message Size Default Message max size is 1MB If you try to send message \u0026gt; 1MB then MessageSizeTooLargeException is thrown Suppose you want to send a 15MB of message from producer to broker to consumer successfully then you need to configure:- message.max.bytes=15728640 (broker/topic config) - is max size of message that can be received by the broker from producer. replica.fetch.max.bytes=15728640 (broker config) - is max size of message that can be replicated across brokers. fetch.message.max.bytes=15728640 (consumer config) - is max size of message that can be fetched by consumer. Consumer Per thread one consumer is the rule. Consumer must not be multi threaded. Each consumer is assigned to different partition in same consumer group. If there are 5 consumers of same consumer group consuming from a topic with 10 partition then 2 partitions will be assigned to each consumer and no consumer will remain idle. If there are 10 consumers of same consumer group consuming from a topic with 5 partition then 5 partition will be assigned to 5 consumers and rest 5 consumers will remain idle. records-lag-max (monitoring metrics) The maximum lag in terms of number of records for any partition in this window. An increasing value over time is your best indication that the consumer group is not keeping up with the producers. Consumer Group If two applications want to process all the messages independently from a kafka topic having 4 partition, then you should create 2 consumer groups with 4 consumers in each group for optimal performance. Consumer Offset When consumer in a group has processed the data received from Kafka, it commits the offset in Kafka topic named _consumer_offset which is used when a consumer dies, it will be able to read back from where it left off.\nDelivery Semantics At most once Offset are committed as soon as message batch is received. If the processing goes wrong, the message will be lost (it won\u0026rsquo;t be read again) At least once (default) Offset are committed after the message is processed. If the processing goes wrong, the message will be read again. This can result in duplicate processing of message. Make sure your processing is idempotent. (i.e. re-processing the message won\u0026rsquo;t impact your systems). For most of the application, we use this and ensure processing are idempotent. Exactly once Can only be achieved for Kafka=\u0026gt;Kafka workflows using Kafka Streams API. For Kafka=\u0026gt;Sink workflows, use an idempotent consumer. Consumer Offset commit strategy enable.auto.commit=true \u0026amp; synchronous processing of batches with auto commit, offset will be committed automatically for you at regular interval (auto.commit.interval.ms=5000 by default) every time you call .poll(). If you don\u0026rsquo;t use synchronous processing, you will be in \u0026ldquo;at most once\u0026rdquo; behavior because offsets will be committed before your data is processed. enable.auto.commit=false \u0026amp; manual commit of offsets (recommended) Consumer Offset reset behavior auto.offset.reset=latest will read from the end of the log auto.offset.reset=earliest will read from the start of the log auto.offset.reset=none will throw exception of no offset is found Consumer offset can be lost if hasn\u0026rsquo;t read new data in 7 days. This can be controlled by broker setting offset.retention.minutes Consumer Poll Behavior fetch.min.bytes = 1 (default), Control how much data you want to pull at least on each request. Help improving throughput and decreasing request number. At the cost of latency. max.poll.records = 500 (default), Controls how many records to receive per poll request. Increase if your messages are very small and have a lot of available RAM. max.partition.fetch.bytes = 1MB (default), Maximum data returned by broker per partition. If you read from 100 partition, you will need a lot of memory (RAM) fetch.max.bytes = 50MB (default), Maximum data returned for each fetch request (covers multiple partition). Consumer performs multiple fetches in parallel. Consumer Heartbeat Thread Heartbeat mechanism is used to detect if consumer application in dead. session.timeout.ms=10s (default), If heartbeat is not sent in 10 second period, the consumer is considered dead. Set lower value to faster consumer rebalances heartbeat.interval.ms=3s (default), Heartbeat is sent in every 3 seconds interval. Usually 1/3rd of session.timeout.ms Consumer Poll Thread Poll mechanism is also used to detect if consumer application is dead. max.poll.interval.ms = 5min (default), Max amount of time between two .poll() calls before declaring consumer dead. If processing of message batch takes more time in general in application then should increase the interval. Kafka Guarantees Messages are appended to a topic-partition in the order they are sent Consumer read the messages in the order stored in topic-partition With a replication factor of N, producers and consumers can tolerate upto N-1 brokers being down As long as number of partitions remains constant for a topic ( no new partition), the same key will always go to same partition Client Bi-Directional Compatibility an Older client (1.1) can talk to Newer broker (2.0) a Newer client (2.0) can talk to Older broker (1.1) Kafka Connect Source connect Get data from common data source to Kafka for e.g. import data from external database to kafka Sink connect Publish data from Kafka to common data source for e.g. export data from Kafka to external database Zookeeper ZooKeeper servers will be deployed on multiple nodes. This is called an ensemble. An ensemble is a set of 2n + 1 ZooKeeper servers where n is any number greater than 0. The odd number of servers allows ZooKeeper to perform majority elections for leadership. At any given time, there can be up to n failed servers in an ensemble and the ZooKeeper cluster will keep quorum. If at any time, quorum is lost, the ZooKeeper cluster will go down.\nZookeeper cluster to withstand loss of 2 server, require total of 2*2+1 = 5 servers.\nIn Zookeeper multi-node configuration, initLimit and syncLimit are used to govern how long following ZooKeeper servers can take to initialize with the current leader and how long they can be out of sync with the leader. If tickTime=2000, initLimit=5 and syncLimit=2 then a follower can take (tickTimeinitLimit) = 10000ms to initialize and may be out of sync for up to (tickTimesyncLimit) = 4000ms\nIn Zookeeper multi-node configuration, The server.* properties set the ensemble membership. The format is\nserver.\u0026lt;myid\u0026gt;=\u0026lt;hostname\u0026gt;:\u0026lt;leaderport\u0026gt;:\u0026lt;electionport\u0026gt;, where:\nmyid is the server identification number. In this example, there are three servers, so each one will have a different myid with values 1, 2, and 3 respectively. The myid is set by creating a file named myid in the dataDir that contains a single integer in human readable ASCII text. This value must match one of the myid values from the configuration file. If another ensemble member has already been started with a conflicting myid value, an error will be thrown upon startup. leaderport is used by followers to connect to the active leader. This port should be open between all ZooKeeper ensemble members. electionport is used to perform leader elections between ensemble members. This port should be open between all ZooKeeper ensemble members. KAFKA CLI kafka-topics.sh allows you to create, modify, delete, and list information about topic in the cluster. kafka-console-producer.sh allows you to write messages into a kafka topic in your cluster. kafka-console-consumer.sh allows you to consume messages out of one or more topics in your Kafka cluster. kafka-consumer-groups.sh allows you to list consumer groups, describe, specific groups, delete consumer groups or specific group info, or reset consumer group offset information. kafka-configs.sh allows you to change the configuration dynamically for --entity-types topics, brokers, users, and clients. You are required to provide the cluster connection string and port through the --bootstrap-server option. Provide cluster information through -zookeeper for older version of Kafka. Zookeeper is deprecated.\nFollowing are few examples of Kafka command line:-\n① Start a zookeeper at default port 2181\n$bin/zookeeper-server-start.sh config/zookeeper.properties Please note that Zookeeper is no longer required to manage Apache Kafka cluster in latest version. You still need it for older versions.\n② Start a kafka server at default port 9092\n$bin/kafka-server-start.sh config/server.properties ③ Create a kafka topic \u0026lsquo;my-first-topic\u0026rsquo; with 3 partitions and 3 replicas\n$bin/kafka-topics.sh \\ --bootstrap-server localhost:9092 \\ --topic my-first-topic \\ --create \\ --replication-factor 3 \\ --partitions 3 ④ List all kafka topics in a cluster\n$bin/kafka-topics.sh \\ --bootstrap-server localhost:9092 \\ --list ⑤ Modify kafka topic \u0026lsquo;my-first-topic\u0026rsquo;\n$bin/kafka-topics.sh \\ --bootstrap-server localhost:9092 \\ --topic my-first-topic \\ --partitions 9 \\ --alter This will add 6 more partition to the topic, which is already having 3 partition\n⑥ Delete kafka topic \u0026lsquo;my-first-topic\u0026rsquo;\n$bin/kafka-topics.sh \\ --bootstrap-server localhost:9092 \\ --topic my-first-topic \\ --delete This will have no impact if delete.topic.enable is not set to true\n⑦ Describe kafka topic \u0026lsquo;my-first-topic\u0026rsquo; details\n$bin/kafka-topics.sh \\ --bootstrap-server localhost:9092 \\ --topic my-first-topic \\ --describe Find out all the partitions without a leader\n$bin/kafka-topics.sh \\ --bootstrap-server localhost:9092 \\ --describe --unavailable-partitions ⑧ Produce messages to Kafka topic \u0026lsquo;my-first-topic\u0026rsquo;\n$bin/kafka-console-producer.sh \\ --bootstrap-server localhost:9092 \\ --topic my-first-topic \\ --producer-property acks=all \u0026gt; message 1 \u0026gt; message 2 \u0026gt; ^C ⑨ Start Consuming messages from kafka topic \u0026lsquo;my-first-topic\u0026rsquo;\n$bin/kafka-console-consumer.sh \\ --bootstrap-server localhost:9092 \\ --topic my-first-topic \\ --from-beginning \u0026gt; message 1 \u0026gt; message 2 ⑩ Start Consuming messages in a consumer group from kafka topic \u0026lsquo;my-first-topic\u0026rsquo;\n$bin/kafka-console-consumer.sh \\ --bootstrap-server localhost:9092 \\ --topic my-first-topic \\ --group my-first-consumer-group \\ --from-beginning ⑪ List all consumer groups\n$bin/kafka-consumer-groups.sh \\ --bootstrap-server localhost:9092 \\ --list ⑫ Describe consumer group\n$bin/kafka-consumer-groups.sh \\ --bootstrap-server localhost:9092 \\ -group my-first-consumer-group \\ --describe ⑬ Reset offset of consumer group to replay all messages\n$bin/kafka-consumer-groups.sh \\ --bootstrap-server localhost:9092 \\ --group my-first-consumer-group \\ --reset-offsets --to-earliest \\ --topic my-first-topic \\ --execute ⑭ Shift offsets by 2 (forward) of a consumer group\n$bin/kafka-consumer-groups \\ --bootstrap-server localhost:9092 \\ --group my-first-consumer-group \\ --reset-offsets --shift-by 2 \\ --topic my-first_topic \\ --execute You can shift offsets by 2 (backward) using --shift-by -2\n⑮ Setting the retention for the topic named \u0026lsquo;my-first-topic\u0026rsquo; to 1 hour (3,600,000 ms):\n$bin/kafka-configs.sh \\ --bootstrap-server localhost:9092 --entity-type topics --entity-name my-first-topic --add-config retention.ms=3600000 --alter Kafka Streams Kafka Streams is used for building streaming applications which transform data of one Kafka topics and feeds to another Kafka topic.\n1. Stateless Operators branch filter inverseFilter flatMap flatMapValues foreach groupByKey groupBy map mapValues 2. Stateful Operators join aggregate count reduce windowing 3. Window 1) Tumbling window Time based, Fixed Size, Non overlapping, Gap less windows For e.g. if window-size=5min and advance-interval =5min then it looks like [0-5min] [5min-10min] [10min-15min]\u0026hellip;.. 2) Hopping window Time based, Fixed Size, Overlapping windows For e.g. if widow-size=5min and advance-interval=3min then it looks like [0-5min] [3min-8min] [6min-11min]\u0026hellip;\u0026hellip; 3) Sliding window Fixed size overlapping window that works on the difference between record timestamp Used only for join operation 4) Session window Session based, Dynamically sized, Non overlapping, Data driven window. Used to aggregate key based events into session. For more information, refer Apache Kafka Documentation on windowing\n4. SerDes data types Kafka stream operations require SerDes (Serializer/Deserializer) to identify data type.\nbyte[] ByteBuffer Double Integer Long String 5. Streams DSL 1) KStream Abstraction of record stream from subset of partitions of topic In database table analogy, interpreted as INSERT statement In an e-commerce application, any type of transactions like purchase, payment should be modeled as KStream 2) KTable Abstraction of changelog stream from subset of partitions of topic In database table analogy, interpreted as UPDATE statement In an e-commerce application, mostly static data like inventory list, customer list and aggregated data like total sales should be modeled as KTable 3) GlobalKTable Abstraction of changelog stream from all partitions of topic In database table analogy, interpreted as UPDATE statement For more information, refer Apache Kafka Documentation on stream DSL\n6. Join Operands Join Operands Output Type co-partition required Join Type KStream-to-KStream KStream Windowed Yes key and window based KTable-to-KTable KTable Non-windowed Yes key or foreign-key based KStream-to-KTable KStream Non-windowed Yes key based KStream-to-GlobalKTable KStream Non-windowed No key or foreign-key based co-partition co-partition means both the left and right join operand topics must have same number of partitions.\nA join between a topic A ( 5 parition ) and topic B (3 partition) is possible using KStream-to-GlobalKTrade since it does not require co-partition.\nKAFKA API Click here to find out how we can create a Safe and high throughput Kafka Producer using Java. Click here to find out how we can create a Kafka consumer using Java with manual auto commit enabled. Confluent Schema Registry 1. Avro Primitive Types 1. null 2. boolean 3. int (32 bit) 4. long (64 bit) 5. float (32 bit) 6. double (64 bit) 7. byte[] (8 bit) 8. string (char sequence) Complex Types 1. record 2. enum 3. array 4. map 5. union 6. fixed Avro Schema Definition namespace (required) type (required) =\u0026gt; record, enum, array, map, union, fixed name (required) doc (optional) aliases (optional) fields (required) { name (required) type (required) doc (optional) default (optional) order (optional) aliases (optional) } 2. Confluent Schema Notes Schema Registry stores all schemas in a Kafka topic _schemas defined by kafkastore.config = _schemas (default) which is a single partition topic with log compacted. The default response media type application/vnd.schemaregistry.v1+json, application/vnd.schemaregistry+json, application/json are used in response header. HTTP and HTTPS client protocol are supported for schema registry. Prefix to apply to metric names for the default JMX reporter kafka.schema.registry Default port for listener is 8081 Confluent support primitive types of null, Boolean, Integer, Long, Float, Double, String, byte[], and complex type of IndexedRecord. Sending data of other types to KafkaAvroSerializer will cause a SerializationException 3. Confluent Schema Compatibility Types BACKWARD Consumer using schema X can process data produced with schema X or X-1. In case of BACKWARD_TRANSITIVE, consumer using schema X can process data produced with all previous schema X, X-1, X-2 and so on Delete field without default value (Required field) is allowed. In this case, Consumer ignore this field. Add field with default value (Optional field) is allowed. In this case, Consumer will assign default value. BACKWARD is default compatibility type in confluent schema registry. There is no assurance that consumers using older schema can read data produced using the new schema. Therefore, upgrade all consumers before you start producing new events. FORWARD Data produced using schema X can be ready by consumers with schema X or X-1. In case of FORWARD_TRANSITIVE, data produced using schema X can be ready by consumers with all previous schema X, X-1, X-2 and so on Add field without default value (Required field) is allowed. In this case, Consumer ignore this field. Delete field with default value (Optional field) is allowed. In this case, Consumer will assign default value. There is no assurance that consumers using the new schema can read data produced using older schema. Therefore, first upgrade all producers to using the new schema and make sure the data already produced using the older schema are not available to consumers, then upgrade the consumers. FULL Backward and forward compatible between schema X and X-1. In case of FULL_TRANSITIVE, backward and forward compatible between all previous schema X and X-1 and X-2 and so on Modify field with default value (Optional field) is allowed. There are assurances that consumers using older schema can read data produced using the new schema and that consumers using the new schema can read data produced using older schema. Therefore, you can upgrade the producers and consumers independently. NONE Compatibility type means schema compatibility checks are disabled. Upgrading Consumer or Producer depends. For example, modifying a field type from Number to String. In this case, you will either need to upgrade all producers and consumers to the new schema version at the same time Default Ports Zookeeper Client Port: 2181 Zookeeper Leader Port: 3888 Zookeeper Election Port (Peer port): 2888 Broker: 9092 REST Proxy: 8082 Schema Registry: 8081 KSQL: 8088 ","permalink":"https://codingnconcepts.com/post/apache-kafka-ccdak-exam-notes/","tags":["CCDAK","Certification"],"title":"Apache Kafka CCDAK Exam Notes"},{"categories":["Javascript"],"contents":"This post explains the DOM, Virtual DOM and Shadow DOM concepts and their differences\u0026hellip;\nDOM DOM in shorthand for Document Object Model - It’s a way of representing a structured content via objects. HTML, XHTML, XML are some of the ways to write structured content.\nWhen browser renders an HTML page, it sort of compiles this HTML behind the scene and generates a DOM Object. This DOM Object can be accessed and manipulated by JavaScript and CSS for e.g. when you click on a button, a DOM click event is triggered which JavaScript can listen and manipulate DOM to show a pop up dialog.\nBy default, when there is any change in the DOM Object, browser re-render the whole page. In this way changes in DOM are expensive in terms of performance.\nTo solve this issue, new concepts came out. Let\u0026rsquo;s look at them:-\nVirtual DOM The virtual DOM is an in-memory representation of the real DOM. Popular UI frameworks React.js and Vue.js, both use Virtual DOM. The concept of virtual DOM is mainly to solve performance issue, Here is how:- Any update in the DOM first applied to Virtual DOM instead of applying directly to actual DOM. Then it compare the changes against actual DOM through a process call diffing and apply the changes efficiently to actual DOM by only re-rendering the changed elements. In additional to that, It allows to collect several changes to be applied at once, so not every single change causes a re-render, but instead re-rendering only happens once after a set of changes were applied from virtual DOM to actual DOM Shadow DOM You can think of the shadow DOM as a DOM within a DOM. A real DOM can have many shadow DOMs but each share DOM has its own isolated DOM tree with its own elements and styles, completely isolated from the real DOM. Concept of Shadow DOM is natively supported by most of the browsers including Firefox, Chrome, Opera and Safari. You can make reusable native web components which follows Shadow DOM concept. Implementation and styling of native web component is hidden within the Shadow DOM and having no impact from the outer DOM. Polymer LitElement and Vaadin provides open source reusable web components built using shadow DOM concept. Shadow DOM is not a new concept Although only recently specified for use by web authors, the shadow DOM has been used by user agents for years to create and style complex components such as form elements. Let’s take the range input element, for example. To create one on the page, all we have to do is add the following element:\n\u0026lt;input type=\u0026#34;range\u0026#34;\u0026gt; That one element results in the following component:\nIf we dig deeper, we will see that this one \u0026lt;input\u0026gt; element is actually made up of several smaller \u0026lt;div\u0026gt; elements, controlling the track and the slider itself.\nThis is achieved using the shadow DOM. The element that is exposed to the host HTML document the simple \u0026lt;input\u0026gt;, but underneath it there are elements and styles related to the component that do not form part of the DOM’s global scope.\n","permalink":"https://codingnconcepts.com/javascript/virtual-dom-vs-shadow-dom/","tags":["Javascript Interview","Javascript Core"],"title":"Virtual Dom vs Shadow Dom"},{"categories":["Java","Kafka"],"contents":"Implementation of Kafka Consumer using Java\nppackage com.abc; import java.time.Duration; import java.util.Collections; import java.util.Properties; import java.util.concurrent.ExecutionException; import org.apache.kafka.clients.consumer.ConsumerConfig; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import org.apache.kafka.common.serialization.StringDeserializer; public class KafkaConsumerTest { public static void main(String[] args) throws InterruptedException, ExecutionException{ //Create consumer property String bootstrapServer = \u0026#34;localhost:9092\u0026#34;; String groupId = \u0026#34;my-first-consumer-group\u0026#34;; String topicName = \u0026#34;my-first-topic\u0026#34;; Properties properties = new Properties(); properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServer); properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupId); properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \u0026#34;earliest\u0026#34;); properties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \u0026#34;false\u0026#34;); //Create consumer KafkaConsumer\u0026lt;String, String\u0026gt; consumer = new KafkaConsumer\u0026lt;\u0026gt;(properties); //Subscribe consumer to topic(s) consumer.subscribe(Collections.singleton(topicName)); //Poll for new data while(true){ ConsumerRecords\u0026lt;String, String\u0026gt; records = consumer.poll(Duration.ofMillis(1000)); for(ConsumerRecord\u0026lt;String, String\u0026gt; record: records){ System.out.println(record.key() + record.value()); System.out.println(record.topic() + record.partition() + record.offset()); } //Commit consumer offset manually (recommended) consumer.commitAsync(); } } } ","permalink":"https://codingnconcepts.com/post/apache-kafka-consumer-using-java/","tags":["Java Kafka","Java Coding"],"title":"Kafka Consumer Using Java"},{"categories":["Java","Kafka"],"contents":"Implementation of Kafka Producer using Java\npackage com.abc.demo; import java.util.Properties; import java.util.concurrent.ExecutionException; import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.ProducerConfig; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.common.serialization.StringSerializer; public class KafkaProducerTest { public static void main(String[] args) throws InterruptedException, ExecutionException{ //Create producer property String bootstrapServer = \u0026#34;localhost:9092\u0026#34;; Properties properties = new Properties(); properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServer); properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); //Create safe producer properties.setProperty(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \u0026#34;true\u0026#34;); properties.setProperty(ProducerConfig.ACKS_CONFIG, \u0026#34;all\u0026#34;); properties.setProperty(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, \u0026#34;5\u0026#34;); properties.setProperty(ProducerConfig.RETRIES_CONFIG, Integer.toString(Integer.MAX_VALUE)); //High throughput producer (at the expense of a bit of latency and CPU usage) properties.setProperty(ProducerConfig.COMPRESSION_TYPE_CONFIG, \u0026#34;snappy\u0026#34;); properties.setProperty(ProducerConfig.LINGER_MS_CONFIG, \u0026#34;20\u0026#34;); //20ms wait time properties.setProperty(ProducerConfig.BATCH_SIZE_CONFIG, Integer.toString(32*1024)); //32KB batch size //Create producer KafkaProducer\u0026lt;String, String\u0026gt; producer = new KafkaProducer\u0026lt;\u0026gt;(properties); //create a producer record ProducerRecord\u0026lt;String, String\u0026gt; record = new ProducerRecord\u0026lt;\u0026gt;(\u0026#34;topicName\u0026#34;, \u0026#34;firstRecord\u0026#34;); //create producer record with key //new ProducerRecord\u0026lt;\u0026gt;(\u0026#34;topicName\u0026#34;, \u0026#34;MessageKey\u0026#34;, \u0026#34;Message\u0026#34;); //create producer record with key and partition number //new ProducerRecord\u0026lt;\u0026gt;(\u0026#34;topicName\u0026#34;, 1 /*partition number*/, \u0026#34;MessageKey\u0026#34;, \u0026#34;Message\u0026#34;); //send data - asynchronous //without callback //producer.send(record); //with callback producer.send(record, (recordMetadata, exception) -\u0026gt; { if(exception == null){ System.out.println(recordMetadata.topic() + \u0026#34;+\u0026#34; + recordMetadata.partition() + \u0026#34;+\u0026#34; + recordMetadata.offset()); }else{ System.err.println(exception.getMessage()); } }); //send data - synchronous //without callback //producer.send(record).get(); //.get() make it synchronous call //flush data producer.flush(); //flush and close producer producer.close(); } } ","permalink":"https://codingnconcepts.com/post/apache-kafka-producer-using-java/","tags":["Java Kafka","Java Coding"],"title":"Kafka Producer Using Java"},{"categories":["Java"],"contents":"In this tutorial, we\u0026rsquo;ll learn about Immutable Class and its benefits in thread-safety, caching and collections. We will also look at rules to create immutable classes and eventually we\u0026rsquo;ll write an Immutable Class from scratch in Java.\nWhat is Immutable Class? Immutable Class means that once an object is initialized from this Class, we cannot change the state of that object.\nIn other words, An immutable object can’t be modified after it has been created. When a new value is needed, the accepted practice is to make a copy of the object that has the new value.\nExamples In Java, All primitive wrapper classes (Integer, Byte, Long, Float, Double, Character, Boolean and Short) and String are immutable in nature.\nString is the most popular Immutable Class known among developers. String object cannot be modified once initialized. Operations like trim(), substring(), replace(), toUpperCase(), toLowerCase() always return a new instance and don’t affect the current instance.\nIn the below example, s1.toUpperCase() returns new instance which need to assign back to s1, if you want s1 to refer to new uppercase instance.\nString s1 = new String(\u0026#34;CodingNConcepts\u0026#34;); String s2 = s1.toUpperCase(); System.out.println(s1 == s2); //false s1 = s2; //assign back to s1 System.out.println(s1 == s2); //true Benefits of Immutable Class Some of the key benefits of Immutable Class are:-\n1. Mutable vs Immutable Object A mutable object starts with one state (initial values of the instance variables). Each mutation of the instance variable takes the object to another state. This brings in the need to document all possible states a mutable object can be in. It is also difficult to avoid inconsistent or invalid states. Hence, all these makes a mutable object difficult to work with.\nWhereas, immutable object have just one state, which is first and foremost benefit of Immutable Class.\n2. Thread safety Immutable objects are inherently thread-safe. They do not require synchronization. Since there is no way the state of an immutable object can change, there is no possibility of one thread observing the effect of another thread. We do not have to deal with the intricacies of sharing an object among threads (like locking, synchronization, making variables volatile etc.). Thus, we can freely share immutable objects. This is the easiest way to achieve thread safety.\n3. Reusable/Cacheable Immutable objects encourage to cache or store the frequently used instances rather than creating one each time. This is because two immutable instances with the same properties/values are equal.\nSome examples of this being applied are as follows:-\nPrimitive wrapper classes\nCreating primitive wrapper objects (Integer, Float, Double etc) using static factory method valueOf does not always return new wrapper instances. In case of Integer, they cache Integer values from -128 to 127 by default.\nInteger oneV1 = new Integer(1); Integer oneV2 = new Integer(1); System.out.println(oneV1 == oneV2); //false System.out.println(oneV1.equals(oneV2)); //true oneV1 = Integer.valueOf(1); //returns cached instance oneV2 = Integer.valueOf(1); //returns cached instance System.out.println(oneV1 == oneV2); //true System.out.println(oneV1.equals(oneV2)); //true BigInteger\nBigInteger stores some common BigInteger values as instance variables.\n/** * The BigInteger constant zero. */ public static final BigInteger ZERO = new BigInteger(new int[0], 0); This reduces the memory footprint and the garbage collection costs.\n4. Building blocks for Collections The immutable objects make a great building block for Collections as compare to mutable objects. Let\u0026rsquo;s understand the problem we face with mutable objects in Collections.\nFirst of all, we create a mutable Person class\nclass Person { String name; public String getName() { return name; } public void setName(String name) { this.name = name; } @Override public String toString() { return \u0026#34;Person { name: \u0026#34; + name + \u0026#34; }\u0026#34;; } } Now let\u0026rsquo;s create some person objects to create Set of persons:-\nPerson person1 = new Person(); person1.setName(\u0026#34;Adam\u0026#34;); Person person2 = new Person(); person2.setName(\u0026#34;Ben\u0026#34;); Set\u0026lt;Person\u0026gt; setOfPerson = new HashSet\u0026lt;\u0026gt;(Arrays.asList(person1, person2)); System.out.println(setOfPerson); person1.setName(\u0026#34;Charlie\u0026#34;); System.out.println(setOfPerson); Output [Person { name: Adam }, Person { name: Ben }] [Person { name: Charlie }, Person { name: Ben }] We wanted to create a Set of persons Adam and Ben but next part of the code has mutated the Adam to Charlie which is not intended. We will solve this problem by making an immutable class ImmutablePerson in subsequent part of the article.\nAs we saw that mutable objects can be mutated even if not intended, immutable objects make a great fit to be used as Keys of a Map, in Set, List, and other collections.\nHow to create Immutable Class? In order to create an Immutable Class, you should keep following points in mind:-\nDeclare the class as final so that it cannot be extended and subclasses will not be able to override methods. Make all the fields as private so direct access in not allowed Make all the fields as final so that value cannot be modified once initialized Provide no setter methods — setter methods are those methods which modify fields or objects referred to by fields. Initialize all the final fields through a constructor and perform a deep copy for mutable objects. If the class holds a mutable object: Don\u0026rsquo;t provide any methods that modify the mutable objects. Always return a copy of mutable object from getter method and never return the actual object reference. Let\u0026rsquo;s apply all the above points and create our immutable class ImmutablePerson\nImmutablePerson.java /** * Immutable class should mark as final so it can not be extended. * Fields should mark as private so direct access is not allowed. * Fields should mark as final so value can not be modified once initialized. **/ public final class ImmutablePerson { // String - immutable private final String name; // Integer - immutable private final Integer weight; // Date - mutable private final Date dateOfBirth; /** * All the final fields are initialized through constructor * Perform a deep copy of immutable objects */ public ImmutablePerson(String name, Integer weight, Date dateOfBirth){ this.name = name; this.weight = weight; this.dateOfBirth = new Date(dateOfBirth.getTime()); } /********************************************** ***********PROVIDE NO SETTER METHODS ********* **********************************************/ /** * String class is immutable so we can return the instance variable as it is **/ public String getName() { return name; } /** * Integer class is immutable so we can return the instance variable as it is **/ public Integer getWeight() { return weight; } /** * Date class is mutable so we need a little care here. * We should not return the reference of original instance variable. * Instead a new Date object, with content copied to it, should be returned. **/ public Date getDateOfBirth() { return new Date(dateOfBirth.getTime()); } @Override public String toString() { return \u0026#34;Person { name: \u0026#34; + name + \u0026#34;, weight: \u0026#34; + weight + \u0026#34;, dateOfBirth: \u0026#34; + new SimpleDateFormat(\u0026#34;dd-MM-yyyy\u0026#34;).format(dateOfBirth) + \u0026#34;}\u0026#34;; } } Now, Let\u0026rsquo;s create immutable person objects to create Set of persons:-\nImmutablePerson person1 = new ImmutablePerson(\u0026#34;Adam\u0026#34;, 55, new SimpleDateFormat(\u0026#34;dd-MM-yyyy\u0026#34;).parse(\u0026#34;01-01-2001\u0026#34;)); ImmutablePerson person2 = new ImmutablePerson(\u0026#34;Ben\u0026#34;, 50, new SimpleDateFormat(\u0026#34;dd-MM-yyyy\u0026#34;).parse(\u0026#34;02-02-2002\u0026#34;)); Set\u0026lt;ImmutablePerson\u0026gt; setOfPerson = new HashSet\u0026lt;\u0026gt;(Arrays.asList(person1, person2)); System.out.println(setOfPerson); /** * ImmutablePerson do not provide setter methods, * no way to mutate name, weight, or date property fields. */ //person1.setName(\u0026#34;Charlie\u0026#34;); //person1.setWeight(90); //person1.setDate(new SimpleDateFormat(\u0026#34;dd-MM-yyyy\u0026#34;).parse(\u0026#34;03-03-2003\u0026#34;)); /** * getDateOfBirth() method returns new instance of date, * setYear() will not change value of person1\u0026#39;s date field. */ Date person1Date = person1.getDateOfBirth(); person1Date.setYear(2020); System.out.println(setOfPerson); Output [Person { name: Adam, weight: 55, dateOfBirth: 01-01-2001}, Person { name: Ben, weight: 50, dateOfBirth: 02-02-2002}] [Person { name: Adam, weight: 55, dateOfBirth: 01-01-2001}, Person { name: Ben, weight: 50, dateOfBirth: 02-02-2002}] We see that we can not mutate the collection of ImmutablePerson once created.\nSummary In this tutorial, we learned about Immutable Class in Java and its benefits. Moreover, Immutable Class is frequently asked interview question to check your understanding on design pattern, immutable \u0026amp; mutable objects and final keyword.\nWhat do you think? As per Oracle Docs for Immutable Classes, Don\u0026rsquo;t allow subclasses to override methods. The simplest way to do this is to declare the class as final. A more sophisticated approach is to make the constructor private and construct instances in factory methods.\nPlease note that we have marked the class as final and constructor as public in our ImmutablePerson class.\nIt is debatable. Should we use private constructor with static factory method to create instances? My view is we are restricting the instance creation by using private constructor, which is not a desired scenario for immutability.\nWhat\u0026rsquo;s your thoughts on this? Please comment.\nReferences: Java Developer Central ","permalink":"https://codingnconcepts.com/java/immutable-class-using-java/","tags":["Java Design Pattern"],"title":"Immutable Class in Java"},{"categories":["Java"],"contents":"Fibonacci series implementation in java is frequently asked question in interview at fresher level. Moreover, it is a very famous example to show how to use recursive function in java.\nRecursive Approach public class Fibonacci { public static void main(String[] args) { fibonacci(10); } /** * print fibonacci series from 0 to n * @param n */ private static void fibonacci(int n) { for (int i = 0; i \u0026lt;= n; i++) { System.out.print(fib(i) + \u0026#34;, \u0026#34;); } } /** * Recursive approach f(n) = f(n-1) + f(n-2) * @param n * @return */ public static int fib(int n) { if (n \u0026lt;= 1) { return n; } return fib(n - 1) + fib(n - 2); } } Output 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, Recursive approach time complexity is approx 𝘖(2ⁿ)\nDynamic Approach We can use dynamic approach for linear time complexity i.e. 𝘖(n)\npublic static int fibonacci(int n) { if (n \u0026lt;= 1) { return n; } int[] fib = new int[n + 1]; fib[0] = 0; fib[1] = 1; for (int i = 2; i \u0026lt;= n; i++) { fib[i] = fib[i - 1] + fib[i - 2]; } return fib[n]; } ","permalink":"https://codingnconcepts.com/java/fibonacci-series-using-recursive/","tags":["Java Algorithm","Recursive"],"title":"Fibonacci Series Using Recursive function"},{"categories":["Java"],"contents":"Why it is called bubble sort ? Bubble Sort is nothing but a comparison algorithm where -\nAt the end of first iteration, largest element in the array get placed at last index At the end of second iteration, second largest element in the array get placed at second last index and so on\u0026hellip; This way large elements are moving towards the last indexes and hence small elements are moving towards the starting indexes which is also termed as smaller elements \u0026ldquo;bubble\u0026rdquo; to the top of the list that is why it is called bubble sort.\nStep-by-step example Let us take the array of numbers \u0026ldquo;5 1 4 2 8\u0026rdquo;, and sort the array from lowest number to greatest number using bubble sort. In each step, elements written in bold are being compared. Three passes will be required.\nFirst Pass: ( 5 1 4 2 8 ) \u0026mdash;\u0026mdash; ( 1 5 4 2 8 ), Here, algorithm compares the first two elements, and swaps since 5 \u0026gt; 1.\n( 1 5 4 2 8 ) \u0026mdash;\u0026mdash; ( 1 4 5 2 8 ), Swap since 5 \u0026gt; 4\n( 1 4 5 2 8 ) \u0026mdash;\u0026mdash; ( 1 4 2 5 8 ), Swap since 5 \u0026gt; 2\n( 1 4 2 5 8 ) \u0026mdash;\u0026mdash; ( 1 4 2 5 8 ), Now, since these elements are already in order (8 \u0026gt; 5), algorithm does not swap them.\nSecond Pass: ( 1 4 2 5 8 ) \u0026mdash;\u0026mdash; ( 1 4 2 5 8 )\n( 1 4 2 5 8 ) \u0026mdash;\u0026mdash; ( 1 2 4 5 8 ), Swap since 4 \u0026gt; 2\n( 1 2 4 5 8 ) \u0026mdash;\u0026mdash; ( 1 2 4 5 8 )\n( 1 2 4 5 8 ) \u0026mdash;\u0026mdash; ( 1 2 4 5 8 )\nNow, the array is already sorted, but the algorithm does not know if it is completed. The algorithm needs one whole pass without any swap to know it is sorted.\nThird Pass: ( 1 2 4 5 8 ) \u0026mdash;\u0026mdash; ( 1 2 4 5 8 )\n( 1 2 4 5 8 ) \u0026mdash;\u0026mdash; ( 1 2 4 5 8 )\n( 1 2 4 5 8 ) \u0026mdash;\u0026mdash; ( 1 2 4 5 8 )\n( 1 2 4 5 8 ) \u0026mdash;\u0026mdash; ( 1 2 4 5 8 )\npackage com.abc; public class BubbleSort { public static void main(String[] args){ int[] array = new int[]{5, 1, 12, -5, 16}; BubbleSort.sort(array); } private static void sort(int[] array){ int count = 0; for(int i = array.length ; i \u0026gt; 0 ; i --, count++){ for(int j = 0 ; j \u0026lt; array.length-1 ; j++, count++){ if(array[j] \u0026gt; array[j+1]){ swapNumbers(j, j+1, array); } } } printNumbers(array, count); } private static void swapNumbers(int i, int j, int[] array) { int temp; temp = array[i]; array[i] = array[j]; array[j] = temp; } private static void printNumbers(int[] array, int count) { System.out.print(\u0026#34;Sorted Array : {\u0026#34;); for(int i : array){ System.out.print(i + \u0026#34; \u0026#34;); } System.out.print(\u0026#34;}, n : \u0026#34; + array.length + \u0026#34;, comparisons : \u0026#34; + count); } } Output Sorted Array : {-5 1 5 12 16 }, n : 5, comparisons : 25 Time Complexity for worst case is O(n2). Algorithm can be further improved :\nprivate static void sort(int[] array){ int count = 0; for(int i = array.length ; i \u0026gt; 0 ; i --, count++){ for(int j = 0 ; j \u0026lt; i-1 ; j++, count++){ if(array[j] \u0026gt; array[j+1]){ swapNumbers(j, j+1, array); } } } printNumbers(array, count); } Output Sorted Array : {-5 1 5 12 16 }, n : 5, comparisons : 15 ","permalink":"https://codingnconcepts.com/java/bubble-sort-using-java/","tags":["Sorting","Java Algorithm"],"title":"Bubble Sort"},{"categories":["Java"],"contents":"Implementation of Elevator or Lift has been asked in many interviews. I have tried to implement it using muti-threading and TreeSet. TreeSet is used to store incoming request. It is a good choice here as it removes the duplicate requests and implements NavigableSet which provide you methods such as floor and ceiling.\nElevator in this program implements following features -\nIf elevator is going up or down, it checks for nearest floor request to process first in that direction. If there is no request to process, it waits at last processed floor. If a new request comes while elevator is processing a request. It process the new request first if it is nearest than the processing floor in same direction. import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.util.TreeSet; public class MyLift { public static void main(String[] args) { System.out.println(\u0026#34;Welcome to MyLift\u0026#34;); // RequestListenerThread to read requested floor and add to Set Thread requestListenerThread = new Thread(new RequestListener(), \u0026#34;RequestListenerThread\u0026#34;); // RequestProcessorThread to read Set and process requested floor Thread requestProcessorThread = new Thread(new RequestProcessor(), \u0026#34;RequestProcessorThread\u0026#34;); Elevator.getInstance().setRequestProcessorThread(requestProcessorThread); requestListenerThread.start(); requestProcessorThread.start(); } } class Elevator { private static Elevator elevator = null; private TreeSet requestSet = new TreeSet(); private int currentFloor = 0; private Direction direction = Direction.UP; private Elevator() {}; private Thread requestProcessorThread; /** * @return singleton instance */ static Elevator getInstance() { if (elevator == null) { elevator = new Elevator(); } return elevator; } /** * Add request to Set * * @param floor */ public synchronized void addFloor(int f) { requestSet.add(f); if(requestProcessorThread.getState() == Thread.State.WAITING){ // Notify processor thread that a new request has come if it is waiting notify(); }else{ // Interrupt Processor thread to check if new request should be processed before current request or not. requestProcessorThread.interrupt(); } } /** * @return next request to process based on elevator current floor and direction */ public synchronized int nextFloor() { Integer floor = null; if (direction == Direction.UP) { if (requestSet.ceiling(currentFloor) != null) { floor = requestSet.ceiling(currentFloor); } else { floor = requestSet.floor(currentFloor); } } else { if (requestSet.floor(currentFloor) != null) { floor = requestSet.floor(currentFloor); } else { floor = requestSet.ceiling(currentFloor); } } if (floor == null) { try { System.out.println(\u0026#34;Waiting at Floor :\u0026#34; + getCurrentFloor()); wait(); } catch (InterruptedException e) { e.printStackTrace(); } } else { // Remove the request from Set as it is the request in Progress. requestSet.remove(floor); } return (floor == null) ? -1 : floor; } public int getCurrentFloor() { return currentFloor; } /** * Set current floor and direction based on requested floor * * @param currentFloor * @throws InterruptedException */ public void setCurrentFloor(int currentFloor) throws InterruptedException { if (this.currentFloor \u0026gt; currentFloor) { setDirection(Direction.DOWN); } else { setDirection(Direction.UP); } this.currentFloor = currentFloor; System.out.println(\u0026#34;Floor : \u0026#34; + currentFloor); Thread.sleep(3000); } public Direction getDirection() { return direction; } public void setDirection(Direction direction) { this.direction = direction; } public Thread getRequestProcessorThread() { return requestProcessorThread; } public void setRequestProcessorThread(Thread requestProcessorThread) { this.requestProcessorThread = requestProcessorThread; } public TreeSet getRequestSet() { return requestSet; } public void setRequestSet(TreeSet requestSet) { this.requestSet = requestSet; } } class RequestProcessor implements Runnable { @Override public void run() { while (true) { Elevator elevator = Elevator.getInstance(); int floor = elevator.nextFloor(); int currentFloor = elevator.getCurrentFloor(); try{ if (floor \u0026gt;= 0) { if (currentFloor \u0026gt; floor) { while (currentFloor \u0026gt; floor) { elevator.setCurrentFloor(--currentFloor); } } else { while (currentFloor \u0026lt; floor) { elevator.setCurrentFloor(++currentFloor); } } System.out.println(\u0026#34;Welcome to Floor : \u0026#34; + elevator.getCurrentFloor()); } }catch(InterruptedException e){ // If a new request has interrupted a current request processing then check - // -if the current request is already processed // -otherwise add it back in request Set if(elevator.getCurrentFloor() != floor){ elevator.getRequestSet().add(floor); } } } } } class RequestListener implements Runnable { @Override public void run() { while (true) { String floorNumberStr = null; try { // Read input from console BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(System.in)); floorNumberStr = bufferedReader.readLine(); } catch (IOException e) { e.printStackTrace(); } if (isValidFloorNumber(floorNumberStr)) { System.out.println(\u0026#34;User Pressed : \u0026#34; + floorNumberStr); Elevator elevator = Elevator.getInstance(); elevator.addFloor(Integer.parseInt(floorNumberStr)); } else { System.out.println(\u0026#34;Floor Request Invalid : \u0026#34; + floorNumberStr); } } } /** * This method is used to define maximum floors this elevator can process. * @param s - requested floor * @return true if requested floor is integer and upto two digits. (max floor = 99) */ private boolean isValidFloorNumber(String s) { return (s != null) \u0026amp;\u0026amp; s.matches(\u0026#34;\\\\d{1,2}\u0026#34;); } } enum Direction { UP, DOWN } ","permalink":"https://codingnconcepts.com/java/elevator-implementation-using-java/","tags":["Java Coding"],"title":"Design Elevator (Lift) in Java"},{"categories":["Puzzles"],"contents":"This is my favorite weight puzzle which have been asked from me in many interviews over the past few years.\nPuzzle You have 12 balls identical in size and appearance but 1 is an odd weight (could be either light or heavy).\nYou have a weighing scale with no measurements so you can just compare weight of balls against each other. You have only 3 chances to weigh the balls in any combination using the scales. Determine which ball is the odd one and if it’s heavier or lighter than the rest. How do you do it?\nSolution First of all we will give a number to each ball i.e. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, and 12\nThe trick to solve these kind of weight problem is to divide them in groups. We will divide these 12 balls in 3 groups:\nGroup Ball Numbers group 1 1, 2, 3, 4 group 2 5, 6, 7, 8 group 3 9, 10, 11, 12 Now we keep the group 3 aside and put group 1 balls on one side of scale and group 2 balls on another side of scale [1] with three possible outcomes:-\n1. Scale is balanced That means each ball in group 1 and group 2 are identical in weight and defective one is from group 3.\nWe know that defective one if from group 3 but we don\u0026rsquo;t know that defective one is lighter or heavier as compare to others. To solve this we need to weigh some balls from group 3 against some balls from either group 1 or group 2\nLet\u0026rsquo;s weight group 3 (9, 10, 11) vs group 2 (6, 7, 8) with three possible outcomes:-\nIf group 3 (9, 10, 11) and group 2 (6, 7, 8) balances, then 12 is the odd ball. Weigh it against any other ball to determine if heavy or light. If group 3 (9, 10, 11) is heavy then they contain a heavy ball. Weigh 9 vs 10, if balanced then 11 is the odd heavy ball, else the heavier of 9 or 10 is the odd heavy ball. If group 3 (9, 10, 11) is light then they contain a light ball. Weigh 9 vs 10, if balanced then 11 is the odd light ball, else the lighter of 9 or 10 is the odd light ball. 2. group 1 is heavier then group 2 That means either group 1 (heavy ball) or group 2 (light ball) contains the defective ball and group 3 has all good balls.\nLet\u0026rsquo;s weigh 5,6,1 vs 7,2,12 with 3 possible outcomes:-\nIf 5,6,1 vs 7,2,12 balances, then either 8 is the odd light ball or 3 or 4 is the odd heavy ball. Weigh 3 vs 4, if they balance then 8 is the odd light ball, or the heaviest of 3 vs 4 is the odd heavy ball. If 7,2,12 is heavy then either 2 is the odd heavy ball or 5 or 6 is the odd light ball. Weigh 5 vs 6, if balanced then 2 is the odd heavy ball, or the lightest of 5 vs 6 is the odd light ball. If 7,2,12 is light then either 7 is light or 1 is heavy. Weigh 7 against any other ball, if balanced then 1 is the odd heavy ball else 7 is the odd light ball. 3. group 2 is heavier then group 1 That means either group 2 (heavy ball) or group 1 (light ball) contains the defective ball and group 3 has all good balls.\nLet\u0026rsquo;s weigh 1,2,5 vs 3,6,12 with 3 possible outcomes:-\nIf 1,2,5 vs 3,6,12 balances, then either 4 is the odd light ball or 7 or 8 is the odd heavy ball. Weigh 7 vs 8, if they balance then 4 is the odd light ball, or the heaviest of 7 vs 8 is the odd heavy ball. If 3,6,12 is heavy then either 6 is the odd heavy ball or 1 or 2 is the odd light ball. Weigh 1 vs 2, if balanced then 6 is the odd heavy ball, or the lightest of 1 vs 2 is the odd light ball. If 3,6,12 is light then either 3 is light or 5 is heavy. Weigh 3 against any other ball, if balanced then 5 is the odd heavy ball else 3 is the odd light ball. ","permalink":"https://codingnconcepts.com/puzzle/12-balls-weight-puzzle/","tags":["Interview Puzzle","Weight Puzzle"],"title":"12 Balls Weight Puzzle (Solved)"},{"categories":["Puzzles"],"contents":"This is my favorite weight puzzle which have been asked from me in many interviews over the past few years.\nPuzzle You have 8 balls identical in size and appearance. One of them is defective and weighs heavy than the others. You have a weighing scale with no measurements so you can just compare weight of balls against each other. How would you find the defective ball given only 2 chances to use the scale?\nSolution First of all we will give a number to each ball i.e. 1, 2, 3, 4, 5, 6, 7, and 8\nThe trick to solve these kind of weight problem is to divide them in groups. We will divide these 8 balls in 3 groups:\nGroup Ball Numbers group 1 1, 2, 3 group 2 4, 5, 6 group 3 7, 8 Now we keep the group 3 aside and put group 1 balls on one side of scale and group 2 balls on another side of scale [1st chance] with three possible outcomes:-\n1. Scale is balanced That means each ball in group 1 and group 2 are identical in weight and defective one is from group 3.\nLet\u0026rsquo;s put group 3 balls 7 and 8 on each side of scale [2nd chance] with two possible outcomes:-\nIf 7 is heavy then it is defective one If 8 is heavy then it is defective one 2. group 1 is heavier then group 2 That means defective balls is from group 1 i.e. either 1 or 2 or 3.\nLet\u0026rsquo;s keep number 3 aside and put balls 1 and 2 on each side of scale [2nd chance] with three possible outcomes:-\nIf 1 and 2 balances, then 3 is defective one If 1 is heavy then it is defective one If 2 is heavy then it is defective one 3. group 2 is heavier then group 1 That means defective balls is from group 2 i.e. either 4 or 5 or 6.\nLet\u0026rsquo;s keep number 6 aside and put balls 4 and 5 on each side of scale [2nd chance] with three possible outcomes:-\nIf 4 and 5 balances, then 6 is defective one If 4 is heavy then it is defective one If 5 is heavy then it is defective one Conclusion That\u0026rsquo;s it champs. We have found the defective balls out of 8 balls in only 2 weighing.\nIf by now you understood the trick of dividing balls in group and keeping some balls aside then you can solve weight puzzle with any number of balls. Here is the cheat sheet:-\nCheat Sheet N Balls Weight Puzzle Groups Min Weighing (Best Case) Min Weighing (Worst Case) N = 2 [1] [2] 1 1 N = 3 [1] [2] [3] 1 1 N = 4 [1] [2] [3,4] 1 2 N = 5 [1,2] [3,4] [5] 1 2 N = 6 [1,2] [3,4] [5,6] 2 2 N = 6 [1,2,3] [4,5,6] [7] 1 2 N = 8 [1,2,3] [4,5,6] [7,8] 2 2 N = 9 [1,2,3] [4,5,6] [7,8,9] 2 2 N = 10 [1,2,3,4] [5,6,7,8] [9,10] 2 3 N = 11 [1,2,3,4] [5,6,7,8] [9,10,11] 2 3 N = 12 [1,2,3,4] [5,6,7,8] [9,10,11,12] 3 3 If you are interested in how to solve 9 or 12 balls weight puzzle then check out these posts:-\nHow to solve 9 balls weight puzzle How to solve 12 balls weight puzzle ","permalink":"https://codingnconcepts.com/puzzle/8-balls-weight-puzzle/","tags":["Interview Puzzle","Weight Puzzle"],"title":"8 Balls Weight Puzzle (Solved)"},{"categories":["Puzzles"],"contents":"This puzzle is a mix of weight and math puzzle.\nPuzzle What is the minimum number of weights needed for a scale that will be able to weigh objects from the weight of one pound to 100 pounds?\nSolution If only one side pan of the balance-scale is used,\nWeights are : 2^n \u0026lt;100 i.e. {1,2,4,8,16,32,64} = 7\nIf both side pan of balance-scale is used,\nWeights are : 3^ n \u0026lt;100 i.e. {1,3,9,27,81} = 5\nMathematical Explanation For each weight, there are three things you can do:-\nput it on the left pan, the right pan, or not on the balance at all.\nSo, if you have n weights, there are 3n things you can do with them.\nOne of those things is not putting any weights on the scale, which is good if you want to weigh a 0-pound object, so really there are only (3n – 1) arrangements.\nThen, for each arrangement there’s also its mirror image (where all the weights are switched to the opposite pan of the scale), so there are at most (3n – 1)/2 arrangements of n weights.\nThat’s enough to prove that 4 weights can weigh at most 40 different things … 40 is really the upper limit for 4 weights.\nWith a fifth weight, you should be able to get up to (35 – 1)/2 =121 pounds.\n","permalink":"https://codingnconcepts.com/puzzle/minimum-set-of-weights/","tags":["Weight Puzzle","Maths Puzzle"],"title":"Minimum set of weight puzzle"},{"categories":["Puzzles"],"contents":"The Contaminated Pills Puzzle also one of my favorite puzzle which is frequently asked in interviews.\nPuzzle You have 5 jars of full of pills. Each pill weighs 10 gram, except for contaminated pills contained in one jar, where each pill weighs 9 gm.\nGiven a scale, how could you tell which jar had the contaminated pills in just one measurement?\nSolution Mark the jars with numbers 1, 2, 3, 4, and 5. Take 1 pill from jar 1, take 2 pills from jar 2, take 3 pills from jar 3, take 4 pills from jar 4 and take 5 pills from jar 5. Put all of them on the scale at once and take the measurement. Now, subtract the measurement from 150 ( 110 + 210 + 310 + 410 + 5*10) The result will give you the jar number which has contaminated pill. ","permalink":"https://codingnconcepts.com/puzzle/contaminated-pills/","tags":["Interview Puzzle","Weight Puzzle"],"title":"Contaminated Pills Puzzle"},{"categories":null,"contents":"नारी क्या है ये \u0026hellip;अबला या बला ?\nअबला है तो प्यार ही पूजा है, बला है तो आज ये तो कल कोई दूजा है\nअबला है तो पति के चरणों में परमेश्वर है, बला है तो परमेश्वर के चरणों में पति है\nअबला है तो सावन ही सावन, बला है तो पतझड़ ही पतझड़\nअबला है तो सत्कार सेवा सम्मान है, बला है तो मुसीबतों की दुकान है\nअबला हो या बला हो वक़्त ने यही बताया है, इसके अंतर्मन को कोई नर समझ न पाया है !\n~~ आशीष लाहोटी ( १६ जुलाई २०११ ) ~~\n","permalink":"https://codingnconcepts.com/poetry/abla-ya-bala/","tags":null,"title":"अबला या बला ?"},{"categories":null,"contents":"साडे यारां दी यारी है अनमोल\nसाडे यारां दा टशन ही कुछ होर\nअस्सी करदे हां यारां नाल बहारां\nयारां बिन हुन अपना ना गुजारा\nयार ता साडे दिल चे बसदे ने\nयारी चे अस्सी जीन्दे मरदे ने\nये पल दो पल दी रिश्तेदारी नहीं\nऐ फ़र्ज़ ता उमरा तक निभांदे ने\nओ रब्बा मेरी इक अरदास सुन ले\nनसीबां विच यारां दा साथ लिख दे\nविछड़ना गंवारा नहीं अब यारां दा\nभावें दुखड़े होर हज़ार लिख दे\n~~ Ashish Lahoti ( 07 Aug 2011, Friendship day ) ~~\n","permalink":"https://codingnconcepts.com/poetry/yaar-anmulle/","tags":null,"title":"Yaar Anmulle"},{"categories":null,"contents":"पापा, क्या लिखुँ मैं आपके बारे में,\nजिन हाथों ने इन पैरों को चलना सिखाया,\nजिनकी छांव में खुद को बढ़ता हुआ पाया,\nजिन्हे देख देख जिंदगी का मायना समझ में आया।\nखुद की खुशियों को ताक पे रख,\nहमारे लिए वो जीते मरते है।\nशायद हमारी एक ख़ुशी की झलक देख,\nवो अपने सब दुख भूल जाते है।\nवो इस भाग दौड़ की नौकरी में,\nहमें वक़्त नहीं दे पाते है।\nशायद वो भी कंही कुछ पलों को चुराकर,\nहमारे साथ जिंदगी जीना चाहते है।\nफिर क्यों माँ के प्यार के आगे,\nउनका प्यार फीका पड़ जाता है।\nशायद उनका प्यार जताने का तरीका है अलग,\nजो हमें कभी समझ में नहीं आता है।\n~~ आशीष लाहोटी ( १९ जून २०११ फादर्स-डे ) ~~\n","permalink":"https://codingnconcepts.com/poetry/papa/","tags":null,"title":"पापा"},{"categories":null,"contents":"English version of poem \u0026ldquo;छोटा था तो अच्छा था !\u0026rdquo; by Prateek Saxena\nI still remember, the days of my childhood,\nThe innocence and truth,\nThe laughter, the attention, the life of carelessness,\nthose were the days of my youth.\nWhen crying and weeping were my leverages,\nTo get any sweets, any beverages\nWhen I used to fake a headache to miss my school,\nWhen my toys were enough to me to look cool.\nI remember those days, when we used to cry, On mother dearest every small fry,\nWhen she used to pick me up then in a bit,\nThat sense of gratification, the bliss.\nWhen waiting for dad was a pain everyday,\nFor the facade of studying was worth the pay,\nWhen my brother was the best partner in crime,\nWhen we covered our sister dearest everyday in slime.\nThen came the friends, the groups , the games,\nAlong with the fights, the joys, the flames,\nThose days are now gone far behind,\nBut at my heart, I am still a child.\n","permalink":"https://codingnconcepts.com/poetry/i-am-still-a-child/","tags":null,"title":"I'm still a child !!"},{"categories":null,"contents":"जिंदगी क्या है, रिश्तों का जंजाल।\nजितना दूर भागना चाहता हूँ, उतना ही अपने पास पाता हूँ,\nजितना समझने की कोशिश करता हूँ, उतना ही उलझ जाता हूँ।\nजिंदगी क्या है, वक़्त का साया।\nजब चाहता हूँ दूर भाग जाना, वो हर साँस में मेरे साथ रहती है,\nजब चाहता हूँ कुछ और जीना, वो हर रोज उम्र कम कर देती है।\nजिंदगी क्या है, रंगमंच का तमाशा।\nजिसमे हसना है तो रोना भी, रूठना है तो मनाना भी,\nजिसमे प्यार दोस्ती का वादा है तो वहीँ दुश्मनी निभाना भी।\nजिंदगी क्या है, मद का प्याला।\nजितना ज्यादा भरना चाहता हूँ, उतना छलकता जाता है,\nकितना भी पी लूँ, एक जाम तो हमेशा कम पड़ जाता है।\nजिंदगी क्या है, समुंद्र की लहर।\nवो जितना शोर और उफान लेकर आती है, उतने ही धीमे लोट जाती है,\nया फिर रेत,\nजितना मुठी में पकड़ने की कोशिश करता हूँ, उतनी छूटती चली जाती है।\nजिंदगी क्या है, ये हमेशा से एक राज है।\nपता नहीं कभी कोई इसके असली मकसद को समझ पायेगा,\nइसे समझने के लिए तो अगला जनम भी काम पड़ जायेगा।\n~~ आशीष लाहोटी ( ०१ मई २०११ ) ~~\n","permalink":"https://codingnconcepts.com/poetry/jindagi-kya-hai/","tags":null,"title":"जिंदगी क्या है ?"},{"categories":null,"contents":"जन्दगी के इस मोड़ पे, जब हम उस वक़्त से बहुत दूर निकल गए,\nउन पलों को समेटना चाहता हूँ, जो मैंने कॉलेज (VIT) में बिताये।\nवो हॉस्टल में मम्मी पापा का यूँ छोड़ जाना,\nऔर फिर \u0026lsquo;F ब्लॉक\u0026rsquo; के bunk bed पे सोना,\nतब अनजान लोगों का अपना बन जाना याद आता है।\nवो लाइट जाने पे dustbin का नीचे फेंकना,\nऔर फिर तरह तरह की आवाजें निकलना,\nतब \u0026lsquo;राजेश गोविन्दन\u0026rsquo; का डंडे से पीटना याद आता है।\nवो बस ५ मिनट और सोच के Alarm का बंद कर देना,\nऔर फिर सुबह की पहली क्लास miss कर देना,\nतब दोस्तों का Class में मेरी Proxy लगाना याद आता है।\nवो जीन्स बस बिना धोये पहनते ही जाना,\nऔर फिर डिओ लगा के बिना नहाये क्लास भाग जाना,\nतब महीने में एक बार धोबी घाट लगाना याद आता है।\nवो प्रोफेसर के लेक्चर सुनते ही नींद आ जाना,\nऔर फिर लास्ट बेंच पे बैठ के मजाक करना,\nतब bunk मारके फ़ूड कोर्ट में veg puff खाना याद आता है।\nवो रात रात भर जागकर CS, Dota खेलना,\nऔर फिर FRIENDS, HIMYM, 24 की series ख़तम कर देना\nतब सुबह का breakfast miss कर देना याद आता है।\nवो दिवाली पे हॉस्टल में आधी रात को बम फोड़ना,\nऔर फिर होली पे सोते हुए दोस्तों पे रंग डालना,\nतब \u0026lsquo;Reviera\u0026rsquo; पे चार दिन खूब मस्ती करना याद आता है।\nवो exam में last moment पे course पता करना,\nऔर फिर साथ में मिलके night out करना,\nतब रात में चाय की घंटी बजने का इंतज़ार करना याद आता है।\nवो mess में लम्बी लाइन में लगना,\nऔर फिर बकवास खाना देख के भाग जाना,\nतब \u0026lsquo;Enzo\u0026rsquo; की maggi और सुनील अन्ना का \u0026lsquo;PD\u0026rsquo; याद आता है।\nवो दोस्त के b\u0026rsquo;day पर उसकी खूब धुलाई करना,\nऔर फिर गले लगा के उसे b\u0026rsquo;day wish करना,\nतब उसकी b\u0026rsquo;day treat पे लम्बा चौड़ा बिल आना याद आता है।\nवो interview clear न होने पे tense हो जाना,\nऔर फिर दोस्तों का मेरी काबिलियत पे भरोसा दिलाना,\nतब job मिलने पर उनका मुझसे ज्यादा खुश होना याद आता है।\nवो वक़्त नहीं ठहर पाया हमारे लिए,\nऔर फिर हम आ गए जिंदगी के एक नए मुकाम पे,\nतब आज फिर उसकी कॉलेज लाइफ में वापिस जाने को दिल चाहता है।\n~~ आशीष लाहोटी ( २३ अप्रैल २०११ ) ~~\n","permalink":"https://codingnconcepts.com/poetry/yaad-aata-hai-vit/","tags":null,"title":"याद आता है - VIT !!!"},{"categories":null,"contents":"कभी मैं भी छोटा बच्चा था, वो पल कितना सच्चा था,\nउमर का थोडा कच्चा था, पर छोटा था तो अच्छा था।\nजब दो आंसु की कीमत पे, जो चाहे वो मिल जाता था,\nतब एक छोटे बहाने से मैं स्कूल से छुट्टी पाता था।\nजब खिलोनों गुड्डे गुड़ियों के चारों और मेरा संसार था,\nतब मिटटी के घरौंदों से मैं अपनी दुनिया सजाता था।\nजब मम्मी की छोटी डांट पे भी मुझे जोर से रोना आता था,\nतब उनकी गोद में सर रखके मैं सारी खुशियाँ पाता था।\nजब पापा के काम से आते ही पढने का नाटक करता था,\nतब उनका सर पे हाथ भी आशीर्वाद बन जाता था।\nजब भाई के साथ में मिल मैं खूब शरारत करता था,\nतब मेरी सारी गलती भी वो अपने सर ले लेता था।\nजब दीदी हर बात पे मेरी खूब खिंचाई करती थी,\nतब अपने हिस्से की चीज़े भी मुझको दे देती थी।\nजब दोस्तों की टोली में मैं खूब धमाल मचाता था,\nतब दोस्ती का वादा भी सच्चे दिल से निभाता था।\nजब छोटा सा संसार था, न कोई जीवन जंजाल था,\nतब मैं बिलकुल नादान था, पर छोटा था तो अच्छा था।\nउन खट्टी मीठी यादों में, मैं आज भी रोता हँसता हूँ,\nवो बचपन फिर न आयेगा, पर आज भी मैं एक बच्चा हूँ।\n~~ आशीष लाहोटी ( 8-अप्रैल-११ ) ~~\n","permalink":"https://codingnconcepts.com/poetry/chota-tha-to-accha-tha/","tags":null,"title":"छोटा था तो अच्छा था !"},{"categories":null,"contents":"","permalink":"https://codingnconcepts.com/search/","tags":null,"title":"Search Results"},{"categories":null,"contents":" Just click on a symbol to copy. Stars Symbols ★ ☆ ✡ ✦ ✧ ✩ ✪ ✫ ✬ ✭ ✮ ✯ ✰ ⁂ ⁎ ⁑ ✢ ✣ ✤ ✥ ✱ ✲ ✳ ✴ ✵ ✶ ✷ ✸ ✹ ✺ ✻ ✼ ✽ ✾ ✿ ❀ ❁ ❂ ❃ ❇ ❈ ❉ ❊ ❋ ❄ ❆ ❅ ⋆ ≛ Copyright, Trademark, Office \u0026amp; Law Symbols © ® ™ ℠ ℡ ℗ ‱ № ℀ ℁ ℅ ℆ ⅍ ☊ ☎ ☏ ⌨ ✁ ✂ ✃ ✄ ✆ ✇ ✈ ✉ ✎ ✏ ✐ ✑ ✒ ‰ § ¶ ✌ ☝ ☞ ☛ ☟ ☜ ☚ ✍ Currency Symbols ¢ $ € £ ¥ ₮ ৲ ৳ ௹ ฿ ៛ ₠ ₡ ₢ ₣ ₤ ₥ ₦ ₧ ₨ ₩ ₪ ₫ ₭ ₯ ₰ ₱ ₲ ₳ ₴ ₵ ￥ ﷼ ¤ ƒ Bracket Symbols 〈 〉 《 》 「 」 『 』 【 】 〔 〕 ︵ ︶ ︷ ︸ ︹ ︺ ︻ ︼ ︽ ︾ ︿ ﹀ ﹁ ﹂ ﹃ ﹄ ﹙ ﹚ ﹛ ﹜ ﹝ ﹞ ﹤ ﹥ （ ） ＜ ＞ ｛ ｝ 〖 〗 〘 〙 〚 〛 « » ‹ › 〈 〉 〱 Chess \u0026amp; Card Symbols ♔ ♕ ♖ ♗ ♘ ♙ ♚ ♛ ♜ ♝ ♞ ♟ ♤ ♠ ♧ ♣ ♡ ♥ ♢ ♦ Musical Notes \u0026amp; Music Symbols ♩ ♪ ♫ ♬ ♭ ♮ ♯ ° ø ؂ ≠ ≭ Degree, Weather \u0026amp; Unit Symbols ° ℃ ℉ ϟ ☀ ☁ ☂ ☃ ☉ ☼ ☽ ☾ ♁ ♨ ❄ ❅ ❆ ☇ ☈ ☄ ㎎ ㎏ ㎜ ㎝ ㎞ ㎡ ㏄ ㏎ ㏑ ㏒ ㏕ Arrows Symbols ↕ ↖ ↗ ↘ ↙ ↚ ↛ ↜ ↝ ↞ ↟ ↠ ↡ ↢ ↣ ↤ ↥ ↦ ↧ ↨ ↩ ↪ ↫ ↬ ↭ ↮ ↯ ↰ ↱ ↲ ↳ ↴ ↶ ↷ ↸ ↹ ↺ ↻ ↼ ↽ ↾ ↿ ⇀ ⇁ ⇂ ⇃ ⇄ ⇅ ⇆ ⇇ ⇈ ⇉ ⇊ ⇋ ⇌ ⇍ ⇎ ⇏ ⇕ ⇖ ⇗ ⇘ ⇙ ⇚ ⇛ ⇜ ⇝ ⇞ ⇟ ⇠ ⇡ ⇢ ⇣ ⇤ ⇥ ⇦ ⇧ ⇨ ⇩ ⇪ ⌅ ⌆ ⌤ ⏎ ▶ ☇ ☈ ☊ ☋ ☌ ☍ ➔ ➘ ➙ ➚ ➛ ➜ ➝ ➞ ➟ ➠ ➡ ➢ ➣ ➤ ➥ ➦ ➧ ➨ ➩ ➪ ➫ ➬ ➭ ➮ ➯ ➱ ➲ ➳ ➴ ➵ ➶ ➷ ➸ ➹ ➺ ➻ ➼ ➽ ➾ ⤴ ⤵ ↵ ↓ ↔ ← → ↑ ⌦ ⌫ ⌧ ⇰ ⇫ ⇬ ⇭ ⇳ ⇮ ⇯ ⇱ ⇲ ⇴ ⇵ ⇷ ⇸ ⇹ ⇺ ⇑ ⇓ ⇽ ⇾ ⇿ ⬳ ⟿ ⤉ ⤈ ⇻ ⇼ ⬴ ⤀ ⬵ ⤁ ⬹ ⤔ ⬺ ⤕ ⬶ ⤅ ⬻ ⤖ ⬷ ⤐ ⬼ ⤗ ⬽ ⤘ ⤝ ⤞ ⤟ ⤠ ⤡ ⤢ ⤣ ⤤ ⤥ ⤦ ⤪ ⤨ ⤧ ⤩ ⤭ ⤮ ⤯ ⤰ ⤱ ⤲ ⤫ ⤬ ⬐ ⬎ ⬑ ⬏ ⤶ ⤷ ⥂ ⥃ ⥄ ⭀ ⥱ ⥶ ⥸ ⭂ ⭈ ⭊ ⥵ ⭁ ⭇ ⭉ ⥲ ⭋ ⭌ ⥳ ⥴ ⥆ ⥅ ⥹ ⥻ ⬰ ⥈ ⬾ ⥇ ⬲ ⟴ ⥷ ⭃ ⥺ ⭄ ⥉ ⥰ ⬿ ⤳ ⥊ ⥋ ⥌ ⥍ ⥎ ⥏ ⥐ ⥑ ⥒ ⥓ ⥔ ⥕ ⥖ ⥗ ⥘ ⥙ ⥚ ⥛ ⥜ ⥝ ⥞ ⥟ ⥠ ⥡ ⥢ ⥤ ⥣ ⥥ ⥦ ⥨ ⥧ ⥩ ⥮ ⥯ ⥪ ⥬ ⥫ ⥭ ⤌ ⤍ ⤎ ⤏ ⬸ ⤑ ⬱ ⟸ ⟹ ⟺ ⤂ ⤃ ⤄ ⤆ ⤇ ⤊ ⤋ ⭅ ⭆ ⟰ ⟱ ⇐ ⇒ ⇔ ⇶ ⟵ ⟶ ⟷ ⬄ ⬀ ⬁ ⬂ ⬃ ⬅ ⬆ ⬇ ⬈ ⬉ ⬊ ⬋ ⬌ ⬍ ⟻ ⟼ ⤒ ⤓ ⤙ ⤚ ⤛ ⤜ ⥼ ⥽ ⥾ ⥿ ⤼ ⤽ ⤾ ⤿ ⤸ ⤺ ⤹ ⤻ ⥀ ⥁ ⟲ ⟳ Astrological \u0026 Zodiac Sign Symbols ☮ ☸ ♈ ♉ ☪ ♊ ♋ ♌ ♍ ♎ ♏ ♐ ♑ ♒ ♓ ☤ ☥ ☧ ☨ ☩ ☫ ☬ ☭ ☯ ☽ ☾ ✙ ✚ ✛ ✜ ✝ ✞ ✟ † ⊹ ‡ ♁ ♆ ❖ ♅ ✠ ✡ ✢ 卍 卐 〷 ☠ ☢ ☣ ☦ Heart Symbols ♥ ♡ ❤ ❥ ❣ ❦ ❧ დ ღ ۵ ლ ও ლ ❤️️ 💙 🧡 💚 💛 💜 🖤 💗 💓 💔 💟 💕 💖 ❣️ 💘 💝 💞 Check mark \u0026amp; Tick Symbols ✓ ✔ ✗ ✘ ☓ ∨ √ ✇ ☐ ☑ ☒ 〤 〥 Male, Female, People \u0026amp; Smiley Symbols ♀ ♂ ☹ ☺ ☻ ☿ 〠 ヅ ツ ㋡ 웃 유 ü Ü ت シ ッ ㋛ 웃̟͟ ꑇ ꐦ ꐠ ꐡ ꐕ ꌇ ꌈ ꉕ ꈋ ꈌ ꆛ ꆜ ꃼ ☠ ☃ 〲 〴 ϡ ﭢ ⍢ ⍣ ⍤ ⍥ ⍨ ⍩ ὃ ὕ ὣ Ѷ Ӫ ӫ ⚣ ⚤ ⚥ ⚦ ⚧ ⚨ ⚢ Punctuation Symbols · ‑ ‒ – — ― ‗ ‘ ’ ‚ ‛ “ ” „ ‟ • ‣ ․ ‥ … ‧ ′ ″ ‴ ‵ ‶ ‷ ❛ ❜ ❝ ❞ ʹ ʺ ʻ ʼ ʽ ʾ ʿ ˀ ˁ ˂ ˃ ˄ ˅ ˆ ˇ ˈ ˉ ˊ ˋ ˌ ˍ ˎ ˏ ː ˑ ˒ ˓ ˔ ˕ ˖ ˗ ˘ ˙ ˚ ˛ ˜ ˝ ˞ ˠ ˡ ～ ¿ ﹐ ﹒ ﹔ ﹕ ！ ＃ ＄ ％ ＆ ＊ ， ． ： ； ？ ＠ 、 。 〃 〝 〞 ︰ Maths Symbols π ∞ Σ √ ∛ ∜ ∫ ∬ ∭ ∮ ∯ ∰ ∱ ∲ ∳ ∀ ∁ ∂ ∃ ∄ ∅ ∆ ∇ ∈ ∉ ∊ ∋ ∌ ∍ ∎ ∏ ∐ ∑ − ∓ ∔ ∕ ∖ ∗ ∘ ∙ ∝ ∟ ∠ ∡ ∢ ∣ ∤ ∥ ∦ ∧ ∨ ∩ ∪ ∴ ∵ ∶ ∷ ∸ ∹ ∺ ∻ ∼ ∽ ∾ ∿ ≀ ≁ ≂ ≃ ≄ ≅ ≆ ≇ ≈ ≉ ≊ ≋ ≌ ≍ ≎ ≏ ≐ ≑ ≒ ≓ ≔ ≕ ≖ ≗ ≘ ≙ ≚ ≛ ≜ ≝ ≞ ≟ ≠ ≡ ≢ ≣ ≤ ≥ ≦ ≧ ≨ ≩ ≪ ≫ ≬ ≭ ≮ ≯ ≰ ≱ ≲ ≳ ≴ ≵ ≶ ≷ ≸ ≹ ≺ ≻ ≼ ≽ ≾ ≿ ⊀ ⊁ ⊂ ⊃ ⊄ ⊅ ⊆ ⊇ ⊈ ⊉ ⊊ ⊋ ⊌ ⊍ ⊎ ⊏ ⊐ ⊑ ⊒ ⊓ ⊔ ⊕ ⊖ ⊗ ⊘ ⊙ ⊚ ⊛ ⊜ ⊝ ⊞ ⊟ ⊠ ⊡ ⊢ ⊣ ⊤ ⊥ ⊦ ⊧ ⊨ ⊩ ⊪ ⊫ ⊬ ⊭ ⊮ ⊯ ⊰ ⊱ ⊲ ⊳ ⊴ ⊵ ⊶ ⊷ ⊸ ⊹ ⊺ ⊻ ⊼ ⊽ ⊾ ⊿ ⋀ ⋁ ⋂ ⋃ ⋄ ⋅ ⋆ ⋇ ⋈ ⋉ ⋊ ⋋ ⋌ ⋍ ⋎ ⋏ ⋐ ⋑ ⋒ ⋓ ⋔ ⋕ ⋖ ⋗ ⋘ ⋙ ⋚ ⋛ ⋜ ⋝ ⋞ ⋟ ⋠ ⋡ ⋢ ⋣ ⋤ ⋥ ⋦ ⋧ ⋨ ⋩ ⋪ ⋫ ⋬ ⋭ ⋮ ⋯ ⋰ ⋱ ⁺ ⁻ ⁼ ⁽ ⁾ ⁿ ₊ ₋ ₌ ₍ ₎ ✖ ﹢ ﹣ ＋ － ／ ＝ ÷ ± × Number Symbols Ⅰ Ⅱ Ⅲ Ⅳ Ⅴ Ⅵ Ⅶ Ⅷ Ⅸ Ⅹ Ⅺ Ⅻ Ⅼ Ⅽ Ⅾ Ⅿ ⅰ ⅱ ⅲ ⅳ ⅴ ⅵ ⅶ ⅷ ⅸ ⅹ ⅺ ⅻ ⅼ ⅽ ⅾ ⅿ ↀ ↁ ↂ ➀ ➁ ➂ ➃ ➄ ➅ ➆ ➇ ➈ ➉ ➊ ➋ ➌ ➍ ➎ ➏ ➐ ➑ ➒ ➓ ⓵ ⓶ ⓷ ⓸ ⓹ ⓺ ⓻ ⓼ ⓽ ⓾ ⓿ ❶ ❷ ❸ ❹ ❺ ❻ ❼ ❽ ❾ ❿ ⁰ ¹ ² ³ ⁴ ⁵ ⁶ ⁷ ⁸ ⁹ ₀ ₁ ₂ ₃ ₄ ₅ ₆ ₇ ₈ ₉ ⓪ ① ② ③ ④ ⑤ ⑥ ⑦ ⑧ ⑨ ⑩ ⑪ ⑫ ⑬ ⑭ ⑮ ⑯ ⑰ ⑱ ⑲ ⑳ ⑴ ⑵ ⑶ ⑷ ⑸ ⑹ ⑺ ⑻ ⑼ ⑽ ⑾ ⑿ ⒀ ⒁ ⒂ ⒃ ⒄ ⒅ ⒆ ⒇ ⒈ ⒉ ⒊ ⒋ ⒌ ⒍ ⒎ ⒏ ⒐ ⒑ ⒒ ⒓ ⒔ ⒕ ⒖ ⒗ ⒘ ⒙ ⒚ ⒛ ㈠ ㈡ ㈢ ㈣ ㈤ ㈥ ㈦ ㈧ ㈨ ㈩ ㊀ ㊁ ㊂ ㊃ ㊄ ㊅ ㊆ ㊇ ㊈ ㊉ ０ １ ２ ３ ４ ５ ６ ７ ８ ９ ⁱ ₐ ₑ ₒ ₓ ₔ Fraction Symbols ⅟ ½ ⅓ ⅕ ⅙ ⅛ ⅔ ⅖ ⅚ ⅜ ¾ ⅗ ⅝ ⅞ ⅘ ¼ ⅐ ⅑ ⅒ ↉ % ℅ ‰ ‱ Comparison Symbols ≤ ≥ ≦ ≧ ≨ ≩ ⊰ ⊱ ⋛ ⋚ ≂ ≃ ≄ ≅ ≆ ≇ ≈ ≉ ≊ ≋ ≌ ≍ ≎ ≏ ≐ ≑ ≒ ≓ ≔ ≕ ≖ ≗ ≘ ≙ ≚ ≛ ≜ ≝ ≞ ≟ ≠ ≡ ≢ ≣ Technical Symbols ⌀ ⌂ ⌃ ⌄ ⌅ ⌆ ⌇ ⌈ ⌉ ⌊ ⌋ ⌌ ⌍ ⌎ ⌏ ⌐ ⌑ ⌒ ⌓ ⌔ ⌕ ⌖ ⌗ ⌘ ⌙ ⌚ ⌛ ⌜ ⌝ ⌞ ⌟ ⌠ ⌡ ⌢ ⌣ ⌤ ⌥ ⌦ ⌧ ⌨ ⌫ ⌬ ⌭ ⌮ ⌯ ⌰ ⌱ ⌲ ⌳ ⌴ ⌵ ⌶ ⌷ ⌸ ⌹ ⌺ ⌻ ⌼ ⌽ ⌾ ⌿ ⍀ ⍁ ⍂ ⍃ ⍄ ⍅ ⍆ ⍇ ⍈ ⍉ ⍊ ⍋ ⍌ ⍍ ⍎ ⍏ ⍐ ⍑ ⍒ ⍓ ⍔ ⍕ ⍖ ⍗ ⍘ ⍙ ⍚ ⍛ ⍜ ⍝ ⍞ ⍟ ⍠ ⍡ ⍢ ⍣ ⍤ ⍥ ⍦ ⍧ ⍨ ⍩ ⍪ ⍫ ⍬ ⍭ ⍮ ⍯ ⍰ ⍱ ⍲ ⍳ ⍴ ⍵ ⍶ ⍷ ⍸ ⍹ ⍺ ﹘ ﹝ ﹞ ﹟ ﹡ 〶 ␛ ␡ ␚ ␟ ␘ ␠ ␤ ␋ ␌ ␍ ␎ ␏ ␐ ␑ ␒ ␓ ␔ ␕ ␖ ␗ ␙ ␜ ␝ ␞ ␀ ␁ ␂ ␃ ␄ ␅ ␆ ␇ ␈ ␉ ␊ ␢ ␣ ⎋  ᴴᴰ Square \u0026amp; Rectangle Symbols ❏ ❐ ❑ ❒ ▀ ▁ ▂ ▃ ▄ ▅ ▆ ▇ ▉ ▊ ▋ █ ▌ ▐ ▍ ▎ ▏ ▕ ░ ▒ ▓ ▔ ▬ ▢ ▣ ▤ ▥ ▦ ▧ ▨ ▩ ▪ ▫ ▭ ▮ ▯ ☰ ☲ ☱ ☴ ☵ ☶ ☳ ☷ ▰ ▱ ◧ ◨ ◩ ◪ ◫ ∎ ■ □ ⊞ ⊟ ⊠ ⊡ ❘ ❙ ❚ 〓 ◊ ◈ ◇ ◆ ⎔ ⎚ ☖ ☗ Triangle Symbols ◄ ▲ ▼ ► ◀ ◣ ◥ ◤ ◢ ▶ ◂ ▴ ▾ ▸ ◁ △ ▽ ▷ ∆ ∇ ⊳ ⊲ ⊴ ⊵ ◅ ▻ ▵ ▿ ◃ ▹ ◭ ◮ ⫷ ⫸ ⋖ ⋗ ⋪ ⋫ ⋬ ⋭ ⊿ ◬ ≜ ⑅ Line Symbols │ ┃ ╽ ╿ ╏ ║ ╎ ┇ ︱ ┊ ︳ ┋ ┆ ╵ 〡 〢 ╹ ╻ ╷ 〣 ☰ ☱ ☲ ☳ ☴ ☵ ☶ ☷ ≡ ✕ ═ ━ ─ ╍ ┅ ┉ ┄ ┈ ╌ ╴ ╶ ╸ ╺ ╼ ╾ ﹉ ﹍ ﹊ ﹎ ︲ ⑆ ⑇ ⑈ ⑉ ⑊ ⑄ ⑀ ︴ ﹏ ﹌ ﹋ ╳ ╲ ╱ ︶ ︵ 〵 〴 〳 〆 ` ᐟ ‐ ⁃ ⎯ 〄 Corner Symbols ﹄ ﹃ ﹂ ﹁ ┕ ┓ └ ┐ ┖ ┒ ┗ ┑ ┍ ┙ ┏ ┛ ┎ ┚ ┌ ┘ 「 」 『 』 ˩ ˥ ├ ┝ ┞ ┟ ┠ ┡ ┢ ┣ ┤ ┥ ┦ ┧ ┨ ┩ ┪ ┫ ┬ ┭ ┮ ┯ ┰ ┱ ┲ ┳ ┴ ┵ ┶ ┷ ┸ ┹ ┺ ┻ ┼ ┽ ┾ ┿ ╀ ╁ ╂ ╃ ╄ ╅ ╆ ╇ ╈ ╉ ╊ ╋ ╒ ╕ ╓ ╖ ╔ ╗ ╘ ╛ ╙ ╜ ╚ ╝ ╞ ╡ ╟ ╢ ╠ ╣ ╥ ╨ ╧ ╤ ╦ ╩ ╪ ╫ ╬ 〒 ⊢ ⊣ ⊤ ⊥ ╭ ╮ ╯ ╰ ⊦ ⊧ ⊨ ⊩ ⊪ ⊫ ⊬ ⊭ ⊮ ⊯ ⊺ 〦 〧 〨 ˦ ˧ ˨ ⑁ ⑂ ⑃ ∟ Circle Symbols ◉ ○ ◌ ◍ ◎ ● ◐ ◑ ◒ ◓ ◔ ◕ ◖ ◗ ❂ ☢ ⊗ ⊙ ◘ ◙ ◚ ◛ ◜ ◝ ◞ ◟ ◠ ◡ ◯ 〇 〶 ⚫ ⬤ ◦ ∅ ∘ ⊕ ⊖ ⊘ ⊚ ⊛ ⊜ ⊝ ❍ ⦿ Phonetic Symbols ʌ ɑ: æ e ə ɜ: ɪ i: ɒ ɔ: ʊ u: aɪ aʊ eɪ oʊ ɔɪ eə ɪə ʊə b d f g h j k l m n ŋ p r s ʃ t tʃ θ ð v w z ʒ dʒ Greek Symbols \u0026alpha; \u0026beta; \u0026gamma; \u0026delta; \u0026epsilon; \u0026zeta; \u0026eta; \u0026theta; \u0026iota; \u0026kappa; \u0026lambda; \u0026mu; \u0026nu; \u0026xi; \u0026omicron; \u0026pi; \u0026rho; \u0026sigmaf; \u0026sigma; \u0026tau; \u0026upsilon; \u0026phi; \u0026chi; \u0026psi; \u0026omega; \u0026Alpha; \u0026Beta; \u0026Gamma; \u0026Delta; \u0026Epsilon; \u0026Zeta; \u0026Eta; \u0026Theta; \u0026Iota; \u0026Kappa; \u0026Lambda; \u0026Mu; \u0026Nu; \u0026Xi; \u0026Omicron; \u0026Pi; \u0026Rho; \u0026Sigma; \u0026Tau; \u0026Upsilon; \u0026Phi; \u0026Chi; \u0026Psi; \u0026Omega; Latin Symbols ą č Ĥ ħ ĩ Ň Ř Ť Ŵ Ž ⒜ ⒝ ⒞ ⒟ ⒠ ⒡ ⒢ ⒣ ⒤ ⒥ ⒦ ⒧ ⒨ ⒩ ⒪ ⒫ ⒬ ⒭ ⒮ ⒯ ⒰ ⒱ ⒲ ⒳ ⒴ ⒵ Ⓐ Ⓑ Ⓒ Ⓓ Ⓔ Ⓕ Ⓖ Ⓗ Ⓘ Ⓙ Ⓚ Ⓛ Ⓜ Ⓝ Ⓞ Ⓟ Ⓠ Ⓡ Ⓢ Ⓣ Ⓤ Ⓥ Ⓦ Ⓧ Ⓨ Ⓩ ⓐ ⓑ ⓒ ⓓ ⓔ ⓕ ⓖ ⓗ ⓘ ⓙ ⓚ ⓛ ⓜ ⓝ ⓞ ⓟ ⓠ ⓡ ⓢ ⓣ ⓤ ⓥ ⓦ ⓧ ⓨ ⓩ Ａ Ｂ Ｃ Ｄ Ｅ Ｆ Ｇ Ｈ Ｉ Ｊ Ｋ Ｌ Ｍ Ｎ Ｏ Ｐ Ｑ Ｒ Ｓ Ｔ Ｕ Ｖ Ｗ Ｘ Ｙ Ｚ ａ ｂ ｃ ｄ ｅ ｆ ｇ ｈ ｉ ｊ ｋ ｌ ｍ ｎ ｏ ｐ ｑ ｒ ｓ ｔ ｕ ｖ ｗ ｘ ｙ ｚ á â æ à å ã ä ç é ê è ð ë í î ì ï ñ ó ô ò ø õ ö ß þ ú û ù ü ý ÿ ᴀ ʙ ᴄ ᴅ ᴇ ғ ɢ ʜ ɪ ᴊ ᴋ ʟ ᴍ ɴ ᴏ ᴏ ᴘ ǫ ʀ s ᴛ ᴜ ᴠ ᴡ x ʏ ᴢ 𝓐 𝓑 𝓒 𝓓 𝓔 𝓕 𝓖 𝓗 𝓘 𝓙 𝓚 𝓛 𝓜 𝓝 𝓞 𝓟 𝓠 𝓡 𝓢 𝓣 𝓤 𝓥 𝓦 𝓧 𝓨 𝓩 𝓪 𝓫 𝓬 𝓭 𝓮 𝓯 𝓰 𝓱 𝓲 𝓳 𝓴 𝓵 𝓶 𝓷 𝓸 𝓹 𝓺 𝓻 𝓼 𝓽 𝓾 𝓿 𝔀 𝔁 𝔂 𝔃 𝒜 𝐵 𝒞 𝒟 𝐸 𝐹 𝒢 𝐻 𝐼 𝒥 𝒦 𝐿 𝑀 𝒩 𝒪 𝒫 𝒬 𝑅 𝒮 𝒯 𝒰 𝒱 𝒲 𝒳 𝒴 𝒵 𝒶 𝒷 𝒸 𝒹 𝑒 𝒻 𝑔 𝒽 𝒾 𝒿 𝓀 𝓁 𝓂 𝓃 𝑜 𝓅 𝓆 𝓇 𝓈 𝓉 𝓊 𝓋 𝓌 𝓍 𝓎 𝓏 𝐀 𝐁 𝐂 𝐃 𝐄 𝐅 𝐆 𝐇 𝐈 𝐉 𝐊 𝐋 𝐌 𝐍 𝐎 𝐏 𝐐 𝐑 𝐒 𝐓 𝐔 𝐕 𝐖 𝐗 𝐘 𝐙 𝐚 𝐛 𝐜 𝐝 𝐞 𝐟 𝐠 𝐡 𝐢 𝐣 𝐤 𝐥 𝐦 𝐧 𝐨 𝐩 𝐪 𝐫 𝐬 𝐭 𝐮 𝐯 𝐰 𝐱 𝐲 𝐳 𝔸 𝔹 ℂ 𝔻 𝔼 𝔽 𝔾 ℍ 𝕀 𝕁 𝕂 𝕃 𝕄 ℕ 𝕆 ℙ ℚ ℝ 𝕊 𝕋 𝕌 𝕍 𝕎 𝕏 𝕐 ℤ 𝕒 𝕓 𝕔 𝕕 𝕖 𝕗 𝕘 𝕙 𝕚 𝕛 𝕜 𝕝 𝕞 𝕟 𝕠 𝕡 𝕢 𝕣 𝕤 𝕥 𝕦 𝕧 𝕨 𝕩 𝕪 𝕫 ᴭ ᴮ ᴯ ᴰ ᴱ ᴲ ᴳ ᴴ ᴵ ᴶ ᴷ ᴸ ᴹ ᴺ ᴻ ᴼ ᴽ ᴾ ᴿ ᵀ ᵁ ᵂ ᵃ ᵄ ᵆ ᵇ ᵈ ᵉ ᵊ ᵋ ᵌ ᵍ ʱ ʰ ᵢ ᵎ ʲ ᵏ ᵐ ᵑ ᵒ ᵓ ᵔ ᵕ ᵖ ʳ ʴ ᵗ ʵ ᵘ ᵙ ᵛ ᵚ ᵜ ᵝ ᵞ ᵟ ᵠ ᵡ ᶛ ᶜ ᶝ ᶞ ᶟ ᶠ ᶡ ᶢ ᶣ ᶤ ᶥ ᶦ ᶧ ᶨ ᶩ ᶪ ᶫ ᗩ ℊ ℎ ℓ ℘ ℮ ℄ ℇ ℈ ℏ ℔ ℞ ℟ ℣ ℥ Ω ℧ ℩ K Å Ⅎ ℵ ℶ ℷ ℸ ♃ ♄ ☡ ♇ ❡ 🅰 🅱 🅾 🅿 𝕬 𝕭 𝕮 𝕯 𝕰 𝕱 𝕲 𝕳 𝕴 𝕵 𝕶 𝕷 𝕸 𝕹 𝕺 𝕻 𝕼 𝕽 𝕾 𝕿 𝖀 𝖁 𝖂 𝖃 𝖄 𝖅 𝖆 𝖇 𝖈 𝖉 𝖊 𝖋 𝖌 𝖍 𝖎 𝖏 𝖐 𝖑 𝖒 𝖓 𝖔 𝖕 𝖖 𝖗 𝖘 𝖙 𝖚 𝖛 𝖜 𝖝 𝖞 𝖟 𝔄 𝔅 ℭ 𝔇 𝔈 𝔉 𝔊 ℌ ℑ 𝔍 𝔎 𝔏 𝔐 𝔑 𝔒 𝔓 𝔔 ℜ 𝔖 𝔗 𝔘 𝔙 𝔚 𝔛 𝔜 ℨ 𝔞 𝔟 𝔠 𝔡 𝔢 𝔣 𝔤 𝔥 𝔦 𝔧 𝔨 𝔩 𝔪 𝔫 𝔬 𝔭 𝔮 𝔯 𝔰 𝔱 𝔲 𝔳 𝔴 𝔵 𝔶 𝔷 𝘈 𝘉 𝘊 𝘋 𝘌 𝘍 𝘎 𝘏 𝘐 𝘑 𝘒 𝘓 𝘔 𝘕 𝘖 𝘗 𝘘 𝘙 𝘚 𝘛 𝘜 𝘝 𝘞 𝘟 𝘠 𝘡 𝘢 𝘣 𝘤 𝘥 𝘦 𝘧 𝘨 𝘩 𝘪 𝘫 𝘬 𝘭 𝘮 𝘯 𝘰 𝘱 𝘲 𝘳 𝘴 𝘵 𝘶 𝘷 𝘸 𝘹 𝘺 𝘻 𝘼 𝘽 𝘾 𝘿 𝙀 𝙁 𝙂 𝙃 𝙄 𝙅 𝙆 𝙇 𝙈 𝙉 𝙊 𝙋 𝙌 𝙍 𝙎 𝙏 𝙐 𝙑 𝙒 𝙓 𝙔 𝙕 𝙖 𝙗 𝙘 𝙙 𝙚 𝙛 𝙜 𝙝 𝙞 𝙟 𝙠 𝙡 𝙢 𝙣 𝙤 𝙥 𝙦 𝙧 𝙨 𝙩 𝙪 𝙫 𝙬 𝙭 𝙮 𝙯 Chinese Symbols ㊊ ㊋ ㊌ ㊍ ㊎ ㊏ ㊐ ㊑ ㊒ ㊓ ㊔ ㊕ ㊖ ㊗ ㊘ ㊙ ㊚ ㊛ ㊜ ㊝ ㊞ ㊟ ㊠ ㊡ ㊢ ㊣ ㊤ ㊥ ㊦ ㊧ ㊨ ㊩ ㊪ ㊫ ㊬ ㊭ ㊮ ㊯ ㊰ Japanese Symbols ぁ あ ぃ い ぅ う ぇ え ぉ お か が き ぎ く ぐ け げ こ ご さ ざ し じ す ず せ ぜ そ ぞ た だ ち ぢ っ つ づ て で と ど な に ぬ ね の は ば ぱ ひ び ぴ ふ ぶ ぷ へ べ ぺ ほ ぼ ぽ ま み む め も ゃ や ゅ ゆ ょ よ ら り る れ ろ ゎ わ ゐ ゑ を ん ゔ ゕ ゖ ゚ ゛ ゜ ゝ ゞ ゟ ゠ ァ ア ィ イ ゥ ウ ェ エ ォ オ カ ガ キ ギ ク グ ケ ゲ コ ゴ サ ザ シ ジ ス ズ セ ゼ ソ ゾ タ ダ チ ヂ ッ ツ ヅ テ デ ト ド ナ ニ ヌ ネ ノ ハ バ パ ヒ ビ ピ フ ブ プ ヘ ベ ペ ホ ボ ポ マ ミ ム メ モ ャ ヤ ュ ユ ョ ヨ ラ リ ル レ ロ ヮ ワ ヰ ヱ ヲ ン ヴ ヵ ヶ ヷ ヸ ヹ ヺ ・ ー ヽ ヾ ヿ ㍐ ㍿ Korean Symbols ㄱ ㄲ ㄳ ㄴ ㄵ ㄶ ㄷ ㄸ ㄹ ㄺ ㄻ ㄼ ㄽ ㄾ ㄿ ㅀ ㅁ ㅂ ㅃ ㅄ ㅅ ㅆ ㅇ ㅈ ㅉ ㅊ ㅋ ㅌ ㅍ ㅎ ㅏ ㅐ ㅑ ㅒ ㅓ ㅔ ㅕ ㅖ ㅗ ㅘ ㅙ ㅚ ㅛ ㅜ ㅝ ㅞ ㅟ ㅠ ㅡ ㅢ ㅥ ㅦ ㅧ ㅨ ㅩ ㅪ ㅫ ㅬ ㅭ ㅮ ㅯ ㅰ ㅱ ㅲ ㅳ ㅴ ㅵ ㅶ ㅷ ㅸ ㅹ ㅺ ㅻ ㅼ ㅽ ㅾ ㅿ ㆀ ㆁ ㆂ ㆃ ㆄ ㆅ ㆆ ㆇ ㆈ ㆉ ㆊ Popular Facebook Symbols ♡ ♥ ღ ❥ ℳ Ⓐ ℒ ۵ ❁ ❀ ℰ ⋆ ℬ Ⓔ ❦ ℯ ↬ ℛ ℐ ➳ ✰ ℱ ℴ ❝ » ᴬ Ⓘ Ⓛ ✧ ✞ † Ⓢ ❞ ༄ ↳ Ⓡ ™ 【 Ⓝ ℋ ♛ ☾ ✿ ⚘ ★ 】 ¹ ᵅ → ➵ ❤️ 😍 🖤 ⚔️ 😂 💕 🎧 🌹 📌 🏳️‍🌈 👑 💙 💎 😎 ✨ 💻 ✔️ 🌙 🤣 🎤 💜 🎮 🎼 💛 😊 🧡 📞 🔊 ❣️ 💫 💖 🤩 💞 ☕️ 💔 ⚜️ 🌸 🔪 ⚠️ 😭 ","permalink":"https://codingnconcepts.com/tools/unicode-characters/","tags":null,"title":"Unicode Characters"}]